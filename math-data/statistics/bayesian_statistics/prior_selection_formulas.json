[
  {
    "id": "stat_for_prior_selection_004",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "prior_selection",
    "title": "Prior Selection in Bayesian Statistics",
    "contentHtml": "<p>In Bayesian statistics, prior selection is a crucial step in modeling uncertainty. We'll explore the different types of priors and when to use them.</p>",
    "formula": {
      "latex": "\\[ p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{p(x)} \\]",
      "name": "Bayes' theorem"
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we want to model the prior distribution of a coin's bias. What type of prior would be suitable?</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Choose an informative prior",
          "mathHtml": "",
          "explanation": "An informative prior is useful when we have some knowledge about the parameter"
        }
      ],
      "finalAnswer": "A uniform prior"
    },
    "intuition": "Priors help us encode our initial beliefs or assumptions about a model's parameters. Informative priors can be helpful when we have domain expertise, while non-informative priors are useful for avoiding bias.",
    "realWorldApplications": [
      "In natural language processing, informative priors can be used to model the prior distribution of word frequencies."
    ],
    "tags": [
      "Bayesian statistics",
      "prior selection"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:16:22.065Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_prior_selection_005",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "prior_selection",
    "title": "Prior Selection in Bayesian Statistics",
    "contentHtml": "<p>In Bayesian statistics, prior selection is a crucial step in modeling uncertainty. A good prior can significantly impact the posterior distribution and subsequent inference.</p>",
    "formula": "{",
    "latex": "\\[ \\pi(x) = \\frac{\\alpha}{\\beta + x} \\]\",",
    "name": "Informative Prior",
    "variants": "[ {\"latex\": \"\\[ \\pi(x) = \\mathcal{N}(x | 0, 1) \\]\", \"description\": \"Non-informative prior\"} ] },",
    "intuition": "A informative prior provides additional information about the distribution of interest. This can be useful when domain knowledge is available.",
    "realWorldApplications": [
      "In natural language processing, a informative prior can help improve topic modeling"
    ],
    "tags": [
      "Bayesian Statistics",
      "Prior Selection"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:16:34.662Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_prior_selection_006",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "prior_selection",
    "title": "Prior Selection in Bayesian Statistics",
    "contentHtml": "<p>In Bayesian statistics, prior selection is a crucial step in modeling uncertainty. A good prior can greatly impact the results of our inference.</p><p>But how do we choose an informative yet non-informative prior? Enter Jeffreys' prior!</p>",
    "formula": "{",
    "latex": "\\[ p(\\theta) \\propto \\exp\\left(-\\frac{1}{2} \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\pi(x) dx \\right) \\]\",",
    "name": "Jeffreys' Prior\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of exam scores with mean 80 and standard deviation 10. We want to model the prior distribution of students' abilities.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Identify the problem\", \"mathHtml\": \"\", \"explanation\": \"\"}, {\"stepNumber\": 2, \"description\": \"Choose a conjugate prior\", \"mathHtml\": \"\\[ p(\\theta) \\propto \\exp\\left(-\\frac{1}{2} \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\pi(x) dx \\right) \\]\", \"explanation\": \"Jeffreys' prior is a popular choice for continuous distributions\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "A good prior should reflect our initial understanding of the problem, but not be too informative.",
    "tags": [
      "Bayesian Statistics",
      "Prior Selection"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:16:54.714Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_prior_selection_007",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "prior_selection",
    "title": "Prior Selection in Bayesian Statistics",
    "contentHtml": "<p>In Bayesian statistics, prior selection is a crucial step in modeling uncertainty. A good prior can greatly impact the resulting posterior distribution.</p><ul><li>A proper prior is one that integrates to 1, while an improper prior does not.</li><li>Jeffreys' prior is a popular choice for certain problems, but it's not always suitable.</li></ul>",
    "formula": "{",
    "latex": "\\[ \\mathcal{P}(x) = \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\int \\text{Likelihood} \\cdot \\text{Prior} dx} \\]\",",
    "name": "Bayes' Theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we're modeling the probability of a coin landing heads-up. We have some prior knowledge that the coin is biased towards heads.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the likelihood function\", \"mathHtml\": \"\\[ \\text{Likelihood} = \\frac{1}{2} + \\frac{\\theta}{2} \\]\", \"explanation\": \"The likelihood represents our updated understanding of the coin's bias.\"}, {\"stepNumber\": 2, \"description\": \"Choose a prior distribution for Î¸\", \"mathHtml\": \"\\[ \\text{Prior} = \\mathcal{N}(0.5, 0.1) \\]\", \"explanation\": \"A normal distribution with mean 0.5 and variance 0.1 represents our initial uncertainty about the coin's bias.\"}, {\"stepNumber\": 3, \"description\": \"Update the prior using Bayes' theorem\", \"mathHtml\": \"\\[ \\mathcal{P}(x) = \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\int \\text{Likelihood} \\cdot \\text{Prior} dx} \\]\", \"explanation\": \"The updated posterior distribution takes into account our prior knowledge and the observed data.\"} ],",
    "finalAnswer": "The resulting posterior distribution represents our updated understanding of the coin's bias.\" },",
    "intuition": "A good prior can help or hurt your model, so it's essential to choose one that reflects your domain expertise.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:17:20.692Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]