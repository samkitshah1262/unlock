[
  {
    "id": "stat_con_bayesian_paradigm_001",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics that underlies many machine learning and artificial intelligence applications.</p><p>In this framework, we update our knowledge about the world based on new data by incorporating prior information and likelihoods. This process yields a posterior distribution that reflects our updated understanding.</p>",
    "formula": {
      "latex": "\\[ P(\\theta | x) = \\frac{P(x | \\theta) P(\\theta)}{P(x)} \\]",
      "name": "Bayes' theorem"
    },
    "intuition": "The Bayesian paradigm is about updating our knowledge based on new data while incorporating prior information. This process allows us to make more informed decisions and predictions.",
    "realWorldApplications": [
      "Bayesian inference in natural language processing"
    ],
    "commonMistakes": [
      "Confusing Bayes' theorem with the likelihood principle",
      "Ignoring the importance of prior distributions"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:10:17.906Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_bayesian_paradigm_002",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in statistical inference that guides our thinking about uncertainty and decision-making.</p><p>In this framework, we update our knowledge based on new data by combining prior information with the likelihood of observing that data. This process yields a posterior distribution that reflects our updated understanding.</p>",
    "formula": {
      "latex": "\\[ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\]",
      "name": "Bayes' theorem"
    },
    "intuition": "The Bayesian approach acknowledges that our knowledge is always uncertain and incomplete. By updating our prior beliefs with new data, we can refine our understanding of the world.",
    "realWorldApplications": [
      "Bayesian methods are widely used in machine learning for tasks like natural language processing and computer vision."
    ],
    "commonMistakes": [
      "Confusing Bayesian inference with frequentist statistics"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:10:31.671Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_bayesian_paradigm_003",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics that underlies many machine learning and artificial intelligence techniques.</p><p>In classical statistics, we often assume a fixed model and use data to estimate its parameters. In contrast, the Bayesian approach views the model as uncertain and updates our knowledge about it based on new data.</p>",
    "formula": {
      "latex": "\\[ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\]",
      "name": "Bayes' theorem"
    },
    "intuition": "The Bayesian paradigm is all about updating our knowledge about a model based on new data. It's like refining our understanding of the world as we learn more.",
    "realWorldApplications": [
      "In machine learning, the Bayesian approach can be used for hyperparameter tuning and model selection."
    ],
    "commonMistakes": [
      "Not accounting for uncertainty in models"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:10:45.123Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_paradigm_004",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a philosophical approach to statistical inference that views probability as a measure of uncertainty or degree of belief.</p><p>It's based on Bayes' theorem, which updates the prior distribution with new data using the likelihood function.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' Theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a prior belief that the average height of adults is 175 cm, and we take a random sample of 10 people. If our sample mean is 172 cm, what's the updated distribution for the average height?</p>",
    "steps": "[ {",
    "stepNumber": 3,
    "description": "Calculate the posterior",
    "mathHtml": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "explanation": "We apply Bayes' theorem to update our prior with the new data\" } ],",
    "finalAnswer": "The updated distribution for the average height is a normal distribution centered at 172 cm with some uncertainty.\" },",
    "intuition": "Bayesian inference provides a flexible and intuitive way to update our beliefs based on new data, allowing us to incorporate prior knowledge and uncertainty.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:11:11.373Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_paradigm_005",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics that guides our thinking about probability and inference.</p><p>In this framework, we update our knowledge of the world based on new data using Bayes' theorem. The key components are:</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of landing heads up, and we flip it 10 times. If the result is 7 heads and 3 tails, what's our updated belief about the coin's fairness?</p>",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Define the prior distribution over the coin's fairness",
        "mathHtml": "",
        "explanation": "We start with a vague idea of the coin's fairness."
      }
    ],
    "finalAnswer": "\" },",
    "intuition": "The Bayesian paradigm is about updating our knowledge based on new data, using Bayes' theorem to combine prior beliefs and likelihoods.",
    "realWorldApplications": [
      "Bayesian inference in machine learning models"
    ],
    "tags": [
      "bayes",
      "inference",
      "probability"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:11:29.466Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_paradigm_006",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in statistical inference that guides our thinking about uncertainty and decision-making.</p><p>At its core, the Bayesian approach involves updating prior knowledge with new data to form a posterior distribution. This process relies on three key components: the prior distribution, likelihood function, and posterior distribution.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{\\int P(D | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' theorem",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of landing heads-up, denoted by θ. We observe the outcomes of 10 flips and get 6 heads.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\(P(\\theta) = \\frac{1}{2} I_{[0,1]}\\)\", \"explanation\": \"We assume a uniform prior for the coin's probability.\"}, {\"stepNumber\": 2, \"description\": \"Update with the likelihood function\", \"mathHtml\": \"\\(P(D | \\theta) = (θ)^6 (1-θ)^4\\)\", \"explanation\": \"The likelihood is the product of binomial probabilities for each flip.\"} ],",
    "finalAnswer": "The updated posterior distribution\" },",
    "intuition": "The Bayesian paradigm encourages us to think about uncertainty and decision-making in a more nuanced way, recognizing that our prior knowledge can be updated with new data.",
    "realWorldApplications": [
      "Bayesian inference is widely used in machine learning for tasks like model selection and hyperparameter tuning."
    ],
    "tags": [
      "bayes",
      "inference"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:11:52.002Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_paradigm_007",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics that guides our thinking about probability and inference.</p><p>In this framework, we update our knowledge about a parameter or hypothesis based on new data using Bayes' theorem.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' Theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we want to estimate the probability of a coin being fair given some observations.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define our prior distribution for the coin's fairness\", \"mathHtml\": \"\\[P(\\theta) = \\frac{1}{2}\\]\", \"explanation\": \"We assume equal probability of the coin being fair or biased\"}, {\"stepNumber\": 2, \"description\": \"Update our prior with new data (e.g., observing heads 5 times out of 10 flips)\", \"mathHtml\": \"\\[P(X | \\theta) = \\frac{1}{2}^{5+3}\\]\", \"explanation\": \"We update the likelihood using the observed data\"} ],",
    "finalAnswer": "The updated posterior distribution reflects our new understanding of the coin's fairness\" },",
    "intuition": "Bayes' theorem helps us quantify how new information affects our beliefs about a parameter or hypothesis",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:12:11.652Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_paradigm_008",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "problem": {
      "statementHtml": "Consider a Bayesian model with prior distribution <i>P(θ)</i> and likelihood function <i>L(θ|x)</i>. Given observed data <i>x</i>, derive the posterior distribution <i>P(θ|x)</i>.",
      "hints": [
        "Start by applying Bayes' theorem: <i>P(θ|x) ∝ P(x|θ)P(θ)</i>",
        "Recognize that the likelihood function can be written as a product of conditional probabilities.",
        "Use the chain rule to expand the posterior distribution."
      ],
      "solutionHtml": "<p>Applying Bayes' theorem, we get:</p><p><i>P(θ|x) ∝ P(x|θ)P(θ)</i></p><p>Now, recognize that the likelihood function can be written as a product of conditional probabilities:</p><p><i>L(θ|x) = ∏<sub>i</sub> P(xi | θ)</i></p><p>Using the chain rule to expand the posterior distribution:</p><p><i>P(θ|x) ∝ ∫ P(x|θ, θ')P(θ')dθ'</i></p>",
      "answerShort": "<i>P(θ|x) ∝ L(θ|x)P(θ)</i>"
    },
    "commonMistakes": [
      "Forgetting to apply Bayes' theorem",
      "Not recognizing the likelihood function as a product of conditional probabilities"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:12:29.872Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_paradigm_009",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "problem": {
      "statementHtml": "<p>Consider a Bayesian paradigm where we update our prior distribution <i>p</i>(<i>θ</i>) based on new data <i>D</i>. Write down the posterior distribution <i>p</i>(<i>θ</i>|<i>D</i>). What is the key difference between this approach and a frequentist one?</p>",
      "hints": [
        "<p>Start by writing down Bayes' theorem: <i>p</i>(<i>θ</i>|<i>D</i>) = <i>p</i>(<i>D</i>|<i>θ</i>) <i>p</i>(<i>θ</i>) / <i>p</i>(<i>D</i>)</p>",
        "<p>Notice that the likelihood <i>p</i>(<i>D</i>|<i>θ</i>) plays a crucial role in updating the prior.</p>",
        "<p>Think about how this approach differs from a frequentist one, which would focus on estimating population parameters rather than updating beliefs.</p>"
      ],
      "solutionHtml": "<p>To derive the posterior distribution, we can plug in our prior and likelihood functions. Let's assume <i>p</i>(<i>D</i>|<i>θ</i>) = <i>N</i>(<i>D</i>|<i>μ</i>, <i>σ</i><sup>2</sup>). Then, the posterior distribution is:</p>\n<p><i>p</i>(<i>θ</i>|<i>D</i>) ∝ <i>p</i>(<i>D</i>|<i>θ</i>) <i>p</i>(<i>θ</i>) = <i>N</i>(<i>D</i>|<i>μ</i>, <i>σ</i><sup>2</sup>) <i>N</i>(<i>θ</i>|<i>μ</i>, <i>τ</i><sup>2</sup>)</p>",
      "answerShort": "<i>p</i>(<i>θ</i>|<i>D</i>) ∝ <i>N</i>(<i>D</i>|<i>μ</i>, <i>σ</i><sup>2</sup>) <i>N</i>(<i>θ</i>|<i>μ</i>, <i>τ</i><sup>2</sup>)"
    },
    "commonMistakes": [
      "<p>Failing to recognize the importance of the prior distribution in Bayesian inference.</p>",
      "<p>Misunderstanding the role of likelihood functions in updating the posterior distribution.</p>"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:12:59.792Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_paradigm_010",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "problem": "{",
    "statementHtml": "<p>Consider a Bayesian model with prior distribution <i>P</i>(<i>θ</i>) and likelihood function <i>L</i>(<i>x</i>|<i>θ</i>). What is the posterior distribution <i>P</i>(<i>θ</i>|<i>x</i>)?</p>",
    "hints": [
      "<p>The prior distribution represents our initial uncertainty about the parameter <i>θ</i>.</p>",
      "<p>The likelihood function encodes our knowledge about the data <i>x</i> given the parameter <i>θ</i>.</p>",
      "<p>The posterior distribution combines the prior and likelihood information to update our belief about <i>θ</i>.</p>"
    ],
    "solutionHtml": "<p>To find the posterior distribution, we can use Bayes' theorem:</p>\\n\\ <p><i>P</i>(<i>θ</i>|<i>x</i>) = <i>P</i>(<i>θ</i>) &middot; <i>L</i>(<i>x</i>|<i>θ</i>) / ∫<i>P</i>(<i>θ</i>) &middot; <i>L</i>(<i>x</i>|<i>θ</i>) d<i>θ</i></p>\\n\\ <p>The integral is the normalizing constant, which ensures that the posterior distribution integrates to 1.</p>\",",
    "answerShort": "<i>P</i>(<i>θ</i>|<i>x</i>)\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:13:20.509Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_paradigm_011",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "problem": {
      "statementHtml": "<p>Consider a Bayesian model with prior distribution <i>P</i>(<i>θ</i>) and likelihood function <i>L</i>(<i>y</i>|<i>θ</i>). Derive the posterior distribution <i>P</i>(<i>θ</i>|<i>y</i>). Compare this to a frequentist approach.</p>",
      "hints": [
        "<p>Start by writing down Bayes' theorem: <i>P</i>(<i>θ</i>|<i>y</i>) = <i>P</i>(<i>y</i>|<i>θ</i>) <i>P</i>(<i>θ</i>) / <i>P</i>(<i>y</i>)</p>",
        "<p>Now, apply the prior and likelihood functions to Bayes' theorem.</p>",
        "<p>Compare your result to a frequentist approach. What are the key differences?</p>"
      ],
      "solutionHtml": "<p>To derive the posterior distribution, we can plug in the given prior and likelihood functions:</p><p><i>P</i>(<i>θ</i>|<i>y</i>) = <i>L</i>(<i>y</i>|<i>θ</i>) <i>P</i>(<i>θ</i>) / ∫<i>L</i>(<i>y</i>|<i>θ</i>) <i>P</i>(<i>θ</i>) d<i>θ</i></p><p>Simplify the expression to get the desired posterior distribution.</p>",
      "answerShort": "<i>P</i>(<i>θ</i>|<i>y</i>) = ... (fill in the answer)"
    },
    "commonMistakes": [
      "Forgetting to normalize the posterior distribution",
      "Not recognizing that the prior and likelihood functions are conjugate"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:13:42.869Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_paradigm_012",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics.</p>",
    "formula": "{",
    "latex": "\\[ P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)} \\]",
    "name": "Bayes' Theorem\" },",
    "problem": "{",
    "statementHtml": "<p>Suppose we have a coin with an unknown probability of heads, and we flip it 10 times. If the number of heads is 7, what's our updated belief about the probability?</p>",
    "hints": [
      "Think about updating your prior based on the data"
    ],
    "solutionHtml": "<p>We can use Bayes' Theorem to update our prior.</p>",
    "answerShort": "The answer\" },",
    "workedExample": "{",
    "problemHtml": "<p>Let's say we have a prior distribution of \\(\\beta \\sim \\mathcal{N}(0,1)\\). We flip the coin 10 times and get 7 heads.</p>",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:14:06.251Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_paradigm_013",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian statistics, we update our knowledge about a model's parameters based on new data.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of landing heads up. We flip the coin 5 times and get 3 heads. What is the updated probability of getting heads?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define our prior distribution\", \"mathHtml\": \"\\[p(\\theta) = \\frac{1}{2} \\cdot I_{\\theta \\in [0, 1]} \\]\", \"explanation\": \"We choose a uniform prior since we have no prior knowledge about the coin's probability.\"}, {\"stepNumber\": 2, \"description\": \"Observe the data and define the likelihood\", \"mathHtml\": \"\\[p(D | \\theta) = \\prod_{i=1}^5 \\frac{1}{2}^{1 + \\delta_i \\cdot \\theta} \\]\", \"explanation\": \"The likelihood is the probability of observing 3 heads given our prior belief about the coin's probability.\"}, {\"stepNumber\": 3, \"description\": \"Update the posterior distribution\", \"mathHtml\": \"\\[p(\\theta | D) = \\frac{p(D | \\theta) p(\\theta)}{\\int_{\\theta=0}^1 p(D | \\theta) p(\\theta) d\\theta} \\]\", \"explanation\": \"We use Bayes' theorem to update our prior distribution with the new data.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the posterior mean\", \"mathHtml\": \"\\[\\int_0^1 \\theta p(\\theta | D) d\\theta = \\frac{3}{5} \\]\", \"explanation\": \"The updated probability of getting heads is now more concentrated around 0.6.\"}, ],",
    "finalAnswer": "The updated probability of getting heads is approximately 0.6.\" },",
    "intuition": "Bayesian statistics allows us to incorporate new data and update our understanding of a model's parameters.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:14:30.959Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_paradigm_014",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian statistics, we update our knowledge about a system based on new data using Bayes' theorem.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose you have a coin that is either fair or biased. You flip the coin 10 times and get 8 heads. What's the probability it's a fair coin?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior\", \"mathHtml\": \"\\[P(Fair) = 0.5\\]\", \"explanation\": \"We start with an equal chance of the coin being fair or biased.\"}, {\"stepNumber\": 2, \"description\": \"Observe the data (8 heads)\", \"mathHtml\": \"\", \"explanation\": \"We get 8 heads out of 10 flips.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the likelihood\", \"mathHtml\": \"\\[P(Heads | Fair) = \\left(\\frac{1}{2}\\right)^8\\]\", \"explanation\": \"The probability of getting 8 heads if it's a fair coin is very low.\"}, {\"stepNumber\": 4, \"description\": \"Update the prior with the likelihood\", \"mathHtml\": \"\\[P(Fair | Heads) = \\frac{P(Heads | Fair) P(Fair)}{P(Heads)}\\]\", \"explanation\": \"We multiply the prior by the likelihood and normalize.\"}, {\"stepNumber\": 5, \"description\": \"Calculate the posterior\", \"mathHtml\": \"\\[P(Fair | Heads) = \\frac{\\left(\\frac{1}{2}\\right)^8 \\cdot 0.5}{\\int \\left(\\frac{1}{2}\\right)^10 p(x) dx}\\]\", \"explanation\": \"We get a new probability distribution for the coin being fair given the data.\"}, ],",
    "finalAnswer": "The updated probability is lower than our initial prior, indicating that the biased coin hypothesis becomes more plausible.\", },",
    "intuition": "Bayes' theorem helps us update our knowledge about a system based on new data by combining our prior beliefs with the likelihood of observing the data given those beliefs.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:14:56.456Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_paradigm_015",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_paradigm",
    "title": "Bayesian Paradigm: Prior, Likelihood, Posterior",
    "contentHtml": "<p>The Bayesian paradigm is a fundamental concept in mathematical statistics.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of heads, <i>p</i>. We flip the coin 10 times and get 7 heads. What's the updated probability of getting heads?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[p_0 \\sim \\text{Uniform}(0, 1)\\]\", \"explanation\": \"We assume a uniform prior since we have no prior knowledge.\"}, {\"stepNumber\": 2, \"description\": \"Update with the likelihood\", \"mathHtml\": \"\\[L(p) = (p)^7 (1-p)^3\\]\", \"explanation\": \"The likelihood is the probability of observing 7 heads and 3 tails given <i>p</i>.\"}, {\"stepNumber\": 3, \"description\": \"Compute the posterior distribution\", \"mathHtml\": \"\\[p|L \\sim \\frac{L(p) p_0}{\\int L(x) dx}\\]\", \"explanation\": \"We use Bayes' theorem to update the prior with the likelihood.\"}, {\"stepNumber\": 4, \"description\": \"Evaluate the posterior distribution\", \"mathHtml\": \"\\[p|L = \\frac{(p)^8 (1-p)^3}{\\int (x)^8 (1-x)^3 dx}\\]\", \"explanation\": \"We can evaluate the integral numerically or use a computational method.\"}, {\"stepNumber\": 5, \"description\": \"Get the final answer\", \"mathHtml\": \"\\[E[p|L] = \\frac{\\int p|L dp}{\\int p|L dp}\\]\", \"explanation\": \"The expected value of the posterior distribution gives us the updated probability of getting heads.\"} ],",
    "finalAnswer": "The updated probability is approximately 0.73.\" },",
    "intuition": "The Bayesian paradigm allows us to update our knowledge about a parameter given new data.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:15:20.837Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]