[
  {
    "id": "stat_con_bayesian_estimation_001",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation",
    "contentHtml": "<p>In Bayesian statistics, point estimation is a crucial concept that allows us to make informed decisions about unknown parameters based on observed data.</p><p>Given a prior distribution and a likelihood function, we can update our knowledge using Bayes' theorem. The resulting posterior distribution often has a mean or mode that serves as a reliable estimate of the true parameter value.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\text{Posterior Mean} = \\\\int p(\\\\theta | D) \\\\theta d\\\\theta\\]\",",
    "name": "Bayes' Theorem\" },",
    "whyMatters": "<p>Accurate point estimation is vital in many fields, including machine learning and artificial intelligence. By correctly estimating model parameters, we can improve predictive performance, optimize hyperparameters, and make more informed decisions.</p>",
    "intuition": "Think of Bayesian point estimation as a continuous process of updating our knowledge based on new data. The posterior mean represents the most likely value of the parameter given the observed data.",
    "commonMistakes": [
      "Failing to account for uncertainty in estimates",
      "Ignoring the prior distribution"
    ],
    "realWorldApplications": [
      "Hyperparameter tuning in neural networks",
      "Estimating model parameters in Gaussian mixture models"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:27:19.119Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_bayesian_estimation_002",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation",
    "contentHtml": "<p>In Bayesian statistics, point estimation is a fundamental concept that helps us make informed decisions about unknown parameters.</p><p>We're going to explore how Bayesian methods can be used for point estimation and the importance of understanding posterior means and maximum a posteriori (MAP) estimates.</p>",
    "formula": {
      "latex": "\\(\\mathbb{E}[x|y] = \\int x p(x|y) dx\\)",
      "name": "Posterior Mean"
    },
    "whyMatters": "<p>Bayesian point estimation is crucial in machine learning and artificial intelligence, as it allows us to make probabilistic statements about unknown parameters.</p>",
    "geometricIntuition": "<p>A simple way to visualize Bayesian point estimation is to imagine a probability distribution over possible values of the parameter. The posterior mean represents the 'center of gravity' of this distribution.</p>",
    "realWorldApplications": [
      "In natural language processing, Bayesian methods can be used for topic modeling and sentiment analysis."
    ],
    "commonMistakes": [
      "Failing to account for uncertainty in estimates",
      "Ignoring the importance of prior distributions"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:27:34.910Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_bayesian_estimation_003",
    "subject": "statistics",
    "type": "concept",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation",
    "contentHtml": "<p>Bayesian point estimation is a fundamental concept in Bayesian statistics that allows us to make informed decisions about unknown parameters based on observed data.</p><p>Given a prior distribution over the parameter space, we can update our beliefs using Bayes' theorem and arrive at a posterior distribution. The <i>posterior mean</i> is then used as an estimate of the true value.</p>",
    "formula": {
      "latex": "\\mathbb{E}[\\theta|D] = \\frac{\\int_{\\theta} \\theta p(\\theta|D) d\\theta}{\\int_{\\theta} p(\\theta|D) d\\theta}",
      "name": "Posterior Mean"
    },
    "whyMatters": "<p>This concept matters because it provides a principled way to update our beliefs in the face of new data, incorporating both prior knowledge and observed evidence.</p>",
    "intuition": "Think of Bayesian point estimation as using Bayes' theorem to 'zoom in' on the true value of an unknown parameter.",
    "realWorldApplications": [
      "In machine learning, this concept is crucial for tasks like hyperparameter tuning and model selection."
    ],
    "commonMistakes": [
      "Don't confuse posterior mean with maximum a posteriori (MAP) estimation; they're related but distinct concepts."
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:27:52.627Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_estimation_004",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation",
    "contentHtml": "<p>In Bayesian statistics, point estimation involves calculating a single value that best represents a distribution's mean or mode.</p><p>This is particularly useful when dealing with complex models and large datasets.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x p(x | y) dx \\]\",",
    "name": "Posterior Mean\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of exam scores and want to estimate the average score for students who scored above 80.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the prior distribution\", \"mathHtml\": \"\\( p(x) \\)\", \"explanation\": \"We start with an initial understanding of the data.\"} ],",
    "finalAnswer": "The estimated average score\" },",
    "intuition": "Bayesian point estimation provides a way to update our beliefs about a parameter based on new data, allowing for more accurate predictions.",
    "realWorldApplications": [
      "Estimating user engagement in online platforms"
    ],
    "tags": [
      "bayes",
      "point estimation"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:28:09.523Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_estimation_005",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation: Posterior Mean and MAP",
    "contentHtml": "<p>In Bayesian statistics, we often need to estimate population parameters from a sample. The posterior mean and maximum a posteriori (MAP) are two key concepts in Bayesian point estimation.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} xP(X=x|y) dx, \\]\",",
    "name": "Posterior Mean",
    "variants": "[ {\"latex\": \"\\[ P(\\theta|x) = \\frac{1}{Z} p(x|\\theta) \\pi(\\theta), \\]\", \"description\": \"Prior distribution\" } ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of exam scores and want to estimate the true average score. We assume a normal distribution with unknown mean and variance.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the likelihood function\", \"mathHtml\": \"\\( p(x|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)^2/2\\sigma^2} \\)\", \"explanation\": \"The likelihood function describes the probability of observing our data given the model parameters.\" }, {\"stepNumber\": 2, \"description\": \"Specify the prior distribution\", \"mathHtml\": \"\\( \\pi(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\tau} e^{-(\\theta-\\mu_0)^2/2\\tau^2} \\)\", \"explanation\": \"The prior distribution encodes our initial beliefs about the model parameters.\" } ],",
    "finalAnswer": "The posterior mean is the expected value of the posterior distribution.\" },",
    "intuition": "Bayesian point estimation provides a principled way to update our beliefs about unknown parameters based on new data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:28:32.848Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_bayesian_estimation_006",
    "subject": "statistics",
    "type": "formula",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation",
    "contentHtml": "<p>Bayesian point estimation is a fundamental concept in Bayesian statistics that allows us to update our knowledge about an unknown parameter based on new data.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbb{E}[X|y] = \\int x p(x|y) dx \\]\",",
    "name": "Posterior Mean\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we want to estimate the mean height of a population based on a sample of heights.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define our prior distribution for the mean height\", \"mathHtml\": \"\\( \\)\", \"explanation\": \"We start with some initial understanding of the population's height.\"} ],",
    "finalAnswer": "The updated estimate\" },",
    "intuition": "Bayesian point estimation allows us to incorporate new data and update our understanding of an unknown parameter, which is crucial in many real-world applications.",
    "realWorldApplications": [
      "Estimating user preferences in recommendation systems"
    ],
    "tags": [
      "bayes",
      "point estimation"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:28:48.500Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_thm_bayesian_estimation_007",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation Theorem",
    "contentHtml": "<p>The Bayesian point estimation theorem provides a framework for finding the optimal estimate of an unknown parameter given data.</p>",
    "formula": {
      "latex": "\\[ \\mathbb{E}[x|y] = \\int x p(x|y) dx \\]",
      "name": "Posterior Mean"
    },
    "theorem": {
      "statement": "\\[ \\text{The posterior mean } \\mathbb{E}[x|y] \\text{ is the optimal point estimate of } x \\text{ given data } y. \\]"
    },
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:28:58.958Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_thm_bayesian_estimation_008",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation Theorem",
    "contentHtml": "<p>The Bayesian point estimation theorem provides a framework for updating prior distributions to posterior distributions based on observed data.</p>",
    "formula": {
      "latex": "\\[\\mathbb{E}[X|y] = \\int x p(x|y) dx\\]",
      "name": "Posterior Mean"
    },
    "theorem": {
      "statement": "\\[p(\\theta|x) \\propto p(x|\\theta) p(\\theta)\\]",
      "proofSketch": "The proof involves showing that the posterior distribution is proportional to the product of the likelihood and prior distributions."
    },
    "intuition": "This theorem allows us to update our uncertainty about a parameter given new data, incorporating both the observed data and our initial beliefs.",
    "realWorldApplications": [
      "Using Bayesian methods in natural language processing for topic modeling"
    ],
    "tags": [
      "Bayesian Statistics",
      "Point Estimation"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:29:12.672Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_estimation_009",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "problem": "{",
    "statementHtml": "Find the posterior mean and MAP estimates for a normal distribution with prior <i>N(0,1)</i> and likelihood <i>f(x|θ) = N(θ, 2)</i>.",
    "hints": [
      "Start by finding the conjugate prior.",
      "Use the fact that the posterior is also normally distributed.",
      "Don't forget to normalize the MAP estimate."
    ],
    "solutionHtml": "<p>To find the posterior mean, we need to find the expected value of <i>θ</i> given the data.</p>\\[E[\\theta|x] = \\frac{\\int\\theta f(x|\\theta)N(\\theta|0,1)d\\theta}{\\int f(x|\\theta)N(\\theta|0,1)d\\theta]\\]<p>To find the MAP estimate, we need to find the value of <i>θ</i> that maximizes the posterior distribution.</p>\\[MAP = \\arg\\max_{\\theta} N(\\theta|0,1)f(x|\\theta)\\]</p>The final answer is:</p>\",",
    "answerShort": "The posterior mean is 0.5 and the MAP estimate is 0.\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:29:30.327Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_estimation_010",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "problem": {
      "statementHtml": "<p>Given a likelihood function <i>p(y | θ)</i>, find the posterior mean and MAP estimate of <i>θ</i>.</p>",
      "hints": [
        "Start by considering the Bayes' theorem formula.",
        "Think about how to apply the prior distribution to the likelihood function.",
        "Use the concept of expected value to derive the posterior mean."
      ],
      "solutionHtml": "<p>To find the posterior mean, we need to evaluate the integral <i>∫ p(θ | y) dθ</i>. We can do this by using the prior distribution and the likelihood function.</p><p>The MAP estimate is simply the value of <i>θ</i> that maximizes the posterior density <i>p(θ | y)</i>.</p>",
      "answerShort": "The posterior mean is given by the expected value of the posterior distribution, while the MAP estimate is the mode."
    },
    "commonMistakes": [
      "Forgetting to normalize the posterior distribution.",
      "Not considering the effect of the prior distribution on the posterior mean and MAP."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:29:45.300Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_prb_bayesian_estimation_011",
    "subject": "statistics",
    "type": "problem",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "problem": "{",
    "statementHtml": "<p>Estimate the posterior mean of a parameter given a prior and likelihood.</p>",
    "hints": [
      "Start by recalling Bayes' theorem.",
      "Think about how to combine the prior and likelihood to get the posterior distribution.",
      "Don't forget to normalize the posterior!"
    ],
    "solutionHtml": "<p>To find the posterior mean, we need to compute the expected value of the posterior distribution.</p>\\[ \\mu_{\\text{post}} = \\int x p(x | y) dx \\]\\n<p>Substituting our prior and likelihood, we get:</p>\\[ \\mu_{\\text{post}} = \\frac{\\int x p(y | x) p(x) dx}{\\int p(y | x) p(x) dx} \\]\\n<p>We can simplify this expression by recognizing it as the MAP estimate.</p>\",",
    "answerShort": "The posterior mean is equal to the MAP estimate.\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:30:00.282Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_estimation_012",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation: Posterior Mean and MAP",
    "contentHtml": "<p>In Bayesian statistics, we often need to estimate parameters of a distribution based on observed data.</p>",
    "formula": "{",
    "latex": "\\( \\mathbb{E}[x | y] = \\int x p(x | y) dx \\)\",",
    "name": "Posterior Mean\" },",
    "problem": "{",
    "statementHtml": "<p>Suppose we have a normal distribution with unknown mean and variance. We observe \\(n\\) data points, and want to estimate the mean using Bayes' theorem.</p>\",",
    "hints": [
      "Hint: Start by defining the prior and likelihood"
    ],
    "solutionHtml": "<p>To solve this problem, we'll follow these steps:</p><ul><li>Step 1: Define the prior distribution for the mean. We'll use a normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\).</li><li>Step 2: Define the likelihood function based on our observed data points.</li><li>Step 3: Compute the posterior distribution using Bayes' theorem.</li><li>Step 4: Calculate the posterior mean, which is the expected value of the posterior distribution.</li></ul>\",",
    "answerShort": "The posterior mean is \\( \\frac{1}{n} \\sum_{i=1}^n x_i + (1-\\frac{1}{n})\\mu_0 \\)\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a normal distribution with unknown mean and variance. We observe \\(5\\) data points: \\(2, 3, 4, 6, 7\\). We want to estimate the mean using Bayes' theorem.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution for the mean\", \"mathHtml\": \"\\( p(\\mu) = \\mathcal{N}(0, \\sigma^2) \\)\", \"explanation\": \"We choose a normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\), representing our initial uncertainty about the mean.\"}, {\"stepNumber\": 2, \"description\": \"Define the likelihood function\", \"mathHtml\": \"\\( p(x | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}} \\)\", \"explanation\": \"The likelihood function represents the probability of observing our data points given the mean.\"}, {\"stepNumber\": 3, \"description\": \"Compute the posterior distribution\", \"mathHtml\": \"\\( p(\\mu | x) = \\frac{p(x | \\mu) p(\\mu)}{\\int p(x | \\mu) p(\\mu) d\\mu} \\)\", \"explanation\": \"We use Bayes' theorem to update our prior distribution with the likelihood function.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the posterior mean\", \"mathHtml\": \"\\( \\mathbb{E}[x | y] = \\frac{\\int x p(x | \\mu) p(\\mu) d\\mu}{\\int p(x | \\mu) p(\\mu) d\\mu} \\)\", \"explanation\": \"The posterior mean is the expected value of our updated distribution.\"} ],",
    "finalAnswer": "The answer is \\( \\frac{1}{5} (2+3+4+6+7) + (1-\\frac{1}{5})0 = 4 \\)\" },",
    "intuition": "Bayesian point estimation provides a way to incorporate prior knowledge and uncertainty into our estimates, making it a powerful tool for many real-world applications.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:30:41.994Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_estimation_013",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation: Posterior Mean and MAP",
    "contentHtml": "<p>In Bayesian statistics, we often want to estimate a parameter based on some observed data.</p>",
    "formula": "{",
    "latex": "\\( \\mathbb{E}[x | y] = \\int x p(x | y) dx \\)\",",
    "name": "Posterior mean",
    "variants": "[ {\"latex\": \"\\( \\text{MAP} = \\arg\\max_p p(x | y) \\)\", \"description\": \"Maximum a posteriori estimation\"} ] },",
    "problem": "{",
    "statementHtml": "<p>Suppose we have a normal distribution \\( N(\\mu, \\sigma^2) \\) and we observe some data points. How can we estimate the mean \\( \\mu \\) using Bayesian methods?</p>\",",
    "hints": [
      "Think about Bayes' theorem",
      "Use conjugate priors"
    ],
    "solutionHtml": "<p>We'll use a prior distribution for \\( \\mu \\) and update it based on our observations.</p>\",",
    "answerShort": "The posterior mean is the weighted average of the prior mean and the observed data.\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a normal distribution \\( N(0, 1) \\) and we observe the data points \\( x_1 = 2, x_2 = 3, x_3 = 4 \\). How can we estimate the mean \\( \\mu \\)?</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify a prior distribution for \\( \\mu \\)\", \"mathHtml\": \"\\( p(\\mu) = N(0, 10) \\)\", \"explanation\": \"We choose a conjugate prior that's also normal.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior using Bayes' theorem\", \"mathHtml\": \"\\( p(\\mu | x) \\propto p(x | \\mu) p(\\mu) \\)\", \"explanation\": \"We use the likelihood function and our prior to get the posterior.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the posterior mean\", \"mathHtml\": \"\\( \\mathbb{E}[\\mu | x] = \\frac{\\int \\mu p(x | \\mu) p(\\mu) d\\mu}{\\int p(x | \\mu) p(\\mu) d\\mu} \\)\", \"explanation\": \"We take the weighted average of our prior mean and the observed data.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the MAP estimate\", \"mathHtml\": \"\\( \\text{MAP} = \\arg\\max_p p(\\mu | x) \\)\", \"explanation\": \"We find the value that maximizes the posterior probability.\"} ],",
    "finalAnswer": "The estimated mean is approximately 2.5.\" },",
    "intuition": "Bayesian point estimation helps us incorporate prior knowledge and update it based on new data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:31:15.621Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_bayesian_estimation_014",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "bayesian_statistics",
    "topic": "bayesian_estimation",
    "title": "Bayesian Point Estimation: Posterior Mean and MAP",
    "contentHtml": "<p>In Bayesian statistics, we often want to estimate parameters of a model based on observed data.</p><ul><li>We'll explore two common methods for point estimation: the posterior mean and maximum a posteriori (MAP).</li></ul>",
    "problem": "{",
    "statementHtml": "<p>Suppose we have a normal distribution with unknown mean μ and known variance σ^2. We observe n data points x_1, ..., x_n. Find the Bayesian point estimate for μ.</p>",
    "hints": [
      "Consider the prior distribution of μ",
      "Think about how to incorporate the observed data"
    ],
    "solutionHtml": "<p>We'll use a conjugate prior for μ, such as a normal distribution with mean 0 and variance τ^2. The posterior distribution is also normal:</p><p>\\[p(\\mu | x) = \\mathcal{N}(\\mu | \\bar{x}, \\frac{\\tau^2}{n + \\tau^2})\\]</p>\",",
    "answerShort": "The Bayesian point estimate for μ is the mean of the posterior distribution, which is \\(\\bar{x}\\).\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a normal distribution with unknown mean μ and known variance σ^2. We observe two data points x_1 = 3 and x_2 = 5. Find the Bayesian point estimate for μ using a prior distribution with mean 0 and variance 4.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the prior distribution\", \"mathHtml\": \"\\[p(\\mu) = \\mathcal{N}(\\mu | 0, 4)\\]\", \"explanation\": \"We choose a conjugate prior that is also normal.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with the observed data\", \"mathHtml\": \"\\[p(\\mu | x) = \\mathcal{N}(\\mu | \\frac{\\tau^2\\bar{x}}{n + \\tau^2}, \\frac{\\tau^2}{n + \\tau^2})\\]\", \"explanation\": \"We use Bayes' theorem to update the prior with the likelihood of the observed data.\"}, {\"stepNumber\": 3, \"description\": \"Find the posterior mean\", \"mathHtml\": \"\\[\\bar{x} = \\frac{\\tau^2\\bar{x}}{n + \\tau^2}\\]\", \"explanation\": \"The posterior mean is simply the weighted average of the prior mean and the observed data.\"}, {\"stepNumber\": 4, \"description\": \"Find the MAP estimate\", \"mathHtml\": \"\\[p(\\mu | x) = \\mathcal{N}(\\mu | 3.5, 0.25)\\]\", \"explanation\": \"The MAP estimate is the value of μ that maximizes the posterior distribution.\"}, {\"stepNumber\": 5, \"description\": \"Compare the two estimates\", \"mathHtml\": \"\", \"explanation\": \"We can see that both estimates are close to each other, but the MAP estimate is slightly more precise due to its concentration around a single value.\"} ],",
    "finalAnswer": "The Bayesian point estimate for μ is approximately 3.5.\" },",
    "intuition": "Bayesian point estimation provides a way to incorporate prior knowledge and observed data to make informed decisions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:31:53.587Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]