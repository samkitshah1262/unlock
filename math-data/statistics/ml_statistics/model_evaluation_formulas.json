[
  {
    "id": "stat_for_model_evaluation_004",
    "subject": "statistics",
    "type": "formula",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>Evaluating machine learning models is crucial to understand their performance and make informed decisions. In this formula card, we'll explore key metrics used in model evaluation.</p>",
    "formula": "{",
    "latex": "\\[Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}\\]\",",
    "name": "Accuracy",
    "variants": "[ {\"latex\": \"\\[Precision = \\frac{TP}{TP + FP}\\]\", \"description\": \"Precision measures the proportion of true positives among all positive predictions.\"}, {\"latex\": \"\\[Recall = \\frac{TP}{TP + FN}\\]\", \"description\": \"Recall measures the proportion of true positives among all actual positive instances.\"}, {\"latex\": \"\\[F1-score = 2 \\* \\frac{Precision \\* Recall}{Precision + Recall}\\]\", \"description\": \"The F1-score is the harmonic mean of precision and recall, providing a balanced measure.\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification model that predicts whether a customer will churn or not. We have 100 actual positive instances (customers who churned) and 80 true positives (correctly predicted by the model). Calculate the accuracy, precision, recall, and F1-score for this model.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Count the number of true positives\", \"mathHtml\": \"\\[TP = 80\\]\", \"explanation\": \"This is the number of customers who actually churned and were correctly predicted by the model.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the total number of positive predictions\", \"mathHtml\": \"\\[TP + FP = ?\\]\", \"explanation\": \"We need to find the total number of customers who were predicted to churn (true positives plus false positives).\"} ],",
    "finalAnswer": "Accuracy: 0.8, Precision: 0.9, Recall: 1.0, F1-score: 0.95\" },",
    "intuition": "Model evaluation metrics provide a way to quantify the performance of machine learning models and help us understand their strengths and weaknesses.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:08:56.411Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_model_evaluation_005",
    "subject": "statistics",
    "type": "formula",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics: Accuracy, Precision, Recall, F1, AUC-ROC, Calibration",
    "contentHtml": "<p>When evaluating machine learning models, it's crucial to use the right metrics to measure their performance. In this card, we'll explore six essential model evaluation metrics: accuracy, precision, recall, F1 score, AUC-ROC, and calibration.</p>",
    "formula": "{",
    "latex": "\\[Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}\\]\",",
    "name": "Accuracy",
    "variants": "[ {\"latex\": \"\\[Precision = \\frac{TP}{TP + FP}\\]\", \"description\": \"Precision measures the proportion of true positives among all predicted positive instances.\"}, {\"latex\": \"\\[Recall = \\frac{TP}{TP + FN}\\]\", \"description\": \"Recall measures the proportion of true positives among all actual positive instances.\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification model that predicts whether a customer will churn or not. We want to evaluate its performance on a test set.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN)\", \"mathHtml\": \"\\[...]\\\", \"explanation\": \"We count the instances where the model correctly predicted churn or non-churn.\"} ],",
    "finalAnswer": "The accuracy score for our model is 0.85.\" },",
    "intuition": "These metrics help us understand how well a model generalizes to new, unseen data and identify areas for improvement.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:09:17.716Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_model_evaluation_006",
    "subject": "statistics",
    "type": "formula",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>Model evaluation metrics are crucial in machine learning to assess the performance of a model.</p><p>A good model should balance accuracy and precision, while also considering recall and F1 score.</p>",
    "formula": "{",
    "latex": "\\[Accuracy = \\frac{TP + TN}{P + N}\\]\",",
    "name": "Accuracy",
    "variants": "[ {\"latex\": \"\\[Precision = \\frac{TP}{P}\\]\", \"description\": \"Precision measures the proportion of true positives among all positive predictions.\"}, {\"latex\": \"\\[Recall = \\frac{TP}{P + FN}\\]\", \"description\": \"Recall measures the proportion of true positives among all actual positive instances.\"}, {\"latex\": \"\\[F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\]\", \"description\": \"F1 score is the harmonic mean of precision and recall.\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification model that predicts whether a customer will churn or not.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the number of true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP).\"}, {\"stepNumber\": 2, \"description\": \"Use the formula for accuracy to calculate the model's performance.\", \"mathHtml\": \"\\[Accuracy = \\frac{TP + TN}{P + N}\\]\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "Model evaluation metrics provide a way to quantify the performance of a model and make informed decisions about its deployment.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:09:38.732Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_for_model_evaluation_007",
    "subject": "statistics",
    "type": "formula",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>In machine learning, evaluating model performance is crucial to ensure it generalizes well to new data. This card covers key metrics: accuracy, precision, recall, F1 score, AUC-ROC, and calibration.</p>",
    "formula": "{",
    "latex": "\\[Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}\\]\",",
    "name": "Accuracy",
    "variants": "[ {\"latex\": \"\\[Precision = \\frac{TP}{TP + FP}\\]\", \"description\": \"True positives over total positive predictions\"}, {\"latex\": \"\\[Recall = \\frac{TP}{TP + FN}\\]\", \"description\": \"True positives over total actual positive instances\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification model predicting whether a customer will churn or not. We have the following true labels and predictions:</p><ul><li>Actual: [0, 1, 1, 0, 1]</li><li>Predicted: [0, 1, 1, 0, 1]</li></ul>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Count true positives (TP) and false negatives (FN)\", \"mathHtml\": \"\\[TP = 3\\]\", \"explanation\": \"We correctly predicted 3 customers who actually churned\"}, {\"stepNumber\": 2, \"description\": \"Count true negatives (TN) and false positives (FP)\", \"mathHtml\": \"\\[TN = 2\\], \\[FP = 1\\]\", \"explanation\": \"We correctly predicted 2 customers who didn't churn and incorrectly predicted 1 customer to churn\"} ],",
    "finalAnswer": "Accuracy is calculated as the sum of TP, TN over the total instances\" },",
    "intuition": "These metrics help you understand your model's strengths and weaknesses, making it easier to improve its performance.",
    "tags": [
      "machine learning",
      "model evaluation"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:10:03.080Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]