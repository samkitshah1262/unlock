[
  {
    "id": "stat_thm_bias_variance_tradeoff_008",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "ml_statistics",
    "topic": "bias_variance_tradeoff",
    "title": "Bias-Variance Tradeoff",
    "contentHtml": "<p>The Bias-Variance Tradeoff is a fundamental concept in statistical learning.</p><p>It states that as we increase the complexity of our model to better fit the training data (reduce bias), we also increase the risk of overfitting and poor generalization (increase variance).</p>",
    "formula": "{",
    "latex": "\\\\[\\\\text{Bias} + \\\\text{Variance} = \\\\text{Error}\\]\",",
    "name": "Bias-Variance Decomposition\" },",
    "theorem": "{",
    "statement": "\\\\[\\\\min_{h \\in H} R(h) = \\\\min_{h \\in H} (T(h) + V(h))\\\\]\",",
    "proofSketch": "The proof involves showing that the expected error of a model is bounded by its bias and variance.\" },",
    "intuition": "In simple terms, as we add more features or complexity to our model, it becomes better at fitting the training data but also starts to memorize noise, leading to poor generalization.",
    "realWorldApplications": [
      "Reducing overfitting in neural networks",
      "Selecting the right hyperparameters for a model"
    ],
    "tags": [
      "Bias-Variance Tradeoff",
      "Model Selection",
      "Overfitting"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:58:48.344Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_thm_bias_variance_tradeoff_009",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "ml_statistics",
    "topic": "bias_variance_tradeoff",
    "title": "Bias-Variance Tradeoff",
    "contentHtml": "<p>The Bias-Variance Tradeoff is a fundamental concept in statistical learning.</p><p>It states that as we increase the complexity of our model to fit the training data better (reducing bias), we risk overfitting and increasing the variance of our predictions.</p>",
    "formula": "{",
    "latex": "\\[\\text{Bias} + \\text{Variance} = \\text{MSE}\\]",
    "name": "MSE\" },",
    "theorem": "{",
    "statement": "\\\\[\\\\min_{f \\in H} \\\\mathbb{E}_{x,y} [(y - f(x))^2] + \\\\lambda ||f||^2\\\\]\",",
    "proofSketch": "The proof involves showing that the first term is minimized when the model is too complex, and the second term is minimized when the model is too simple.\" },",
    "intuition": "In essence, we're trying to find a balance between fitting the training data well (low bias) and avoiding overfitting (low variance).",
    "realWorldApplications": [
      "This tradeoff has significant implications for model selection in machine learning, as it highlights the need for careful regularization and validation techniques."
    ],
    "tags": [
      "Bias-Variance Tradeoff",
      "Model Selection"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T23:59:05.510Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]