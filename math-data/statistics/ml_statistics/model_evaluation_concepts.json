[
  {
    "id": "stat_con_model_evaluation_001",
    "subject": "statistics",
    "type": "concept",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>In machine learning, evaluating model performance is crucial to ensure it generalizes well to new data. We'll explore six essential metrics: accuracy, precision, recall, F1-score, AUC-ROC, and calibration.</p>",
    "formula": "{",
    "latex": "\\(Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}\\)\",",
    "name": "Accuracy",
    "variants": "[ {\"latex\": \"\\(Precision = \\frac{TP}{TP + FP}\\)\", \"description\": \"True positives over total positives and false positives\"}, {\"latex\": \"\\(Recall = \\frac{TP}{TP + FN}\\)\", \"description\": \"True positives over total actual positive instances\"} ] },",
    "intuition": "These metrics help you understand how well your model performs on a specific task. Accuracy is the most straightforward, while precision and recall are useful for imbalanced datasets.",
    "realWorldApplications": [
      "Classifying spam vs. non-spam emails",
      "Identifying fraudulent transactions"
    ],
    "commonMistakes": [
      "Not considering class imbalance when evaluating precision and recall"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:07:52.052Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_model_evaluation_002",
    "subject": "statistics",
    "type": "concept",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>In machine learning, evaluating model performance is crucial to ensure it generalizes well to new data. We'll explore five essential metrics: accuracy, precision, recall, F1 score, and AUC-ROC.</p><p>These metrics help us understand how well our model classifies instances correctly. Accuracy measures the proportion of correct predictions, while precision and recall focus on positive predictions (e.g., true positives). The F1 score is a harmonic mean of precision and recall, providing a balanced view. Finally, AUC-ROC visualizes the receiver operating characteristic curve, helping us understand how well our model separates classes.</p>",
    "formula": {
      "latex": "\\[\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\\]",
      "name": "Accuracy",
      "variants": [
        {
          "latex": "\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]",
          "description": "Positive predictive value"
        },
        {
          "latex": "\\[\\text{Recall} = \\frac{TP}{TP + FN}\\]",
          "description": "True positive rate"
        },
        {
          "latex": "\\[\\text{F1 Score} = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\]",
          "description": "Harmonic mean of precision and recall"
        }
      ]
    },
    "intuition": "These metrics provide a nuanced understanding of model performance, helping us identify strengths and weaknesses.",
    "realWorldApplications": [
      "Classifying spam vs. non-spam emails in email filters"
    ],
    "commonMistakes": [
      "Confusing accuracy with precision or recall",
      "Not considering class imbalance when evaluating recall"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:08:12.867Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_con_model_evaluation_003",
    "subject": "statistics",
    "type": "concept",
    "chapter": "ml_statistics",
    "topic": "model_evaluation",
    "title": "Model Evaluation Metrics",
    "contentHtml": "<p>In machine learning, evaluating model performance is crucial to ensure it generalizes well to new data. We'll explore five essential metrics: accuracy, precision, recall, F1 score, and AUC-ROC.</p><p>Accuracy measures the proportion of correctly classified instances. Precision calculates the ratio of true positives to total predicted positive instances. Recall, on the other hand, is the proportion of true positives to actual positive instances.</p>",
    "formula": {
      "latex": "\\[\\text{F1} = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\]",
      "name": "F1 Score",
      "variants": [
        {
          "latex": "\\[\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\\]",
          "description": "Accuracy formula"
        }
      ]
    },
    "intuition": "These metrics provide a nuanced understanding of model performance, helping you identify strengths and weaknesses.",
    "realWorldApplications": [
      "In ML/AI, these metrics guide hyperparameter tuning and model selection for better generalization."
    ],
    "commonMistakes": [
      "Confusing accuracy with precision or recall",
      "Not considering class imbalance when evaluating"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-28T00:08:29.781Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]