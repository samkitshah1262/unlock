[
  {
    "id": "stat_wex_fisher_information_012",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "point_estimation",
    "topic": "fisher_information",
    "title": "Fisher Information: Definition and Computation",
    "contentHtml": "<p>Fisher information is a crucial concept in statistical inference.</p>",
    "formula": {
      "latex": "\\[ I(F;\\theta) = E\\left[- \\frac{\\partial^2}{\\partial \\theta^2} \\log p(X|\\theta) \\right] \\]",
      "name": "Fisher Information"
    },
    "problem": {
      "statementHtml": "<p>Given a random variable X with density p(x|\\theta), compute the Fisher information I(F;\\theta) for some parameter \\theta.</p>",
      "hints": [
        "Hint: Start by taking the derivative of the log likelihood"
      ],
      "solutionHtml": "",
      "answerShort": ""
    },
    "workedExample": {
      "problemHtml": "<p>Let X be a random variable with density p(x|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)^2/(2\\sigma^2)}.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Take the derivative of the log likelihood",
          "mathHtml": "\\[ \\frac{d}{d\\theta} \\log p(x|\\theta) = ... \\]",
          "explanation": "We're doing this to find the score function."
        },
        {
          "stepNumber": 2,
          "description": "Compute the second derivative",
          "mathHtml": "\\[ \\frac{d^2}{d\\theta^2} \\log p(x|\\theta) = ... \\]",
          "explanation": "This is crucial for finding the Fisher information."
        },
        {
          "stepNumber": 3,
          "description": "Evaluate the expectation",
          "mathHtml": "\\[ E\\left[- \\frac{\\partial^2}{\\partial \\theta^2} \\log p(x|\\theta) \\right] = ... \\]",
          "explanation": "This is where we use the given density to simplify the expression."
        },
        {
          "stepNumber": 4,
          "description": "Simplify and compute the Fisher information",
          "mathHtml": "\\[ I(F;\\theta) = ... \\]",
          "explanation": "Now we can plug in our values and find the final answer."
        }
      ],
      "finalAnswer": ""
    },
    "intuition": "Fisher information measures how well a statistical model is able to estimate a parameter.",
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T09:48:53.060Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_fisher_information_013",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "point_estimation",
    "topic": "fisher_information",
    "title": "Fisher Information",
    "contentHtml": "<p>Fisher information is a fundamental concept in mathematical statistics that measures the amount of information an experiment provides about a parameter.</p>",
    "formula": {
      "latex": "\\[ I(\\theta) = E\\left[- \\frac{d}{d\\theta} \\ln p(X | \\theta) \\right] \\]",
      "name": "Fisher Information"
    },
    "problem": {
      "statementHtml": "<p>Compute the Fisher information for a binomial distribution with probability of success \\(p\\).</p>",
      "hints": [
        "Hint: Start by finding the log-likelihood"
      ],
      "solutionHtml": "<p>To compute the Fisher information, we first find the log-likelihood:</p><ul><li>\\[ \\ln p(X | p) = (x + 1) \\ln(\\frac{p}{1-p}) + (n-x) \\ln(1-p) \\]</li></ul><p>Then, we take the derivative with respect to \\(p\\):</p><ul><li>\\[ - \\frac{d}{dp} \\ln p(X | p) = \\frac{x+1}{p(1-p)} - \\frac{n-x}{(1-p)^2} \\]</li></ul><p>The Fisher information is the expectation of this derivative:</p><ul><li>\\[ I(p) = E\\left[- \\frac{d}{dp} \\ln p(X | p) \\right] \\]</li></ul>",
      "answerShort": "The answer"
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a coin with probability of heads \\(0.5\\). We flip the coin \\(n=10\\) times and observe \\(x=6\\) heads.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Find the log-likelihood",
          "mathHtml": "\\[ \\ln p(X | p) = (x + 1) \\ln(\\frac{p}{1-p}) + (n-x) \\ln(1-p) \\]",
          "explanation": "This is the logarithmic likelihood function for a binomial distribution."
        },
        {
          "stepNumber": 2,
          "description": "Take the derivative with respect to \\(p\\)",
          "mathHtml": "\\[ - \\frac{d}{dp} \\ln p(X | p) = \\frac{x+1}{p(1-p)} - \\frac{n-x}{(1-p)^2} \\]",
          "explanation": "This is the derivative of the log-likelihood with respect to \\(p\\)."
        },
        {
          "stepNumber": 3,
          "description": "Compute the Fisher information",
          "mathHtml": "\\[ I(p) = E\\left[- \\frac{d}{dp} \\ln p(X | p) \\right] \\]",
          "explanation": "This is the expectation of the derivative, which gives us the Fisher information."
        },
        {
          "stepNumber": 4,
          "description": "Evaluate the Fisher information",
          "mathHtml": "\\[ I(p) = ? \\]",
          "explanation": "We can evaluate the Fisher information by plugging in the values we know and simplifying."
        }
      ],
      "finalAnswer": "The answer"
    },
    "intuition": "Fisher information measures how much an experiment tells us about a parameter.",
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T09:49:30.296Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_wex_fisher_information_014",
    "subject": "statistics",
    "type": "worked_example",
    "chapter": "point_estimation",
    "topic": "fisher_information",
    "title": "Fisher Information: Definition and Computation",
    "contentHtml": "<p>Fisher information is a fundamental concept in statistical inference.</p>",
    "formula": {
      "latex": "\\[ I(F_\\theta) = E_{\\theta}[-\\nabla \\log p(X | \\theta)]^T [-\\nabla \\log p(X | \\theta)] \\]",
      "name": "Fisher Information Matrix"
    },
    "problem": {
      "statementHtml": "<p>Compute the Fisher information matrix for a normal distribution.</p>",
      "hints": [
        "Hint: Use the log-likelihood function"
      ],
      "solutionHtml": "",
      "answerShort": ""
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a random sample \\(X_1, X_2, \\ldots, X_n\\) from a normal distribution \\(N(\\mu, \\sigma^2)\\). Compute the Fisher information matrix.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Define the log-likelihood function",
          "mathHtml": "\\[ \\log p(X | \\mu, \\sigma) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^n (X_i - \\mu)^2/\\sigma^2 \\]",
          "explanation": "This is the log-likelihood function for a normal distribution."
        },
        {
          "stepNumber": 2,
          "description": "Compute the derivative of the log-likelihood function",
          "mathHtml": "\\[ \\frac{\\partial}{\\partial \\mu} \\log p(X | \\mu, \\sigma) = -\\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i - \\mu) \\]",
          "explanation": "We take the derivative with respect to \\(\\mu\\), which gives us the score function."
        },
        {
          "stepNumber": 3,
          "description": "Compute the expected value of the score function",
          "mathHtml": "\\[ E_{\\mu}[-\\nabla \\log p(X | \\mu, \\sigma)] = -\\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i - \\mu) \\]",
          "explanation": "We take the expected value of the score function, which gives us the expected value of the derivative."
        },
        {
          "stepNumber": 4,
          "description": "Compute the Fisher information matrix",
          "mathHtml": "\\[ I(F_\\theta) = E_{\\mu}[-\\nabla \\log p(X | \\mu, \\sigma)]^T [-\\nabla \\log p(X | \\mu, \\sigma)] \\]",
          "explanation": "We compute the Fisher information matrix by taking the expected value of the score function and multiplying it by its transpose."
        }
      ],
      "finalAnswer": "\\[ I(F_\\theta) = \\frac{n}{\\sigma^2} \\]"
    },
    "intuition": "Fisher information measures the amount of information that an experiment provides about a parameter.",
    "visualDescription": "",
    "commonMistakes": [
      "Forgetting to take the expected value"
    ],
    "realWorldApplications": [
      "Maximum likelihood estimation in machine learning"
    ],
    "tags": [
      "point estimation",
      "statistical inference"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T09:50:06.975Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]