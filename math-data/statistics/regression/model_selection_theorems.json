[
  {
    "id": "stat_thm_model_selection_008",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "regression",
    "topic": "model_selection",
    "title": "The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)",
    "contentHtml": "<p>The Akaike information criterion (AIC) and Bayesian information criterion (BIC) are two widely used model selection methods in regression analysis.</p>",
    "formula": "{",
    "latex": "\\\\[ AIC = -2 \\* log(L) + 2k \\\\]\",",
    "name": "Akaike Information Criterion",
    "variants": "[ {\"latex\": \"\\\\[ BIC = -2 \\* log(L) + k \\* log(n) \\\\]\", \"description\": \"Bayesian Information Criterion\"} ] },",
    "theorem": "{",
    "statement": "\\[ AIC < BIC \\]",
    "proofSketch": "Not applicable for this theorem\" },",
    "intuition": "These criteria help us choose the best model by balancing goodness of fit and complexity.",
    "realWorldApplications": [
      "Model selection in machine learning, e.g., choosing between decision trees and random forests"
    ],
    "tags": [
      "model selection",
      "regression analysis"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T22:52:50.765Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "stat_thm_model_selection_009",
    "subject": "statistics",
    "type": "theorem",
    "chapter": "regression",
    "topic": "model_selection",
    "title": "The Akaike Information Criterion (AIC)",
    "contentHtml": "<p>The Akaike Information Criterion (AIC) is a widely used measure of model quality in regression analysis.</p><p>Given a set of candidate models, AIC selects the one that best balances goodness-of-fit and complexity.</p>",
    "formula": "{",
    "latex": "\\\\[ AIC = -2 \\* ln(L) + 2k \\\\]\",",
    "name": "Akaike Information Criterion\" },",
    "theorem": "{",
    "statement": "\\[ AIC < AIC_0 \\] if and only if the model is more complex than a baseline model",
    "proofSketch": "The proof involves showing that the difference in AIC values is equivalent to the Kullback-Leibler divergence between the two models.\" },",
    "intuition": "AIC helps us choose between models by trading off goodness-of-fit (measured by log-likelihood) against complexity (measured by the number of parameters k).",
    "realWorldApplications": [
      "Selecting the best model for a dataset in machine learning"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T22:53:05.570Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]