[
  {
    "id": "prob_for_probabilistic_models_004",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Probabilistic Generative Models: VAEs",
    "contentHtml": "<p>Latent variable models like Variational Autoencoders (VAEs) are a cornerstone of probabilistic generative models.</p>",
    "formula": "{",
    "latex": "\\\\[ p_{Z}(z) = \\\\frac{1}{\\sqrt{(2\\\\pi)^k \\\\sigma^2}} e^{(-\\\\frac{||z-0||^2}{2\\\\sigma^2})} \\\\]\",",
    "name": "Prior Distribution",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of handwritten digits and want to generate new, similar digits.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\\\[ p_{Z}(z) = \\\\frac{1}{\\sqrt{(2\\\\pi)^k \\\\sigma^2}} e^{(-\\\\frac{||z-0||^2}{2\\\\sigma^2})} \\\\]\", \"explanation\": \"This sets the initial state of our generative model\"} ],",
    "finalAnswer": "\" },",
    "intuition": "VAEs learn to compress and reconstruct data by finding a lower-dimensional representation (latent space) that captures the essential features.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:36:22.321Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_probabilistic_models_005",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Probabilistic Generative Models: VAEs",
    "contentHtml": "<p>Latent variable models like Variational Autoencoders (VAEs) are a cornerstone of probabilistic generative modeling.</p>",
    "formula": "{",
    "latex": "\\[q(z|x) = \\mathcal{N}(\\mu=\\mathbf{h}_\\theta(x), \\sigma=1)\\]\",",
    "name": "Variational Inference\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we want to generate new images of handwritten digits using a VAE.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the generative model\", \"mathHtml\": \"\\[p(x|z) = \\mathcal{N}(\\mu=0, \\sigma=1)\\]\", \"explanation\": \"The prior distribution over the latent variables.\"}, {\"stepNumber\": 2, \"description\": \"Sample from the latent space\", \"mathHtml\": \"\", \"explanation\": \"Use a normal distribution to sample from the latent space.\"} ],",
    "finalAnswer": "\" },",
    "intuition": "VAEs learn a probabilistic representation of the data by mapping inputs to a lower-dimensional latent space.",
    "realWorldApplications": [
      "Image generation for applications like style transfer"
    ],
    "tags": [
      "probabilistic generative models",
      "variational autoencoders"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:36:43.290Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_probabilistic_models_006",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Probabilistic Generative Models",
    "contentHtml": "<p>Latent variable models are a fundamental concept in probabilistic generative modeling.</p><ul><li>A probabilistic generative model is a type of statistical model that can be used to generate new data samples based on the patterns learned from existing data.</li></ul>",
    "formula": "{",
    "latex": "\\\\[q(z | x) = \\\\frac{1}{Z} p(x | z) q(z)\\]\",",
    "name": "Variational Inference\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images and we want to generate new images that are similar to the existing ones.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the prior distribution over the latent variables\", \"mathHtml\": \"\\\\[q(z) = \\\\mathcal{N}(z | 0, I)\\]\", \"explanation\": \"We choose a simple Gaussian distribution as our prior.\"} ],",
    "finalAnswer": "\" },",
    "intuition": "The key insight is that we can use the variational inference to approximate the true posterior distribution over the latent variables.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:37:01.810Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_probabilistic_models_007",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Probabilistic Generative Models: Latent Variable Models",
    "contentHtml": "<p>Latent variable models are a class of probabilistic generative models that aim to learn complex distributions by introducing unobserved variables.</p>",
    "formula": "{",
    "latex": "\\[ p(x, z) = \\prod_{i} p(x_i | z) p(z) \\]\",",
    "name": "Generative Model Formula",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we want to model the distribution of handwritten digits using a probabilistic generative model.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the generative model\", \"mathHtml\": \"\\( p(x, z) = \\prod_{i} p(x_i | z) p(z) \\)\", \"explanation\": \"We assume each digit is generated by a latent variable \\(z\\) and a conditional distribution \\(p(x_i | z)\\)\"}, {\"stepNumber\": 2, \"description\": \"Train the model\", \"mathHtml\": \"\", \"explanation\": \"We train the model using maximum likelihood estimation or variants like VAE\"} ],",
    "finalAnswer": "\" },",
    "intuition": "Latent variable models provide a flexible framework for modeling complex distributions by introducing unobserved variables, which can be useful in machine learning applications.",
    "tags": [
      "latent",
      "generative",
      "model"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:37:23.541Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]