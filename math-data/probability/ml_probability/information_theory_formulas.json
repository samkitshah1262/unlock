[
  {
    "id": "prob_for_information_theory_004",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>Entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Shannon Entropy",
    "variants": "[ {\"latex\": \"\\[H(Q|P) = H(Q) + KL(Q||P)\\]\", \"description\": \"Cross-entropy\"} ] },",
    "intuition": "Entropy is a measure of how much information is contained in a probability distribution. In machine learning, cross-entropy is used to measure the difference between predicted and actual probabilities.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:01.429Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_005",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>Entropy measures the uncertainty or randomness in a probability distribution.</p><p>Cross-entropy is used to measure the difference between two distributions.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Entropy",
    "variants": "[ {\"latex\": \"\\[H(Q) = H(P, Q) = -\\sum_{i} P(i) \\log_2 Q(i)\\]\", \"description\": \"Cross-entropy\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with two classes (0 and 1). The true distribution is P(0) = 0.6, P(1) = 0.4.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the entropy of the true distribution\", \"mathHtml\": \"\\[H(P) = -P(0) \\log_2 P(0) - P(1) \\log_2 P(1)\\]\", \"explanation\": \"We use the formula for entropy.\"} ],",
    "finalAnswer": "The answer is approximately 0.9709\" },",
    "intuition": "Entropy helps us understand how much information we gain from observing a random variable.",
    "realWorldApplications": [
      "Used in neural networks to measure the difference between predicted and actual distributions"
    ],
    "tags": [
      "probability",
      "information theory",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:24.968Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_006",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "subtitle": "Measuring uncertainty in probability distributions",
    "contentHtml": "<p>Entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\[H(P) = - \\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Shannon Entropy",
    "variants": "[ {\"latex\": \"\\[KL(D || G) = D \\log \\frac{D}{G}\\]\", \"description\": \"Kullback-Leibler divergence\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with class probabilities [0.6, 0.4]. Calculate the entropy of this distribution.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the probability mass function\", \"mathHtml\": \"\\(P(0) = 0.6\\)\", \"explanation\": \"We're given the class probabilities.\"}, {\"stepNumber\": 2, \"description\": \"Apply the entropy formula\", \"mathHtml\": \"\\(H(P) = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\\)\", \"explanation\": \"Plug in the values and simplify.\"} ],",
    "finalAnswer": "The answer is approximately 0.9709\" },",
    "intuition": "Entropy provides a way to quantify the uncertainty or randomness in a probability distribution, which is crucial in machine learning for tasks like classification and clustering.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:47.899Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_007",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>Entropy measures the uncertainty or randomness of a probability distribution. In machine learning, cross-entropy is used to measure the difference between predicted and actual probabilities.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Entropy",
    "variants": "[ {\"latex\": \"\\[CE(y, \\hat{y}) = -\\sum_{i} y_i \\log_2 \\hat{y}_i - \\sum_{i} (1-y_i) \\log_2 (1-\\hat{y}_i)\\]\",",
    "description": "Calculate the entropy of the actual probabilities",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with equal class sizes. The true positive rate is 0.8, false positive rate is 0.2, and the negative predictive value is 0.9.</p>",
    "steps": "[ {\"stepNumber\": 1,",
    "mathHtml": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "explanation": "Entropy measures the uncertainty in the data\"} ],",
    "finalAnswer": "\" },",
    "intuition": "Entropy helps us understand how much information is lost when we compress or approximate a probability distribution.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:08:10.523Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]