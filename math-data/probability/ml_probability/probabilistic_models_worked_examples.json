[
  {
    "id": "prob_wex_probabilistic_models_012",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Solving a Probabilistic Generative Model Example",
    "problem": "{",
    "statementHtml": "<p>Given a dataset of handwritten digits, we want to train a probabilistic generative model to generate new samples that resemble the original data.</p>",
    "hints": [
      "Hint: Use the EM algorithm for maximum likelihood estimation."
    ],
    "solutionHtml": "<p>To solve this problem, we'll follow these steps:</p>",
    "answerShort": "The answer is...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of 1000 handwritten digits (0-9) with 784 features each. We want to train a probabilistic generative model to generate new samples.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the probabilistic generative model\", \"mathHtml\": \"\\(P(x|z) = \\prod_{i=1}^{784} N(x_i | \\mu_i, \\sigma_i)\\)\", \"explanation\": \"We choose a probabilistic generative model that assumes each feature is normally distributed given the latent variable.\"}, {\"stepNumber\": 2, \"description\": \"Define the likelihood function\", \"mathHtml\": \"\\(L(\\theta) = \\prod_{i=1}^{1000} P(x_i | z)\\)\", \"explanation\": \"We define the likelihood function as the product of the probabilities of each data point given the latent variable.\"}, {\"stepNumber\": 3, \"description\": \"Use the EM algorithm for maximum likelihood estimation\", \"mathHtml\": \"\\(Q(\\theta) = E_{z} [L(\\theta)]\\)\", \"explanation\": \"We use the EM algorithm to find the maximum likelihood estimate of the model parameters.\"}, {\"stepNumber\": 4, \"description\": \"Compute the posterior distribution over the latent variables\", \"mathHtml\": \"\\(P(z|x) = \\frac{P(x|z)P(z)}{\\sum_{z'} P(x|z')P(z')}\\)\", \"explanation\": \"We compute the posterior distribution over the latent variables using Bayes' rule.\"}, {\"stepNumber\": 5, \"description\": \"Sample from the posterior distribution to generate new samples\", \"mathHtml\": \"\\(x \\sim P(z|x)\\)\", \"explanation\": \"We sample from the posterior distribution to generate new samples that resemble the original data.\"} ],",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:39:29.515Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_probabilistic_models_013",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Solving a Probabilistic Generative Model Problem",
    "contentHtml": "<p>In this worked example, we'll walk through solving a problem involving probabilistic generative models.</p>",
    "problem": "{",
    "statementHtml": "<p>Given a dataset of handwritten digits, use a variational autoencoder (VAE) to reconstruct the input data. What is the optimal number of latent variables for this model?</p>",
    "hints": [
      "Consider the trade-off between complexity and expressiveness.",
      "Think about how many clusters you might expect in the data."
    ],
    "solutionHtml": "<p>To solve this problem, we'll follow these steps:</p>",
    "answerShort": "The optimal number of latent variables is...\" },",
    "workedExample": "{",
    "problemHtml": "",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the VAE model\", \"mathHtml\": \"\\[ p(x) = \\int_{z} p(z) p(x|z) dz \\]\", \"explanation\": \"We start by defining the generative process for our VAE.\" }, {\"stepNumber\": 2, \"description\": \"Specify the likelihood function\", \"mathHtml\": \"\\[ p(x|z) = \\mathcal{N}(x; \\mu_z, \\sigma_z^2) \\]\", \"explanation\": \"Next, we specify the likelihood function for our data given the latent variables.\" }, {\"stepNumber\": 3, \"description\": \"Choose a prior distribution\", \"mathHtml\": \"\\[ p(z) = \\mathcal{N}(z; 0, I) \\]\", \"explanation\": \"We then choose a prior distribution over the latent variables.\" }, {\"stepNumber\": 4, \"description\": \"Use the EM algorithm to optimize the model\", \"mathHtml\": \"\", \"explanation\": \"Finally, we use the EM algorithm to optimize our model parameters.\" } ],",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:39:56.913Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_probabilistic_models_014",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Probabilistic Generative Models: VAE Preview",
    "contentHtml": "<p>In this worked example, we'll explore a simplified version of the Variational Autoencoder (VAE) using latent variables.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we want to generate synthetic images of handwritten digits. We have a dataset of labeled images and want to learn a probabilistic model that can generate new, realistic images.",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the generative model\", \"mathHtml\": \"\\[p(x) = \\int p(x|z) p(z) dz\\]\", \"explanation\": \"We're modeling the probability of an image x as a function of its latent representation z.\"}, {\"stepNumber\": 2, \"description\": \"Introduce the inference network\", \"mathHtml\": \"\\[q(z|x) = \\frac{p(x|z) p(z)}{\\int p(x|z') p(z') dz'}\\]\", \"explanation\": \"We're defining a probabilistic inference network that maps an image x to its latent representation z.\"}, {\"stepNumber\": 3, \"description\": \"Use the evidence lower bound (ELBO)\", \"mathHtml\": \"\\[L = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\\]\", \"explanation\": \"We're using the ELBO to optimize the VAE's parameters.\"}, {\"stepNumber\": 4, \"description\": \"Minimize the KL divergence\", \"mathHtml\": \"\\[D_{KL}(q(z|x) || p(z)) = \\int q(z|x) \\log \\frac{q(z|x)}{p(z)} dz\\]\", \"explanation\": \"We're minimizing the KL divergence between our inferred distribution and the prior to encourage the VAE to learn meaningful latent representations.\"}, ],",
    "finalAnswer": "By optimizing the ELBO, we can train a VAE that generates realistic images of handwritten digits.\" },",
    "intuition": "VAEs are powerful generative models that can be used for image synthesis, data augmentation, and anomaly detection.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:40:27.691Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_probabilistic_models_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "probabilistic_models",
    "title": "Solving a Probabilistic Generative Model",
    "contentHtml": "<p>In this example, we'll work through solving a probabilistic generative model using the Expectation-Maximization (EM) algorithm.</p>",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset of images and corresponding labels, we want to train a probabilistic generative model that can generate new images with similar characteristics. The model should be able to learn the underlying distribution of the data and generate samples from it.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the likelihood function\", \"mathHtml\": \"\\[ p(x | z) = \\prod_{i} N(x_i | \\mu(z), \\sigma^2) \\]\", \"explanation\": \"We define the likelihood function as a product of normal distributions for each data point.\"}, {\"stepNumber\": 2, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[ p(z) = N(z | 0, I) \\]\", \"explanation\": \"We assume a standard normal distribution for the latent variables.\"}, {\"stepNumber\": 3, \"description\": \"Compute the Q-function\", \"mathHtml\": \"\\[ Q(z | x) = \\int p(x | z) p(z) dz \\]\", \"explanation\": \"The Q-function is the expected log-likelihood of the data given the latent variables.\"}, {\"stepNumber\": 4, \"description\": \"Update the posterior distribution\", \"mathHtml\": \"\\[ p(z | x) = Q(z | x) / Z(x) \\]\", \"explanation\": \"We update the posterior distribution by normalizing the Q-function with a partition function.\"}, {\"stepNumber\": 5, \"description\": \"Update the model parameters\", \"mathHtml\": \"\"}, \"explanation\": \"We update the model parameters using the new posterior distribution.\"} ],",
    "finalAnswer": "The trained probabilistic generative model can be used to generate new images with similar characteristics.\" },",
    "intuition": "Probabilistic generative models provide a powerful framework for modeling complex data distributions and generating new samples that are consistent with those distributions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:40:57.179Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]