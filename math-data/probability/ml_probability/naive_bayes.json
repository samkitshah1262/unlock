[
  {
    "id": "prob_con_naive_bayes_001",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier",
    "contentHtml": "<p>The Naive Bayes classifier is a popular probabilistic approach in machine learning that assumes independence between features.</p><p>This assumption simplifies the calculation of probabilities and makes it more feasible for large datasets.</p>",
    "formula": {
      "latex": "\\(P(y|x) = \\frac{P(x|y) P(y)}{P(x)}\\)",
      "name": "Bayes' theorem"
    },
    "intuition": "The Naive Bayes classifier is 'naive' because it assumes independence between features, which might not always be the case in real-world data.",
    "realWorldApplications": [
      "Text classification",
      "Image classification"
    ],
    "commonMistakes": [
      "Forgetting to consider feature correlations"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:53:24.713Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_naive_bayes_002",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier",
    "contentHtml": "<p>The Naive Bayes classifier is a popular probabilistic approach in machine learning that assumes independence between features.</p><p>This assumption simplifies the calculation of conditional probabilities and makes it more feasible to apply Bayes' theorem for classification.</p>",
    "formula": {
      "latex": "\\mathbf{P}(y | x) = \\frac{\\mathbf{P}(x | y) \\mathbf{P}(y)}{\\mathbf{P}(x)}",
      "name": "Bayes' theorem"
    },
    "intuition": "The Naive Bayes classifier is a great example of how simplifying assumptions can lead to efficient and effective algorithms in machine learning.",
    "realWorldApplications": [
      "Text classification",
      "Sentiment analysis"
    ],
    "commonMistakes": [
      "Forgetting to apply the independence assumption",
      "Not considering class prior probabilities"
    ],
    "tags": [
      "machine learning",
      "probability theory"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:53:40.765Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_naive_bayes_003",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier",
    "contentHtml": "<p>The Naive Bayes classifier is a popular machine learning algorithm that uses Bayes' theorem to classify instances based on their features.</p><p>It assumes independence between the features, which simplifies the calculation of probabilities. This assumption is often reasonable for many real-world problems.</p>",
    "formula": {
      "latex": "\\[ P(y|X) = \\frac{P(X|y) P(y)}{P(X)} \\]",
      "name": "Bayes' Theorem"
    },
    "intuition": "The Naive Bayes classifier is a simple yet effective way to classify data when the features are independent. This assumption allows us to calculate the probability of each class given the features, which can be useful in many machine learning applications.",
    "realWorldApplications": [
      "Text classification",
      "Image classification"
    ],
    "commonMistakes": [
      "Assuming dependent features without justification"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:53:56.573Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_naive_bayes_004",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier",
    "contentHtml": "<p>The Naive Bayes classifier is a popular probabilistic approach in machine learning that assumes independence between features.</p>",
    "formula": "{",
    "latex": "\\\\[P(y \\mid x) = \\\\frac{P(x \\mid y) P(y)}{P(x)}\\\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images with labels (0 or 1), and we want to classify new images.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior probability P(y)\", \"mathHtml\": \"\\\\[P(y) = \\\\frac{1}{2}\\\\]\", \"explanation\": \"This is our initial belief about the class distribution\"}, {\"stepNumber\": 2, \"description\": \"Calculate the likelihood P(x \\mid y)\", \"mathHtml\": \"\\\\[P(x \\mid y) = \\\\prod_{i} P(x_i \\mid y)\\\\]\", \"explanation\": \"We assume independence between features\"} ],",
    "finalAnswer": "Classify new images based on their feature values\" },",
    "intuition": "The Naive Bayes classifier is a simple yet effective approach that works well when the features are independent.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:54:17.668Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_naive_bayes_005",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier Formula",
    "contentHtml": "<p>The Naive Bayes classifier is a probabilistic machine learning model that assumes independence between features.</p><p>This formula is used for training and prediction in the Naive Bayes algorithm.</p>",
    "formula": "{",
    "latex": "\\[P(y | x) = \\frac{1}{P(x)} \\prod_{i=1}^n P(x_i | y)\\]\",",
    "name": "Bayes' Theorem",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images labeled as either 'dog' or 'cat'. We want to train a Naive Bayes classifier to predict the class label given an image.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the prior probability of each class\", \"mathHtml\": \"\\\\[P(y) = \\\\frac{1}{2}\\]\", \"explanation\": \"The prior probability is the probability of the class before considering any features.\"} ],",
    "finalAnswer": "The trained classifier\" },",
    "intuition": "This formula represents the probability of a class given an instance, which is used to make predictions in the Naive Bayes algorithm.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:54:37.647Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_naive_bayes_006",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier",
    "contentHtml": "<p>The Naive Bayes classifier is a probabilistic machine learning algorithm that assumes independence between features.</p>",
    "formula": "{",
    "latex": "\\\\[ P(y \\mid x) = \\\\frac{P(x \\mid y) P(y)}{P(x)} \\\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset of handwritten digits, train a Naive Bayes classifier to classify new images.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the prior probability of each class\", \"mathHtml\": \"\\\\[ P(y) = \\\\frac{\\\\text{\\# instances in class } y}{\\\\text{\\# total instances}} \\\\]\", \"explanation\": \"This represents our initial understanding of the classes.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the likelihood of each feature given a class\", \"mathHtml\": \"\\\\[ P(x \\mid y) = \\\\frac{\\\\text{\\# instances in class } y \\\\text{ with feature } x}{P(y)} \\\\]\", \"explanation\": \"This represents how likely we are to see a certain feature given a class.\"} ],",
    "finalAnswer": "The trained classifier\" },",
    "intuition": "By assuming independence between features, the Naive Bayes classifier becomes computationally efficient and easy to interpret.",
    "realWorldApplications": [
      "Image classification",
      "Text classification"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:54:59.622Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_naive_bayes_007",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier Formula",
    "contentHtml": "<p>The Naive Bayes classifier is a popular probabilistic approach in machine learning.</p><p>It's based on the Bayes theorem and assumes independence between features.</p>",
    "formula": "{",
    "latex": "\\[P(y \\mid x) = \\frac{1}{P(x)} \\prod_{i=1}^n P(x_i \\mid y)\\]\",",
    "name": "Bayes' Theorem for Naive Bayes\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images with labels (0 or 1) representing whether an image is of a cat or not.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior probability P(y=0) and P(y=1)\", \"mathHtml\": \"\\(P(y=0) = 0.5\\), \\(P(y=1) = 0.5\\)\", \"explanation\": \"We assume equal prior probabilities\"}, {\"stepNumber\": 2, \"description\": \"Calculate the likelihoods P(x_i \\mid y)\"\", \"mathHtml\": \"\\(P(x_1 \\mid y=0) = 0.7\\), \\(P(x_1 \\mid y=1) = 0.3\\)\", \"explanation\": \"These are conditional probabilities of features given class labels\"} ],",
    "finalAnswer": "The posterior probability P(y=0|x) is calculated using the formula\" },",
    "intuition": "The Naive Bayes classifier assumes independence between features, which simplifies the calculation. This assumption is often reasonable in many real-world scenarios.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:55:24.739Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_naive_bayes_008",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with features \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), ..., \\(\\mathbf{x}_n\\) and class labels \\(y_1\\), \\(y_2\\), ..., \\(y_m\\), design a Naive Bayes classifier under the assumption of independence between features.</p>\",",
    "hints": [
      "<p>Start by modeling each feature using a Gaussian distribution.</p>",
      "<p>Use Bayes' theorem to calculate the posterior probability of each class given the input features.</p>",
      "<p>Assume that the class prior probabilities are known and equal.</p>"
    ],
    "solutionHtml": "<p>To train the classifier, compute the mean \\(\\mu_i\\) and variance \\(\\sigma_i^2\\) for each feature \\(x_i\\). Then, for a new input \\(\\mathbf{x}\\), calculate the likelihood of each class given the features using the Gaussian distribution.</p><p>For example:</p>\\[\\text{P}(y|\\mathbf{x}) = \\frac{\\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_{iy}, \\sigma_{iy}^2)}{\\sum_{j=1}^m \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_{ij}, \\sigma_{ij}^2)}\\]",
    "answerShort": "The trained classifier is a probabilistic model that predicts the class label given the input features.\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:55:49.246Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_naive_bayes_009",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "problem": "{",
    "statementHtml": "A Naive Bayes classifier is trained on a dataset with features X1, ..., Xn and labels y. Assuming independence between features, derive the predictive distribution P(y|x) for a new input x.",
    "hints": [
      "Start by writing down the joint probability density function of the features and labels.",
      "Use the chain rule to factorize the joint density into a product of conditional densities.",
      "Apply Bayes' theorem to get the predictive distribution."
    ],
    "solutionHtml": " The joint density is given by P(x, y) = ‚àèi P(xi | yi)P(yi). Applying Bayes' theorem, we get: \\[P(y|x) = \\frac{P(x|y)P(y)}{\\sum_{y'} P(x|y')P(y')}\\] This is the predictive distribution for a new input x. \",",
    "answerShort": "P(y|x)\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:56:05.769Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_naive_bayes_010",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with features X1, X2, ..., Xn and labels y, train a Naive Bayes classifier assuming independence between features.</p>",
    "hints": [
      "<p>Start by calculating the prior probability P(y).</p>",
      "<p>Then, for each feature Xi, calculate the likelihood P(Xi|y) using the Gaussian distribution.</p>",
      "<p>Finally, use Bayes' theorem to obtain the posterior probability P(y|X).</p>"
    ],
    "solutionHtml": "<p>To train a Naive Bayes classifier, we first need to calculate the prior probability P(y). This is simply the proportion of instances in each class.</p><p>\\[P(y) = \\frac{\\sum_{i=1}^N I[y_i=y]}{N}\\]</p><p>Next, for each feature Xi, we calculate the likelihood P(Xi|y) using the Gaussian distribution. This is done by calculating the mean and standard deviation of each feature for each class.</p><p>\\[P(Xi|y) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{(-(\\xi_i - \\mu)^2 / (2\\sigma^2))}\\]</p><p>Finally, we use Bayes' theorem to obtain the posterior probability P(y|X). This is done by multiplying the prior probability P(y) with the likelihood P(X|y).</p><p>\\[P(y|X) = \\frac{P(X|y)P(y)}{\\sum_{k=1}^K P(X|y_k)P(y_k)}\\]</p>\",",
    "answerShort": "The trained Naive Bayes classifier\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:56:32.458Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_naive_bayes_011",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "problem": "{",
    "statementHtml": "<p>Given a set of features X and their corresponding class labels y, develop a Naive Bayes classifier that assumes independence between the features.</p>",
    "hints": [
      "Start by defining the joint probability distribution over X and y.",
      "Use the chain rule to factorize the joint distribution into a product of conditional distributions.",
      "Assume each feature is conditionally independent given the class label."
    ],
    "solutionHtml": "<p>To develop the Naive Bayes classifier, we first define the joint probability distribution over X and y:</p>\\n\\ \\[P(X, y) = P(y) \\prod_{i=1}^n P(x_i | y).\\]\\n\\ <p>Next, we use the chain rule to factorize the joint distribution into a product of conditional distributions:</p>\\n\\ \\[P(X, y) = P(y) \\prod_{i=1}^n P(x_i | y) = P(y) \\prod_{i=1}^n P(x_i | c).\\]\\n\\ <p>Assuming each feature is conditionally independent given the class label, we can further factorize:</p>\\n\\ \\[P(X, y) = P(y) \\prod_{i=1}^n P(x_i | c) = P(y) \\prod_{i=1}^n \\prod_{c'} P(x_i | c').\\]\\n\\ <p>The final step is to compute the class probabilities and Bayes' theorem:</p>\\n\\ \\[P(c | X) = \\frac{P(X | c) P(c)}{\\sum_{c'} P(X | c') P(c')}.\\]",
    "answerShort": "The Naive Bayes classifier assumes independence between features.\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:56:59.239Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_naive_bayes_012",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier: Training and Prediction",
    "problem": {
      "statementHtml": "Given a dataset of features X and labels Y, train a Naive Bayes classifier to predict the label for a new instance.",
      "hints": [
        "Assume independence between features",
        "Use Bayes' theorem"
      ],
      "solutionHtml": "<p>To train the classifier, we calculate the prior probability P(Y) and the likelihoods P(X|Y).</p><p>For prediction, we calculate the posterior probabilities P(Y|X) for each class and choose the one with the highest probability.</p>",
      "answerShort": "The trained classifier can be used to make predictions."
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a dataset of handwritten digits (0-9) with features like width, height, and curvature. Train a Naive Bayes classifier to predict the digit for a new instance.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the prior probability P(Y)",
          "mathHtml": "\\[P(Y) = \\frac{\\sum_{i=0}^9 n_i}{N}\\]",
          "explanation": "The prior probability is the proportion of instances in each class."
        },
        {
          "stepNumber": 2,
          "description": "Calculate the likelihoods P(X|Y)",
          "mathHtml": "\\[P(X|Y) = \\prod_{i=1}^m P(x_i|Y)\\]",
          "explanation": "The likelihood is the probability of the features given the class."
        },
        {
          "stepNumber": 3,
          "description": "Calculate the posterior probabilities P(Y|X)",
          "mathHtml": "\\[P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}\\]",
          "explanation": "The posterior probability is the updated probability of the class given the features."
        },
        {
          "stepNumber": 4,
          "description": "Make predictions by choosing the class with the highest posterior probability",
          "mathHtml": "\\[\\hat{Y} = \\arg\\max_{y \\in Y} P(Y|X)\\]",
          "explanation": "The predicted label is the one with the highest posterior probability."
        }
      ],
      "finalAnswer": "The trained classifier can be used to make predictions."
    },
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:57:31.928Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_naive_bayes_013",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier Worked Example",
    "problem": "{",
    "statementHtml": "<p>A company wants to classify emails as either spam or not spam using a Naive Bayes classifier. Given the following training data:</p><ul><li>Spam: 100 emails with words 'buy', 'offer', and 'discount'</li><li>Not Spam: 200 emails without these words</li></ul>",
    "hints": [],
    "solutionHtml": "<p>To train the classifier, we calculate the probability of each word given spam or not spam. Then, for a new email with words 'buy' and 'offer', we predict the class based on the product of probabilities.</p><ul><li>What is the probability of 'buy' given Spam?</li><li>How do we handle zero-frequency events in this classifier?</li></ul>",
    "answerShort": "\" },",
    "workedExample": "{",
    "problemHtml": "<p>We have a new email with words 'buy', 'offer', and 'discount'. Predict the class using the trained Naive Bayes classifier.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the probability of each word given spam or not spam\", \"mathHtml\": \"\\[P(\\text{buy}|\\text{Spam}) = \\frac{\\text{# buy in Spam}}{\\text{# emails in Spam}},\\]\", \"explanation\": \"We calculate the probability by dividing the number of times 'buy' appears in spam emails by the total number of spam emails.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the product of probabilities for each word\", \"mathHtml\": \"\\[P(\\text{Spam}|\\text{buy},\\text{offer}) = P(\\text{buy}|\\text{Spam}) \\cdot P(\\text{offer}|\\text{Spam}),\\]\", \"explanation\": \"We multiply the probabilities of each word given spam to get the probability of the email being spam.\"}, {\"stepNumber\": 3, \"description\": \"Handle zero-frequency events by adding a smoothing term\", \"mathHtml\": \"\\[P(\\text{buy}|\\text{Spam}) = \\frac{\\text{# buy in Spam} + 1}{\\text{# emails in Spam} + 2},\\]\", \"explanation\": \"We add a small value to the numerator and denominator to avoid division by zero.\"}, {\"stepNumber\": 4, \"description\": \"Compare the product of probabilities for each class\", \"mathHtml\": \"\", \"explanation\": \"We compare the product of probabilities for spam and not spam classes to determine the predicted class.\"} ],",
    "finalAnswer": "Spam\" },",
    "intuition": "The Naive Bayes classifier assumes independence between features, which simplifies calculations but may not always be accurate.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:58:09.308Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_naive_bayes_014",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier: Independence Assumption",
    "contentHtml": "<p>In this worked example, we'll explore how to apply the Naive Bayes classifier with an independence assumption.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a dataset of handwritten digits (0-9) and want to classify new, unseen images. We can use the Naive Bayes classifier with an independence assumption.",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the class-conditional probability distributions\", \"mathHtml\": \"\\[P(y | x) = \\prod_{i=1}^n P(x_i | y)\\]\", \"explanation\": \"We assume that each feature is independent given the class label.\"}, {\"stepNumber\": 2, \"description\": \"Train the model using maximum likelihood estimation\", \"mathHtml\": \"\\[P(y) = \\frac{1}{N} \\sum_{i=1}^N I[y_i=y]\\]\", \"explanation\": \"We calculate the probability of each class given the training data.\"}, {\"stepNumber\": 3, \"description\": \"Make predictions using Bayes' theorem\", \"mathHtml\": \"\\[P(y | x) = \\frac{P(x | y) P(y)}{\\sum_{k=1}^K P(x | k) P(k)}\\]\", \"explanation\": \"We apply Bayes' theorem to calculate the probability of each class given the new, unseen image.\"}, {\"stepNumber\": 4, \"description\": \"Smoothing: handle zero-frequency classes\", \"mathHtml\": \"\\[P(y) = \\frac{1}{N} \\sum_{i=1}^N I[y_i=y] + \\epsilon\\]\", \"explanation\": \"We add a small value to the class probability to avoid division by zero when making predictions.\"}, {\"stepNumber\": 5, \"description\": \"Classify the new image\", \"mathHtml\": \"\", \"explanation\": \"We choose the class with the highest posterior probability as our prediction.\"} ],",
    "finalAnswer": "The predicted class label\" },",
    "intuition": "The Naive Bayes classifier assumes independence between features, making it a simple yet effective approach for classification tasks.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:58:40.605Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_naive_bayes_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "naive_bayes",
    "title": "Naive Bayes Classifier Worked Example",
    "contentHtml": "<p>In this example, we'll walk through a step-by-step solution of a Naive Bayes classifier problem.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a dataset of emails labeled as either spam or not spam. We want to train a Naive Bayes classifier to predict the likelihood of an email being spam based on its features (e.g., subject, sender, content).",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Assume independence between features\", \"mathHtml\": \"\\(P(\\text{spam}|\\mathbf{x}) = P(\\text{subject}|\\mathbf{x}) \\cdot P(\\text{sender}|\\mathbf{x}) \\cdots\\)\", \"explanation\": \"We're making the independence assumption, which simplifies our calculations.\"}, {\"stepNumber\": 2, \"description\": \"Calculate class probabilities\", \"mathHtml\": \"\\(P(\\text{spam}) = \\frac{\\text{# spam emails}}{\\text{total emails}}, P(\\text{not spam}) = 1 - P(\\text{spam})\\)\", \"explanation\": \"We need the prior probabilities of each class.\"}, {\"stepNumber\": 3, \"description\": \"Calculate feature conditional probabilities\", \"mathHtml\": \"\\(P(\\text{subject}|\\text{spam}) = \\frac{\\text{# spam emails with subject}}{\\text{# spam emails}}, P(\\text{sender}|\\text{not spam}) = \\frac{\\text{# not spam emails from sender}}{\\text{# not spam emails}}\\)\", \"explanation\": \"We're calculating the likelihood of each feature given a class.\"}, {\"stepNumber\": 4, \"description\": \"Calculate posterior probabilities\", \"mathHtml\": \"\\(P(\\text{spam}|\\mathbf{x}) = P(\\text{spam}) \\cdot \\prod_{i} P(f_i|\\text{spam}), P(\\text{not spam}|\\mathbf{x}) = P(\\text{not spam}) \\cdot \\prod_{i} P(f_i|\\text{not spam})\\)\", \"explanation\": \"We're combining the class probabilities with the feature conditional probabilities to get our final predictions.\"}, {\"stepNumber\": 5, \"description\": \"Make predictions\", \"mathHtml\": \"\", \"explanation\": \"Now we can use our trained model to predict the likelihood of an email being spam based on its features.\"} ],",
    "finalAnswer": "We've successfully trained and used a Naive Bayes classifier to classify emails as either spam or not spam.\" },",
    "intuition": "The key insight is that by assuming independence between features, we can simplify our calculations and make the training process more efficient.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:59:17.319Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]