[
  {
    "id": "prob_wex_information_theory_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "problem": "{",
    "statementHtml": "Given a probability distribution p(x) over a random variable X, define entropy H(p) as <p>log2(1/p(x))dx.</p>",
    "hints": [
      "Hint: Think about the uncertainty of the outcome"
    ],
    "solutionHtml": "<p>To solve this problem, we'll break it down into smaller steps:</p><ul><li>Step 1: Identify the probability distribution p(x).</li><li>Step 2: Calculate the entropy H(p) using the definition.</li></ul>",
    "answerShort": "The answer is...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with two classes, 0 and 1. We're given a probability distribution p(y|x) over the class label y given the input x.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the entropy H(p) for this problem\", \"mathHtml\": \"\\(H(p) = -\\sum_{y=0}^1 p(y|x) \\log_2 p(y|x)\\)\", \"explanation\": \"We're calculating the uncertainty of the class label given the input.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the entropy for each possible class label\", \"mathHtml\": \"\\(H(p) = -p(0|x) \\log_2 p(0|x) - p(1|x) \\log_2 p(1|x)\\)\", \"explanation\": \"We're breaking down the entropy into two parts, one for each class.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the average entropy across all inputs\", \"mathHtml\": \"\\(H(p) = -\\sum_{x} \\frac{1}{n} \\sum_{y=0}^1 p(y|x) \\log_2 p(y|x)\\)\", \"explanation\": \"We're averaging the entropy over all possible inputs.\"}, {\"stepNumber\": 4, \"description\": \"Find the cross-entropy between two probability distributions\", \"mathHtml\": \"\\(H(p, q) = -\\sum_{y=0}^1 \\left[p(y|x) \\log_2 p(y|x) + (1-p(y|x)) \\log_2 (1-p(y|x))\\right)\\)\", \"explanation\": \"We're calculating the difference between two probability distributions.\"} ],",
    "finalAnswer": "The final answer is...\" },",
    "intuition": "Entropy measures the uncertainty or randomness in a probability distribution. Cross-entropy measures the difference between two such distributions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:11:14.300Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_016",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness of a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\)\",",
    "name": "Entropy Formula\" },",
    "problem": "{",
    "statementHtml": "<p>Given two distributions P and Q, calculate the cross-entropy between them.</p>",
    "hints": [
      "Hint: Use the definition of entropy"
    ],
    "solutionHtml": "<p>Solution steps:</p><ul><li>We'll use the formula for entropy to define cross-entropy.</li><li>Substitute P with P(i) and Q with Q(j).</li><li>Evaluate the sum using the properties of logarithms.</li></ul>",
    "answerShort": "The answer is...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have two distributions P and Q over a binary outcome (0 or 1). The probability mass functions are:</p><ul><li>P(0) = 0.6, P(1) = 0.4</li><li>Q(0) = 0.7, Q(1) = 0.3</li></ul>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define cross-entropy\", \"mathHtml\": \"\\(CE(P,Q) = H(P,Q)\\)\", \"explanation\": \"We're using the entropy formula to define cross-entropy.\"}, {\"stepNumber\": 2, \"description\": \"Substitute P and Q\", \"mathHtml\": \"\\(CE(P,Q) = -\\sum_{i} P(i) \\log_2 Q(i)\\)\", \"explanation\": \"Substituting P with P(i) and Q with Q(j) gives us the cross-entropy formula.\"}, {\"stepNumber\": 3, \"description\": \"Evaluate the sum\", \"mathHtml\": \"\\(CE(P,Q) = -0.6 \\log_2 0.7 - 0.4 \\log_2 0.3\\)\", \"explanation\": \"Evaluating the sum using logarithmic properties gives us the final answer.\"}, {\"stepNumber\": 4, \"description\": \"Simplify and calculate\", \"mathHtml\": \"\\(CE(P,Q) = 0.6 \\log_2 1.43 + 0.4 \\log_2 0.67\\)\", \"explanation\": \"Simplifying the expression gives us the final answer.\"} ],",
    "finalAnswer": "The cross-entropy is...\" },",
    "intuition": "Entropy measures uncertainty, while cross-entropy measures the difference between two distributions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:11:51.588Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_017",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\)\",",
    "name": "Entropy\" },",
    "problem": "{",
    "statementHtml": "<p>Given two distributions p and q, calculate the cross-entropy between them.</p>",
    "hints": [
      "Hint: Use the formula for entropy"
    ],
    "solutionHtml": "<p>To solve this problem, we'll use the definition of cross-entropy:</p><p>\\(H(p,q) = -\\sum_{i} p_i \\log_2 \\frac{p_i}{q_i}\\)</p>\",",
    "answerShort": "The answer\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have two distributions over a binary outcome: p = [0.6, 0.4] and q = [0.7, 0.3]. Calculate the cross-entropy between them.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the entropy of each distribution\", \"mathHtml\": \"\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\) and \\(H(q) = -\\sum_{i} q_i \\log_2 q_i\\)\", \"explanation\": \"We're calculating the uncertainty in each distribution\"}, {\"stepNumber\": 2, \"description\": \"Calculate the cross-entropy\", \"mathHtml\": \"\\(H(p,q) = -\\sum_{i} p_i \\log_2 \\frac{p_i}{q_i}\\)\", \"explanation\": \"This measures how much the two distributions differ\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "Entropy and cross-entropy are essential in machine learning for tasks like classification, regression, and clustering.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:12:20.913Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_018",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and KL Divergence in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness of a probability distribution.</p>",
    "formula": {
      "latex": "\\[ H(P) = - \\sum_{i} P(i) log_2 P(i) \\]",
      "name": "Entropy"
    },
    "problem": {
      "statementHtml": "<p>Consider two distributions, P and Q. Calculate the KL divergence between them.</p>",
      "hints": [
        "Hint: Use the definition of KL divergence"
      ],
      "solutionHtml": "<p>To calculate the KL divergence, we need to sum over all possible values:</p><ul><li>\\[ D_{KL}(P \\Vert Q) = \\sum_{i} P(i) log_2 \\frac{P(i)}{Q(i)} \\]</li></ul>",
      "answerShort": "The answer is..."
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have two distributions, P and Q:</p><ul><li>P: [0.5, 0.3, 0.2] \\[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\]</li><li>Q: [0.7, 0.2, 0.1] \\[\\frac{7}{10}, \\frac{1}{5}, \\frac{1}{10}\\]</li></ul>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the entropy of P",
          "mathHtml": "\\[ H(P) = - \\sum_{i} P(i) log_2 P(i) \\]",
          "explanation": "We're calculating the uncertainty or randomness of the distribution P."
        },
        {
          "stepNumber": 2,
          "description": "Calculate the entropy of Q",
          "mathHtml": "\\[ H(Q) = - \\sum_{i} Q(i) log_2 Q(i) \\]",
          "explanation": "We're calculating the uncertainty or randomness of the distribution Q."
        },
        {
          "stepNumber": 3,
          "description": "Calculate the KL divergence",
          "mathHtml": "\\[ D_{KL}(P \\Vert Q) = \\sum_{i} P(i) log_2 \\frac{P(i)}{Q(i)} \\]",
          "explanation": "We're calculating the difference between the two distributions."
        }
      ],
      "finalAnswer": "The KL divergence is..."
    },
    "intuition": "Entropy and KL divergence are used to measure the similarity or dissimilarity of probability distributions.",
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:12:56.833Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_019",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\)\",",
    "name": "Shannon Entropy\" },",
    "problem": "{",
    "statementHtml": "<p>Given a binary classification problem with equal class probabilities, calculate the cross-entropy loss.</p>",
    "hints": [
      "Hint: Use the formula for entropy"
    ],
    "solutionHtml": "",
    "answerShort": "\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset with 50 positive and 50 negative examples. Calculate the cross-entropy loss when our model predicts:</p><ul><li>Positive class: 0.5</li><li>Negative class: 0.5</li></ul>",
    "steps": "[ {",
    "stepNumber": 2,
    "description": "Calculate the cross-entropy loss",
    "mathHtml": "\\(L = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1\\)\",",
    "explanation": "We use the formula for cross-entropy and plug in the predicted probabilities\" } ],",
    "finalAnswer": "The cross-entropy loss is 1\" },",
    "intuition": "Entropy measures the amount of information gained from observing a random variable. Cross-entropy measures the difference between our predictions and the true distribution.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:13:25.427Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]