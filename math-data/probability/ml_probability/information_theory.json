[
  {
    "id": "prob_con_information_theory_001",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Information Theory",
    "contentHtml": "<p>Information theory is a branch of mathematics that deals with quantifying the amount of information in a given message or data set. Entropy, cross-entropy, KL divergence, and mutual information are fundamental concepts in this field.</p><p>In essence, entropy measures the uncertainty or randomness in a probability distribution. Cross-entropy is used to measure the difference between two probability distributions. The Kullback-Leibler (KL) divergence is a way to quantify the distance between two probability distributions. Mutual information measures the amount of information that one random variable contains about another.</p>",
    "formula": "{",
    "latex": "\\[ H(P) = - \\sum_{i} P(i) \\log_2 P(i) \\]\",",
    "name": "Entropy",
    "variants": "[ {\"latex\": \"\\[ D(P \\| Q) = \\sum_{i} P(i) \\log_2 \\frac{P(i)}{Q(i)} \\]\", \"description\": \"KL divergence\"} ] },",
    "intuition": "Think of entropy as a measure of how much surprise or uncertainty is present in a probability distribution. For instance, if you flip a fair coin, the entropy would be high because there's equal chance of getting heads or tails.",
    "realWorldApplications": [
      "In machine learning, entropy is used to evaluate the quality of a model's predictions.",
      "KL divergence is used in text classification and topic modeling."
    ],
    "tags": [
      "information theory",
      "entropy"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:06:11.848Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_information_theory_002",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Information Theory",
    "contentHtml": "<p>Information theory is a branch of mathematics that deals with quantifying the amount of information in a given message or dataset. At its core lies the concept of entropy, which measures the uncertainty or randomness of a probability distribution.</p><p>In this context, entropy is often used to describe the amount of information present in a random variable. The higher the entropy, the more uncertain or unpredictable the outcome.</p>",
    "formula": {
      "latex": "\\(H(X) = - \\sum_{i} p(x_i) \\log_2 p(x_i)\\)",
      "name": "Entropy Formula"
    },
    "intuition": "Think of entropy as a measure of how much surprise or uncertainty is present in a random variable. The more unpredictable the outcome, the higher the entropy.",
    "realWorldApplications": [
      "In machine learning, entropy is used to evaluate the quality of a model's predictions and to determine the information gained from new data."
    ],
    "commonMistakes": [
      "Don't confuse entropy with variance or standard deviation; they measure different aspects of uncertainty."
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:06:30.066Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_information_theory_003",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Information Theory",
    "contentHtml": "<p>Information theory is a fundamental concept in probability theory that helps us quantify the amount of information or uncertainty in a random variable.</p><p>Entropy, cross-entropy, KL divergence, and mutual information are all measures of information that play crucial roles in machine learning.</p>",
    "formula": "{",
    "latex": "\\(H(X) = - \\sum_{x} p(x) \\log_2 p(x)\\)\",",
    "name": "Shannon Entropy",
    "variants": "[ {\"latex\": \"\\(KL(P \\| Q) = D_P(Q)\\)\", \"description\": \"Kullback-Leibler divergence\"} ] },",
    "intuition": "Entropy measures the uncertainty or randomness in a random variable. A high entropy indicates more uncertainty, while a low entropy means less uncertainty.",
    "realWorldApplications": [
      "In machine learning, entropy is used to evaluate the quality of clustering algorithms and to determine the optimal number of clusters."
    ],
    "commonMistakes": [
      "Confusing entropy with cross-entropy",
      "Not understanding that KL divergence is not a metric"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:06:48.013Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_004",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>Entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Shannon Entropy",
    "variants": "[ {\"latex\": \"\\[H(Q|P) = H(Q) + KL(Q||P)\\]\", \"description\": \"Cross-entropy\"} ] },",
    "intuition": "Entropy is a measure of how much information is contained in a probability distribution. In machine learning, cross-entropy is used to measure the difference between predicted and actual probabilities.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:01.429Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_005",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>Entropy measures the uncertainty or randomness in a probability distribution.</p><p>Cross-entropy is used to measure the difference between two distributions.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Entropy",
    "variants": "[ {\"latex\": \"\\[H(Q) = H(P, Q) = -\\sum_{i} P(i) \\log_2 Q(i)\\]\", \"description\": \"Cross-entropy\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with two classes (0 and 1). The true distribution is P(0) = 0.6, P(1) = 0.4.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the entropy of the true distribution\", \"mathHtml\": \"\\[H(P) = -P(0) \\log_2 P(0) - P(1) \\log_2 P(1)\\]\", \"explanation\": \"We use the formula for entropy.\"} ],",
    "finalAnswer": "The answer is approximately 0.9709\" },",
    "intuition": "Entropy helps us understand how much information we gain from observing a random variable.",
    "realWorldApplications": [
      "Used in neural networks to measure the difference between predicted and actual distributions"
    ],
    "tags": [
      "probability",
      "information theory",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:24.968Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_006",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "subtitle": "Measuring uncertainty in probability distributions",
    "contentHtml": "<p>Entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\[H(P) = - \\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Shannon Entropy",
    "variants": "[ {\"latex\": \"\\[KL(D || G) = D \\log \\frac{D}{G}\\]\", \"description\": \"Kullback-Leibler divergence\"} ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with class probabilities [0.6, 0.4]. Calculate the entropy of this distribution.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the probability mass function\", \"mathHtml\": \"\\(P(0) = 0.6\\)\", \"explanation\": \"We're given the class probabilities.\"}, {\"stepNumber\": 2, \"description\": \"Apply the entropy formula\", \"mathHtml\": \"\\(H(P) = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\\)\", \"explanation\": \"Plug in the values and simplify.\"} ],",
    "finalAnswer": "The answer is approximately 0.9709\" },",
    "intuition": "Entropy provides a way to quantify the uncertainty or randomness in a probability distribution, which is crucial in machine learning for tasks like classification and clustering.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:07:47.899Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_information_theory_007",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>Entropy measures the uncertainty or randomness of a probability distribution. In machine learning, cross-entropy is used to measure the difference between predicted and actual probabilities.</p>",
    "formula": "{",
    "latex": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "name": "Entropy",
    "variants": "[ {\"latex\": \"\\[CE(y, \\hat{y}) = -\\sum_{i} y_i \\log_2 \\hat{y}_i - \\sum_{i} (1-y_i) \\log_2 (1-\\hat{y}_i)\\]\",",
    "description": "Calculate the entropy of the actual probabilities",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with equal class sizes. The true positive rate is 0.8, false positive rate is 0.2, and the negative predictive value is 0.9.</p>",
    "steps": "[ {\"stepNumber\": 1,",
    "mathHtml": "\\[H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\]\",",
    "explanation": "Entropy measures the uncertainty in the data\"} ],",
    "finalAnswer": "\" },",
    "intuition": "Entropy helps us understand how much information is lost when we compress or approximate a probability distribution.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:08:10.523Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_information_theory_008",
    "subject": "probability",
    "type": "theorem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "KL Divergence Theorem",
    "contentHtml": "<p>The KL divergence theorem is a fundamental concept in information theory that helps us understand the relationship between two probability distributions.</p>",
    "formula": {
      "latex": "\\[ KL(P \\| Q) = \\int P(x) \\log\\frac{P(x)}{Q(x)} dx \\]",
      "name": "KL Divergence"
    },
    "theorem": {
      "statement": "\\[ \\textbf{Theorem: } KL(P \\| Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)} \\leq \\log\\frac{1}{Q(\\text{argmax}_x P(x))} \\]",
      "proofSketch": "\\[ The proof involves showing that the KL divergence is non-negative and then using the fact that \\log 1 = 0 to prove the inequality. \\]"
    },
    "intuition": "<p>The KL divergence theorem tells us that the difference between two probability distributions can be bounded by a term involving the log of the ratio of their maximum probabilities.</p>",
    "realWorldApplications": [
      "KL divergence is used in many machine learning algorithms, such as logistic regression and neural networks."
    ],
    "tags": [
      "information theory",
      "probability theory"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:08:31.089Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_information_theory_009",
    "subject": "probability",
    "type": "theorem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Kullback-Leibler Divergence",
    "contentHtml": "<p>The Kullback-Leibler (KL) divergence measures the difference between two probability distributions.</p>",
    "formula": {
      "latex": "\\[ KL(P\\|Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)} \\]",
      "name": "Kullback-Leibler Divergence"
    },
    "theorem": {
      "statement": "\\[ KL(P\\|Q) \\geq 0, with equality if and only if P=Q \\]"
    },
    "intuition": "The KL divergence represents the expected change in information when we switch from using Q to use P. It's a measure of how much more 'surprised' we are by P than by Q.",
    "realWorldApplications": [
      "Used in many machine learning algorithms, such as maximum likelihood estimation and Bayesian inference"
    ],
    "tags": [
      "information theory",
      "probability",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:08:47.470Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_information_theory_010",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "problem": "{",
    "statementHtml": "<p>Compute the cross-entropy between two discrete probability distributions <code>P</code> and <code>Q</code>.</p>",
    "hints": [
      "Start by recalling the definition of cross-entropy.",
      "Think about how you would compute the entropy of a single distribution.",
      "Use the chain rule to relate the cross-entropy to the entropies of each individual distribution."
    ],
    "solutionHtml": "<p>To begin, recall that the cross-entropy between <code>P</code> and <code>Q</code> is given by:</p>\\[\\mathcal{H}(P,Q) = -\\sum_{i} P(i) \\log Q(i).\\]<p>Next, consider how you would compute the entropy of a single distribution <code>P</code>:</p>\\[\\mathcal{H}(P) = -\\sum_{i} P(i) \\log P(i).\\]<p>Now, use the chain rule to relate the cross-entropy to the entropies of each individual distribution:</p>\\[\\mathcal{H}(P,Q) = -\\sum_{i} P(i) \\log Q(i) = -\\sum_{i} P(i) (\\log P(i) + \\log (Q(i)/P(i))) = \\mathcal{H}(P) + \\KL(P || Q).\\]",
    "answerShort": "The cross-entropy is given by the formula above.\" },",
    "commonMistakes": [
      "Forgetting to use the chain rule",
      "Not recognizing that <code>Q</code> is a probability distribution"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:09:11.854Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_information_theory_011",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "problem": "{",
    "statementHtml": "<p>Consider a binary classification problem where we have two classes: positive and negative.</p>",
    "hints": [
      "Start by calculating the entropy of each class.",
      "Use the fact that entropy is additive for independent events.",
      "Think about how you can use this to define cross-entropy."
    ],
    "solutionHtml": "<p>To calculate the entropy of each class, we need to sum the probabilities of each outcome multiplied by their logarithms.</p>\\n\\[H(Y) = -\\sum_{i=1}^2 p_i \\log_2 p_i\\]\\n<p>Now, let's say we have two classes with probabilities 0.7 and 0.3 for positive and negative respectively.</p>\\n<p>We can calculate the entropy of each class:</p>\\n\\[H(Positive) = -0.7 \\log_2 0.7 - 0.3 \\log_2 0.3\\]\\n<p>Next, we'll use the fact that entropy is additive for independent events to define cross-entropy.</p>\\n\\[CE = H(Y) + H(\\neg Y)\\]\",",
    "answerShort": "The answer\" },",
    "commonMistakes": [
      "Forgetting to normalize probabilities",
      "Not accounting for logarithmic base"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:09:32.498Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_information_theory_012",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "problem": "{",
    "statementHtml": "<p>Calculate the cross-entropy between two discrete distributions P and Q.</p>",
    "hints": [
      "Start by recalling the definition of cross-entropy.",
      "Use the logarithmic identity to simplify the expression.",
      "Compare your answer with the KL-divergence formula."
    ],
    "solutionHtml": "<p>Let's denote the probability mass functions as <i>P</i>(x) and <i>Q</i>(x). The cross-entropy is:</p>\\n\\[H(P, Q) = -\\sum_x P(x) \\log \\frac{Q(x)}{P(x)}.\\]\\n<p>To simplify this expression, we can use the logarithmic identity:</p>\\n\\[0 \\leq x \\log \\frac{x}{y} + (1-x) \\log \\frac{1-x}{1-y}\\]\\n<p>Substituting <i>P</i>(x) and <i>Q</i>(x), we get:</p>\\n\\[\\sum_x P(x) \\log \\frac{P(x)}{Q(x)} + \\sum_x (1-P(x)) \\log \\frac{1-P(x)}{1-Q(x)}.\\]\\n<p>This is the cross-entropy formula.</p>\",",
    "answerShort": "-\\sum_x P(x) \\log \\frac{Q(x)}{P(x)}\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:09:54.366Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_information_theory_013",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "problem": "{",
    "statementHtml": "<p>Compute the cross-entropy between two discrete distributions <code>P</code> and <code>Q</code>.</p>",
    "hints": [
      "Start by recalling the definition of cross-entropy.",
      "Think about how to relate this concept to KL divergence.",
      "Use the fact that cross-entropy is a measure of the difference between two distributions."
    ],
    "solutionHtml": "<p>Let <code>H(P)</code> and <code>H(Q)</code> be the entropies of <code>P</code> and <code>Q</code>, respectively. Then, the cross-entropy between <code>P</code> and <code>Q</code> is:</p>\\[H(P,Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}.\\]<p>To compute this quantity, we can use the definition of entropy:</p>\\[H(P) = -\\sum_{i} P(i) \\log P(i).\\]\\[H(Q) = -\\sum_{i} Q(i) \\log Q(i).\\]Substituting these expressions into the cross-entropy formula, we get:</p>\\[H(P,Q) = -\\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}.\\]<p>This is the final answer.</p>\",",
    "answerShort": "The cross-entropy between two discrete distributions <code>P</code> and <code>Q</code> is given by the formula above.\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:10:18.975Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_information_theory_014",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "problem": "{",
    "statementHtml": "<p>Compute the cross-entropy between two discrete distributions <code>P</code> and <code>Q</code>.</p>",
    "hints": [
      "Recall that cross-entropy is related to KL divergence.",
      "Think about how you can use the definition of entropy to simplify the problem.",
      "Use the fact that <code>P</code> and <code>Q</code> are discrete distributions."
    ],
    "solutionHtml": "<p>To compute the cross-entropy, we need to sum over all possible outcomes:</p>\\n\\ \\[ H(P,Q) = -\\sum_{i} P(i) \\log Q(i) + P(i) \\log P(i) \\]\\n\\ <p>We can simplify this expression by combining like terms:</p>\\n\\ \\[ H(P,Q) = -\\sum_{i} P(i) \\log \\frac{Q(i)}{P(i)} \\]\\n\\ <p>This is equal to the KL divergence between <code>P</code> and <code>Q</code>, up to a constant factor.</p>\",",
    "answerShort": "H(P,Q)\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:10:38.531Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "problem": "{",
    "statementHtml": "Given a probability distribution p(x) over a random variable X, define entropy H(p) as <p>log2(1/p(x))dx.</p>",
    "hints": [
      "Hint: Think about the uncertainty of the outcome"
    ],
    "solutionHtml": "<p>To solve this problem, we'll break it down into smaller steps:</p><ul><li>Step 1: Identify the probability distribution p(x).</li><li>Step 2: Calculate the entropy H(p) using the definition.</li></ul>",
    "answerShort": "The answer is...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a binary classification problem with two classes, 0 and 1. We're given a probability distribution p(y|x) over the class label y given the input x.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the entropy H(p) for this problem\", \"mathHtml\": \"\\(H(p) = -\\sum_{y=0}^1 p(y|x) \\log_2 p(y|x)\\)\", \"explanation\": \"We're calculating the uncertainty of the class label given the input.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the entropy for each possible class label\", \"mathHtml\": \"\\(H(p) = -p(0|x) \\log_2 p(0|x) - p(1|x) \\log_2 p(1|x)\\)\", \"explanation\": \"We're breaking down the entropy into two parts, one for each class.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the average entropy across all inputs\", \"mathHtml\": \"\\(H(p) = -\\sum_{x} \\frac{1}{n} \\sum_{y=0}^1 p(y|x) \\log_2 p(y|x)\\)\", \"explanation\": \"We're averaging the entropy over all possible inputs.\"}, {\"stepNumber\": 4, \"description\": \"Find the cross-entropy between two probability distributions\", \"mathHtml\": \"\\(H(p, q) = -\\sum_{y=0}^1 \\left[p(y|x) \\log_2 p(y|x) + (1-p(y|x)) \\log_2 (1-p(y|x))\\right)\\)\", \"explanation\": \"We're calculating the difference between two probability distributions.\"} ],",
    "finalAnswer": "The final answer is...\" },",
    "intuition": "Entropy measures the uncertainty or randomness in a probability distribution. Cross-entropy measures the difference between two such distributions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:11:14.300Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_016",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness of a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(P) = -\\sum_{i} P(i) \\log_2 P(i)\\)\",",
    "name": "Entropy Formula\" },",
    "problem": "{",
    "statementHtml": "<p>Given two distributions P and Q, calculate the cross-entropy between them.</p>",
    "hints": [
      "Hint: Use the definition of entropy"
    ],
    "solutionHtml": "<p>Solution steps:</p><ul><li>We'll use the formula for entropy to define cross-entropy.</li><li>Substitute P with P(i) and Q with Q(j).</li><li>Evaluate the sum using the properties of logarithms.</li></ul>",
    "answerShort": "The answer is...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have two distributions P and Q over a binary outcome (0 or 1). The probability mass functions are:</p><ul><li>P(0) = 0.6, P(1) = 0.4</li><li>Q(0) = 0.7, Q(1) = 0.3</li></ul>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define cross-entropy\", \"mathHtml\": \"\\(CE(P,Q) = H(P,Q)\\)\", \"explanation\": \"We're using the entropy formula to define cross-entropy.\"}, {\"stepNumber\": 2, \"description\": \"Substitute P and Q\", \"mathHtml\": \"\\(CE(P,Q) = -\\sum_{i} P(i) \\log_2 Q(i)\\)\", \"explanation\": \"Substituting P with P(i) and Q with Q(j) gives us the cross-entropy formula.\"}, {\"stepNumber\": 3, \"description\": \"Evaluate the sum\", \"mathHtml\": \"\\(CE(P,Q) = -0.6 \\log_2 0.7 - 0.4 \\log_2 0.3\\)\", \"explanation\": \"Evaluating the sum using logarithmic properties gives us the final answer.\"}, {\"stepNumber\": 4, \"description\": \"Simplify and calculate\", \"mathHtml\": \"\\(CE(P,Q) = 0.6 \\log_2 1.43 + 0.4 \\log_2 0.67\\)\", \"explanation\": \"Simplifying the expression gives us the final answer.\"} ],",
    "finalAnswer": "The cross-entropy is...\" },",
    "intuition": "Entropy measures uncertainty, while cross-entropy measures the difference between two distributions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:11:51.588Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_017",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the amount of uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\)\",",
    "name": "Entropy\" },",
    "problem": "{",
    "statementHtml": "<p>Given two distributions p and q, calculate the cross-entropy between them.</p>",
    "hints": [
      "Hint: Use the formula for entropy"
    ],
    "solutionHtml": "<p>To solve this problem, we'll use the definition of cross-entropy:</p><p>\\(H(p,q) = -\\sum_{i} p_i \\log_2 \\frac{p_i}{q_i}\\)</p>\",",
    "answerShort": "The answer\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have two distributions over a binary outcome: p = [0.6, 0.4] and q = [0.7, 0.3]. Calculate the cross-entropy between them.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the entropy of each distribution\", \"mathHtml\": \"\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\) and \\(H(q) = -\\sum_{i} q_i \\log_2 q_i\\)\", \"explanation\": \"We're calculating the uncertainty in each distribution\"}, {\"stepNumber\": 2, \"description\": \"Calculate the cross-entropy\", \"mathHtml\": \"\\(H(p,q) = -\\sum_{i} p_i \\log_2 \\frac{p_i}{q_i}\\)\", \"explanation\": \"This measures how much the two distributions differ\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "Entropy and cross-entropy are essential in machine learning for tasks like classification, regression, and clustering.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:12:20.913Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_018",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and KL Divergence in Machine Learning",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness of a probability distribution.</p>",
    "formula": {
      "latex": "\\[ H(P) = - \\sum_{i} P(i) log_2 P(i) \\]",
      "name": "Entropy"
    },
    "problem": {
      "statementHtml": "<p>Consider two distributions, P and Q. Calculate the KL divergence between them.</p>",
      "hints": [
        "Hint: Use the definition of KL divergence"
      ],
      "solutionHtml": "<p>To calculate the KL divergence, we need to sum over all possible values:</p><ul><li>\\[ D_{KL}(P \\Vert Q) = \\sum_{i} P(i) log_2 \\frac{P(i)}{Q(i)} \\]</li></ul>",
      "answerShort": "The answer is..."
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have two distributions, P and Q:</p><ul><li>P: [0.5, 0.3, 0.2] \\[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\]</li><li>Q: [0.7, 0.2, 0.1] \\[\\frac{7}{10}, \\frac{1}{5}, \\frac{1}{10}\\]</li></ul>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the entropy of P",
          "mathHtml": "\\[ H(P) = - \\sum_{i} P(i) log_2 P(i) \\]",
          "explanation": "We're calculating the uncertainty or randomness of the distribution P."
        },
        {
          "stepNumber": 2,
          "description": "Calculate the entropy of Q",
          "mathHtml": "\\[ H(Q) = - \\sum_{i} Q(i) log_2 Q(i) \\]",
          "explanation": "We're calculating the uncertainty or randomness of the distribution Q."
        },
        {
          "stepNumber": 3,
          "description": "Calculate the KL divergence",
          "mathHtml": "\\[ D_{KL}(P \\Vert Q) = \\sum_{i} P(i) log_2 \\frac{P(i)}{Q(i)} \\]",
          "explanation": "We're calculating the difference between the two distributions."
        }
      ],
      "finalAnswer": "The KL divergence is..."
    },
    "intuition": "Entropy and KL divergence are used to measure the similarity or dissimilarity of probability distributions.",
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:12:56.833Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_information_theory_019",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "information_theory",
    "title": "Entropy and Cross-Entropy",
    "contentHtml": "<p>In information theory, entropy measures the uncertainty or randomness in a probability distribution.</p>",
    "formula": "{",
    "latex": "\\(H(p) = -\\sum_{i} p_i \\log_2 p_i\\)\",",
    "name": "Shannon Entropy\" },",
    "problem": "{",
    "statementHtml": "<p>Given a binary classification problem with equal class probabilities, calculate the cross-entropy loss.</p>",
    "hints": [
      "Hint: Use the formula for entropy"
    ],
    "solutionHtml": "",
    "answerShort": "\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset with 50 positive and 50 negative examples. Calculate the cross-entropy loss when our model predicts:</p><ul><li>Positive class: 0.5</li><li>Negative class: 0.5</li></ul>",
    "steps": "[ {",
    "stepNumber": 2,
    "description": "Calculate the cross-entropy loss",
    "mathHtml": "\\(L = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1\\)\",",
    "explanation": "We use the formula for cross-entropy and plug in the predicted probabilities\" } ],",
    "finalAnswer": "The cross-entropy loss is 1\" },",
    "intuition": "Entropy measures the amount of information gained from observing a random variable. Cross-entropy measures the difference between our predictions and the true distribution.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:13:25.427Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]