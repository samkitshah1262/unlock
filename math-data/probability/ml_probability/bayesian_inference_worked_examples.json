[
  {
    "id": "prob_wex_bayesian_inference_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "problem": "{",
    "statementHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) and likelihood function <i>L</i>(<i>D</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>).",
    "hints": [
      "Consider Bayes' theorem"
    ],
    "solutionHtml": "<p>To solve this problem, we'll follow these steps:</p><ul><li>Step 1: Define the prior and likelihood functions.</li>\\[P(\\\\theta) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2)}\\]\\[L(D|\\\\theta) = \\\\prod_{i=1}^n \\\\mathcal{N}(d_i | \\\\theta, \\\\sigma^2)\\]</li><li>Step 2: Use Bayes' theorem to update the prior with the likelihood.</li>\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]</li><li>Step 3: Simplify the posterior distribution by combining like terms.</li>\\[P(\\\\theta|D) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2 + n\\\\sigma^2)}\\]</li><li>Step 4: Calculate the MAP estimate by finding the mode of the posterior distribution.</li>\\[\\\\hat{\\\\theta}_{MAP} = \\\\mu + \\\\frac{n}{n+1}\\\\sigma^2(\\\\theta - \\\\mu)\\]</li></ul>\",",
    "answerShort": "The posterior distribution is a Gaussian with mean <i>μ</i> and variance <i>σ^2 + n/σ^2</i>, and the MAP estimate is given by the formula above.\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a prior distribution <i>P(\\theta)</i> = <i>N(\\mu, \\sigma^2)</i> for the mean of a Gaussian distribution, and we observe data points <i>D</i> = {<i>d_1</i>, ..., <i>d_n</i>}.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior and likelihood functions.\", \"mathHtml\": \"\\\\[P(\\\\theta) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2)}\\]\\[L(D|\\\\theta) = \\\\prod_{i=1}^n \\\\mathcal{N}(d_i | \\\\theta, \\\\sigma^2)\\]\", \"explanation\": \"We start by defining the prior distribution and the likelihood function given the observed data.\"}, {\"stepNumber\": 2, \"description\": \"Use Bayes' theorem to update the prior with the likelihood.\", \"mathHtml\": \"\\\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]\", \"explanation\": \"We apply Bayes' theorem to update the prior distribution with the likelihood function.\"}, {\"stepNumber\": 3, \"description\": \"Simplify the posterior distribution by combining like terms.\", \"mathHtml\": \"\\\\[P(\\\\theta|D) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2 + n\\\\sigma^2)}\\]\", \"explanation\": \"We simplify the posterior distribution by combining like terms.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the MAP estimate by finding the mode of the posterior distribution.\", \"mathHtml\": \"\\\\[\\\\hat{\\\\theta}_{MAP} = \\\\mu + \\\\frac{n}{n+1}\\\\sigma^2(\\\\theta - \\\\mu)\\]\", \"explanation\": \"We calculate the MAP estimate by finding the mode of the posterior distribution.\"} ],",
    "finalAnswer": "The MAP estimate is given by the formula above.\" },",
    "intuition": "Bayesian inference provides a principled way to update our beliefs about a parameter <i>θ</i> based on new data <i>D</i>, while incorporating prior knowledge and uncertainty.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:50:58.163Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_016",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian inference, we update our knowledge about a parameter given new data using Bayes' theorem.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of heads, p. We flip the coin 10 times and get 7 heads. What is the updated probability distribution for p?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[p_0 \\sim \\text{Uniform}(0, 1)\\]\", \"explanation\": \"We assume a uniform prior since we have no information about the coin.\"}, {\"stepNumber\": 2, \"description\": \"Observe the data and calculate the likelihood\", \"mathHtml\": \"\\[L(p) = p^7 (1-p)^3\\]\", \"explanation\": \"The likelihood is the probability of observing 7 heads given a particular value of p.\"}, {\"stepNumber\": 3, \"description\": \"Update the prior using Bayes' theorem\", \"mathHtml\": \"\\[p|D \\propto L(p) p_0\\]\", \"explanation\": \"We multiply the likelihood by the prior and normalize to get the posterior.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the posterior distribution\", \"mathHtml\": \"\\[p|D \\sim \\text{Beta}(8, 4)\\]\", \"explanation\": \"The posterior is a beta distribution with parameters 8 and 4.\"}, {\"stepNumber\": 5, \"description\": \"Find the maximum a posteriori (MAP) estimate\", \"mathHtml\": \"\\[\\hat{p} = \\frac{8}{12}\\]\", \"explanation\": \"The MAP estimate is the most likely value of p given the data.\"} ],",
    "finalAnswer": "The updated probability distribution for p is Beta(8, 4). The MAP estimate is 2/3.\" },",
    "intuition": "Bayesian inference allows us to update our knowledge about a parameter based on new data. It's a powerful tool in machine learning and AI.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:51:29.328Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_017",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian inference, we update our knowledge about a parameter based on new data.</p>",
    "workedExample": "{",
    "problemHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) = 0.5<sup><i>θ</i></sup>, and a likelihood function <i>L</i>(<i>x</i>|<i>θ</i>) = (1 + <i>e</i><sup>-<i>x</i></sup>)<sup>-2</sup>, find the posterior distribution <i>P</i>(<i>θ</i>|<i>x</i>).",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Write down Bayes' theorem.\", \"mathHtml\": \"\\[ P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{\\int P(x|\\theta)P(\\theta)d\\theta} \\]\", \"explanation\": \"We're updating our prior knowledge about <i>θ</i> based on the new data <i>x</i>. Bayes' theorem gives us a formula to do this.\"}, {\"stepNumber\": 2, \"description\": \"Integrate out the likelihood function.\", \"mathHtml\": \"\\[ \\int P(x|\\theta)P(\\theta)d\\theta = \\int (1 + e^{-x})^{-2}0.5^{\\theta}d\\theta \\]\", \"explanation\": \"We're essentially normalizing our prior distribution by summing over all possible values of <i>θ</i>. This gives us a constant that we can use to update our prior.\"}, {\"stepNumber\": 3, \"description\": \"Multiply the likelihood function with the prior.\", \"mathHtml\": \"\\[ P(x|\\theta)P(\\theta) = (1 + e^{-x})^{-2}0.5^{\\theta} \\]\", \"explanation\": \"We're combining our new data <i>x</i> with our old knowledge about <i>θ</i>. This gives us the numerator of Bayes' theorem.\"}, {\"stepNumber\": 4\", \"description\": \"Divide by the normalization constant.\", \"mathHtml\": \"\\[ P(\\theta|x) = \\frac{(1 + e^{-x})^{-2}0.5^{\\theta}}{\\int (1 + e^{-x})^{-2}0.5^{\\theta}d\\theta} \\]\", \"explanation\": \"We're now updating our prior distribution based on the new data <i>x</i>. This gives us our final posterior distribution.\"}, ],",
    "finalAnswer": "The answer is a complex expression involving the prior, likelihood, and normalization constant.\" },",
    "intuition": "Bayesian inference allows us to update our knowledge about a parameter based on new data. It's like updating our mental model of the world as we learn more.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:52:09.641Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_018",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Solving a Problem Step-by-Step",
    "contentHtml": "<p>In this worked example, we'll apply Bayesian inference to solve a problem step-by-step.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of heads, and we flip it 5 times. The results are: H, T, H, H, T.",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[p(\\theta) = \\frac{1}{\\sqrt{10}} e^{-|\\theta - 0.5|^2/2}\\]\", \"explanation\": \"We assume a normal distribution with mean 0.5 and variance 0.25.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with the likelihood\", \"mathHtml\": \"\\[p(\\text{data}|\\theta) = \\prod_{i=1}^5 p(y_i|\\theta)\\]\", \"explanation\": \"We calculate the likelihood of observing each flip given the unknown probability.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the posterior distribution\", \"mathHtml\": \"\\[p(\\theta|\\text{data}) \\propto p(\\text{data}|\\theta) p(\\theta)\\]\", \"explanation\": \"We update the prior with the likelihood to get the posterior.\"}, {\"stepNumber\": 4, \"description\": \"Find the MAP estimate\", \"mathHtml\": \"\\[\\hat{\\theta} = \\arg\\max_{\\theta} p(\\theta|\\text{data})\\]\", \"explanation\": \"We find the most likely value of θ given the data.\"} ],",
    "finalAnswer": "The MAP estimate is approximately 0.42.\" },",
    "intuition": "Bayesian inference allows us to update our prior knowledge with new data, and it's a fundamental concept in machine learning.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:52:37.852Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_019",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Solving a Problem Step-by-Step",
    "contentHtml": "<p>In this worked example, we'll walk through solving a Bayesian inference problem using prior, likelihood, and posterior distributions.</p>",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of heads, $p$. We flip the coin 10 times and get 7 heads. What's our updated estimate of $p$?</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[ p_0 \\sim \\text{Beta}(a_0, b_0) \\]\", \"explanation\": \"We choose a conjugate prior that's easy to work with.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with new data\", \"mathHtml\": \"\\[ p | D \\propto p^{7+a_0} (1-p)^{3+b_0} \\]\", \"explanation\": \"We use Bayes' theorem to update our prior with the likelihood of observing 7 heads.\"}, {\"stepNumber\": 3, \"description\": \"Normalize the posterior\", \"mathHtml\": \"\\[ p | D = \\frac{p^{7+a_0} (1-p)^{3+b_0}}{\\int_0^1 p^{7+a_0} (1-p)^{3+b_0} dp} \\]\", \"explanation\": \"We need to normalize the posterior to ensure it integrates to 1.\"}, {\"stepNumber\": 4, \"description\": \"Find the maximum a posteriori (MAP) estimate\", \"mathHtml\": \"\\[ p_{\\text{MAP}} = \\arg\\max_p p | D \\]\", \"explanation\": \"We find the value of $p$ that maximizes our updated distribution.\"}, {\"stepNumber\": 5, \"description\": \"Calculate the MAP estimate numerically\", \"mathHtml\": \"Numerical computation yields...\", \"explanation\": \"We use numerical methods to find the exact value of the MAP estimate.\"} ],",
    "finalAnswer": "The estimated probability of heads is...\" },",
    "intuition": "Bayesian inference provides a powerful way to update our beliefs based on new data, allowing us to make more informed decisions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:53:09.570Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]