[
  {
    "id": "prob_con_bayesian_inference_001",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, and Posterior",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in probability theory that allows us to update our knowledge about a random variable based on new data.</p><p>Given a prior distribution over the possible values of the variable, we can combine it with the likelihood function (the probability of observing the data given the variable's value) using Bayes' theorem. This results in a posterior distribution that reflects our updated beliefs.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{\\int P(D | \\theta') P(\\theta') d\\theta'}\\]\",",
    "name": "Bayes' Theorem\" },",
    "intuition": "Think of Bayesian inference as updating your mental model of the world based on new observations. You start with a prior belief (your initial understanding), then incorporate new data to refine your thinking.",
    "realWorldApplications": [
      "In natural language processing, Bayesian inference is used in topic modeling and sentiment analysis"
    ],
    "commonMistakes": [
      "Failing to normalize the likelihood function",
      "Ignoring the importance of conjugate priors"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:45:23.859Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_bayesian_inference_002",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in probability theory that allows us to update our knowledge about a parameter given new data.</p><p>The process involves three key components:</p>",
    "formula": "{",
    "latex": "\\(P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\\)\",",
    "name": "Bayes' theorem\" },",
    "intuition": "Think of Bayesian inference as a process of updating our prior knowledge about a parameter based on new data. The likelihood function represents the probability of observing the data given the parameter, while the prior distribution represents our initial understanding of the parameter.",
    "realWorldApplications": [
      "Bayesian methods are widely used in machine learning for tasks such as natural language processing and computer vision"
    ],
    "commonMistakes": [
      "Failing to normalize the likelihood function",
      "Not considering the impact of prior distributions on the posterior"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:45:40.235Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_bayesian_inference_003",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in probability theory and machine learning. It's the process of updating our knowledge about an unknown parameter or event based on new data.</p><p>Imagine you're trying to determine the probability that someone will like a certain movie. You have some prior information, such as their past preferences, and then you get some new data, like their reaction after watching the movie. Bayesian inference helps you update your understanding of this person's taste in movies based on this new information.</p>",
    "formula": {
      "latex": "\\[ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\]",
      "name": "Bayes' theorem"
    },
    "intuition": "The key insight is that Bayesian inference allows us to incorporate prior knowledge and update it based on new data. This makes it a powerful tool for making predictions in uncertain environments.",
    "realWorldApplications": [
      "Recommendation systems",
      "Natural language processing"
    ],
    "commonMistakes": [
      "Failing to account for the prior distribution",
      "Ignoring the likelihood function"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:45:58.416Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_bayesian_inference_004",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in machine learning that allows us to update our knowledge about a probability distribution based on new data.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of heads, and we flip it 10 times. If the results are 7 heads and 3 tails, what is our updated estimate of the probability?</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[P(\\theta) = \\frac{1}{2}\\]\", \"explanation\": \"We assume a uniform prior.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with the likelihood\", \"mathHtml\": \"\\[P(X | \\theta) = \\binom{10}{7} \\theta^7 (1-\\theta)^3\\]\", \"explanation\": \"The likelihood is the probability of observing 7 heads and 3 tails given the true probability.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the posterior\", \"mathHtml\": \"\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\", \"explanation\": \"We apply Bayes' theorem to update our prior with the new data.\"} ],",
    "finalAnswer": "The updated estimate of the probability is approximately 0.7\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:46:25.400Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_bayesian_inference_005",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in machine learning that allows us to update our knowledge about a probability distribution based on new data.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{\\int P(X | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a prior distribution over the mean of a normal distribution, and we want to update this distribution based on some new data.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Specify the prior distribution\", \"mathHtml\": \"\\(P(\\mu) = \\mathcal{N}(\\mu | 0, 10^2)\\)\", \"explanation\": \"We choose a normal distribution with mean 0 and variance 100.\"} ],",
    "finalAnswer": "The updated posterior distribution\" },",
    "intuition": "Bayesian inference allows us to incorporate new data into our understanding of a probability distribution while preserving the uncertainty inherent in the data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:46:43.944Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_bayesian_inference_006",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in machine learning that allows us to update our beliefs about a model's parameters based on new data.</p><p>The key idea is to combine prior knowledge with likelihood from the observed data to obtain a posterior distribution over the model's parameters.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{\\int P(D | \\theta) P(\\theta) d\\theta}\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of heads, p. We flip the coin 10 times and get 7 heads.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution over p\", \"mathHtml\": \"\\[P(p) = \\frac{1}{2} I_{(0, 1)}\\]\", \"explanation\": \"We assume a uniform prior.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with the likelihood from the observed data\", \"mathHtml\": \"\\[P(D | p) = (p^7 (1-p)^3)\\]\", \"explanation\": \"The likelihood is the probability of observing 7 heads and 3 tails given the coin's probability.\"}, {\"stepNumber\": 3, \"description\": \"Compute the posterior distribution\", \"mathHtml\": \"\\[P(p | D) = \\frac{p^7 (1-p)^3}{\\int p^7 (1-p)^3 dp}\\]\", \"explanation\": \"We use Bayes' theorem to update our prior with the likelihood.\"} ],",
    "finalAnswer": "The posterior distribution over p is updated to reflect our new knowledge about the coin's probability.\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:47:10.726Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_for_bayesian_inference_007",
    "subject": "probability",
    "type": "formula",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference",
    "contentHtml": "<p>Bayesian inference is a fundamental concept in machine learning and statistics that allows us to update our knowledge about a probability distribution based on new data.</p>",
    "formula": "{",
    "latex": "\\[P(\\theta | X, y) = \\frac{P(X, y | \\theta) P(\\theta)}{P(X, y)}\\]\",",
    "name": "Bayes' theorem\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a prior distribution for the mean of a Gaussian distribution and we want to update it based on some new data.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the likelihood function\", \"mathHtml\": \"\\(P(X | \\theta) = \\prod_{i=1}^n N(x_i | \\mu, \\sigma)\\)\", \"explanation\": \"This represents our new data\"}, {\"stepNumber\": 2, \"description\": \"Update the prior distribution\", \"mathHtml\": \"\\(P(\\theta | X) \\propto P(X | \\theta) P(\\theta)\\)\", \"explanation\": \"Using Bayes' theorem\"} ],",
    "finalAnswer": "The updated posterior distribution\" },",
    "intuition": "Bayesian inference allows us to incorporate new data into our prior knowledge and update our understanding of the underlying probability distribution.",
    "realWorldApplications": [
      "Estimating parameters in Gaussian mixture models"
    ],
    "tags": [
      "bayes",
      "inference",
      "ml",
      "ai"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:47:34.134Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_bayesian_inference_008",
    "subject": "probability",
    "type": "theorem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayes' Theorem",
    "contentHtml": "<p>Bayes' theorem is a fundamental concept in Bayesian inference that allows us to update our knowledge about a probability distribution based on new evidence.</p>",
    "formula": {
      "latex": "\\[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\]",
      "name": "Bayes' Theorem"
    },
    "theorem": {
      "statement": "\\[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\]"
    },
    "intuition": "This theorem shows us how to update our prior knowledge about a probability distribution based on new evidence, represented by the likelihood. It's like updating our mental model of the world based on new data.",
    "realWorldApplications": [
      "Bayesian spam filtering",
      "Medical diagnosis"
    ],
    "tags": [
      "bayes",
      "inference",
      "probability"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:47:50.344Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_bayesian_inference_009",
    "subject": "probability",
    "type": "theorem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayes' Theorem",
    "contentHtml": "<p>Bayes' theorem is a fundamental concept in probability theory that allows us to update our beliefs about a random event based on new information.</p>",
    "formula": {
      "latex": "\\[ P(A | B) = \\frac{P(B | A) P(A)}{P(B)} \\]",
      "name": "Bayes' Theorem"
    },
    "theorem": {
      "statement": "\\[ P(A | B) = \\frac{P(B | A) P(A)}{P(B)} \\]"
    },
    "intuition": "This theorem provides a way to update our prior probability of an event given new evidence. It's like adjusting the weights in our mental scale based on new information.",
    "realWorldApplications": [
      "Bayesian inference is used in natural language processing, image recognition, and other AI applications."
    ],
    "tags": [
      "probability",
      "bayes"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:48:06.189Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_010",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "<p>Given a prior distribution <i>P</i>(<i>θ</i>) and a likelihood function <i>L</i>(<i>D</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>). Assume a conjugate prior.</p>",
    "hints": [
      "<p>The key to Bayesian inference is updating the prior with new data.</p>",
      "<p>Conjugate priors simplify the calculation of the posterior distribution.</p>",
      "<p>MAP estimation can be used when we're interested in the most likely value, rather than the full distribution.</p>"
    ],
    "solutionHtml": "<p>To find the posterior, we need to calculate the product of the prior and likelihood functions. Since our prior is conjugate, we can use Bayes' theorem:</p>\\n\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]\\n<p>We'll assume a normal-normal model for simplicity. The posterior distribution will also be normal.</p>\",",
    "answerShort": "<i>P</i>(<i>θ</i>|<i>D</i>) = ... (calculate the posterior mean and variance)\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:48:28.069Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_011",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": {
      "statementHtml": "Suppose we have a prior distribution <i>P</i>(<i>θ</i>) over a parameter <i>θ</i>, and we observe data <i>D</i>. We want to update our prior to obtain the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>).",
      "hints": [
        "Think about Bayes' rule.",
        "Recall that the likelihood is proportional to the product of the probability density functions (PDFs) of the observed data.",
        "The prior and likelihood are combined using the product rule."
      ],
      "solutionHtml": "<p>Using Bayes' rule, we have:</p><p><i>P</i>(<i>θ</i>|<i>D</i>) = <i>P</i>(<i>D</i>|<i>θ</i>) <i>P</i>(<i>θ</i>) / <i>P</i>(<i>D</i>)</p><p>We can simplify this expression by recognizing that the denominator is a constant:</p><p><i>P</i>(<i>θ</i>|<i>D</i>) = <i>P</i>(<i>D</i>|<i>θ</i>) <i>P</i>(<i>θ</i>)</p>",
      "answerShort": "The posterior distribution is proportional to the product of the likelihood and prior."
    },
    "commonMistakes": [
      "Forgetting that the denominator in Bayes' rule is a constant.",
      "Not recognizing that the prior and likelihood are combined using the product rule."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:48:51.300Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_012",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "Suppose we have a Bayesian network with variables X and Y, where <i>P(X)</i> is our prior distribution over X. If we observe data <i>d</i>, what is the updated posterior distribution <i>P(X | d)</i>?",
    "hints": [
      "Start by thinking about how to update the prior using Bayes' rule.",
      "Consider the conjugate prior for this problem, and how it simplifies the calculation.",
      "Don't forget to normalize the result!"
    ],
    "solutionHtml": "<p>To solve this problem, we can use Bayes' rule:</p>\\(\\frac{P(X | d) P(d)}{P(d)} = \\frac{P(X) P(d | X)}{\\int P(X) P(d | X) dx}\\)<br><p>Since our prior is conjugate to the likelihood, we can simplify this expression.</p>\",",
    "answerShort": "The posterior distribution <i>P(X | d)</i> is proportional to <i>P(X) P(d | X)</i>\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:49:10.056Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_013",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": {
      "statementHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) and a likelihood function <i>L</i>(<i>y</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>y</i>).",
      "hints": [
        "Start by applying Bayes' theorem.",
        "Recall that the prior and likelihood functions are conjugate if they share the same functional form.",
        "Use the fact that the posterior distribution is proportional to the product of the prior and likelihood."
      ],
      "solutionHtml": "<p>To find the posterior distribution, we can apply Bayes' theorem:</p><p><i>P</i>(<i>θ</i>|<i>y</i>) ∝ <i>P</i>(<i>θ</i>) × <i>L</i>(<i>y</i>|<i>θ</i>)</p><p>Since the prior and likelihood are conjugate, we can assume they both follow a normal distribution with mean <i>μ</i> and variance <i>σ</i>.</p><p>The posterior distribution will also be normal with mean:</p><p><i>μ<sub>posterior</sub></i> = (<i>μ</i> × <i>P</i>(<i>θ</i>)) + (<i>y</i> × <i>L</i>(<i>y</i>|<i>θ</i>))</p><p>and variance:</p><p><i>σ<sub>posterior</sub></i> = (1/<i>P</i>(<i>θ</i>)) + (<i>L</i>(<i>y</i>|<i>θ</i>))</p>",
      "answerShort": "The posterior distribution is proportional to the product of the prior and likelihood."
    },
    "commonMistakes": [
      "Forgetting that the prior and likelihood are conjugate.",
      "Not recognizing that the posterior distribution will also follow a normal distribution."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:49:39.219Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_014",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "<p>Given a Gaussian prior distribution with mean $\\mu$ and variance $\\sigma^2$, and a likelihood function $f(x|\\theta)$ parameterized by $\\theta$, find the posterior distribution using Bayes' theorem.</p>\",",
    "hints": [
      "Start by writing down Bayes' theorem",
      "Recognize that the prior is conjugate to the likelihood",
      "Use the properties of Gaussian distributions to simplify the expression"
    ],
    "solutionHtml": "<p>To find the posterior distribution, we can apply Bayes' theorem:</p>\\n\\[p(\\theta|x) = \\frac{f(x|\\theta) p(\\theta)}{\\int f(x|\\theta) p(\\theta) d\\theta}\\]\\n<p>Since the prior is conjugate to the likelihood, we know that the posterior will also be Gaussian with mean and variance:</p>\\n\\[m = \\frac{\\mu}{\\sigma^2 + 1}, \\quad s^2 = \\frac{1}{\\sigma^2 + 1}\\]\\n<p>Finally, we can write down the full posterior distribution:</p>\\n\\[p(\\theta|x) = N(m, s^2)\\]\",",
    "answerShort": "The posterior distribution is a Gaussian with mean $m$ and variance $s^2$\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:50:00.943Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_015",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "problem": "{",
    "statementHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) and likelihood function <i>L</i>(<i>D</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>).",
    "hints": [
      "Consider Bayes' theorem"
    ],
    "solutionHtml": "<p>To solve this problem, we'll follow these steps:</p><ul><li>Step 1: Define the prior and likelihood functions.</li>\\[P(\\\\theta) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2)}\\]\\[L(D|\\\\theta) = \\\\prod_{i=1}^n \\\\mathcal{N}(d_i | \\\\theta, \\\\sigma^2)\\]</li><li>Step 2: Use Bayes' theorem to update the prior with the likelihood.</li>\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]</li><li>Step 3: Simplify the posterior distribution by combining like terms.</li>\\[P(\\\\theta|D) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2 + n\\\\sigma^2)}\\]</li><li>Step 4: Calculate the MAP estimate by finding the mode of the posterior distribution.</li>\\[\\\\hat{\\\\theta}_{MAP} = \\\\mu + \\\\frac{n}{n+1}\\\\sigma^2(\\\\theta - \\\\mu)\\]</li></ul>\",",
    "answerShort": "The posterior distribution is a Gaussian with mean <i>μ</i> and variance <i>σ^2 + n/σ^2</i>, and the MAP estimate is given by the formula above.\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a prior distribution <i>P(\\theta)</i> = <i>N(\\mu, \\sigma^2)</i> for the mean of a Gaussian distribution, and we observe data points <i>D</i> = {<i>d_1</i>, ..., <i>d_n</i>}.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior and likelihood functions.\", \"mathHtml\": \"\\\\[P(\\\\theta) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2)}\\]\\[L(D|\\\\theta) = \\\\prod_{i=1}^n \\\\mathcal{N}(d_i | \\\\theta, \\\\sigma^2)\\]\", \"explanation\": \"We start by defining the prior distribution and the likelihood function given the observed data.\"}, {\"stepNumber\": 2, \"description\": \"Use Bayes' theorem to update the prior with the likelihood.\", \"mathHtml\": \"\\\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]\", \"explanation\": \"We apply Bayes' theorem to update the prior distribution with the likelihood function.\"}, {\"stepNumber\": 3, \"description\": \"Simplify the posterior distribution by combining like terms.\", \"mathHtml\": \"\\\\[P(\\\\theta|D) = \\\\frac{1}{Z} e^{-(\\\\theta - \\\\mu)^2 / (2\\\\sigma^2 + n\\\\sigma^2)}\\]\", \"explanation\": \"We simplify the posterior distribution by combining like terms.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the MAP estimate by finding the mode of the posterior distribution.\", \"mathHtml\": \"\\\\[\\\\hat{\\\\theta}_{MAP} = \\\\mu + \\\\frac{n}{n+1}\\\\sigma^2(\\\\theta - \\\\mu)\\]\", \"explanation\": \"We calculate the MAP estimate by finding the mode of the posterior distribution.\"} ],",
    "finalAnswer": "The MAP estimate is given by the formula above.\" },",
    "intuition": "Bayesian inference provides a principled way to update our beliefs about a parameter <i>θ</i> based on new data <i>D</i>, while incorporating prior knowledge and uncertainty.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:50:58.163Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_016",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian inference, we update our knowledge about a parameter given new data using Bayes' theorem.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of heads, p. We flip the coin 10 times and get 7 heads. What is the updated probability distribution for p?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[p_0 \\sim \\text{Uniform}(0, 1)\\]\", \"explanation\": \"We assume a uniform prior since we have no information about the coin.\"}, {\"stepNumber\": 2, \"description\": \"Observe the data and calculate the likelihood\", \"mathHtml\": \"\\[L(p) = p^7 (1-p)^3\\]\", \"explanation\": \"The likelihood is the probability of observing 7 heads given a particular value of p.\"}, {\"stepNumber\": 3, \"description\": \"Update the prior using Bayes' theorem\", \"mathHtml\": \"\\[p|D \\propto L(p) p_0\\]\", \"explanation\": \"We multiply the likelihood by the prior and normalize to get the posterior.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the posterior distribution\", \"mathHtml\": \"\\[p|D \\sim \\text{Beta}(8, 4)\\]\", \"explanation\": \"The posterior is a beta distribution with parameters 8 and 4.\"}, {\"stepNumber\": 5, \"description\": \"Find the maximum a posteriori (MAP) estimate\", \"mathHtml\": \"\\[\\hat{p} = \\frac{8}{12}\\]\", \"explanation\": \"The MAP estimate is the most likely value of p given the data.\"} ],",
    "finalAnswer": "The updated probability distribution for p is Beta(8, 4). The MAP estimate is 2/3.\" },",
    "intuition": "Bayesian inference allows us to update our knowledge about a parameter based on new data. It's a powerful tool in machine learning and AI.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:51:29.328Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_017",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Prior, Likelihood, Posterior",
    "contentHtml": "<p>In Bayesian inference, we update our knowledge about a parameter based on new data.</p>",
    "workedExample": "{",
    "problemHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) = 0.5<sup><i>θ</i></sup>, and a likelihood function <i>L</i>(<i>x</i>|<i>θ</i>) = (1 + <i>e</i><sup>-<i>x</i></sup>)<sup>-2</sup>, find the posterior distribution <i>P</i>(<i>θ</i>|<i>x</i>).",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Write down Bayes' theorem.\", \"mathHtml\": \"\\[ P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{\\int P(x|\\theta)P(\\theta)d\\theta} \\]\", \"explanation\": \"We're updating our prior knowledge about <i>θ</i> based on the new data <i>x</i>. Bayes' theorem gives us a formula to do this.\"}, {\"stepNumber\": 2, \"description\": \"Integrate out the likelihood function.\", \"mathHtml\": \"\\[ \\int P(x|\\theta)P(\\theta)d\\theta = \\int (1 + e^{-x})^{-2}0.5^{\\theta}d\\theta \\]\", \"explanation\": \"We're essentially normalizing our prior distribution by summing over all possible values of <i>θ</i>. This gives us a constant that we can use to update our prior.\"}, {\"stepNumber\": 3, \"description\": \"Multiply the likelihood function with the prior.\", \"mathHtml\": \"\\[ P(x|\\theta)P(\\theta) = (1 + e^{-x})^{-2}0.5^{\\theta} \\]\", \"explanation\": \"We're combining our new data <i>x</i> with our old knowledge about <i>θ</i>. This gives us the numerator of Bayes' theorem.\"}, {\"stepNumber\": 4\", \"description\": \"Divide by the normalization constant.\", \"mathHtml\": \"\\[ P(\\theta|x) = \\frac{(1 + e^{-x})^{-2}0.5^{\\theta}}{\\int (1 + e^{-x})^{-2}0.5^{\\theta}d\\theta} \\]\", \"explanation\": \"We're now updating our prior distribution based on the new data <i>x</i>. This gives us our final posterior distribution.\"}, ],",
    "finalAnswer": "The answer is a complex expression involving the prior, likelihood, and normalization constant.\" },",
    "intuition": "Bayesian inference allows us to update our knowledge about a parameter based on new data. It's like updating our mental model of the world as we learn more.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:52:09.641Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_018",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Solving a Problem Step-by-Step",
    "contentHtml": "<p>In this worked example, we'll apply Bayesian inference to solve a problem step-by-step.</p>",
    "workedExample": "{",
    "problemHtml": "Suppose we have a coin with an unknown probability of heads, and we flip it 5 times. The results are: H, T, H, H, T.",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[p(\\theta) = \\frac{1}{\\sqrt{10}} e^{-|\\theta - 0.5|^2/2}\\]\", \"explanation\": \"We assume a normal distribution with mean 0.5 and variance 0.25.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with the likelihood\", \"mathHtml\": \"\\[p(\\text{data}|\\theta) = \\prod_{i=1}^5 p(y_i|\\theta)\\]\", \"explanation\": \"We calculate the likelihood of observing each flip given the unknown probability.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the posterior distribution\", \"mathHtml\": \"\\[p(\\theta|\\text{data}) \\propto p(\\text{data}|\\theta) p(\\theta)\\]\", \"explanation\": \"We update the prior with the likelihood to get the posterior.\"}, {\"stepNumber\": 4, \"description\": \"Find the MAP estimate\", \"mathHtml\": \"\\[\\hat{\\theta} = \\arg\\max_{\\theta} p(\\theta|\\text{data})\\]\", \"explanation\": \"We find the most likely value of θ given the data.\"} ],",
    "finalAnswer": "The MAP estimate is approximately 0.42.\" },",
    "intuition": "Bayesian inference allows us to update our prior knowledge with new data, and it's a fundamental concept in machine learning.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:52:37.852Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_wex_bayesian_inference_019",
    "subject": "probability",
    "type": "worked_example",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "title": "Bayesian Inference: Solving a Problem Step-by-Step",
    "contentHtml": "<p>In this worked example, we'll walk through solving a Bayesian inference problem using prior, likelihood, and posterior distributions.</p>",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a coin with an unknown probability of heads, $p$. We flip the coin 10 times and get 7 heads. What's our updated estimate of $p$?</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the prior distribution\", \"mathHtml\": \"\\[ p_0 \\sim \\text{Beta}(a_0, b_0) \\]\", \"explanation\": \"We choose a conjugate prior that's easy to work with.\"}, {\"stepNumber\": 2, \"description\": \"Update the prior with new data\", \"mathHtml\": \"\\[ p | D \\propto p^{7+a_0} (1-p)^{3+b_0} \\]\", \"explanation\": \"We use Bayes' theorem to update our prior with the likelihood of observing 7 heads.\"}, {\"stepNumber\": 3, \"description\": \"Normalize the posterior\", \"mathHtml\": \"\\[ p | D = \\frac{p^{7+a_0} (1-p)^{3+b_0}}{\\int_0^1 p^{7+a_0} (1-p)^{3+b_0} dp} \\]\", \"explanation\": \"We need to normalize the posterior to ensure it integrates to 1.\"}, {\"stepNumber\": 4, \"description\": \"Find the maximum a posteriori (MAP) estimate\", \"mathHtml\": \"\\[ p_{\\text{MAP}} = \\arg\\max_p p | D \\]\", \"explanation\": \"We find the value of $p$ that maximizes our updated distribution.\"}, {\"stepNumber\": 5, \"description\": \"Calculate the MAP estimate numerically\", \"mathHtml\": \"Numerical computation yields...\", \"explanation\": \"We use numerical methods to find the exact value of the MAP estimate.\"} ],",
    "finalAnswer": "The estimated probability of heads is...\" },",
    "intuition": "Bayesian inference provides a powerful way to update our beliefs based on new data, allowing us to make more informed decisions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:53:09.570Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]