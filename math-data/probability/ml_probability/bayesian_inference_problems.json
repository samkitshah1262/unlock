[
  {
    "id": "prob_prb_bayesian_inference_010",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "<p>Given a prior distribution <i>P</i>(<i>θ</i>) and a likelihood function <i>L</i>(<i>D</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>). Assume a conjugate prior.</p>",
    "hints": [
      "<p>The key to Bayesian inference is updating the prior with new data.</p>",
      "<p>Conjugate priors simplify the calculation of the posterior distribution.</p>",
      "<p>MAP estimation can be used when we're interested in the most likely value, rather than the full distribution.</p>"
    ],
    "solutionHtml": "<p>To find the posterior, we need to calculate the product of the prior and likelihood functions. Since our prior is conjugate, we can use Bayes' theorem:</p>\\n\\[P(\\\\theta|D) = \\\\frac{P(D|\\\\theta) P(\\\\theta)}{P(D)}\\]\\n<p>We'll assume a normal-normal model for simplicity. The posterior distribution will also be normal.</p>\",",
    "answerShort": "<i>P</i>(<i>θ</i>|<i>D</i>) = ... (calculate the posterior mean and variance)\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:48:28.069Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_011",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": {
      "statementHtml": "Suppose we have a prior distribution <i>P</i>(<i>θ</i>) over a parameter <i>θ</i>, and we observe data <i>D</i>. We want to update our prior to obtain the posterior distribution <i>P</i>(<i>θ</i>|<i>D</i>).",
      "hints": [
        "Think about Bayes' rule.",
        "Recall that the likelihood is proportional to the product of the probability density functions (PDFs) of the observed data.",
        "The prior and likelihood are combined using the product rule."
      ],
      "solutionHtml": "<p>Using Bayes' rule, we have:</p><p><i>P</i>(<i>θ</i>|<i>D</i>) = <i>P</i>(<i>D</i>|<i>θ</i>) <i>P</i>(<i>θ</i>) / <i>P</i>(<i>D</i>)</p><p>We can simplify this expression by recognizing that the denominator is a constant:</p><p><i>P</i>(<i>θ</i>|<i>D</i>) = <i>P</i>(<i>D</i>|<i>θ</i>) <i>P</i>(<i>θ</i>)</p>",
      "answerShort": "The posterior distribution is proportional to the product of the likelihood and prior."
    },
    "commonMistakes": [
      "Forgetting that the denominator in Bayes' rule is a constant.",
      "Not recognizing that the prior and likelihood are combined using the product rule."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:48:51.300Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_012",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "Suppose we have a Bayesian network with variables X and Y, where <i>P(X)</i> is our prior distribution over X. If we observe data <i>d</i>, what is the updated posterior distribution <i>P(X | d)</i>?",
    "hints": [
      "Start by thinking about how to update the prior using Bayes' rule.",
      "Consider the conjugate prior for this problem, and how it simplifies the calculation.",
      "Don't forget to normalize the result!"
    ],
    "solutionHtml": "<p>To solve this problem, we can use Bayes' rule:</p>\\(\\frac{P(X | d) P(d)}{P(d)} = \\frac{P(X) P(d | X)}{\\int P(X) P(d | X) dx}\\)<br><p>Since our prior is conjugate to the likelihood, we can simplify this expression.</p>\",",
    "answerShort": "The posterior distribution <i>P(X | d)</i> is proportional to <i>P(X) P(d | X)</i>\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:49:10.056Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_013",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": {
      "statementHtml": "Given a prior distribution <i>P</i>(<i>θ</i>) and a likelihood function <i>L</i>(<i>y</i>|<i>θ</i>), find the posterior distribution <i>P</i>(<i>θ</i>|<i>y</i>).",
      "hints": [
        "Start by applying Bayes' theorem.",
        "Recall that the prior and likelihood functions are conjugate if they share the same functional form.",
        "Use the fact that the posterior distribution is proportional to the product of the prior and likelihood."
      ],
      "solutionHtml": "<p>To find the posterior distribution, we can apply Bayes' theorem:</p><p><i>P</i>(<i>θ</i>|<i>y</i>) ∝ <i>P</i>(<i>θ</i>) × <i>L</i>(<i>y</i>|<i>θ</i>)</p><p>Since the prior and likelihood are conjugate, we can assume they both follow a normal distribution with mean <i>μ</i> and variance <i>σ</i>.</p><p>The posterior distribution will also be normal with mean:</p><p><i>μ<sub>posterior</sub></i> = (<i>μ</i> × <i>P</i>(<i>θ</i>)) + (<i>y</i> × <i>L</i>(<i>y</i>|<i>θ</i>))</p><p>and variance:</p><p><i>σ<sub>posterior</sub></i> = (1/<i>P</i>(<i>θ</i>)) + (<i>L</i>(<i>y</i>|<i>θ</i>))</p>",
      "answerShort": "The posterior distribution is proportional to the product of the prior and likelihood."
    },
    "commonMistakes": [
      "Forgetting that the prior and likelihood are conjugate.",
      "Not recognizing that the posterior distribution will also follow a normal distribution."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:49:39.219Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_prb_bayesian_inference_014",
    "subject": "probability",
    "type": "problem",
    "chapter": "ml_probability",
    "topic": "bayesian_inference",
    "problem": "{",
    "statementHtml": "<p>Given a Gaussian prior distribution with mean $\\mu$ and variance $\\sigma^2$, and a likelihood function $f(x|\\theta)$ parameterized by $\\theta$, find the posterior distribution using Bayes' theorem.</p>\",",
    "hints": [
      "Start by writing down Bayes' theorem",
      "Recognize that the prior is conjugate to the likelihood",
      "Use the properties of Gaussian distributions to simplify the expression"
    ],
    "solutionHtml": "<p>To find the posterior distribution, we can apply Bayes' theorem:</p>\\n\\[p(\\theta|x) = \\frac{f(x|\\theta) p(\\theta)}{\\int f(x|\\theta) p(\\theta) d\\theta}\\]\\n<p>Since the prior is conjugate to the likelihood, we know that the posterior will also be Gaussian with mean and variance:</p>\\n\\[m = \\frac{\\mu}{\\sigma^2 + 1}, \\quad s^2 = \\frac{1}{\\sigma^2 + 1}\\]\\n<p>Finally, we can write down the full posterior distribution:</p>\\n\\[p(\\theta|x) = N(m, s^2)\\]\",",
    "answerShort": "The posterior distribution is a Gaussian with mean $m$ and variance $s^2$\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:50:00.943Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]