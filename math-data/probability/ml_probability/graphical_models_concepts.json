[
  {
    "id": "prob_con_graphical_models_001",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "graphical_models",
    "title": "Probabilistic Graphical Models",
    "subtitle": "A fundamental concept in Bayesian networks and machine learning",
    "contentHtml": "<p>Probabilistic graphical models (PGMs) are a powerful tool for representing complex probabilistic relationships between variables.</p><p>In this context, 'graphical' refers to the use of directed acyclic graphs (DAGs) to encode conditional dependencies between random variables.</p>",
    "formula": {
      "latex": "\\[P(X|Y) = \\frac{P(Y|X) P(X)}{P(Y)}\\]",
      "name": "Bayes' theorem"
    },
    "intuition": "PGMs provide a compact and interpretable way to model complex probabilistic relationships, making them essential in many machine learning applications.",
    "visualDescription": "A DAG with nodes representing random variables and directed edges indicating conditional dependencies",
    "commonMistakes": [
      "Confusing PGMs with Bayesian networks",
      "Ignoring the importance of conditional independence"
    ],
    "realWorldApplications": [
      "Bayesian inference in natural language processing",
      "Structural learning in computer vision"
    ],
    "tags": [
      "PGMs",
      "Bayesian networks",
      "Machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:59:36.352Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_graphical_models_002",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "graphical_models",
    "title": "Probabilistic Graphical Models",
    "contentHtml": "<p>Probabilistic graphical models (PGMs) are a powerful tool for representing complex probability distributions in machine learning and artificial intelligence. They consist of nodes and edges that represent variables and their conditional dependencies.</p><p>In this concept, we'll explore the basics of PGMs, including Bayesian networks and factor graphs, as well as the concept of d-separation.</p>",
    "formula": {
      "latex": "\\[P(X | Y) = \\frac{P(Y | X) P(X)}{P(Y)}\\]",
      "name": "Bayes' theorem"
    },
    "intuition": "PGMs provide a compact and interpretable way to represent complex probability distributions, making them essential in many machine learning applications.",
    "visualDescription": "A diagram showing a Bayesian network with nodes representing variables and edges representing conditional dependencies",
    "commonMistakes": [
      "Confusing PGMs with traditional graphical models"
    ],
    "realWorldApplications": [
      "Bayesian networks for natural language processing",
      "Factor graphs for computer vision"
    ],
    "tags": [
      "probabilistic graphical models",
      "PGMs",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T02:59:54.488Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_con_graphical_models_003",
    "subject": "probability",
    "type": "concept",
    "chapter": "ml_probability",
    "topic": "graphical_models",
    "title": "Probabilistic Graphical Models",
    "contentHtml": "<p>Probabilistic graphical models (PGMs) are a powerful tool in machine learning to represent complex probability distributions. They consist of nodes and edges that encode conditional dependencies between variables.</p><p>In PGMs, each node represents a random variable, and the edges represent probabilistic relationships between these variables.</p>",
    "formula": {
      "latex": "\\[P(X|Y) = \\frac{P(Y|X)P(X)}{P(Y)}\\]",
      "name": "Bayes' theorem"
    },
    "intuition": "PGMs allow us to compactly represent complex probability distributions by capturing conditional dependencies between variables.",
    "visualDescription": "A diagram showing nodes and edges representing probabilistic relationships between random variables",
    "commonMistakes": [
      "Not considering the direction of edges",
      "Ignoring edge weights"
    ],
    "realWorldApplications": [
      "Bayesian networks for decision-making",
      "Factor graphs for computer vision"
    ],
    "tags": [
      "machine learning",
      "probability theory"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T03:00:11.236Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]