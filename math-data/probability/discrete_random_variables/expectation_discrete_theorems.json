[
  {
    "id": "prob_thm_expectation_discrete_008",
    "subject": "probability",
    "type": "theorem",
    "chapter": "discrete_random_variables",
    "topic": "expectation_discrete",
    "title": "Linearity of Expectation",
    "contentHtml": "<p>The expectation of a discrete random variable is linear in the sense that it can be represented as a weighted sum of individual outcomes.</p>",
    "formula": {
      "latex": "\\[\\mathbb{E}[X] = \\sum_{x} xP(X=x)\\]",
      "name": "Linear Expectation"
    },
    "theorem": {
      "statement": "\\[\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b\\]"
    },
    "intuition": "The linearity of expectation allows us to break down complex random variables into simpler components and calculate their expectations independently.",
    "realWorldApplications": [
      "In machine learning, the linearity of expectation is crucial for understanding the behavior of neural networks and other models."
    ],
    "tags": [
      "expectation",
      "linearity"
    ],
    "estimatedMinutes": 2,
    "difficulty": 2,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T22:45:55.051Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_expectation_discrete_009",
    "subject": "probability",
    "type": "theorem",
    "chapter": "discrete_random_variables",
    "topic": "expectation_discrete",
    "title": "Linearity of Expectation",
    "contentHtml": "<p>The linearity of expectation states that the expected value of a weighted sum of random variables is equal to the weighted sum of their individual expected values.</p>",
    "formula": {
      "latex": "\\[\\mathbb{E}[aX + bY] = a \\mathbb{E}[X] + b \\mathbb{E}[Y]\\]",
      "name": "Linearity of Expectation"
    },
    "theorem": {
      "statement": "\\[\\mathbb{E}[aX + bY] = a \\mathbb{E}[X] + b \\mathbb{E}[Y]\\]"
    },
    "intuition": "This theorem is crucial in probability theory as it allows us to break down complex expectations into simpler components. It has numerous applications in machine learning, such as calculating the expected loss of a model.",
    "realWorldApplications": [
      "Calculating the expected accuracy of a classification model"
    ],
    "tags": [
      "Expectation",
      "Linearity",
      "Probability Theory"
    ],
    "estimatedMinutes": 2,
    "difficulty": 2,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T22:46:11.598Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]