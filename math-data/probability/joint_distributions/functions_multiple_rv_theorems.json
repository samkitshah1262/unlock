[
  {
    "id": "prob_thm_functions_multiple_rv_007",
    "subject": "probability",
    "type": "theorem",
    "chapter": "joint_distributions",
    "topic": "functions_multiple_rv",
    "title": "Functions of Multiple RVs",
    "contentHtml": "<p>In probability theory, we often encounter situations where we need to work with functions of multiple random variables (RVs). This theorem provides a powerful tool for doing so.</p>",
    "formula": {
      "latex": "\\[f(X_1, X_2) = \\sum_{i=1}^n f_i(x_{i,1}, x_{i,2})\\]",
      "name": "Sum of Independent RVs"
    },
    "theorem": {
      "statement": "\\[f(X_1, X_2) = \\sum_{i=1}^n f_i(X_{1,i}, X_{2,i})\\] where {X_{1,i}, X_{2,i}} are independent RVs",
      "proofSketch": "The proof involves showing that the expected value of the function is equal to the sum of the expected values of each component function."
    },
    "intuition": "This theorem allows us to decompose a complex function into simpler components, making it easier to work with and analyze.",
    "realWorldApplications": [
      "In machine learning, this theorem has applications in modeling complex systems and combining multiple features."
    ],
    "tags": [
      "joint distributions",
      "independent RVs"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "important",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T01:31:55.914Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "prob_thm_functions_multiple_rv_008",
    "subject": "probability",
    "type": "theorem",
    "chapter": "joint_distributions",
    "topic": "functions_multiple_rv",
    "title": "Sum and Difference of Independent RVs",
    "contentHtml": "<p>The sum and difference of independent random variables (RVs) is a fundamental concept in probability theory.</p>",
    "formula": "{",
    "latex": "\\[ X + Y = \\sum_{i=1}^n x_i + \\sum_{j=1}^m y_j \\]",
    "name": "\" },",
    "theorem": "{",
    "statement": "\\\\[ P(X+Y \\leq z) = \\\\int_{-\\\\infty}^{z} f_X(x) f_Y(z-x) dx \\\\]\",",
    "proofSketch": "The proof involves applying the definition of a joint distribution and integrating out the variables.\" },",
    "intuition": "Intuitively, when we sum or difference independent RVs, each term is drawn from its respective distribution. The resulting distribution is a convolution of the individual distributions.",
    "realWorldApplications": [
      "In machine learning, this concept is used in modeling complex systems where multiple components interact."
    ],
    "tags": [
      "joint distributions",
      "independent RVs"
    ],
    "difficulty": 3,
    "mlRelevance": "important",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-27T01:32:12.998Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]