[
  {
    "id": "la_for_vector_projections_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "vectors_foundations",
    "topic": "vector_projections",
    "title": "Vector Projections",
    "contentHtml": "<p>Vector projections are a fundamental concept in linear algebra, allowing us to decompose vectors into their orthogonal and parallel components.</p>",
    "formula": {
      "latex": "\\[ \\text{proj}_\\mathbf{a} \\mathbf{b} = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{||\\mathbf{a}||^2} \\mathbf{a} \\]",
      "name": "Scalar Projection"
    },
    "workedExample": {
      "problemHtml": "<p>Find the scalar projection of vector \\mathbf{b} onto vector \\mathbf{a}</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the dot product",
          "mathHtml": "\\[ \\mathbf{a}\\cdot\\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots \\]",
          "explanation": "This gives us the magnitude of the projection"
        },
        {
          "stepNumber": 2,
          "description": "Calculate the squared norm of vector \\mathbf{a}",
          "mathHtml": "\\[ ||\\mathbf{a}||^2 = a_1^2 + a_2^2 + \\cdots \\]",
          "explanation": "This gives us the normalization factor"
        }
      ],
      "finalAnswer": "The scalar projection is then given by the dot product divided by the squared norm"
    },
    "intuition": "Vector projections help us understand how vectors can be decomposed into their orthogonal and parallel components, which has many applications in machine learning and computer vision.",
    "estimatedMinutes": 2,
    "difficulty": 2,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T09:28:45.989Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_vector_projections_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "vectors_foundations",
    "topic": "vector_projections",
    "title": "Vector Projections",
    "contentHtml": "<p>Vector projections are a fundamental concept in linear algebra, allowing us to find the component of a vector that lies along another direction.</p>",
    "formula": "{",
    "latex": "\\[\\mathbf{P}_{\\mathbf{v}} \\mathbf{u} = (\\mathbf{v} \\cdot \\mathbf{\\hat{v}}) \\mathbf{\\hat{v}}\\]\",",
    "name": "Vector Projection Formula\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the projection of vector <code>\\mathbf{u} = (2,3)</code> onto the direction <code>\\mathbf{v} = (1,0)</code>.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the magnitude of the direction\", \"mathHtml\": \"\\\\[|\\mathbf{\\hat{v}}| = \\\\sqrt{(1)^2 + (0)^2} = 1\\\\]\", \"explanation\": \"We normalize the direction vector to get a unit vector.\"}, {\"stepNumber\": 2, \"description\": \"Compute the dot product\", \"mathHtml\": \"\\\\[ (\\mathbf{u} \\cdot \\mathbf{\\hat{v}}) = (2)(1) + (3)(0) = 2\\\\]\", \"explanation\": \"This gives us the component of <code>\\mathbf{u}</code> that lies along <code>\\mathbf{v}</code>. \"} ],",
    "finalAnswer": "\\\\[ \\mathbf{P}_{\\mathbf{v}} \\mathbf{u} = (2)(1) = 2\\\\]\" },",
    "intuition": "Vector projections help us isolate the part of a vector that aligns with a specific direction, which is crucial in many machine learning and computer vision applications.",
    "difficulty": 2,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T09:29:09.538Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_vector_projections_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "vectors_foundations",
    "topic": "vector_projections",
    "title": "Vector Projections",
    "contentHtml": "<p>Vector projections are a fundamental concept in linear algebra and have numerous applications in machine learning and artificial intelligence.</p><p>In this formula card, we'll explore scalar and vector projections, projection onto lines and planes.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{proj}_{\\mathbf{v}}(\\mathbf{w}) = \\frac{\\mathbf{w} \\cdot \\mathbf{v}}{\\| \\mathbf{v} \\|^{2}} \\mathbf{v} \\]\",",
    "name": "Vector Projection Formula",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Find the projection of vector <math>\\mathbf{x}</math> onto vector <math>\\mathbf{y}</math>.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the dot product\", \"mathHtml\": \"\\[ \\mathbf{x} \\cdot \\mathbf{y} = x_{1} y_{1} + x_{2} y_{2} + ... + x_{n} y_{n} \\]\", \"explanation\": \"The dot product is a scalar value that represents the amount of 'similarity' between two vectors.\"} ],",
    "finalAnswer": "\" },",
    "intuition": "Vector projections help us find the component of one vector in the direction of another. This has numerous applications in machine learning, such as finding the direction of maximum variance or projecting high-dimensional data onto a lower-dimensional space.",
    "difficulty": 2,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T09:29:31.768Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]