[
  {
    "id": "la_con_pca_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a set of correlated variables into a new set of uncorrelated variables called principal components.</p><p>The goal of PCA is to retain the most important information in the data while reducing its complexity by projecting it onto a lower-dimensional space.</p>",
    "formula": {
      "latex": "\\[ \\text{Variance Explained} = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sigma^2} \\]",
      "name": "Variance Explained"
    },
    "intuition": "PCA helps us identify the most important features in our data by capturing the directions of maximum variance. This is useful in machine learning, as it can help reduce overfitting and improve model interpretability.",
    "visualDescription": "A scatter plot showing the original data points with their principal components would be helpful to visualize the concept.",
    "commonMistakes": [
      "Assuming PCA only works for high-dimensional spaces",
      "Thinking that PCA is a feature selection method"
    ],
    "realWorldApplications": [
      "Reducing noise in image datasets",
      "Identifying key factors in financial modeling"
    ],
    "tags": [
      "pca",
      "dimensionality reduction"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:08.656Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_pca_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining most of the original information.</p><p>This is achieved by finding the directions of maximum variance in the data, known as principal components, and projecting the data onto these axes.</p>",
    "formula": {
      "latex": "\\[\\text{Variance explained} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]",
      "name": "Variance explanation"
    },
    "intuition": "Think of PCA as a way to compress your data while keeping the important features intact. Imagine you're trying to summarize a long article by highlighting the main points – that's what PCA does for your data.",
    "realWorldApplications": [
      "Dimensionality reduction in image recognition",
      "Feature extraction in text analysis"
    ],
    "commonMistakes": [
      "Thinking PCA is just a simple linear transformation",
      "Not understanding the importance of variance in the selection of principal components"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:25.511Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_pca_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original data into a new set of features, called principal components, which capture most of the variability in the data.</p><p>The goal of PCA is to reduce the number of features while retaining as much information as possible. This is achieved by finding the directions of maximum variance in the data and projecting the data onto these directions.</p>",
    "formula": {
      "latex": "\\[\\mathbf{W} = \\argmax_{\\mathbf{W}} \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})^T \\mathbf{W} \\mathbf{W}^T (\\mathbf{x}_i - \\bar{\\mathbf{x}})\\]",
      "name": "PCA objective function"
    },
    "intuition": "Think of PCA as a way to compress the data into a smaller set of features that capture most of the variation. This is useful in machine learning, where high-dimensional data can be difficult to work with.",
    "realWorldApplications": [
      "Dimensionality reduction for image and text data",
      "Feature extraction for recommender systems"
    ],
    "commonMistakes": [
      "Not understanding the difference between PCA and other dimensionality reduction techniques",
      "Not normalizing the data before applying PCA"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:44.904Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p><p>Given a dataset with high-dimensional features, PCA aims to find the most important directions of variation and project the data onto those directions.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w}_k = \\arg\\max_{\\mathbf{w}} \\frac{\\left| \\mathbf{w}^T \\mathbf{X} \\right|^2}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "Principal Component\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images with 1000 features each. We want to reduce the dimensionality to 50 while retaining most of the information.</p>",
    "steps": "[ {",
    "stepNumber": 3,
    "description": "Project the data onto the principal components",
    "mathHtml": "\\[ \\mathbf{X}_\\text{reduced} = \\mathbf{W}^T \\mathbf{X} \\]\",",
    "explanation": "We project the original data onto the reduced dimensionality.\" } ],",
    "finalAnswer": "The resulting 50-dimensional representation captures most of the information in the original dataset.\" },",
    "intuition": "PCA helps us identify the most important features and reduce the noise in our data.",
    "realWorldApplications": [
      "Image compression",
      "Anomaly detection"
    ],
    "tags": [
      "dimensionality reduction",
      "feature extraction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:15.539Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>PCA is a widely used dimensionality reduction technique in machine learning.</p><p>It helps to identify the most important features in your data and reduce its dimensionality while retaining most of the information.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w}_k = \\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\Sigma \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "PCA formula\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 10 features, reduce its dimensionality to 3 using PCA.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the covariance matrix\", \"mathHtml\": \"\\\\[ \\Sigma = \\\\frac{1}{n-1} X^T X \\\\]\", \"explanation\": \"The covariance matrix helps us understand how features are correlated.\"} ],",
    "finalAnswer": "3 principal components\" },",
    "intuition": "PCA finds the directions of maximum variance in your data and projects it onto those directions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:34.260Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining most of the original information.</p>",
    "formula": "{",
    "latex": "\\[ W = \\sigma^{-1} U^T Σ^2 U\\]\",",
    "name": "Principal Component Analysis\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 100 features, reduce it to 3 principal components.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\[ Σ = \\frac{1}{n-1} X^T X\\]\", \"explanation\": \"This step is crucial for PCA.\"}, {\"stepNumber\": 2, \"description\": \"Find the eigenvectors and eigenvalues of the covariance matrix\", \"mathHtml\": \"\\[ Σv_i = λ_i v_i\\]\", \"explanation\": \"The eigenvectors represent the principal components.\"} ],",
    "finalAnswer": "The top 3 principal components\" },",
    "intuition": "PCA helps to identify the most important features in your data and reduce the dimensionality, making it easier to visualize and analyze.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:52.067Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = \\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\Sigma \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "PCA Objective Function",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 100 features, how do we reduce it to the top 5 most informative components?</p>",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Compute the covariance matrix",
        "mathHtml": "\\[ \\Sigma = \\frac{1}{n-1} X^T X \\]",
        "explanation": "This is the starting point for PCA."
      }
    ],
    "finalAnswer": "The top 5 principal components\" },",
    "intuition": "PCA helps us identify the most important features in a dataset by maximizing the variance explained.",
    "visualDescription": "A scatter plot showing the original and transformed data",
    "tags": [
      "pca",
      "dimensionality reduction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:15:10.072Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_pca_008",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{C} = \\\\frac{1}{N-1} \\\\sum_{i=1}^N (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}}) (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T \\\\]\",",
    "name": "covariance matrix\" },",
    "theorem": "{",
    "statement": "\\[ \\mathbf{v}_1 = \\arg\\max_{\\mathbf{v} \\in \\mathbb{R}^n} \\frac{||\\mathbf{C}\\mathbf{v}||}{||\\mathbf{v}||} \\]",
    "proofSketch": "The proof involves showing that the principal components are the eigenvectors of the covariance matrix, and then using the Rayleigh quotient to find the maximum variance direction.\" },",
    "intuition": "PCA finds the directions of maximum variance in the data, allowing for dimensionality reduction while preserving most of the information.",
    "realWorldApplications": [
      "Image compression",
      "Feature extraction"
    ],
    "tags": [
      "dimensionality reduction",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:15:29.357Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_pca_009",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p><p>Given a dataset with correlated features, PCA helps to identify the most important directions of variation and project the data onto a lower-dimensional space.</p>",
    "formula": {
      "latex": "\\[ \\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\]",
      "name": "Variance Formula"
    },
    "theorem": {
      "statement": "\\[ \\mathbf{W} \\Lambda \\mathbf{W}^T = \\mathbf{C} \\]",
      "proofSketch": "The proof involves showing that the eigenvectors of the covariance matrix are orthogonal and form a basis for the lower-dimensional space."
    },
    "intuition": "PCA finds the directions in which the data varies most, allowing us to reduce the dimensionality while retaining most of the information.",
    "realWorldApplications": [
      "Dimensionality reduction for image compression",
      "Feature extraction for text classification"
    ],
    "tags": [
      "dimensionality reduction",
      "feature extraction"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:15:46.273Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_pca_010",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "pca",
    "problem": {
      "statementHtml": "Given a dataset with covariance matrix <i>C</i>, find the first two principal components that explain at least 80% of the variance.",
      "hints": [
        "Start by computing the eigenvectors and eigenvalues of <i>C</i>",
        "The top two eigenvalues should account for most of the variance",
        "Use the corresponding eigenvectors to define your principal components"
      ],
      "solutionHtml": "<p>Let's compute the eigenvectors and eigenvalues of <i>C</i>. We can do this using the power iteration method or a library like NumPy.</p><p>Once we have the top two eigenvalues, we can use them to define our principal components.</p>",
      "answerShort": "The first two principal components are..."
    },
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:15:59.179Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_pca_011",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "pca",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with covariance matrix Σ, find the first two principal components using PCA.</p>",
    "hints": [
      "Start by standardizing the data.",
      "Compute the eigenvectors and eigenvalues of Σ.",
      "Select the top two eigenvectors as the principal components."
    ],
    "solutionHtml": "<p>To solve this problem, we first standardize the data to have zero mean and unit variance.</p>\\n<p>Next, we compute the eigenvectors and eigenvalues of Σ using the formula:</p>\\n\\[ \\mathbf{v}_i = \\frac{\\mathbf{u}_i}{\\sqrt{\\lambda_i} } \\]\\n<p>where λi is the ith eigenvalue.</p>\\n<p>We then select the top two eigenvectors as the principal components.</p>\",",
    "answerShort": "The first two principal components are...\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:16:13.901Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_pca_012",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "pca",
    "problem": {
      "statementHtml": "<p>Given a dataset with covariance matrix Σ, find the first two principal components using PCA.</p>",
      "hints": [
        "Start by finding the eigenvectors and eigenvalues of Σ.",
        "The eigenvectors corresponding to the largest eigenvalues will be the principal components.",
        "Use the fact that the variance explained by each component is given by its corresponding eigenvalue."
      ],
      "solutionHtml": "<p>To find the first two principal components, we need to find the eigenvectors and eigenvalues of Σ. Let λ1, ..., λn be the eigenvalues and v1, ..., vn be the corresponding eigenvectors.</p><p>We can do this by solving the equation (Σ - λI)v = 0 for each λ.</p><p>The first two principal components are then given by the eigenvectors corresponding to the largest eigenvalues.</p>",
      "answerShort": "The first two principal components are the eigenvectors corresponding to the largest eigenvalues."
    },
    "commonMistakes": [
      "Forgetting that the variance explained by each component is given by its corresponding eigenvalue.",
      "Not normalizing the eigenvectors to have length 1."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:16:30.584Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_pca_013",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "pca",
    "problem": {
      "statementHtml": "<p>Given a dataset with covariance matrix Σ, find the first two principal components using PCA.</p>",
      "hints": [
        "Start by computing the eigenvectors and eigenvalues of Σ.",
        "The eigenvectors corresponding to the largest eigenvalues will be the principal components.",
        "Use these principal components to transform your data into a lower-dimensional space."
      ],
      "solutionHtml": "<p>To find the first two principal components, we need to compute the eigenvectors and eigenvalues of Σ. This can be done by solving the characteristic equation <i>Σv = λv</i>, where v is an eigenvector and λ is an eigenvalue.</p><p>The eigenvectors corresponding to the largest eigenvalues will be the principal components. Let's denote these as w1 and w2.</p><p>We can use these principal components to transform our data into a lower-dimensional space by projecting it onto w1 and w2.</p>",
      "answerShort": "The first two principal components are w1 and w2."
    },
    "commonMistakes": [
      "Forgetting to normalize the eigenvectors.",
      "Using the wrong method to compute the eigenvectors and eigenvalues."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:16:47.361Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_014",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA: A Worked Example",
    "contentHtml": "<p>In this example, we'll walk through a step-by-step solution to a Principal Component Analysis (PCA) problem.</p>",
    "workedExample": "{",
    "problemHtml": "<p>Given the covariance matrix Σ = [[4, 2, 0], [2, 6, 1], [0, 1, 3]] and the data points X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]], compute the first two principal components.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix Σ\", \"mathHtml\": \"\\[Σ = \\frac{1}{n-1} (X - \\bar{x})^T (X - \\bar{x})\\]\", \"explanation\": \"We need to compute the covariance matrix to find the principal components.\"}, {\"stepNumber\": 2, \"description\": \"Find the eigenvectors and eigenvalues of Σ\", \"mathHtml\": \"\\[\\Sigma v_i = λ_i v_i\\]\", \"explanation\": \"The eigenvectors and eigenvalues tell us about the directions and magnitudes of the principal components.\"}, {\"stepNumber\": 3, \"description\": \"Sort the eigenvectors by their corresponding eigenvalues\", \"mathHtml\": \"\", \"explanation\": \"We want to keep the most important principal components, which are those with the largest eigenvalues.\"}, {\"stepNumber\": 4, \"description\": \"Take the top two eigenvectors as the first two principal components\", \"mathHtml\": \"\", \"explanation\": \"The first two principal components capture the most variance in the data.\"} ],",
    "finalAnswer": "The first two principal components are [0.7071, 0.7071] and [-0.7071, -0.7071].\" },",
    "intuition": "PCA helps us reduce dimensionality by retaining the most important features in our data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:17:13.256Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_015",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this worked example, we'll demonstrate how to solve a Principal Component Analysis (PCA) problem using the covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{C} = \\\\frac{1}{n-1}\\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T\\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset X with n samples and p features, compute the top k principal components using the covariance matrix.</p>",
    "hints": [
      "Hint: Use the eigenvalue decomposition of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll use the following steps:</p><ul><li>Compute the covariance matrix C.</li><li>Eigen-decompose C to obtain its eigenvectors and eigenvalues.</li><li>Select the top k eigenvectors as the principal components.</li></ul>",
    "answerShort": "The first k principal components\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset X with n=100 samples and p=50 features. Compute the top k=3 principal components using the covariance matrix C.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\\\[\\\\mathbf{C} = \\\\frac{1}{n-1}\\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T\\\\]\", \"explanation\": \"This is the starting point for our PCA analysis.\"}, {\"stepNumber\": 2, \"description\": \"Eigen-decompose the covariance matrix\", \"mathHtml\": \"\\\\[\\\\mathbf{C} = \\\\sum_{i=1}^p \\\\lambda_i \\\\mathbf{u}_i \\\\mathbf{u}_i^T\\\\]\", \"explanation\": \"We're looking for the eigenvectors and eigenvalues of C.\"}, {\"stepNumber\": 3, \"description\": \"Select the top k eigenvectors\", \"mathHtml\": \"\\\\[\\\\text{Top }k\\\\text{ principal components} = \\\\{\\\\mathbf{u}_1, \\\\ldots, \\\\mathbf{u}_k\\\\}\\\\]\", \"explanation\": \"These are our desired principal components.\"}, {\"stepNumber\": 4, \"description\": \"Verify the results\", \"mathHtml\": \"\", \"explanation\": \"We can verify that the top k eigenvectors indeed capture the most variance in the data.\"} ],",
    "finalAnswer": "The first three principal components\" },",
    "intuition": "PCA helps reduce dimensionality by retaining the most important features, which is crucial for many machine learning algorithms.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:17:50.395Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_016",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this example, we'll demonstrate how to apply Principal Component Analysis (PCA) using a covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{C} = \\frac{1}{N-1} \\\\sum_{i=1}^N (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T \\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with covariance matrix <code>C</code>, find the top two principal components that explain at least 80% of the variance.</p>",
    "hints": [
      "Hint: Start by finding the eigenvectors and eigenvalues of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll use the following steps:</p><ul><li>Find the eigenvectors and eigenvalues of <code>C</code>.</li><li>Sort the eigenvalues in descending order.</li><li>Choose the top two eigenvectors corresponding to the largest eigenvalues.</li><li>Compute the variance explained by these principal components.</li></ul>",
    "answerShort": "The top two principal components explain at least 80% of the variance.\" },",
    "workedExample": "{",
    "problemHtml": "<p>Let's consider a dataset with covariance matrix:</p><code>\\[ \\\\mathbf{C} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 2 \\\\ \\\\end{bmatrix} \\\\]</code>\",",
    "steps": "[ {",
    "stepNumber": 4,
    "description": "Compute the variance explained by these principal components",
    "mathHtml": "\\[ \\text{variance} = \\sum_{i=1}^2 \\lambda_i \\]",
    "explanation": "This gives us an idea of how much of the original data is captured by these two principal components.\" } ],",
    "finalAnswer": "The top two principal components explain at least 80% of the variance.\" },",
    "intuition": "PCA helps reduce dimensionality while retaining most of the information in the dataset.",
    "visualDescription": "A diagram showing the covariance matrix and its eigenvectors would help illustrate the concept.",
    "commonMistakes": [
      "Forgetting to sort eigenvalues",
      "Choosing too few principal components"
    ],
    "realWorldApplications": [
      "Dimensionality reduction for image classification"
    ],
    "tags": [
      "PCA",
      "dimensionality reduction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:18:33.751Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_017",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this example, we'll demonstrate how to solve a Principal Component Analysis (PCA) problem using the covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{C} = \\frac{1}{n-1} \\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T \\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with mean vector \\\\( \\\\bar{\\mathbf{x}} \\\\) and covariance matrix \\\\( \\\\mathbf{C} \\\\), find the first two principal components that explain at least 80% of the variance.</p>\",",
    "hints": [
      "Hint: Start by computing the eigenvectors and eigenvalues of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll follow these steps:</p><ul><li>Compute the eigenvectors and eigenvalues of \\( \\mathbf{C} \\)</li><li>Sort the eigenvalues in descending order</li><li>Select the top two eigenvectors corresponding to the largest eigenvalues</li><li>Standardize the selected eigenvectors to obtain the principal components</li></ul>",
    "answerShort": "The first two principal components are...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset with mean vector \\( [1, 2]^T \\) and covariance matrix:</p><p>\\[ \\mathbf{C} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\]</p>",
    "steps": "[ {",
    "stepNumber": 4,
    "description": "Standardize the selected eigenvectors",
    "mathHtml": "\\[ \\mathbf{p}_1 = \\frac{\\mathbf{v}_1}{\\sqrt{2.5}}, \\mathbf{p}_2 = \\frac{\\mathbf{v}_2}{\\sqrt{0.5}} \\]",
    "explanation": "We're scaling the eigenvectors to have unit length\" } ],",
    "finalAnswer": "The first two principal components are...\" },",
    "intuition": "PCA helps reduce dimensionality by retaining most of the variance in the data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:19:16.261Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]