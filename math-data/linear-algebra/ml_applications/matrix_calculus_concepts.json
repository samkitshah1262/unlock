[
  {
    "id": "la_con_matrix_calculus_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Gradients",
    "contentHtml": "<p>In linear algebra, we often encounter matrix expressions that depend on some parameters or variables. To optimize these expressions, we need to compute their derivatives with respect to those variables.</p><p>Formally, the derivative of a matrix expression is another matrix that captures how the original expression changes when its inputs change.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} [A(x)] = \\frac{d}{dx} \\left[ \\sum_{i=1}^n A_i x^i \\right] \\]",
      "name": "Matrix Expression Derivative",
      "variants": []
    },
    "intuition": "Think of the derivative as a 'sensitivity' matrix that tells us how much each output changes when we perturb the inputs.",
    "realWorldApplications": [
      "Gradient descent in neural networks"
    ],
    "commonMistakes": [
      "Forgetting to chain rule derivatives across matrices"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:25:45.500Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_matrix_calculus_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>When working with matrix expressions in machine learning, it's crucial to understand how to compute derivatives and Jacobians. This concept is fundamental to optimizing model parameters.</p><p>In this card, we'll explore the basics of matrix calculus, including gradients of quadratic forms, Jacobians, and Hessians.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (x^T A x) = 2x^T A\\]",
      "name": "Gradient of Quadratic Form"
    },
    "intuition": "Think of the Jacobian as a matrix that captures how each input affects the output. This is particularly important in neural networks, where we need to compute gradients for backpropagation.",
    "realWorldApplications": [
      "Optimizing model parameters in neural networks"
    ],
    "commonMistakes": [
      "Forgetting to transpose matrices when computing derivatives"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:00.873Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_matrix_calculus_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Gradients",
    "contentHtml": "<p>Matrix calculus is a fundamental concept in machine learning, allowing us to compute gradients of matrix expressions. This enables efficient optimization of complex models.</p><p>In this card, we'll explore the derivatives of matrix expressions, including the gradient of quadratic forms, Jacobian, and Hessian.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (A x) = A^T \\]",
      "name": "Matrix derivative"
    },
    "intuition": "Think of a matrix as a set of linear transformations. The derivative of a matrix expression represents the rate of change of these transformations with respect to some input.",
    "realWorldApplications": [
      "Gradient descent for neural networks"
    ],
    "commonMistakes": [
      "Failing to account for the matrix structure in computations",
      "Incorrectly applying chain rule"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:14.884Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]