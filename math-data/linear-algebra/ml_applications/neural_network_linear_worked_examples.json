[
  {
    "id": "la_wex_neural_network_linear_012",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "neural_network_linear",
    "title": "Linear Algebra in Neural Networks: Forward Pass",
    "contentHtml": "<p>In neural networks, forward pass is a crucial step where input data flows through layers of neurons to produce an output.</p>",
    "formula": {
      "latex": "\\[Wx + b = y\\]",
      "name": "Forward Pass Formula"
    },
    "problem": {
      "statementHtml": "<p>Given a neural network with weight matrix W, input x, bias term b, and output y, perform the forward pass to compute y.</p>",
      "hints": [
        "Hint: Think of it as a series of matrix multiplications."
      ],
      "solutionHtml": "<p>To perform the forward pass, we can write:</p><ul><li>Step 1: Compute z = Wx + b</li><li>Step 2: Apply activation function to get y = sigmoid(z)</li></ul>",
      "answerShort": "y = sigmoid(Wx + b)"
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a neural network with weight matrix W = \\[\\begin{bmatrix} 0.5 & 0.2 \\\\ -0.3 & 0.9 \\end{bmatrix}\\], input x = \\[\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\], bias term b = 0, and output y.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Compute z",
          "mathHtml": "\\[z = Wx + b = \\begin{bmatrix} 0.5 & 0.2 \\\\ -0.3 & 0.9 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + 0\\] = \\[\\begin{bmatrix} 1.8 \\\\ 1.7 \\end{bmatrix}\\]",
          "explanation": "We're applying the weight matrix to the input, then adding the bias term."
        },
        {
          "stepNumber": 2,
          "description": "Apply activation function",
          "mathHtml": "\\[y = sigmoid(z) = \\frac{1}{1 + e^{-z}}\\] = ?"
        },
        {
          "stepNumber": 3,
          "description": "Calculate y",
          "mathHtml": "? = ?"
        },
        {
          "stepNumber": 4,
          "description": "Final answer",
          "mathHtml": "\\[y = \\frac{1}{1 + e^{-(1.8)}}\\]"
        }
      ],
      "finalAnswer": "\\[y = \\frac{1}{1 + e^{-(1.8)}}\\]"
    },
    "intuition": "The forward pass is a series of matrix multiplications that allow the input data to flow through the network, producing an output.",
    "visualDescription": "A diagram showing the neural network with input x, weight matrix W, bias term b, and output y would help illustrate this concept.",
    "commonMistakes": [
      "Forgetting to apply the activation function",
      "Not accounting for bias terms"
    ],
    "realWorldApplications": [
      "Image classification using convolutional neural networks"
    ],
    "tags": [
      "neural networks",
      "forward pass"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:39:09.032Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_neural_network_linear_013",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "neural_network_linear",
    "title": "Linear Algebra in Neural Networks: Forward Pass",
    "contentHtml": "<p>In neural networks, forward pass is a crucial step where input data flows through layers of neurons to produce an output.</p>",
    "workedExample": "{",
    "problemHtml": "Consider a simple feedforward neural network with two inputs <i>x</i> and <i>y</i>, one hidden layer with two neurons, and one output neuron. The weights for the first hidden neuron are <i>w<sub>11</sub></i> = 0.5, <i>w<sub>12</sub></i> = 1.2, and <i>w<sub>21</sub></i> = -0.8.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the input to the first hidden neuron\", \"mathHtml\": \"\\[z_1 = w_{11}x + w_{12}y\\]\", \"explanation\": \"We're applying the weights to the inputs and summing them up.\"}, {\"stepNumber\": 2, \"description\": \"Apply the activation function to get the output of the first hidden neuron\", \"mathHtml\": \"\\[h_1 = \\sigma(z_1) = \\sigma(0.5x + 1.2y - 0.8)\\]\", \"explanation\": \"We're using some activation function like sigmoid or ReLU.\"}, {\"stepNumber\": 3, \"description\": \"Repeat the process for the second hidden neuron\", \"mathHtml\": \"\\[z_2 = w_{21}x + w_{22}y\\]\", \"explanation\": \"Same idea as before, but with different weights.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the output of the network by applying the final weights and activation function\", \"mathHtml\": \"\\[o = \\sigma(w_{31}h_1 + w_{32}h_2)\\]\", \"explanation\": \"We're combining the outputs of the hidden neurons with some final weights and applying another activation function.\"}, ],",
    "finalAnswer": "The output <i>o</i>\" },",
    "intuition": "Forward pass in neural networks is essentially matrix multiplication, where input data flows through layers of neurons. This example illustrates how we can break down the process into smaller steps.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:39:38.419Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_neural_network_linear_014",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "neural_network_linear",
    "title": "Linear Algebra in Neural Networks: Forward Pass",
    "contentHtml": "<p>In neural networks, forward pass is a crucial step where input data flows through layers of weights and biases to produce output.</p>",
    "workedExample": "{",
    "problemHtml": "Consider a simple neural network with two inputs <i>x</i> and <i>y</i>, one hidden layer with one neuron, and one output layer. The weight matrix for the hidden layer is <i>W = [[w11, w12], [w21, w22]]</i>. Suppose we have input data <i>x = [x1, x2]</i> and <i>y = [y1, y2]</i>. What is the output of the forward pass?",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the pre-activation values for the hidden layer\", \"mathHtml\": \"\\[z = Wx + b\\]\", \"explanation\": \"We're using the weight matrix to dot product with the input data and adding the bias term.\"}, {\"stepNumber\": 2, \"description\": \"Apply the activation function to get the output of the hidden layer\", \"mathHtml\": \"\\[h = \\sigma(z)\\]\", \"explanation\": \"The activation function introduces non-linearity in the model.\"}, {\"stepNumber\": 3, \"description\": \"Calculate the pre-activation values for the output layer\", \"mathHtml\": \"\\[o = Wo + c\\]\", \"explanation\": \"We're using the weight matrix to dot product with the hidden layer output and adding the bias term.\"}, {\"stepNumber\": 4, \"description\": \"Apply the activation function to get the final output\", \"mathHtml\": \"\\[y = \\sigma(o)\\]\", \"explanation\": \"The final output is the result of applying the activation function to the pre-activation values.\"} ],",
    "finalAnswer": "The final output <i>y</i> is calculated by applying the activation function to the pre-activation values obtained from the weight matrix and bias term.\" },",
    "intuition": "Forward pass in neural networks can be thought of as a series of dot products and element-wise operations that transform input data into meaningful representations.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:40:06.047Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_neural_network_linear_015",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "neural_network_linear",
    "title": "Linear Algebra in Neural Networks: Forward Pass",
    "contentHtml": "<p>In neural networks, forward pass is a crucial step where input data flows through layers of neurons to produce output.</p>",
    "formula": {
      "latex": "\\[W \\cdot x + b\\]",
      "name": "Weight Matrix Multiplication"
    },
    "problem": {
      "statementHtml": "<p>Suppose we have a neural network with two hidden layers, each having 3 neurons. The weight matrix for the first layer is given as:</p><p>\\[W_1 = \\begin{bmatrix} 0.5 & -0.2 & 0.8 \\\\ 0.3 & 0.9 & -0.4 \\\\ -0.7 & 0.6 & 0.1 \\end{bmatrix}\\]</p>",
      "hints": [
        "Consider the matrix operations"
      ],
      "solutionHtml": "<p>To perform the forward pass, we need to compute the output of each layer.</p><ul><li>First, we multiply the input data with the weight matrix:</li><li>\\[W_1 \\cdot x = \\begin{bmatrix} 0.5x_1 - 0.2x_2 + 0.8x_3 \\\\ 0.3x_1 + 0.9x_2 - 0.4x_3 \\\\ -0.7x_1 + 0.6x_2 + 0.1x_3 \\end{bmatrix}\\]</li><li>Next, we add the bias term:</li><li>\\[W_1 \\cdot x + b = \\begin{bmatrix} 0.5x_1 - 0.2x_2 + 0.8x_3 + 0.2 \\\\ 0.3x_1 + 0.9x_2 - 0.4x_3 + 0.5 \\\\ -0.7x_1 + 0.6x_2 + 0.1x_3 + 0.8 \\end{bmatrix}\\]</li></ul>",
      "answerShort": "The output of the first layer is a 3-dimensional vector."
    },
    "workedExample": {
      "problemHtml": "<p>For this example, assume input data x = [1, 2, 3].</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Compute the output of the first layer",
          "mathHtml": "\\[W_1 \\cdot x + b\\]",
          "explanation": "We multiply the input data with the weight matrix and add the bias term."
        },
        {
          "stepNumber": 2,
          "description": "Repeat for each hidden layer",
          "mathHtml": "",
          "explanation": "The process is repeated for each subsequent layer."
        }
      ],
      "finalAnswer": "The output of the forward pass is a vector representing the output of the neural network."
    },
    "intuition": "Forward pass in neural networks can be thought of as matrix operations, where input data flows through layers to produce output.",
    "visualDescription": "A diagram showing the flow of input data through multiple hidden layers and an output layer would help illustrate this concept.",
    "commonMistakes": [
      "Forgetting to add bias terms",
      "Not considering batch processing"
    ],
    "realWorldApplications": [
      "Image classification using convolutional neural networks"
    ],
    "tags": [
      "neural networks",
      "forward pass",
      "linear algebra"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:40:49.480Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]