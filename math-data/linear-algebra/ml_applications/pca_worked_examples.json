[
  {
    "id": "la_wex_pca_014",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA: A Worked Example",
    "contentHtml": "<p>In this example, we'll walk through a step-by-step solution to a Principal Component Analysis (PCA) problem.</p>",
    "workedExample": "{",
    "problemHtml": "<p>Given the covariance matrix Σ = [[4, 2, 0], [2, 6, 1], [0, 1, 3]] and the data points X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]], compute the first two principal components.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix Σ\", \"mathHtml\": \"\\[Σ = \\frac{1}{n-1} (X - \\bar{x})^T (X - \\bar{x})\\]\", \"explanation\": \"We need to compute the covariance matrix to find the principal components.\"}, {\"stepNumber\": 2, \"description\": \"Find the eigenvectors and eigenvalues of Σ\", \"mathHtml\": \"\\[\\Sigma v_i = λ_i v_i\\]\", \"explanation\": \"The eigenvectors and eigenvalues tell us about the directions and magnitudes of the principal components.\"}, {\"stepNumber\": 3, \"description\": \"Sort the eigenvectors by their corresponding eigenvalues\", \"mathHtml\": \"\", \"explanation\": \"We want to keep the most important principal components, which are those with the largest eigenvalues.\"}, {\"stepNumber\": 4, \"description\": \"Take the top two eigenvectors as the first two principal components\", \"mathHtml\": \"\", \"explanation\": \"The first two principal components capture the most variance in the data.\"} ],",
    "finalAnswer": "The first two principal components are [0.7071, 0.7071] and [-0.7071, -0.7071].\" },",
    "intuition": "PCA helps us reduce dimensionality by retaining the most important features in our data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:17:13.256Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_015",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this worked example, we'll demonstrate how to solve a Principal Component Analysis (PCA) problem using the covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{C} = \\\\frac{1}{n-1}\\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T\\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset X with n samples and p features, compute the top k principal components using the covariance matrix.</p>",
    "hints": [
      "Hint: Use the eigenvalue decomposition of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll use the following steps:</p><ul><li>Compute the covariance matrix C.</li><li>Eigen-decompose C to obtain its eigenvectors and eigenvalues.</li><li>Select the top k eigenvectors as the principal components.</li></ul>",
    "answerShort": "The first k principal components\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset X with n=100 samples and p=50 features. Compute the top k=3 principal components using the covariance matrix C.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\\\[\\\\mathbf{C} = \\\\frac{1}{n-1}\\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T\\\\]\", \"explanation\": \"This is the starting point for our PCA analysis.\"}, {\"stepNumber\": 2, \"description\": \"Eigen-decompose the covariance matrix\", \"mathHtml\": \"\\\\[\\\\mathbf{C} = \\\\sum_{i=1}^p \\\\lambda_i \\\\mathbf{u}_i \\\\mathbf{u}_i^T\\\\]\", \"explanation\": \"We're looking for the eigenvectors and eigenvalues of C.\"}, {\"stepNumber\": 3, \"description\": \"Select the top k eigenvectors\", \"mathHtml\": \"\\\\[\\\\text{Top }k\\\\text{ principal components} = \\\\{\\\\mathbf{u}_1, \\\\ldots, \\\\mathbf{u}_k\\\\}\\\\]\", \"explanation\": \"These are our desired principal components.\"}, {\"stepNumber\": 4, \"description\": \"Verify the results\", \"mathHtml\": \"\", \"explanation\": \"We can verify that the top k eigenvectors indeed capture the most variance in the data.\"} ],",
    "finalAnswer": "The first three principal components\" },",
    "intuition": "PCA helps reduce dimensionality by retaining the most important features, which is crucial for many machine learning algorithms.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:17:50.395Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_016",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this example, we'll demonstrate how to apply Principal Component Analysis (PCA) using a covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{C} = \\frac{1}{N-1} \\\\sum_{i=1}^N (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T \\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with covariance matrix <code>C</code>, find the top two principal components that explain at least 80% of the variance.</p>",
    "hints": [
      "Hint: Start by finding the eigenvectors and eigenvalues of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll use the following steps:</p><ul><li>Find the eigenvectors and eigenvalues of <code>C</code>.</li><li>Sort the eigenvalues in descending order.</li><li>Choose the top two eigenvectors corresponding to the largest eigenvalues.</li><li>Compute the variance explained by these principal components.</li></ul>",
    "answerShort": "The top two principal components explain at least 80% of the variance.\" },",
    "workedExample": "{",
    "problemHtml": "<p>Let's consider a dataset with covariance matrix:</p><code>\\[ \\\\mathbf{C} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 2 \\\\ \\\\end{bmatrix} \\\\]</code>\",",
    "steps": "[ {",
    "stepNumber": 4,
    "description": "Compute the variance explained by these principal components",
    "mathHtml": "\\[ \\text{variance} = \\sum_{i=1}^2 \\lambda_i \\]",
    "explanation": "This gives us an idea of how much of the original data is captured by these two principal components.\" } ],",
    "finalAnswer": "The top two principal components explain at least 80% of the variance.\" },",
    "intuition": "PCA helps reduce dimensionality while retaining most of the information in the dataset.",
    "visualDescription": "A diagram showing the covariance matrix and its eigenvectors would help illustrate the concept.",
    "commonMistakes": [
      "Forgetting to sort eigenvalues",
      "Choosing too few principal components"
    ],
    "realWorldApplications": [
      "Dimensionality reduction for image classification"
    ],
    "tags": [
      "PCA",
      "dimensionality reduction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:18:33.751Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_pca_017",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Solving PCA with Covariance Matrix",
    "contentHtml": "<p>In this example, we'll demonstrate how to solve a Principal Component Analysis (PCA) problem using the covariance matrix.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{C} = \\frac{1}{n-1} \\\\sum_{i=1}^n (\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})(\\\\mathbf{x}_i - \\\\bar{\\mathbf{x}})^T \\\\]\",",
    "name": "covariance matrix\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset with mean vector \\\\( \\\\bar{\\mathbf{x}} \\\\) and covariance matrix \\\\( \\\\mathbf{C} \\\\), find the first two principal components that explain at least 80% of the variance.</p>\",",
    "hints": [
      "Hint: Start by computing the eigenvectors and eigenvalues of the covariance matrix."
    ],
    "solutionHtml": "<p>We'll follow these steps:</p><ul><li>Compute the eigenvectors and eigenvalues of \\( \\mathbf{C} \\)</li><li>Sort the eigenvalues in descending order</li><li>Select the top two eigenvectors corresponding to the largest eigenvalues</li><li>Standardize the selected eigenvectors to obtain the principal components</li></ul>",
    "answerShort": "The first two principal components are...\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset with mean vector \\( [1, 2]^T \\) and covariance matrix:</p><p>\\[ \\mathbf{C} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\]</p>",
    "steps": "[ {",
    "stepNumber": 4,
    "description": "Standardize the selected eigenvectors",
    "mathHtml": "\\[ \\mathbf{p}_1 = \\frac{\\mathbf{v}_1}{\\sqrt{2.5}}, \\mathbf{p}_2 = \\frac{\\mathbf{v}_2}{\\sqrt{0.5}} \\]",
    "explanation": "We're scaling the eigenvectors to have unit length\" } ],",
    "finalAnswer": "The first two principal components are...\" },",
    "intuition": "PCA helps reduce dimensionality by retaining most of the variance in the data.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:19:16.261Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]