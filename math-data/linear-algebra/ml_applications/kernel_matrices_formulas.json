[
  {
    "id": "la_for_kernel_matrices_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "kernel_matrices",
    "title": "Kernel Methods: Gram Matrices and Positive Semi-Definite Kernels",
    "contentHtml": "<p>In machine learning, kernel methods are a powerful tool for transforming non-linearly separable data into a higher-dimensional space where it becomes linearly separable.</p><p>The key to kernel methods is the gram matrix, which represents the dot product of all pairs of input vectors in the original feature space.</p>",
    "formula": "{",
    "latex": "\\[K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\\]\",",
    "name": "Kernel Function",
    "variants": "[ {\"latex\": \"\\[\\sum_{i=1}^n k(x_i, x_j)\\]\", \"description\": \"Gram matrix\" } ] },",
    "workedExample": "{",
    "problemHtml": "<p>Given two input vectors $x_1 = [1, 2]$ and $x_2 = [3, 4]$, find the gram matrix using a linear kernel.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the feature maps for each input vector\", \"mathHtml\": \"\\[\\phi(x_1) = [1, 2], \\quad \\phi(x_2) = [3, 4]\\]\", \"explanation\": \"We define the feature map as a linear transformation of the original input vectors.\" }, {\"stepNumber\": 2, \"description\": \"Compute the dot product between each pair of input vectors\", \"mathHtml\": \"\\[K(x_1, x_1) = \\phi(x_1)^T \\phi(x_1) = [1, 2] \\cdot [1, 2] = 5\\]\", \"explanation\": \"We compute the dot product between each pair of input vectors using the feature maps.\" }, {\"stepNumber\": 3, \"description\": \"Compute the gram matrix\", \"mathHtml\": \"\\[\\begin{bmatrix} K(x_1, x_1) & K(x_1, x_2) \\\\ K(x_2, x_1) & K(x_2, x_2) \\end{bmatrix} = \\begin{bmatrix} 5 & 11 \\\\ 11 & 29 \\end{bmatrix}\\]\", \"explanation\": \"We assemble the dot products into a gram matrix.\" } ],",
    "finalAnswer": "The gram matrix is \\[\\begin{bmatrix} 5 & 11 \\\\ 11 & 29 \\end{bmatrix}\\]\" },",
    "intuition": "Kernel methods allow us to implicitly transform our data into a higher-dimensional space by defining a kernel function that computes the dot product between input vectors.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:33:06.205Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_kernel_matrices_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "kernel_matrices",
    "title": "Kernel Methods: Gram Matrices and Positive Semi-Definite Kernels",
    "contentHtml": "<p>In kernel methods, we often encounter gram matrices that arise from dot products of data points in a higher-dimensional space.</p><p>The key idea is to use positive semi-definite kernels to ensure the resulting gram matrix is always positive semi-definite, which is crucial for many machine learning algorithms.</p>",
    "formula": "{",
    "latex": "\\[ K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) \\]\",",
    "name": "Kernel Function",
    "variants": "[ {\"latex\": \"\\[ K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\]\", \"description\": \"Gaussian kernel\" } ] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have two data points $x_1$ and $x_2$, both in $\\mathbb{R}^3$. We want to compute the gram matrix for a Gaussian kernel with $\\gamma = 0.5$.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the difference vector\", \"mathHtml\": \"\\[ x_2 - x_1 \\]\", \"explanation\": \"This is the key step in computing the gram matrix.\" }, {\"stepNumber\": 2, \"description\": \"Compute the squared Euclidean norm of the difference vector\", \"mathHtml\": \"\\[ ||x_2 - x_1||^2 \\]\", \"explanation\": \"This is used to compute the Gaussian kernel value.\" } ],",
    "finalAnswer": "The gram matrix element for this example would be $K(x_1, x_2) = \\exp(-0.5 ||x_2 - x_1||^2)$\" },",
    "intuition": "The key idea is to use positive semi-definite kernels to ensure the resulting gram matrix is always positive semi-definite.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:33:32.410Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_kernel_matrices_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "kernel_matrices",
    "title": "Kernel Methods: Gram Matrices and Positive Semi-Definite Kernels",
    "contentHtml": "<p>In machine learning, kernel methods are a powerful tool for transforming non-linearly separable data into a higher-dimensional space where it becomes linearly separable.</p><p>The key to kernel methods is the gram matrix, which measures the similarity between data points using a positive semi-definite (PSD) kernel function.</p>",
    "formula": "{",
    "latex": "\\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) \\]\",",
    "name": "Gram Matrix Formula",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Given a set of data points, compute the gram matrix using a radial basis function (RBF) kernel.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the RBF kernel values for each pair of data points\", \"mathHtml\": \"\\( K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2} \\)\", \"explanation\": \"The RBF kernel measures the similarity between two data points based on their distance.\"} ],",
    "finalAnswer": "\" },",
    "intuition": "Kernel methods allow us to implicitly transform our data into a higher-dimensional space, enabling us to model complex relationships and improve classification accuracy.",
    "tags": [
      "kernel",
      "gram matrix",
      "positive semi-definite"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:33:54.292Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]