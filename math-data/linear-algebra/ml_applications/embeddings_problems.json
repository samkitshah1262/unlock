[
  {
    "id": "la_prb_embeddings_008",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "problem": "{",
    "statementHtml": "<p>Given a set of documents and their corresponding labels, design an embedding matrix that captures the semantic meaning of each document.</p>",
    "hints": [
      "Start by representing each document as a vector in a high-dimensional space.",
      "Use a similarity metric to measure the distance between these vectors and ensure they are well-separated.",
      "Consider using a technique like PCA or t-SNE for dimensionality reduction."
    ],
    "solutionHtml": "<p>To solve this problem, we can use matrix factorization techniques. Let's assume we have a set of documents represented as vectors in a high-dimensional space.</p><p>\\[ W = U \\* Σ \\* V^T \\] where <i>W</i> is the original document-term matrix, <i>U</i> and <i>V</i> are the factor matrices, and <i>Σ</i> is a diagonal matrix containing the singular values.</p><p>We can then use techniques like SVD or NMF to decompose the matrix into its factors. This will give us a set of latent factors that capture the semantic meaning of each document.</p>\",",
    "answerShort": "The answer is an embedding matrix that captures the semantic meaning of each document.\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:48:10.593Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_embeddings_009",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "problem": {
      "statementHtml": "<p>Given a set of documents and their corresponding labels, design an embedding that minimizes the distance between similar documents.</p>",
      "hints": [
        "Start by considering the cosine similarity metric.",
        "Think about how you can use matrix factorization to reduce dimensionality.",
        "Use the concept of nearest neighbors to guide your approach."
      ],
      "solutionHtml": "<p>To minimize the distance between similar documents, we can use a technique called matrix factorization. Specifically, we'll use non-negative matrix factorization (NMF) to decompose our document-term matrix into two lower-dimensional matrices: one for the documents and one for the terms.</p>\n<p>Let's denote the original document-term matrix as <code>A</code>. We can then write <code>A ≈ UV^T</code>, where <code>U</code> is a matrix of size <code>m × k</code> (where <code>m</code> is the number of documents and <code>k</code> is the number of topics), and <code>V</code> is a matrix of size <code>n × k</code> (where <code>n</code> is the number of terms).</p>\n<p>We can then use the cosine similarity metric to measure the distance between similar documents. The final step is to optimize the matrices <code>U</code> and <code>V</code> such that the distance between similar documents is minimized.</p>",
      "answerShort": "The answer is..."
    },
    "commonMistakes": [
      "Not considering the non-negativity constraint in NMF.",
      "Using a different dimensionality reduction technique."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:48:37.040Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_embeddings_010",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "problem": "{",
    "statementHtml": "<p>Given a set of <i>n</i> words and their corresponding word embeddings in a <i>d</i>-dimensional space, find the optimal representation for each word using matrix factorization.</p>",
    "hints": "[ \"Start by defining the objective function to minimize.\", \"Use the fact that the dot product between two vectors is equal to \\(\\mathbf{a} \\cdot \\mathbf{b}\\).\", \"Apply the singular value decomposition (SVD) to the word embedding matrix.\" ],",
    "solutionHtml": "<p>To solve this problem, we can use the following steps:</p><ol><li>Define the objective function as the sum of squared errors between the original and reconstructed word embeddings: \\(\\mathcal{L} = \\sum_{i=1}^n ||\\mathbf{x}_i - \\hat{\\mathbf{x}}_i||^2.\\)</li><li>Use the fact that the dot product between two vectors is equal to \\(\\mathbf{a} \\cdot \\mathbf{b}\\) to rewrite the objective function as:</li><ul><li>\\[\\mathcal{L} = ||\\mathbf{X} - \\hat{\\mathbf{X}}||^2.\\]</li></ul><li>Apply the SVD to the word embedding matrix \\(\\mathbf{X}\\): \\[\\mathbf{U}\\Sigma\\mathbf{V}^T = \\mathbf{X}.\\]</li><li>Solve for the optimal representation by minimizing the objective function:</li><ul><li>\\[ \\hat{\\mathbf{x}}_i = \\mathbf{U}\\Sigma\\mathbf{V}^T \\mathbf{v}_i,\\]</li></ul></ol>\",",
    "answerShort": "The optimal representation for each word is given by the columns of the matrix \\(\\mathbf{U}\\Sigma\\)\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:49:08.054Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_embeddings_011",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "problem": "{",
    "statementHtml": "<p>Given a set of documents and their corresponding labels, design an embedding that captures the semantic meaning of each document.</p>",
    "hints": [
      "Start by defining a similarity metric between documents.",
      "Use matrix factorization to reduce dimensionality.",
      "Consider using a non-linear activation function in your neural network."
    ],
    "solutionHtml": "<p>To create an embedding that captures the semantic meaning of each document, we can use matrix factorization. Let's assume we have a set of documents represented as vectors in a high-dimensional space.</p>\\n<p>We can define a similarity metric between documents using the cosine similarity:</p>\\n\\[s(i, j) = \\frac{\\mathbf{v}_i \\cdot \\mathbf{v}_j}{||\\mathbf{v}_i|| ||\\mathbf{v}_j||}\\]\\n<p>Next, we can use matrix factorization to reduce the dimensionality of our document vectors. This can be done using a neural network with a non-linear activation function.</p>\\n<p>The output of this neural network will be a set of lower-dimensional embeddings that capture the semantic meaning of each document.</p>\",",
    "answerShort": "The answer is...\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:49:33.005Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]