[
  {
    "id": "la_con_pca_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a set of correlated variables into a new set of uncorrelated variables called principal components.</p><p>The goal of PCA is to retain the most important information in the data while reducing its complexity by projecting it onto a lower-dimensional space.</p>",
    "formula": {
      "latex": "\\[ \\text{Variance Explained} = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sigma^2} \\]",
      "name": "Variance Explained"
    },
    "intuition": "PCA helps us identify the most important features in our data by capturing the directions of maximum variance. This is useful in machine learning, as it can help reduce overfitting and improve model interpretability.",
    "visualDescription": "A scatter plot showing the original data points with their principal components would be helpful to visualize the concept.",
    "commonMistakes": [
      "Assuming PCA only works for high-dimensional spaces",
      "Thinking that PCA is a feature selection method"
    ],
    "realWorldApplications": [
      "Reducing noise in image datasets",
      "Identifying key factors in financial modeling"
    ],
    "tags": [
      "pca",
      "dimensionality reduction"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:08.656Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_pca_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining most of the original information.</p><p>This is achieved by finding the directions of maximum variance in the data, known as principal components, and projecting the data onto these axes.</p>",
    "formula": {
      "latex": "\\[\\text{Variance explained} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]",
      "name": "Variance explanation"
    },
    "intuition": "Think of PCA as a way to compress your data while keeping the important features intact. Imagine you're trying to summarize a long article by highlighting the main points â€“ that's what PCA does for your data.",
    "realWorldApplications": [
      "Dimensionality reduction in image recognition",
      "Feature extraction in text analysis"
    ],
    "commonMistakes": [
      "Thinking PCA is just a simple linear transformation",
      "Not understanding the importance of variance in the selection of principal components"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:25.511Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_pca_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original data into a new set of features, called principal components, which capture most of the variability in the data.</p><p>The goal of PCA is to reduce the number of features while retaining as much information as possible. This is achieved by finding the directions of maximum variance in the data and projecting the data onto these directions.</p>",
    "formula": {
      "latex": "\\[\\mathbf{W} = \\argmax_{\\mathbf{W}} \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})^T \\mathbf{W} \\mathbf{W}^T (\\mathbf{x}_i - \\bar{\\mathbf{x}})\\]",
      "name": "PCA objective function"
    },
    "intuition": "Think of PCA as a way to compress the data into a smaller set of features that capture most of the variation. This is useful in machine learning, where high-dimensional data can be difficult to work with.",
    "realWorldApplications": [
      "Dimensionality reduction for image and text data",
      "Feature extraction for recommender systems"
    ],
    "commonMistakes": [
      "Not understanding the difference between PCA and other dimensionality reduction techniques",
      "Not normalizing the data before applying PCA"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:13:44.904Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]