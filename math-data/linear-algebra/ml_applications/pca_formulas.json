[
  {
    "id": "la_for_pca_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p><p>Given a dataset with high-dimensional features, PCA aims to find the most important directions of variation and project the data onto those directions.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w}_k = \\arg\\max_{\\mathbf{w}} \\frac{\\left| \\mathbf{w}^T \\mathbf{X} \\right|^2}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "Principal Component\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of images with 1000 features each. We want to reduce the dimensionality to 50 while retaining most of the information.</p>",
    "steps": "[ {",
    "stepNumber": 3,
    "description": "Project the data onto the principal components",
    "mathHtml": "\\[ \\mathbf{X}_\\text{reduced} = \\mathbf{W}^T \\mathbf{X} \\]\",",
    "explanation": "We project the original data onto the reduced dimensionality.\" } ],",
    "finalAnswer": "The resulting 50-dimensional representation captures most of the information in the original dataset.\" },",
    "intuition": "PCA helps us identify the most important features and reduce the noise in our data.",
    "realWorldApplications": [
      "Image compression",
      "Anomaly detection"
    ],
    "tags": [
      "dimensionality reduction",
      "feature extraction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:15.539Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>PCA is a widely used dimensionality reduction technique in machine learning.</p><p>It helps to identify the most important features in your data and reduce its dimensionality while retaining most of the information.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w}_k = \\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\Sigma \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "PCA formula\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 10 features, reduce its dimensionality to 3 using PCA.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Calculate the covariance matrix\", \"mathHtml\": \"\\\\[ \\Sigma = \\\\frac{1}{n-1} X^T X \\\\]\", \"explanation\": \"The covariance matrix helps us understand how features are correlated.\"} ],",
    "finalAnswer": "3 principal components\" },",
    "intuition": "PCA finds the directions of maximum variance in your data and projects it onto those directions.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:34.260Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining most of the original information.</p>",
    "formula": "{",
    "latex": "\\[ W = \\sigma^{-1} U^T Σ^2 U\\]\",",
    "name": "Principal Component Analysis\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 100 features, reduce it to 3 principal components.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\[ Σ = \\frac{1}{n-1} X^T X\\]\", \"explanation\": \"This step is crucial for PCA.\"}, {\"stepNumber\": 2, \"description\": \"Find the eigenvectors and eigenvalues of the covariance matrix\", \"mathHtml\": \"\\[ Σv_i = λ_i v_i\\]\", \"explanation\": \"The eigenvectors represent the principal components.\"} ],",
    "finalAnswer": "The top 3 principal components\" },",
    "intuition": "PCA helps to identify the most important features in your data and reduce the dimensionality, making it easier to visualize and analyze.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:14:52.067Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_pca_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "pca",
    "title": "Principal Component Analysis (PCA)",
    "contentHtml": "<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in machine learning.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = \\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\Sigma \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\]\",",
    "name": "PCA Objective Function",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with 100 features, how do we reduce it to the top 5 most informative components?</p>",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Compute the covariance matrix",
        "mathHtml": "\\[ \\Sigma = \\frac{1}{n-1} X^T X \\]",
        "explanation": "This is the starting point for PCA."
      }
    ],
    "finalAnswer": "The top 5 principal components\" },",
    "intuition": "PCA helps us identify the most important features in a dataset by maximizing the variance explained.",
    "visualDescription": "A scatter plot showing the original and transformed data",
    "tags": [
      "pca",
      "dimensionality reduction"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:15:10.072Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]