[
  {
    "id": "la_for_matrix_calculus_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In linear algebra, derivatives are crucial in optimization problems, such as those encountered in machine learning. This formula allows us to compute the derivative of a matrix expression with respect to another matrix.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A}^T\\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{A}^T\\]\",",
    "name": "Derivative of Matrix Expression",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the matrix expression <code>\\(\\mathbf{X}\\mathbf{w}\\)</code> with respect to <code>\\(\\mathbf{w}\\)</code>, where <code>\\(\\mathbf{X}\\)</code> is a data matrix and <code>\\(\\mathbf{w}\\)</code> is the weight vector.</p>\",",
    "steps": "[ {",
    "stepNumber": 2,
    "description": "Use the product rule",
    "mathHtml": "\\[\\frac{\\partial (\\mathbf{X}\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\",",
    "explanation": "The product rule helps us compute the derivative of a product.\" } ],",
    "finalAnswer": "<code>\\(\\frac{\\partial \\mathbf{X}\\mathbf{w}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\)</code>\" },",
    "intuition": "This formula is essential in optimization problems, such as logistic regression and neural networks.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:43.031Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>The derivative of a matrix expression is crucial in machine learning to optimize model parameters. This formula helps us compute the gradient of quadratic forms and Jacobian/Hessian matrices.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A}^T\\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{A}\\]\",",
    "name": "Chain Rule for Matrix Expressions\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the matrix expression <code>\\mathbf{y} = \\mathbf{X}^T\\mathbf{w}</code>, where <code>\\mathbf{X}</code> is a design matrix and <code>\\mathbf{w}</code> is a weight vector.</p>\",",
    "steps": "[ {",
    "stepNumber": 1,
    "description": "Apply the chain rule",
    "mathHtml": "\\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{w}} = \\frac{\\partial (\\mathbf{X}^T\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\",",
    "explanation": "The chain rule helps us compute the derivative of a composite function.\" } ],",
    "finalAnswer": "\\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\" },",
    "intuition": "This formula is essential in machine learning to optimize model parameters. It helps us compute the gradient of quadratic forms and Jacobian/Hessian matrices, which are crucial for many algorithms.",
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:06.043Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In many machine learning algorithms, we need to compute derivatives of matrix expressions with respect to some parameters. This card introduces the key formulas and concepts for doing so.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A} \\mathbf{x}}{\\partial x_i} = \\mathbf{A} e_i\\]\",",
    "name": "Chain Rule for Matrix Products\" },",
    "workedExample": "{",
    "problemHtml": "<p>Compute the derivative of the matrix product $\\mathbf{W} (\\mathbf{X} \\mathbf{x})$ with respect to $x_1$.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"\\[\\frac{\\partial}{\\partial x_1} \\mathbf{W} (\\mathbf{X} \\mathbf{x}) = \\mathbf{W} \\mathbf{X} e_1\\]\", \"explanation\": \"The derivative of a matrix product is the product of the derivative of the first factor and the second factor.\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "Understanding how to compute derivatives of matrix expressions is crucial in many machine learning algorithms, such as gradient descent.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:24.926Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In linear algebra, derivatives of matrix expressions are crucial in machine learning. This formula helps us compute the gradient of quadratic forms.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{x}^T A \\mathbf{x}}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\]\",",
    "name": "Gradient of Quadratic Form\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the quadratic form <code>\\mathbf{w}^T \\mathbf{x}^T A \\mathbf{x}</code> with respect to <code>\\mathbf{w}</code>.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"\\[ \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}^T A \\mathbf{x}) = ... \\]\", \"explanation\": \"The derivative of a quadratic form with respect to its coefficients is another quadratic form.\"} ],",
    "finalAnswer": "The answer is <code>2\\mathbf{x}\\mathbf{x}^T</code>\" },",
    "intuition": "This formula helps us compute the gradient of quadratic forms, which is essential in machine learning for optimization.",
    "realWorldApplications": [
      "Gradient descent algorithm"
    ],
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:45.178Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]