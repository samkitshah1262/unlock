[
  {
    "id": "la_con_linear_regression_matrix_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "subtitle": "A fundamental concept in machine learning",
    "contentHtml": "<p>Linear regression is a cornerstone of machine learning, and its mathematical foundation lies in linear algebra.</p><p>In this card, we'll explore the normal equations, ridge regression, and matrix formulation of linear regression.</p>",
    "formula": {
      "latex": "\\[ \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} \\]",
      "name": "Normal Equations",
      "variants": []
    },
    "intuition": "Linear regression finds the best-fitting hyperplane to separate classes by minimizing the mean squared error between predicted and actual values.",
    "realWorldApplications": [
      "Image classification",
      "Recommendation systems"
    ],
    "commonMistakes": [
      "Forgetting that ridge regression is a regularization technique"
    ],
    "tags": [
      "linear-algebra",
      "machine-learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:19:32.330Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_linear_regression_matrix_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that involves modeling the relationship between a dependent variable and one or more independent variables using a linear equation.</p><p>In this card, we'll explore how linear algebra provides a powerful framework for solving linear regression problems.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{w} = (\\\\mathbf{X}^T \\\\mathbf{X})^{-1} \\\\mathbf{X}^T \\\\mathbf{y}\\]\",",
    "name": "Normal Equation\" },",
    "intuition": "The normal equation provides a closed-form solution for the weights in linear regression, which is essential for efficient computation and scalability.",
    "realWorldApplications": [
      "Image classification",
      "Recommendation systems"
    ],
    "commonMistakes": [
      "Forgetting to add an intercept term",
      "Not accounting for regularization terms"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:19:46.215Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_linear_regression_matrix_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that involves modeling the relationship between a dependent variable and one or more independent variables. In this card, we'll explore how linear algebra provides a powerful framework for understanding and implementing linear regression.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{w} = (\\\\mathbf{X}^T \\\\mathbf{X})^{-1} \\\\mathbf{X}^T \\\\mathbf{y}\\]\",",
    "name": "Normal Equation\" },",
    "intuition": "The normal equation is a way to find the optimal weights in linear regression by minimizing the mean squared error between predicted and actual values. It's a powerful tool for understanding how the independent variables contribute to the dependent variable.",
    "realWorldApplications": [
      "Predicting house prices based on features like number of bedrooms, square footage, etc."
    ],
    "commonMistakes": [
      "Not accounting for multicollinearity in the independent variables",
      "Failing to regularize the model with techniques like ridge regression"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:20:02.232Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_linear_regression_matrix_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that can be elegantly formulated using linear algebra.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\]\",",
    "name": "Normal Equation\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset of features and labels, find the best-fitting linear model.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\[ \\mathbf{X}^T\\mathbf{X} \\]\", \"explanation\": \"This represents the sum of outer products of the feature vectors.\"}, {\"stepNumber\": 2, \"description\": \"Compute the cross-covariance matrix\", \"mathHtml\": \"\\[ \\mathbf{X}^T\\mathbf{y} \\]\", \"explanation\": \"This represents the sum of dot products between the feature vectors and labels.\"} ],",
    "finalAnswer": "The estimated weights\" },",
    "intuition": "Linear regression can be seen as finding the linear combination of features that best predicts the target variable.",
    "realWorldApplications": [
      "Predicting house prices based on features like number of bedrooms and square footage"
    ],
    "tags": [
      "linear algebra",
      "machine learning",
      "regression"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:20:23.523Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_linear_regression_matrix_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that can be elegantly formulated using linear algebra.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\]\",",
    "name": "Normal Equation\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with features X and target y, find the optimal weights w that minimize the mean squared error.</p>",
    "steps": "[ {",
    "stepNumber": 2,
    "description": "Compute the dot product of X^T and y",
    "mathHtml": "\\[ \\mathbf{X}^T\\mathbf{y} = \\cdots \\]\",",
    "explanation": "This is the weighted sum of the features.\" } ],",
    "finalAnswer": "The optimal weights w\" },",
    "intuition": "Linear regression can be seen as finding the best linear combination of features that minimizes the squared error.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:20:43.665Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_linear_regression_matrix_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that can be elegantly formulated using linear algebra.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\]\",",
    "name": "Normal Equation\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset of input-output pairs, find the best-fitting linear model.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the mean-centered data\", \"mathHtml\": \"\\( \\mathbf{X} = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}} \\)\", \"explanation\": \"This helps with numerical stability and avoids bias\"} ],",
    "finalAnswer": "The estimated coefficients\" },",
    "intuition": "Linear regression can be seen as finding the best linear fit to a set of data points, which is equivalent to minimizing the mean squared error.",
    "realWorldApplications": [
      "Predicting housing prices using features like number of bedrooms and square footage"
    ],
    "tags": [
      "linear-algebra",
      "machine-learning",
      "ridge-regression"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:21:02.114Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_linear_regression_matrix_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>Linear regression is a fundamental concept in machine learning that can be elegantly formulated using linear algebra.</p><p>This formula provides a powerful tool for solving the normal equations and extending to ridge regression.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\]\",",
    "name": "Linear Regression Formula\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a dataset with features X and target y, find the optimal weights w that minimize the mean squared error.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the covariance matrix\", \"mathHtml\": \"\\[ \\mathbf{X}^T\\mathbf{X} \\]\", \"explanation\": \"This represents the sum of outer products of feature vectors.\"} ],",
    "finalAnswer": "The optimal weights w\" },",
    "intuition": "This formula provides a way to solve for the optimal weights by leveraging the properties of matrix inversion.",
    "tags": [
      "linear regression",
      "ridge regression"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:21:19.905Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_linear_regression_matrix_008",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "problem": "{",
    "statementHtml": "<p>Given a set of training data points <span class=\"math\">\\(x_1, y_1\\)</span>, ..., <span class=\"math\">\\(x_n, y_n\\)</span>, find the best-fitting linear regression line using normal equations.</p>",
    "hints": [
      "<p>Start by defining the cost function and its derivative.</p>",
      "<p>Rewrite the derivative in terms of matrix operations.</p>",
      "<p>Solve for the coefficients using the normal equation.</p>"
    ],
    "solutionHtml": "<p>To find the best-fitting linear regression line, we need to minimize the mean squared error (MSE) between our predictions and actual values. The cost function is:</p><span class=\\\"math\\\">\\\\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (h_\\theta(x_i) - y_i)^2\\\\)</span><p>The derivative of the cost function with respect to <span class=\\\"math\\\">\\\\(\\theta_0\\\\)</span> and <span class=\\\"math\\\">\\\\(\\theta_1\\\\)</span> is:</p><span class=\\\"math\\\">\\\\( \\frac{\\partial J}{\\partial \\theta_0} = - \\sum_{i=1}^n (h_\\theta(x_i) - y_i), \\\\frac{\\partial J}{\\partial \\theta_1} = - \\sum_{i=1}^n x_i(h_\\theta(x_i) - y_i)\\\\)</span><p>Rewriting the derivative in terms of matrix operations, we get:</p><span class=\\\"math\\\">\\\\( \\frac{\\partial J}{\\partial \\theta} = X^T (X\\theta - y)\\\\)</span><p>Solving for <span class=\\\"math\\\">\\\\(\\theta\\\\)</span> using the normal equation, we get:</p><span class=\\\"math\\\">\\\\( \\theta = (X^T X)^{-1} X^T y\\\\)</span><p>The final answer is:</p><span class=\\\"math\\\">\\\\( \\hat{y} = \\theta_0 + \\theta_1 x \\\\)</span>\",",
    "answerShort": "<span class=\\\"math\\\">\\\\( \\theta \\\\)</span>\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:21:49.377Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_linear_regression_matrix_009",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "problem": {
      "statementHtml": "<p>Given a dataset of <i>n</i> samples and <i>d</i> features, find the linear regression coefficients using normal equations.</p>",
      "hints": [
        "<p>Start by defining the cost function as the sum of squared errors.</p>",
        "<p>Write down the partial derivatives for each coefficient.</p>",
        "<p>Use matrix operations to simplify the expressions.</p>"
      ],
      "solutionHtml": "<p>To solve the normal equations, we need to find the values of <i>w</i> that minimize the cost function. We can do this by setting the partial derivatives equal to zero and solving for <i>w</i>.</p>\n<p><i>J = (X^T * X) * w - (X^T * y)</i></p>\n<p>Solve for <i>w</i> using matrix operations:</p>\n<p><i>w = (X^T * X)^{-1} * (X^T * y)</i></p>",
      "answerShort": "<i>w</i>"
    },
    "commonMistakes": [
      "Forgetting to add an intercept term",
      "Not using the correct matrix operations"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:22:06.136Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_linear_regression_matrix_010",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "problem": "{",
    "statementHtml": "Find the coefficients <i>w</i> and <i>b</i> in linear regression using normal equations.",
    "hints": [
      "Start by writing down the cost function.",
      "Use the fact that <i>E[(y - (wx + b))^2]</i> is minimized.",
      "Think about how to apply the normal equations for a simple case."
    ],
    "solutionHtml": "<p>To find the coefficients, we need to minimize the cost function:</p>\\(\\mathcal{L} = \\frac{1}{2}\\sum_{i=1}^n (y_i - (wx_i + b))^2\\)<br><p>Using the normal equations, we can write:</p>\\[\\begin{bmatrix} X^T X & X^T 1 \\\\ 1^T X & 1^T 1 \\end{bmatrix}\\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} X^T y \\\\ 1^T y \\end{bmatrix}\\]<br><p>Solving for <i>w</i> and <i>b</i>, we get:</p>\\[\\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} X^T X & X^T 1 \\\\ 1^T X & 1^T 1 \\end{bmatrix}^{-1}\\begin{bmatrix} X^T y \\\\ 1^T y \\end{bmatrix}\\]<br><p>The final answer is:</p>\\[\\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\left(\\frac{1}{n}X^T X + \\lambda I\\right)^{-1}\\left(\\frac{1}{n}X^T y\\right)\\]",
    "answerShort": "The coefficients are found by solving the normal equations.\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:22:32.945Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_linear_regression_matrix_011",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "problem": "{",
    "statementHtml": "<p>Given a set of training examples, find the best-fitting linear model using normal equations.</p>",
    "hints": [
      "Start by writing down the cost function for linear regression.",
      "Use the fact that the derivative of the cost function with respect to the weights is zero at the minimum.",
      "Rearrange the equation to solve for the weights."
    ],
    "solutionHtml": "<p>To find the best-fitting linear model, we need to minimize the cost function:</p>\\(\\mathcal{L} = \\frac{1}{2}\\sum_{i=1}^n (y_i - w^T x_i)^2\\)<br><p>First, take the derivative of the cost function with respect to the weights:</p>\\( \\frac{\\partial \\mathcal{L}}{\\partial w} = -\\sum_{i=1}^n (y_i - w^T x_i) x_i \\)<br><p>Set the derivative equal to zero and solve for $w$:</p>\\(\\begin{bmatrix}\\sum_{i=1}^n x_i x_i & -\\sum_{i=1}^n x_i y_i\\\\\\sum_{i=1}^n x_i y_i & \\sum_{i=1}^n y_i y_i\\end{bmatrix} w = \\begin{bmatrix}\\sum_{i=1}^n x_i y_i\\\\\\sum_{i=1}^n y_i y_i\\end{bmatrix}\\)<br><p>Solve for $w$:</p>\\(w = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\)<br><p>Finally, we can add a regularization term to the cost function to prevent overfitting:</p>\\(\\mathcal{L}_{ridge} = \\frac{1}{2}\\sum_{i=1}^n (y_i - w^T x_i)^2 + \\alpha ||w||^2\\)<br><p>Solve for $w$ using the same method as before:</p>\\(w = (\\mathbf{X}^T \\mathbf{X} + \\alpha I)^{-1} \\mathbf{X}^T \\mathbf{y}\\)<br><p>The final answer is:</p>\\(\\boxed{w = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}}\\),",
    "answerShort": "The weights that minimize the cost function\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:23:05.967Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_linear_regression_matrix_012",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>In this example, we'll solve a linear regression problem using normal equations and matrix formulation.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{X}^T \\\\mathbf{X} \\\\beta = \\\\mathbf{X}^T \\\\mathbf{y}\\]\",",
    "name": "Normal Equation\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset of features and labels, find the best-fitting linear model using normal equations.</p>",
    "hints": [
      "Hint: Use matrix operations to simplify the calculation.",
      "Think about the dot product."
    ],
    "solutionHtml": "<p>Solution goes here...</p>\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of exam scores and corresponding student ages. We want to find the best-fitting linear model that predicts scores based on age.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Write down the normal equation\", \"mathHtml\": \"\\\\[\\\\mathbf{X}^T \\\\mathbf{X} \\\\beta = \\\\mathbf{X}^T \\\\mathbf{y}\\]\", \"explanation\": \"This is the key step in solving the linear regression problem.\"}, {\"stepNumber\": 2, \"description\": \"Calculate the dot product\", \"mathHtml\": \"\\\\[\\\\mathbf{X}^T \\\\mathbf{X} = \\\\sum_{i=1}^{n} (x_i - \\\\bar{x})(x_i - \\\\bar{x})^T\\]\", \"explanation\": \"We're using the dot product to simplify the calculation.\"}, {\"stepNumber\": 3, \"description\": \"Solve for beta\", \"mathHtml\": \"\\\\[\\\\beta = (\\\\mathbf{X}^T \\\\mathbf{X})^{-1} \\\\mathbf{X}^T \\\\mathbf{y}\\]\", \"explanation\": \"Now we have the solution to our linear regression problem.\"}, {\"stepNumber\": 4, \"description\": \"Calculate the predicted values\", \"mathHtml\": \"\\\\[\\\\hat{y} = \\\\mathbf{X}\\\\beta\\]\", \"explanation\": \"This is how we use our model to make predictions.\"} ],",
    "finalAnswer": "The final answer is...\" },",
    "intuition": "Linear regression is a fundamental concept in machine learning, and understanding the normal equation is crucial for building accurate models.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:23:37.285Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_linear_regression_matrix_013",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>In this example, we'll derive the normal equations and matrix formulation of linear regression using linear algebra.</p>",
    "formula": "{",
    "latex": "\\\\[\\\\mathbf{w} = (\\\\mathbf{X}^T \\\\mathbf{X})^{-1} \\\\mathbf{X}^T \\\\mathbf{y}\\]\",",
    "name": "Linear Regression Formula\" },",
    "problem": "{",
    "statementHtml": "<p>Given a dataset of features and labels, find the best-fitting linear model using the normal equations.</p>",
    "hints": [
      "Hint: Start by defining the cost function.",
      "Think about how to minimize it."
    ],
    "solutionHtml": "<p>To solve this problem, we'll first define the cost function as half the mean squared error:</p><ul><li>\\[J(\\mathbf{w}) = \\frac{1}{2} ||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||^2</li></ul><p>Next, we'll take the derivative of J with respect to \\mathbf{w} and set it equal to zero:</p><ul><li>\\[\\frac{dJ}{d\\mathbf{w}} = -\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w})</li></ul><p>Solving for \\mathbf{w}, we get:</p>",
    "answerShort": "The normal equations\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a dataset of features (\\mathbf{x}_1, \\mathbf{x}_2) and labels (y_1, y_2). Find the best-fitting linear model using the normal equations.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Define the cost function\", \"mathHtml\": \"\\\\[J(\\\\mathbf{w}) = \\\\frac{1}{2} ||\\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{w}||^2\\]\", \"explanation\": \"This is the standard mean squared error.\"}, {\"stepNumber\": 2, \"description\": \"Take the derivative of J with respect to \\\\mathbf{w}\", \"mathHtml\": \"\\\\[\\\\frac{dJ}{d\\\\mathbf{w}} = -\\\\mathbf{X}^T (\\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{w})\\]\", \"explanation\": \"This is the gradient of our cost function.\"}, {\"stepNumber\": 3, \"description\": \"Set the derivative equal to zero and solve for \\\\mathbf{w}\", \"mathHtml\": \"\\\\[\\\\frac{dJ}{d\\\\mathbf{w}} = 0\\]\", \"explanation\": \"Now we can find the optimal weights by solving this equation.\"}, {\"stepNumber\": 4, \"description\": \"Use matrix operations to simplify the solution\", \"mathHtml\": \"\\\\[\\\\mathbf{w} = (\\\\mathbf{X}^T \\\\mathbf{X})^{-1} \\\\mathbf{X}^T \\\\mathbf{y}\\]\", \"explanation\": \"This is the final formula for our linear regression model.\"} ],",
    "finalAnswer": "The normal equations give us the optimal weights: \\\\mathbf{w}\" },",
    "intuition": "Linear regression is a fundamental concept in machine learning, and understanding its mathematical underpinnings can help you build more robust models.",
    "visualDescription": "A diagram showing the linear relationship between features and labels would be helpful for visualizing the problem.",
    "commonMistakes": [
      "Forgetting to add an intercept term",
      "Not recognizing that \\mathbf{X}^T \\mathbf{X} is a square matrix"
    ],
    "realWorldApplications": [
      "Predicting house prices based on features like number of bedrooms and square footage."
    ],
    "tags": [
      "linear regression",
      "normal equations",
      "ridge regression"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:24:24.993Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_linear_regression_matrix_014",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>In this worked example, we'll demonstrate how to solve a linear regression problem using linear algebra.</p>",
    "formula": {
      "latex": "\\[\\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y}\\]",
      "name": "Normal Equation"
    },
    "problem": {
      "statementHtml": "<p>Given a dataset of input-output pairs, find the best-fitting linear model using the normal equations.</p>",
      "hints": [
        "Hint: Use the matrix formulation"
      ],
      "solutionHtml": "",
      "answerShort": ""
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a dataset of input-output pairs, where each input is a feature vector \\mathbf{x} and each output is a target value y.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Form the design matrix",
          "mathHtml": "\\[\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n]^T\\]",
          "explanation": "We stack all input vectors into a single matrix."
        },
        {
          "stepNumber": 2,
          "description": "Compute the dot product",
          "mathHtml": "\\[\\mathbf{X}^T \\mathbf{X} = [\\mathbf{x}_1^T, \\mathbf{x}_2^T, ..., \\mathbf{x}_n^T]^T [\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n]\\]",
          "explanation": "This is the covariance matrix of the input features."
        },
        {
          "stepNumber": 3,
          "description": "Compute the cross-product",
          "mathHtml": "\\[\\mathbf{X}^T \\mathbf{y} = [\\mathbf{x}_1^T, \\mathbf{x}_2^T, ..., \\mathbf{x}_n^T]^T y\\]",
          "explanation": "This is the vector of dot products between each input feature and the target values."
        },
        {
          "stepNumber": 4,
          "description": "Solve for the coefficients",
          "mathHtml": "\\[\\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]",
          "explanation": "We use the normal equations to find the optimal coefficients."
        },
        {
          "stepNumber": 5,
          "description": "Compute the predicted values",
          "mathHtml": "\\[\\hat{y} = \\mathbf{x}^T \\beta\\]",
          "explanation": "This is the predicted output value for a given input vector."
        }
      ],
      "finalAnswer": ""
    },
    "intuition": "<p>The key insight here is that linear regression can be formulated as a matrix equation, which allows us to leverage the power of linear algebra to find the optimal coefficients.</p>",
    "visualDescription": "",
    "commonMistakes": [
      "Forgetting to include the bias term"
    ],
    "realWorldApplications": [
      "Predicting housing prices based on features like number of bedrooms and square footage"
    ],
    "tags": [
      "linear regression",
      "normal equations"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:25:03.542Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_linear_regression_matrix_015",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "linear_regression_matrix",
    "title": "Linear Regression via Linear Algebra",
    "contentHtml": "<p>In this worked example, we'll use linear algebra to solve a linear regression problem.</p>",
    "formula": {
      "latex": "\\[\\mathbf{X} \\beta = \\mathbf{y}\\]",
      "name": "Linear Regression Equation"
    },
    "problem": {
      "statementHtml": "<p>Given a dataset of input features X and output labels y, find the coefficients β that minimize the mean squared error (MSE) between the predicted values and actual values.</p>",
      "hints": [
        "Hint: Use the normal equations to solve for β"
      ],
      "solutionHtml": "<p>Solution:</p><ul><li>Step 1: Calculate the covariance matrix Σ = X^T * X</li><li>Step 2: Calculate the cross-covariance vector μ = X^T * y</li><li>Step 3: Solve for β using Σ * β = μ</li></ul>",
      "answerShort": "β = (Σ^(-1) * μ)"
    },
    "workedExample": {
      "problemHtml": "<p>Suppose we have a dataset of exam scores X and corresponding student grades y. We want to find the linear regression coefficients β that best predict the grade given the score.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the covariance matrix Σ",
          "mathHtml": "\\[\\Sigma = \\frac{1}{n} X^T * X\\]",
          "explanation": "We're calculating the variance of the input features."
        },
        {
          "stepNumber": 2,
          "description": "Calculate the cross-covariance vector μ",
          "mathHtml": "\\[\\mu = \\frac{1}{n} X^T * y\\]",
          "explanation": "We're calculating the expected value of the output given the input."
        }
      ],
      "finalAnswer": "β = (Σ^(-1) * μ)"
    },
    "intuition": "Linear regression can be seen as finding the best linear approximation to a set of data points.",
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:25:30.194Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]