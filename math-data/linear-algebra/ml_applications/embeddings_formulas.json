[
  {
    "id": "la_for_embeddings_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "title": "Embedding Formula",
    "contentHtml": "<p>Word embeddings are a fundamental concept in representation learning.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{v} = \\frac{\\sum_{i=1}^{n}\\mathbf{x}_i\\cdot \\mathbf{w}_i}{\\left\\lVert \\sum_{i=1}^{n}\\mathbf{x}_i\\cdot \\mathbf{w}_i \\right\\rVert} \\]\",",
    "name": "Embedding Formula",
    "variants": "[] },",
    "intuition": "This formula represents the process of computing a word's embedding as a weighted sum of its context words, where the weights are learned during training.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:47:06.280Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_embeddings_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "title": "Embedding Formula",
    "contentHtml": "<p>Embeddings are a fundamental concept in representation learning.</p><p>The formula we'll explore today is the core of many embedding algorithms.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{W} = \\sigma(\\mathbf{X} \\mathbf{A}) \\]\",",
    "name": "Embedding Formula\" },",
    "intuition": "This formula maps high-dimensional input data to a lower-dimensional representation, allowing for efficient processing and analysis.",
    "realWorldApplications": [
      "Word embeddings in natural language processing"
    ],
    "tags": [
      "embeddings",
      "representation learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:47:18.036Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_embeddings_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "title": "Embedding Formula",
    "contentHtml": "<p>Word embeddings are a fundamental concept in representation learning.</p><p>The formula we'll explore today is a key component of many popular embedding algorithms.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{v} = \\frac{\\sum_{i=1}^n w_i \\mathbf{x}_i }{\\left\\lVert \\sum_{i=1}^n w_i \\mathbf{x}_i \\right\\rVert}\\]\",",
    "name": "Embedding Formula",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a set of words and their corresponding vectors in a high-dimensional space.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Standardize the word vectors\", \"mathHtml\": \"\\[ \\mathbf{x}_i' = \\frac{\\mathbf{x}_i - \\mu}{\\sigma}\\]\", \"explanation\": \"This helps prevent features with large ranges from dominating the embedding.\"} ],",
    "finalAnswer": "The resulting vector represents the embedded word.\" },",
    "intuition": "Embeddings aim to capture semantic relationships between words. This formula helps preserve those relationships while reducing dimensionality.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:47:36.517Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_embeddings_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "embeddings",
    "title": "Word Embeddings Formula",
    "contentHtml": "<p>Word embeddings transform words into vectors that capture semantic relationships.</p>",
    "formula": "{",
    "latex": "\\\\[ x \\sim W^T w \\\\]\",",
    "name": "Word Embedding Formula\" },",
    "workedExample": "{",
    "problemHtml": "Given a word 'dog' and its embedding vector [0.5, 0.2], find the most similar word.",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the dot product\", \"mathHtml\": \"\\\\[ x \\cdot w \\\\]\", \"explanation\": \"Compare the word's vector with each word's embedding\"} ],",
    "finalAnswer": "The most similar word is 'cat' with an embedding vector [-0.2, 0.3]\" },",
    "intuition": "Word embeddings capture semantic relationships by mapping words to vectors that preserve their context.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:47:50.859Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]