[
  {
    "id": "la_con_matrix_calculus_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Gradients",
    "contentHtml": "<p>In linear algebra, we often encounter matrix expressions that depend on some parameters or variables. To optimize these expressions, we need to compute their derivatives with respect to those variables.</p><p>Formally, the derivative of a matrix expression is another matrix that captures how the original expression changes when its inputs change.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} [A(x)] = \\frac{d}{dx} \\left[ \\sum_{i=1}^n A_i x^i \\right] \\]",
      "name": "Matrix Expression Derivative",
      "variants": []
    },
    "intuition": "Think of the derivative as a 'sensitivity' matrix that tells us how much each output changes when we perturb the inputs.",
    "realWorldApplications": [
      "Gradient descent in neural networks"
    ],
    "commonMistakes": [
      "Forgetting to chain rule derivatives across matrices"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:25:45.500Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_matrix_calculus_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>When working with matrix expressions in machine learning, it's crucial to understand how to compute derivatives and Jacobians. This concept is fundamental to optimizing model parameters.</p><p>In this card, we'll explore the basics of matrix calculus, including gradients of quadratic forms, Jacobians, and Hessians.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (x^T A x) = 2x^T A\\]",
      "name": "Gradient of Quadratic Form"
    },
    "intuition": "Think of the Jacobian as a matrix that captures how each input affects the output. This is particularly important in neural networks, where we need to compute gradients for backpropagation.",
    "realWorldApplications": [
      "Optimizing model parameters in neural networks"
    ],
    "commonMistakes": [
      "Forgetting to transpose matrices when computing derivatives"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:00.873Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_matrix_calculus_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Gradients",
    "contentHtml": "<p>Matrix calculus is a fundamental concept in machine learning, allowing us to compute gradients of matrix expressions. This enables efficient optimization of complex models.</p><p>In this card, we'll explore the derivatives of matrix expressions, including the gradient of quadratic forms, Jacobian, and Hessian.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (A x) = A^T \\]",
      "name": "Matrix derivative"
    },
    "intuition": "Think of a matrix as a set of linear transformations. The derivative of a matrix expression represents the rate of change of these transformations with respect to some input.",
    "realWorldApplications": [
      "Gradient descent for neural networks"
    ],
    "commonMistakes": [
      "Failing to account for the matrix structure in computations",
      "Incorrectly applying chain rule"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:14.884Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In linear algebra, derivatives are crucial in optimization problems, such as those encountered in machine learning. This formula allows us to compute the derivative of a matrix expression with respect to another matrix.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A}^T\\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{A}^T\\]\",",
    "name": "Derivative of Matrix Expression",
    "variants": "[] },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the matrix expression <code>\\(\\mathbf{X}\\mathbf{w}\\)</code> with respect to <code>\\(\\mathbf{w}\\)</code>, where <code>\\(\\mathbf{X}\\)</code> is a data matrix and <code>\\(\\mathbf{w}\\)</code> is the weight vector.</p>\",",
    "steps": "[ {",
    "stepNumber": 2,
    "description": "Use the product rule",
    "mathHtml": "\\[\\frac{\\partial (\\mathbf{X}\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\",",
    "explanation": "The product rule helps us compute the derivative of a product.\" } ],",
    "finalAnswer": "<code>\\(\\frac{\\partial \\mathbf{X}\\mathbf{w}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\)</code>\" },",
    "intuition": "This formula is essential in optimization problems, such as logistic regression and neural networks.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:26:43.031Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>The derivative of a matrix expression is crucial in machine learning to optimize model parameters. This formula helps us compute the gradient of quadratic forms and Jacobian/Hessian matrices.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A}^T\\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{A}\\]\",",
    "name": "Chain Rule for Matrix Expressions\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the matrix expression <code>\\mathbf{y} = \\mathbf{X}^T\\mathbf{w}</code>, where <code>\\mathbf{X}</code> is a design matrix and <code>\\mathbf{w}</code> is a weight vector.</p>\",",
    "steps": "[ {",
    "stepNumber": 1,
    "description": "Apply the chain rule",
    "mathHtml": "\\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{w}} = \\frac{\\partial (\\mathbf{X}^T\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\",",
    "explanation": "The chain rule helps us compute the derivative of a composite function.\" } ],",
    "finalAnswer": "\\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\]\" },",
    "intuition": "This formula is essential in machine learning to optimize model parameters. It helps us compute the gradient of quadratic forms and Jacobian/Hessian matrices, which are crucial for many algorithms.",
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:06.043Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In many machine learning algorithms, we need to compute derivatives of matrix expressions with respect to some parameters. This card introduces the key formulas and concepts for doing so.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{A} \\mathbf{x}}{\\partial x_i} = \\mathbf{A} e_i\\]\",",
    "name": "Chain Rule for Matrix Products\" },",
    "workedExample": "{",
    "problemHtml": "<p>Compute the derivative of the matrix product $\\mathbf{W} (\\mathbf{X} \\mathbf{x})$ with respect to $x_1$.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"\\[\\frac{\\partial}{\\partial x_1} \\mathbf{W} (\\mathbf{X} \\mathbf{x}) = \\mathbf{W} \\mathbf{X} e_1\\]\", \"explanation\": \"The derivative of a matrix product is the product of the derivative of the first factor and the second factor.\"} ],",
    "finalAnswer": "The answer\" },",
    "intuition": "Understanding how to compute derivatives of matrix expressions is crucial in many machine learning algorithms, such as gradient descent.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:24.926Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_matrix_calculus_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In linear algebra, derivatives of matrix expressions are crucial in machine learning. This formula helps us compute the gradient of quadratic forms.</p>",
    "formula": "{",
    "latex": "\\[\\frac{\\partial \\mathbf{x}^T A \\mathbf{x}}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\]\",",
    "name": "Gradient of Quadratic Form\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the quadratic form <code>\\mathbf{w}^T \\mathbf{x}^T A \\mathbf{x}</code> with respect to <code>\\mathbf{w}</code>.</p>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"\\[ \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}^T A \\mathbf{x}) = ... \\]\", \"explanation\": \"The derivative of a quadratic form with respect to its coefficients is another quadratic form.\"} ],",
    "finalAnswer": "The answer is <code>2\\mathbf{x}\\mathbf{x}^T</code>\" },",
    "intuition": "This formula helps us compute the gradient of quadratic forms, which is essential in machine learning for optimization.",
    "realWorldApplications": [
      "Gradient descent algorithm"
    ],
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:27:45.178Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_matrix_calculus_008",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>In this theorem, we explore the fundamental concepts of matrix calculus, specifically derivatives of matrix expressions.</p>",
    "formula": {
      "latex": "\\[ \\frac{\\partial A}{\\partial x} = \\sum_{i=1}^n \\frac{\\partial a_i}{\\partial x} e_i \\]",
      "name": "Matrix Derivative"
    },
    "theorem": {
      "statement": "\\[ \\text{The derivative of } A(x) = \\sum_{i=1}^n a_i(x) e_i \\text{ is } \\frac{\\partial A}{\\partial x} = \\sum_{i=1}^n \\frac{\\partial a_i}{\\partial x} e_i \\]",
      "proofSketch": "The proof involves applying the chain rule and linearity of matrix multiplication."
    },
    "intuition": "This theorem provides a framework for computing derivatives of complex matrix expressions, which is crucial in machine learning when optimizing models.",
    "realWorldApplications": [
      "Gradient descent algorithm"
    ],
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:02.387Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_matrix_calculus_009",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>In linear algebra, we often encounter matrix expressions that depend on some parameters. Taking derivatives of these expressions is crucial in machine learning to optimize model performance.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (AB) = A\\left( \\frac{dB}{dx} \\right) \\]",
      "name": "Chain rule for matrix products"
    },
    "theorem": {
      "statement": "\\[ \\frac{d}{dx} (x^T Ax) = 2x^T A \\]",
      "proofSketch": "The proof involves applying the chain rule and properties of matrix multiplication."
    },
    "intuition": "Understanding derivatives of matrix expressions helps us optimize quadratic forms, which is essential in many machine learning algorithms.",
    "realWorldApplications": [
      "Optimizing neural network weights"
    ],
    "tags": [
      "matrix calculus",
      "linear algebra",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:16.993Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_matrix_calculus_010",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "problem": "{",
    "statementHtml": "<p>Find the derivative of the matrix expression <span class='math'>\\( \\mathbf{A}^T \\mathbf{B} \\)</span>.</p>\",",
    "hints": "[ \"Start by applying the chain rule to the transpose operation.\", \"Focus on the inner product between <span class='math'>\\( \\mathbf{a}_i \\)</span> and <span class='math'>\\( \\mathbf{b}_j \\)</span>.\", \"Use the fact that the derivative of a matrix is another matrix.\" ],",
    "solutionHtml": "<p>To find the derivative, we'll apply the chain rule to the transpose operation:</p><ul><li>We know that <span class='math'>\\( \\frac{\\partial}{\\partial \\mathbf{A}} \\mathbf{A}^T \\)</span> is a matrix with entries <span class='math'>\\( \\frac{\\partial}{\\partial a_{ij}} a_{ji} \\)</span>.</li><li>Now, we'll focus on the inner product between <span class='math'>\\( \\mathbf{a}_i \\)</span> and <span class='math'>\\( \\mathbf{b}_j \\)</span>:</li></ul><p>The derivative of this expression is:</p><span class='math'>\\[ \\frac{\\partial}{\\partial \\mathbf{A}} \\( \\mathbf{A}^T \\mathbf{B} \\) = \\( \\mathbf{B}^T \\otimes \\mathbf{I}_n \\) \\]</span>\",",
    "answerShort": "<span class='math'>\\( \\frac{\\partial}{\\partial \\mathbf{A}} \\( \\mathbf{A}^T \\mathbf{B} \\) = \\( \\mathbf{B}^T \\otimes \\mathbf{I}_n \\) \\)</span>\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:43.751Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_matrix_calculus_011",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "problem": {
      "statementHtml": "<p>Find the derivative of the matrix expression <i>M</i> = <i>A</i><sup>T</sup><i>B</i></p>",
      "hints": [
        "Start by applying the chain rule to the transpose operation.",
        "Recall that the derivative of a product is the sum of products of derivatives and original terms.",
        "Use the fact that the derivative of a matrix is another matrix."
      ],
      "solutionHtml": "<p>To find the derivative, we apply the chain rule:</p>\n<p><i>∂M/∂A</i> = <i>B</i><sup>T</sup></p>\n<p><i>∂M/∂B</i> = <i>A</i></p>",
      "answerShort": "<i>M</i> = <i>A</i><sup>T</sup><i>B</i>"
    },
    "commonMistakes": [
      "Forgetting to apply the chain rule when differentiating the transpose operation.",
      "Not recognizing that the derivative of a matrix is another matrix."
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:59.666Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_matrix_calculus_012",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "problem": {
      "statementHtml": "<p>Find the derivative of the matrix expression <span class=\"latex\">\\(M = X^T Y \\)</span>, where <span class=\"latex\">\\(X \\in \\mathbb{R}^{n \\times d}\\)</span> and <span class=\"latex\">\\(Y \\in \\mathbb{R}^{m \\times d}\\)</span>.</p>",
      "hints": [
        "<p>Start by applying the chain rule to the expression.</p>",
        "<p>Identify the derivative of the matrix product.</p>",
        "<p>Use the fact that the derivative of a scalar is zero, unless it's with respect to one of its variables.</p>"
      ],
      "solutionHtml": "<p>To find the derivative, we'll use the chain rule and the fact that the derivative of a matrix product is the product of the derivatives of the matrices. We get:</p><p><span class=\"latex\">\\(\\frac{dM}{dX} = Y^T \\)</span>.</p>",
      "answerShort": "<span class=\"latex\">\\(Y^T \\)</span>"
    },
    "commonMistakes": [
      "Forgetting to apply the chain rule",
      "Not identifying the derivative of the matrix product correctly"
    ],
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:29:17.517Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_matrix_calculus_013",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "problem": "{",
    "statementHtml": "<p>Find the derivative of the matrix expression <span class='latex'>\\mathbf{X}^T \\mathbf{A} \\mathbf{X}</span>, where <span class='latex'>\\mathbf{X}</span> is a column vector and <span class='latex'>\\mathbf{A}</span> is a square matrix.</p>\",",
    "hints": [
      "<p>The derivative of the dot product is given by the outer product.</p>",
      "<p>Apply the chain rule to the matrix expression.</p>",
      "<p>Use the fact that the derivative of a quadratic form is twice its Hessian.</p>"
    ],
    "solutionHtml": "<p>To find the derivative, we can use the chain rule:</p><ul><li>We differentiate the outer product <span class='latex'>\\mathbf{X}^T \\mathbf{A}</span> with respect to <span class='latex'>\\mathbf{X}</span>, which gives us <span class='latex'>\\mathbf{A}</span>.</li><li>We then multiply this result by <span class='latex'>\\mathbf{X}^T</span> to get the final derivative.</li></ul>\",",
    "answerShort": "<p>The derivative is <span class='latex'>\\mathbf{X}^T \\mathbf{A}</span>.</p>\" },",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:29:39.254Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_matrix_calculus_014",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In this worked example, we'll demonstrate how to compute derivatives of matrix expressions in linear algebra.</p>",
    "workedExample": "{",
    "problemHtml": "Find the derivative of <math>\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}</math> with respect to <math>\\mathbf{x}</math>.\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"<math>\\frac{\\partial}{\\partial \\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\frac{1}{2}\\mathbf{A}</math>\", \"explanation\": \"We're applying the chain rule to differentiate the quadratic form with respect to <math>\\mathbf{x}</math>.\"}, {\"stepNumber\": 2, \"description\": \"Expand the derivative\", \"mathHtml\": \"<math>= \\frac{1}{2}\\mathbf{A}(\\mathbf{x}^T + \\mathbf{x})</math>\", \"explanation\": \"We're expanding the derivative by distributing the matrix <math>\\mathbf{A}</math>.\"}, {\"stepNumber\": 3, \"description\": \"Simplify the expression\", \"mathHtml\": \"<math>= \\frac{1}{2}\\mathbf{A}(\\mathbf{x}^T + \\mathbf{x}) = \\mathbf{A}\\mathbf{x}</math>\", \"explanation\": \"We're simplifying the expression by combining like terms.\"}, {\"stepNumber\": 4, \"description\": \"Check the result\", \"mathHtml\": \"<math>= \\frac{\\partial}{\\partial \\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\mathbf{A}\\mathbf{x}</math>\", \"explanation\": \"The final result is the derivative of the quadratic form with respect to <math>\\mathbf{x}</math>.\"} ],",
    "finalAnswer": "<math>\\frac{\\partial}{\\partial \\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\mathbf{A}\\mathbf{x}</math>\" },",
    "intuition": "Derivatives of matrix expressions are crucial in machine learning, as they help us optimize loss functions and update model parameters.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:30:11.168Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_matrix_calculus_015",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In this worked example, we'll derive the derivative of a matrix expression using the chain rule.</p>",
    "workedExample": "{",
    "problemHtml": "Find the derivative of <math>\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}</math> with respect to <math>\\mathbf{x}</math>\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Apply the chain rule\", \"mathHtml\": \"<math>\\frac{\\partial}{\\partial \\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right)</math>\", \"explanation\": \"We're applying the chain rule to find the derivative of a composite function.\"}, {\"stepNumber\": 2, \"description\": \"Expand the expression\", \"mathHtml\": \"<math>\\frac{1}{2}\\left(\\mathbf{A} + \\mathbf{A}^T\\right)\\mathbf{x}</math>\", \"explanation\": \"We're expanding the expression using the fact that <math>\\mathbf{x}^T\\mathbf{A}</math> is a linear combination.\"}, {\"stepNumber\": 3, \"description\": \"Simplify the derivative\", \"mathHtml\": \"<math>\\left(\\frac{1}{2}\\left(\\mathbf{A} + \\mathbf{A}^T\\right)\\right)</math>\", \"explanation\": \"We're simplifying the derivative by recognizing that it's a constant.\"}, {\"stepNumber\": 4, \"description\": \"Find the gradient\", \"mathHtml\": \"<math>\\nabla_{\\mathbf{x}} \\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\left(\\frac{1}{2}\\left(\\mathbf{A} + \\mathbf{A}^T\\right)\\right)\\mathbf{x}</math>\", \"explanation\": \"We're finding the gradient by taking the derivative with respect to <math>\\mathbf{x}</math>.\"}, {\"stepNumber\": 5, \"description\": \"Find the Hessian\", \"mathHtml\": \"<math>H_{\\mathbf{x}} \\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\mathbf{A}</math>\", \"explanation\": \"We're finding the Hessian by taking the second derivative with respect to <math>\\mathbf{x}</math>.\"} ],",
    "finalAnswer": "<math>\\nabla_{\\mathbf{x}} \\left(\\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) = \\left(\\frac{1}{2}\\left(\\mathbf{A} + \\mathbf{A}^T\\right)\\right)\\mathbf{x}</math>\" },",
    "intuition": "In machine learning, understanding the derivatives of matrix expressions is crucial for optimizing model parameters.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:30:48.798Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_matrix_calculus_016",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In this worked example, we'll explore how to compute derivatives of matrix expressions in machine learning.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (A x) = A \\]",
      "name": "Matrix Derivative"
    },
    "problem": {
      "statementHtml": "<p>Compute the derivative of the quadratic form Q(x) = x^T A x with respect to x.</p>",
      "hints": [
        "Hint: Use the chain rule and matrix properties"
      ],
      "solutionHtml": "<p>To compute the derivative, we'll use the chain rule:</p><ul><li>We'll start by computing the derivative of the quadratic form Q(x) = x^T A x with respect to x.</li><li>Using the chain rule, we can rewrite the derivative as \\( \\frac{d}{dx} (x^T A x) = 2 x^T A \\).</li><li>To find the final answer, we'll multiply both sides by \\( \\frac{1}{2} \\) to obtain \\( \\frac{dQ}{dx} = x^T A \\).</li></ul>",
      "answerShort": "x^T A"
    },
    "workedExample": {
      "problemHtml": "<p>Compute the derivative of the matrix expression M(x) = (A + B)x with respect to x.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Apply the chain rule",
          "mathHtml": "\\[ \\frac{d}{dx} ((A + B) x) = ? \\]",
          "explanation": "We'll use the chain rule to compute the derivative."
        },
        {
          "stepNumber": 2,
          "description": "Expand the expression",
          "mathHtml": "\\[ \\frac{d}{dx} ((A + B) x) = (\\frac{d}{dx} A) x + (\\frac{d}{dx} B) x \\]",
          "explanation": "We'll expand the expression using the definition of matrix multiplication."
        },
        {
          "stepNumber": 3,
          "description": "Simplify the derivative",
          "mathHtml": "\\[ \\frac{d}{dx} ((A + B) x) = A + B \\]",
          "explanation": "We'll simplify the derivative by combining like terms."
        }
      ],
      "finalAnswer": "A + B"
    },
    "intuition": "The key insight is that the derivative of a matrix expression can be computed using the chain rule and matrix properties.",
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:31:20.792Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_matrix_calculus_017",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Derivatives of Matrix Expressions",
    "contentHtml": "<p>In this worked example, we'll derive the derivative of a matrix expression using the chain rule.</p>",
    "formula": "{",
    "latex": "\\\\[ \\\\frac{d}{dx} (A \\* x) = A^T \\\\]\",",
    "name": "Chain Rule for Matrix Expressions\" },",
    "workedExample": "{",
    "problemHtml": "<p>Find the derivative of the matrix expression <code>(x^2 I + A)</code>.</p>",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Apply the chain rule to the outer product",
        "mathHtml": "\\[ \\frac{d}{dx} (x^2 I) = 2xI \\]",
        "explanation": "We apply the chain rule to the outer product, treating <code>x</code> as the input."
      },
      {
        "stepNumber": 2,
        "description": "Derive the derivative of the matrix A",
        "mathHtml": "\\[ \\frac{d}{dx} (A) = 0 \\]",
        "explanation": "Since A is a constant matrix, its derivative is zero."
      },
      {
        "stepNumber": 3,
        "description": "Combine the derivatives using the product rule",
        "mathHtml": "\\[ \\frac{d}{dx} (x^2 I + A) = 2xI \\]",
        "explanation": "We combine the derivatives using the product rule, treating <code>x</code> as the input."
      },
      {
        "stepNumber": 4,
        "description": "Simplify the result",
        "mathHtml": "\\[ \\frac{d}{dx} (x^2 I + A) = 2xI \\]",
        "explanation": "The final answer is a matrix expression that depends on <code>x</code>."
      }
    ],
    "finalAnswer": "2xI\" },",
    "intuition": "Derivatives of matrix expressions are crucial in machine learning, particularly when working with neural networks and optimization algorithms.",
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:31:46.547Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]