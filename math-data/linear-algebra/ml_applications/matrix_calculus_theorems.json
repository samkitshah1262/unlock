[
  {
    "id": "la_thm_matrix_calculus_008",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>In this theorem, we explore the fundamental concepts of matrix calculus, specifically derivatives of matrix expressions.</p>",
    "formula": {
      "latex": "\\[ \\frac{\\partial A}{\\partial x} = \\sum_{i=1}^n \\frac{\\partial a_i}{\\partial x} e_i \\]",
      "name": "Matrix Derivative"
    },
    "theorem": {
      "statement": "\\[ \\text{The derivative of } A(x) = \\sum_{i=1}^n a_i(x) e_i \\text{ is } \\frac{\\partial A}{\\partial x} = \\sum_{i=1}^n \\frac{\\partial a_i}{\\partial x} e_i \\]",
      "proofSketch": "The proof involves applying the chain rule and linearity of matrix multiplication."
    },
    "intuition": "This theorem provides a framework for computing derivatives of complex matrix expressions, which is crucial in machine learning when optimizing models.",
    "realWorldApplications": [
      "Gradient descent algorithm"
    ],
    "tags": [
      "matrix calculus",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:02.387Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_matrix_calculus_009",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "ml_applications",
    "topic": "matrix_calculus",
    "title": "Matrix Calculus: Derivatives and Jacobian",
    "contentHtml": "<p>In linear algebra, we often encounter matrix expressions that depend on some parameters. Taking derivatives of these expressions is crucial in machine learning to optimize model performance.</p>",
    "formula": {
      "latex": "\\[ \\frac{d}{dx} (AB) = A\\left( \\frac{dB}{dx} \\right) \\]",
      "name": "Chain rule for matrix products"
    },
    "theorem": {
      "statement": "\\[ \\frac{d}{dx} (x^T Ax) = 2x^T A \\]",
      "proofSketch": "The proof involves applying the chain rule and properties of matrix multiplication."
    },
    "intuition": "Understanding derivatives of matrix expressions helps us optimize quadratic forms, which is essential in many machine learning algorithms.",
    "realWorldApplications": [
      "Optimizing neural network weights"
    ],
    "tags": [
      "matrix calculus",
      "linear algebra",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 4,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T14:28:16.993Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]