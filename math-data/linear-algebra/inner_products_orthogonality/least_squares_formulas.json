[
  {
    "id": "la_for_least_squares_004",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems: Normal Equations",
    "contentHtml": "<p>In many machine learning and data analysis scenarios, we encounter overdetermined systems of linear equations.</p><p>The normal equations provide a way to solve these systems efficiently.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b} \\]\",",
    "name": "Normal Equation\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a set of data points, find the best-fitting line using least squares.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the mean of each feature\", \"mathHtml\": \"\\( \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\)\", \"explanation\": \"This helps to reduce the impact of outliers\"}, {\"stepNumber\": 2, \"description\": \"Calculate the covariance matrix\", \"mathHtml\": \"\\[ \\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T \\]\", \"explanation\": \"This measures the spread of the data\"} ],",
    "finalAnswer": "The coefficients of the best-fitting line\" },",
    "intuition": "The normal equations provide a way to find the minimum squared error between our model and the actual data.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:26:18.820Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_least_squares_005",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems",
    "contentHtml": "<p>In linear algebra, least squares problems arise when we want to find the best-fitting line or plane that approximates a set of data points.</p><p>This is particularly important in machine learning and statistics where we often encounter noisy data.</p>",
    "formula": "{",
    "latex": "\\[\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\\]\",",
    "name": "Normal Equations\" },",
    "workedExample": "{",
    "problemHtml": "<p>Suppose we have a set of data points (x, y) that are noisy measurements of a linear relationship.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the mean-centered data\", \"mathHtml\": \"\\\\[\\\\bar{x}, \\\\bar{y}\\]\", \"explanation\": \"This helps to reduce the effect of outliers\"}, {\"stepNumber\": 2, \"description\": \"Construct the design matrix A\", \"mathHtml\": \"\\\\[A = [x_1, x_2, ..., x_n; 1, 1, ..., 1]\\]\", \"explanation\": \"The extra column of ones represents the bias term\"}, {\"stepNumber\": 3, \"description\": \"Compute the normal equations\", \"mathHtml\": \"\\\\[\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\\]\", \"explanation\": \"This is a system of linear equations that we can solve for x\"} ],",
    "finalAnswer": "The solution vector x\" },",
    "intuition": "The key insight here is that the normal equations provide a way to find the best-fitting line or plane by minimizing the sum of squared errors.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:26:44.478Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_least_squares_006",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems: Normal Equations",
    "contentHtml": "<p>In linear algebra, least squares problems involve finding the best-fitting line or hyperplane to a set of data points.</p><p>This is achieved by minimizing the sum of squared errors between the actual and predicted values.</p>",
    "formula": "{",
    "latex": "\\[ \\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b} \\]\",",
    "name": "Normal Equation\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a set of data points (x, y), find the best-fitting line.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the mean values for x and y\", \"mathHtml\": \"\\( \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\)\", \"explanation\": \"This helps to center the data\"}, {\"stepNumber\": 2, \"description\": \"Form the augmented matrix\", \"mathHtml\": \"\\( \\begin{bmatrix} x_1 & y_1 \\\\ x_2 & y_2 \\\\ \\vdots & \\vdots \\\\ x_n & y_n \\end{bmatrix} \\)\", \"explanation\": \"This represents the data points\"} ],",
    "finalAnswer": "The coefficients of the best-fitting line\" },",
    "intuition": "Least squares problems are essential in machine learning, where they help to find the optimal hyperplane for classification or regression tasks.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:27:07.111Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_for_least_squares_007",
    "subject": "linear_algebra",
    "type": "formula",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems",
    "contentHtml": "<p>In linear algebra, least squares problems arise when we want to find the best fit line or plane that minimizes the sum of squared errors.</p>",
    "formula": "{",
    "latex": "\\[\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\" },",
    "workedExample": "{",
    "problemHtml": "<p>Given a set of data points {(x_i, y_i)}, find the best fit line that minimizes the sum of squared errors.</p>",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Compute the mean-centered data\", \"mathHtml\": \"\\(y_i - \\bar{y}\\)\", \"explanation\": \"This helps to reduce the impact of outliers\"}, {\"stepNumber\": 2, \"description\": \"Calculate the covariance matrix and cross-product matrix\", \"mathHtml\": \"\\[\\Sigma = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\]\", \"explanation\": \"This helps to capture the linear relationship between x and y\"}, {\"stepNumber\": 3, \"description\": \"Solve for the coefficients using the normal equations\", \"mathHtml\": \"\\[\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\", \"explanation\": \"This gives us the best fit line that minimizes the sum of squared errors\"} ],",
    "finalAnswer": "The coefficients w\" },",
    "intuition": "Least squares problems are a fundamental concept in linear algebra, with applications in machine learning and data analysis.",
    "realWorldApplications": [
      "Regression analysis for predicting stock prices"
    ],
    "tags": [
      "least squares",
      "linear regression",
      "machine learning"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:27:33.147Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]