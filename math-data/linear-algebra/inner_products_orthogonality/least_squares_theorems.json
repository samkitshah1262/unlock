[
  {
    "id": "la_thm_least_squares_008",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems",
    "contentHtml": "<p>In linear algebra, least squares problems arise when we want to find the best-fitting line or plane that minimizes the sum of squared errors.</p>",
    "formula": {
      "latex": "\\[ \\mathbf{A}^\\top \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b} \\]",
      "name": "Normal Equation"
    },
    "theorem": {
      "statement": "\\[ \\text{If } \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, \\mathbf{x} \\in \\mathbb{R}^n, \\text{ and } \\mathbf{b} \\in \\mathbb{R}^m, \\text{ then the least squares solution satisfies } \\mathbf{A}^\\top \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}. \\]"
    },
    "intuition": "The normal equation represents the condition for the best-fitting line or plane to minimize the sum of squared errors.",
    "realWorldApplications": [
      "Core concept in machine learning, used in regression tasks like linear regression and neural networks."
    ],
    "tags": [
      "Linear Algebra",
      "Least Squares",
      "Regression"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:27:53.058Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_least_squares_009",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Theorem",
    "contentHtml": "<p>The least squares theorem provides a way to find the best-fitting linear model in an overdetermined system.</p>",
    "formula": {
      "latex": "\\[\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\\]",
      "name": "Normal Equations"
    },
    "theorem": {
      "statement": "\\[\\min_{\\mathbf{x}} ||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2\\]",
      "proofSketch": "The proof involves showing that the normal equations are a necessary and sufficient condition for the minimum in the least squares problem."
    },
    "intuition": "This theorem shows that the best-fitting linear model is the one that minimizes the sum of squared errors.",
    "realWorldApplications": [
      "Regression analysis",
      "Linear regression"
    ],
    "tags": [
      "linear-algebra",
      "least-squares"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:28:10.059Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]