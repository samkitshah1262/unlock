[
  {
    "id": "la_con_orthogonal_sets_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal to each other.</p><p>Two vectors <math>\\mathbf{u}</math> and <math>\\mathbf{v}</math> are said to be orthogonal if their dot product is zero: <math>\\mathbf{u} \\cdot \\mathbf{v} = 0</math>.</p><p>A set of vectors is called orthonormal if each vector has length one (i.e., it's a unit vector) and all pairs of vectors in the set are orthogonal.</p>\",",
    "formula": "{",
    "latex": "\\\\[ \\\\mathbf{u} \\cdot \\\\mathbf{v} = 0 \\\\]\",",
    "name": "Orthogonality condition\" },",
    "intuition": "Think of orthonormal sets as a 'coordinate system' where each vector represents a direction. Just like how we can represent points in space using coordinates, these vectors help us navigate the space of possible linear transformations.",
    "realWorldApplications": [
      "In machine learning, orthonormal bases are used to transform data into more suitable forms for analysis."
    ],
    "commonMistakes": [
      "Don't confuse orthogonality with parallelism; just because two vectors are orthogonal doesn't mean they're parallel."
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:00:28.588Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_orthogonal_sets_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal to each other.</p><p>Two vectors <math>\\mathbf{u}</math> and <math>\\mathbf{v}</math> are said to be orthogonal if their dot product is zero: <math>\\mathbf{u} \\cdot \\mathbf{v} = 0</math>.</p>\",",
    "formula": "{",
    "latex": "\\mathbf{u} \\perp \\mathbf{v} : \\mathbf{u} \\cdot \\mathbf{v} = 0",
    "name": "Orthogonality\" },",
    "intuition": "Think of orthogonal vectors as being 'perpendicular' to each other in space. This property is crucial in many applications, including machine learning and computer graphics.",
    "realWorldApplications": [
      "In neural networks, the weights between layers are often chosen to be orthogonal to ensure independence and prevent overfitting."
    ],
    "commonMistakes": [
      "Don't confuse orthogonality with parallelism; just because two vectors are orthogonal doesn't mean they're parallel."
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:00:45.893Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_orthogonal_sets_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal.</p><p>Orthogonality refers to the property that the dot product between two vectors is zero. This means that if <math>\\mathbf{a}</math> and <math>\\mathbf{b}</math> are orthogonal, then <math>\\mathbf{a} \\cdot \\mathbf{b} = 0</math>.</p><p>Orthonormal sets take this a step further by ensuring that the vectors have length one (i.e., they are normalized). This property is crucial in many applications, including machine learning and computer graphics.</p>\",",
    "formula": "{",
    "latex": "\\[\\mathbf{a} \\cdot \\mathbf{b} = 0\\]",
    "name": "Orthogonality Condition\" },",
    "intuition": "Think of orthogonal vectors as being 'perpendicular' to each other. This property allows us to decompose a vector into its components in a more efficient and meaningful way.",
    "realWorldApplications": [
      "In machine learning, orthonormal bases are used to reduce dimensionality and improve model performance."
    ],
    "commonMistakes": [
      "Don't confuse orthogonality with parallelism. Just because two vectors are orthogonal doesn't mean they're parallel."
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:01:05.232Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_orthogonal_sets_004",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, orthogonal sets are collections of vectors that satisfy a specific property.</p><ul><li>The dot product between any two distinct vectors in the set is zero.</li></ul>",
    "formula": "{",
    "latex": "\\(\\mathbf{a} \\cdot \\mathbf{b} = 0 \\text{ for } i \\neq j\\)",
    "name": "Orthogonality Condition\" },",
    "theorem": "{",
    "statement": "\\[A set of vectors is orthogonal if and only if the dot product between any two distinct vectors in the set is zero.\\]\",",
    "proofSketch": "The proof involves showing that the condition is sufficient and necessary for orthogonality.\" },",
    "intuition": "Orthogonal sets are useful because they allow us to work with independent directions in a vector space. This property has significant implications in machine learning, where it's used to construct orthogonal bases for feature spaces.",
    "realWorldApplications": [
      "Principal Component Analysis (PCA)"
    ],
    "tags": [
      "linear independence",
      "orthogonality",
      "ML/AI applications"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:01:23.170Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_orthogonal_sets_005",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>Understanding orthogonal and orthonormal sets is crucial in linear algebra.</p><p>In this theorem, we'll explore the properties of these sets and their importance in various applications, including machine learning.</p>",
    "formula": {
      "latex": "\\[\\mathbf{u} \\perp \\mathbf{v} \\Leftrightarrow \\mathbf{u}^T \\mathbf{v} = 0\\]",
      "name": "Orthogonality Condition"
    },
    "theorem": {
      "statement": "\\[\\text{A set } S \\subseteq \\mathbb{R}^n \\text{ is orthogonal if any two vectors in } S \\text{ are orthogonal.}\\]"
    },
    "intuition": "An orthogonal set is a collection of vectors that, when added together, cancel each other out. This property is essential in many applications, including dimensionality reduction and feature extraction.",
    "realWorldApplications": [
      "Principal Component Analysis (PCA)"
    ],
    "tags": [
      "linear algebra",
      "orthogonality",
      "machine learning"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:01:40.132Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_orthogonal_sets_006",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "problem": "{",
    "statementHtml": "<p>Find an orthonormal set of vectors that span a given subspace.</p>",
    "hints": [
      "Start with a basis for the subspace.",
      "Use Gram-Schmidt process to orthogonalize the vectors.",
      "Check that the resulting set is orthonormal."
    ],
    "solutionHtml": "<p>To find an orthonormal set of vectors, we can start with a basis for the subspace. Let's say our subspace is spanned by $\\mathbf{v}_1$ and $\\mathbf{v}_2$. We'll use Gram-Schmidt process to orthogonalize these vectors.</p>\\n<p>First, we normalize $\\mathbf{v}_1$:</p>\\n\\[ \\mathbf{u}_1 = \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|} \\]\\n<p>Then, we subtract the projection of $\\mathbf{v}_2$ onto $\\mathbf{u}_1$ from $\\mathbf{v}_2$:</p>\\n\\[ \\mathbf{v}_2^\\perp = \\mathbf{v}_2 - \\left(\\frac{\\mathbf{v}_2}{\\|\\mathbf{v}_2\\|} \\cdot \\mathbf{u}_1\\right) \\mathbf{u}_1 \\]\\n<p>We normalize the resulting vector:</p>\\n\\[ \\mathbf{u}_2 = \\frac{\\mathbf{v}_2^\\perp}{\\|\\mathbf{v}_2^\\perp\\|} \\]\\n<p>The resulting set $\\{\\mathbf{u}_1, \\mathbf{u}_2\\}$ is orthonormal.</p>\",",
    "answerShort": "The orthonormal set of vectors that spans the subspace.\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:02:04.981Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_orthogonal_sets_007",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "problem": "{",
    "statementHtml": "<p>Find an orthonormal basis for the subspace spanned by vectors \\(\\mathbf{a} = [1, 2, 3]\\) and \\(\\mathbf{b} = [4, -1, 0]\\).</p>\",",
    "hints": [
      "Start with the Gram-Schmidt process.",
      "Use the inner product to normalize each vector.",
      "Check that the resulting basis is orthonormal."
    ],
    "solutionHtml": "<p>To find an orthonormal basis for the subspace spanned by \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), we can use the Gram-Schmidt process.</p><ul><li>First, normalize \\(\\mathbf{a}\\): \\(\\hat{\\mathbf{a}} = [1/\\sqrt{14}, 2/\\sqrt{14}, 3/\\sqrt{14}]\\).</li><li>Next, subtract the projection of \\(\\mathbf{b}\\) onto \\(\\hat{\\mathbf{a}}\\) from \\(\\mathbf{b}\\): \\(\\tilde{\\mathbf{b}} = [4 - (2/7), -1 + (6/7), 0]\\).</li><li>Normalize \\(\\tilde{\\mathbf{b}}\\): \\(\\hat{\\mathbf{b}} = [\\sqrt{5/7}, -\\sqrt{1/7}, 0]\\).</li></ul><p>The orthonormal basis is then given by \\(\\{\\hat{\\mathbf{a}}, \\hat{\\mathbf{b}}\\}\\).</p>\",",
    "answerShort": "The orthonormal basis is \\(\\{\\hat{\\mathbf{a}}, \\hat{\\mathbf{b}}\\}\\).\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:02:29.475Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_prb_orthogonal_sets_008",
    "subject": "linear_algebra",
    "type": "problem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "problem": "{",
    "statementHtml": "Find a set of vectors that is both orthogonal and orthonormal.",
    "hints": [
      "Start by considering two vectors in the set.",
      "Recall that orthogonality means dot products are zero, while orthonormality means magnitudes are 1.",
      "Think about how you can use these properties to construct the desired set."
    ],
    "solutionHtml": " Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ be two vectors in the set. We want them to satisfy \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_2 = 0\\) and \\(\\lVert \\mathbf{v}_1 \\rVert = \\lVert \\mathbf{v}_2 \\rVert = 1\\). Suppose we have a third vector $\\mathbf{v}_3$. We can write it as a linear combination of the first two: \\(\\mathbf{v}_3 = a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2\\). Now, take the dot product with each of the first two vectors:\\begin{align*} 0 &= (\\mathbf{v}_3 \\cdot \\mathbf{v}_1) = (a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2) \\cdot \\mathbf{v}_1\\\\ 0 &= (\\mathbf{v}_3 \\cdot \\mathbf{v}_2) = (a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2) \\cdot \\mathbf{v}_2 \\end{align*}Simplifying, we get $a_1 (\\mathbf{v}_1 \\cdot \\mathbf{v}_1) = 0$ and $a_2 (\\mathbf{v}_2 \\cdot \\mathbf{v}_2) = 0$. Since $\\lVert \\mathbf{v}_1 \\rVert = \\lVert \\mathbf{v}_2 \\rVert = 1$, we have $(\\mathbf{v}_1 \\cdot \\mathbf{v}_1) = (\\mathbf{v}_2 \\cdot \\mathbf{v}_2) = 1$. Therefore, $a_1 = a_2 = 0$ and $\\mathbf{v}_3$ is orthogonal to both $\\mathbf{v}_1$ and $\\mathbf{v}_2$. By induction, we can show that any set of vectors satisfying these properties will be orthogonal and orthonormal. \",",
    "answerShort": "Construct the set by considering linear combinations of two initial vectors.\" },",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:03:03.329Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_orthogonal_sets_009",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal.</p>",
    "workedExample": "{",
    "problemHtml": "Find the coordinates of <math>\\mathbf{v}</math> in terms of an orthonormal basis <math>\\{\\mathbf{e}_1, \\mathbf{e}_2\\}</math>, given that\",",
    "steps": "[ {\"stepNumber\": 1, \"description\": \"Write down the coordinate representation\", \"mathHtml\": \"\\[c_1 \\mathbf{e}_1 + c_2 \\mathbf{e}_2 = \\mathbf{v}\\]\", \"explanation\": \"We want to express <math>\\mathbf{v}</math> in terms of the basis vectors.\"}, {\"stepNumber\": 2, \"description\": \"Dot product both sides with each basis vector\", \"mathHtml\": \"\\[\\left(\\mathbf{e}_1 \\cdot \\mathbf{v}\\right) = c_1\\] and \\[\\left(\\mathbf{e}_2 \\cdot \\mathbf{v}\\right) = c_2\\]\", \"explanation\": \"This allows us to solve for the coordinates <math>c_1</math> and <math>c_2</math>.\"}, {\"stepNumber\": 3, \"description\": \"Solve for the coordinates\", \"mathHtml\": \"\\[c_1 = \\frac{\\mathbf{e}_1 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_1\\|^2}\\] and \\[c_2 = \\frac{\\mathbf{e}_2 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_2\\|^2}\\]\", \"explanation\": \"We use the fact that the basis vectors are orthonormal, which simplifies the calculations.\"}, {\"stepNumber\": 4, \"description\": \"Plug in the values and simplify\", \"mathHtml\": \"\\[c_1 = \\frac{\\mathbf{e}_1 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_1\\|^2}\\] and \\[c_2 = \\frac{\\mathbf{e}_2 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_2\\|^2}\\]\", \"explanation\": \"This gives us the final coordinates of <math>\\mathbf{v}</math> in terms of the orthonormal basis.\"}, ],",
    "finalAnswer": "The coordinates are <math>c_1 = \\frac{\\mathbf{e}_1 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_1\\|^2}</math> and <math>c_2 = \\frac{\\mathbf{e}_2 \\cdot \\mathbf{v}}{\\|\\mathbf{e}_2\\|^2}</math>\" },",
    "intuition": "Orthogonal sets are crucial in many machine learning algorithms, such as PCA and LLE.",
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:03:40.753Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_orthogonal_sets_010",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal.</p>",
    "formula": {
      "latex": "\\[\\mathbf{u} \\perp \\mathbf{v} \\Leftrightarrow \\mathbf{u}^T \\mathbf{v} = 0\\]",
      "name": "Orthogonality Condition"
    },
    "problem": {
      "statementHtml": "<p>Given two vectors, \\mathbf{a} and \\mathbf{b}, determine if they are orthogonal.</p>",
      "hints": [
        "Check the dot product",
        "Use the definition of orthogonality"
      ],
      "solutionHtml": "<p>To check for orthogonality, we can calculate the dot product:</p><p>\\(\\mathbf{a}^T \\mathbf{b} = 0\\)</p><p>If this is true, then \\mathbf{a} and \\mathbf{b} are orthogonal.</p>",
      "answerShort": "Check the dot product"
    },
    "workedExample": {
      "problemHtml": "<p>Consider two vectors, \\mathbf{u} = (1, 2) and \\mathbf{v} = (-3, -4). Determine if they are orthogonal.</p>",
      "steps": [
        {
          "stepNumber": 1,
          "description": "Calculate the dot product",
          "mathHtml": "\\[\\mathbf{u}^T \\mathbf{v} = (1, 2) \\cdot (-3, -4) = -(3) + (-4)(2) = -10\\]",
          "explanation": "We're calculating the dot product to check for orthogonality."
        },
        {
          "stepNumber": 2,
          "description": "Check if the result is zero",
          "mathHtml": "\\[\\mathbf{u}^T \\mathbf{v} = -10 \\neq 0\\]",
          "explanation": "Since the dot product is not zero, \\mathbf{u} and \\mathbf{v} are not orthogonal."
        },
        {
          "stepNumber": 3,
          "description": "Find an orthonormal basis",
          "mathHtml": "\\[\\text{...}\\]",
          "explanation": "We can find an orthonormal basis by normalizing the vectors and then finding a new set of vectors that span the same space."
        }
      ],
      "finalAnswer": "Not orthogonal"
    },
    "intuition": "Orthogonal sets are crucial in many applications, including machine learning where they help with dimensionality reduction and feature extraction.",
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:04:14.934Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_wex_orthogonal_sets_011",
    "subject": "linear_algebra",
    "type": "worked_example",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_sets",
    "title": "Orthogonal and Orthonormal Sets",
    "contentHtml": "<p>In linear algebra, we often encounter sets of vectors that are either orthogonal or orthonormal.</p>",
    "workedExample": "{",
    "problemHtml": "Find the coordinates of <math>\\mathbf{v} = (1,2,3)</math> in an orthonormal basis <math>\\{\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3\\}</math>\",",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Write the standard basis vectors",
        "mathHtml": "\\[\\mathbf{e}_1 = (1,0,0), \\mathbf{e}_2 = (0,1,0), \\mathbf{e}_3 = (0,0,1)\\]",
        "explanation": "We need to write the standard basis vectors in terms of our orthonormal basis."
      },
      {
        "stepNumber": 2,
        "description": "Compute the dot product with each basis vector",
        "mathHtml": "\\[\\mathbf{v} \\cdot \\mathbf{e}_1 = (1,2,3) \\cdot (1,0,0) = 1, \\mathbf{v} \\cdot \\mathbf{e}_2 = (1,2,3) \\cdot (0,1,0) = 2, \\mathbf{v} \\cdot \\mathbf{e}_3 = (1,2,3) \\cdot (0,0,1) = 3\\]",
        "explanation": "We compute the dot product to find the coordinates."
      },
      {
        "stepNumber": 3,
        "description": "Normalize each basis vector",
        "mathHtml": "\\[\\hat{\\mathbf{e}}_1 = \\frac{\\mathbf{e}_1}{||\\mathbf{e}_1||}, \\hat{\\mathbf{e}}_2 = \\frac{\\mathbf{e}_2}{||\\mathbf{e}_2||}, \\hat{\\mathbf{e}}_3 = \\frac{\\mathbf{e}_3}{||\\mathbf{e}_3||}\\]",
        "explanation": "We normalize each basis vector to make it orthonormal."
      },
      {
        "stepNumber": 4,
        "description": "Find the coordinates using the normalized basis",
        "mathHtml": "\\[v_1 = \\frac{(1,2,3) \\cdot (1,0,0)}{||\\mathbf{e}_1||}, v_2 = \\frac{(1,2,3) \\cdot (0,1,0)}{||\\mathbf{e}_2||}, v_3 = \\frac{(1,2,3) \\cdot (0,0,1)}{||\\mathbf{e}_3||}\\]",
        "explanation": "We use the normalized basis to find the coordinates."
      },
      {
        "stepNumber": 5,
        "description": "Simplify the expressions",
        "mathHtml": "\\[v_1 = 1/\\sqrt{3}, v_2 = 2/\\sqrt{3}, v_3 = 3/\\sqrt{3}\\]",
        "explanation": "We simplify the expressions to get the final answer."
      }
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:04:57.549Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]