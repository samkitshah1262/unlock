[
  {
    "id": "la_con_least_squares_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems: Normal Equations and Orthogonality",
    "contentHtml": "<p>In many machine learning applications, we encounter overdetermined systems of linear equations. These arise when we have more observations than features in our data. The goal is to find the best-fitting line or hyperplane that minimizes the sum of squared errors.</p><p>One way to solve these problems is by using normal equations. These are a set of linear equations that can be used to find the coefficients of the optimal solution.</p>",
    "formula": {
      "latex": "\\[ \\mathbf{A}^T\\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b} \\]",
      "name": "Normal Equation"
    },
    "intuition": "The key insight is that the normal equations provide a way to find the optimal solution by minimizing the sum of squared errors.",
    "realWorldApplications": [
      "Linear regression in machine learning"
    ],
    "tags": [
      "linear-algebra",
      "machine-learning",
      "least-squares"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:25:18.656Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_least_squares_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems: Normal Equations and Orthogonality",
    "contentHtml": "<p>In linear algebra, least squares problems are a fundamental concept that arises in many applications, including machine learning and artificial intelligence.</p><p>Given a set of data points, the goal is to find the best-fitting line or hyperplane that minimizes the sum of squared errors between the observed values and the predicted values. This is achieved by solving a system of linear equations known as the normal equations.</p>",
    "formula": {
      "latex": "\\[ \\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b} \\]",
      "name": "Normal Equations"
    },
    "intuition": "The key insight is that the normal equations represent a balance between the sum of squared errors and the constraints imposed by the data points. This balance is achieved when the residual vector is orthogonal to the column space of the design matrix.",
    "realWorldApplications": [
      "Regression analysis in machine learning"
    ],
    "commonMistakes": [
      "Failing to recognize that the normal equations are a system of linear equations",
      "Not understanding the geometric interpretation of orthogonality"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:25:37.051Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_least_squares_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "least_squares",
    "title": "Least Squares Problems: A Geometric View",
    "contentHtml": "<p>In linear algebra, we often encounter overdetermined systems of equations, where the number of equations exceeds the number of variables. One way to solve such systems is by minimizing the sum of squared errors using the least squares method.</p><p>Geometrically, this means finding the closest point in a subspace that best fits a set of data points.</p>",
    "formula": {
      "latex": "\\[ \\mathbf{A}^T\\mathbf{A}\\mathbf{x} = \\mathbf{A}^T\\mathbf{b} \\]",
      "name": "Normal Equations"
    },
    "intuition": "The key insight is that the least squares solution minimizes the perpendicular distance from a point to the subspace, making it a natural choice for many applications.",
    "realWorldApplications": [
      "Regression analysis in machine learning"
    ],
    "commonMistakes": [
      "Failing to recognize overdetermined systems as a fundamental problem type",
      "Not understanding the geometric interpretation of least squares"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:25:53.578Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]