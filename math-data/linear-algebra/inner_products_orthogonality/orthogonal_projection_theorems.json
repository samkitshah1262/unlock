[
  {
    "id": "la_thm_orthogonal_projection_008",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_projection",
    "title": "Orthogonal Projection Theorem",
    "contentHtml": "<p>The Orthogonal Projection Theorem states that given an orthonormal basis \\(\\mathbf{B} = (\\mathbf{b}_1, ..., \\mathbf{b}_k)\\) and a vector \\(\\mathbf{x}\\), the best approximation of \\(\\mathbf{x}\\) in the subspace spanned by \\(\\mathbf{B}\\) is given by the orthogonal projection matrix \\(\\mathbf{P} = \\mathbf{B} \\mathbf{B}^T\\).</p>\",",
    "formula": "{",
    "latex": "\\[\\mathbf{P} = \\mathbf{B} \\mathbf{B}^T\\]\",",
    "name": "Orthogonal Projection Matrix\" },",
    "theorem": "{",
    "statement": "\\[\\text{Given an orthonormal basis } \\mathbf{B} = (\\mathbf{b}_1, ..., \\mathbf{b}_k) \\text{ and a vector } \\mathbf{x}, \\text{ the best approximation of } \\mathbf{x} \\text{ in the subspace spanned by } \\mathbf{B} \\text{ is given by the orthogonal projection matrix } \\mathbf{P} = \\mathbf{B} \\mathbf{B}^T.\\]\",",
    "proofSketch": "The proof involves showing that \\(\\mathbf{P}\\) minimizes the squared error between \\(\\mathbf{x}\\) and its approximation in the subspace.\" },",
    "intuition": "This theorem provides a powerful tool for projecting high-dimensional data onto lower-dimensional subspaces, which is crucial in many machine learning applications.",
    "realWorldApplications": [
      "Dimensionality reduction in computer vision",
      "Feature extraction in natural language processing"
    ],
    "tags": [
      "Linear Algebra",
      "Orthogonal Projections",
      "Machine Learning"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:15:07.407Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_thm_orthogonal_projection_009",
    "subject": "linear_algebra",
    "type": "theorem",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_projection",
    "title": "Orthogonal Projection Theorem",
    "contentHtml": "<p>The Orthogonal Projection Theorem states that given a subspace <em>V</em> and a vector <em>x</em>, there exists a unique orthogonal projection matrix <em>P</em> such that <code>\\[P x\\]</code> is the best approximation of <em>x</em> in <em>V</em>.</p>\",",
    "formula": "{",
    "latex": "\\[P = H\\]",
    "name": "Orthogonal Projection Matrix\" },",
    "theorem": "{",
    "statement": "\\\\[\\\\text{For any }x \\\\in \\\\mathbb{R}^n, \\\\text{there exists a unique }P \\\\in \\\\mathcal{M}_{n}\\\\text{ such that }Px \\\\perp V_{\\\\bot}\\]\",",
    "proofSketch": "The proof involves showing that <code>P</code> is the best approximation by minimizing the squared error between <em>x</em> and its projection onto <em>V</em>\" },",
    "intuition": "This theorem provides a way to project any vector onto a subspace, which is crucial in many machine learning algorithms such as principal component analysis (PCA) and independent component analysis (ICA).",
    "realWorldApplications": [
      "Principal Component Analysis",
      "Independent Component Analysis"
    ],
    "tags": [
      "Linear Algebra",
      "Orthogonality",
      "Projection"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:15:27.259Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]