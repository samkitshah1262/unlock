[
  {
    "id": "la_con_orthogonal_projection_001",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_projection",
    "title": "Orthogonal Projection",
    "contentHtml": "<p>When dealing with subspaces in linear algebra, it's crucial to understand how to project vectors onto these subspaces. This concept is fundamental to many machine learning and AI applications.</p><p>Given a vector <i>v</i> and a subspace <i>S</i>, the orthogonal projection of <i>v</i> onto <i>S</i> finds the point in <i>S</i> that minimizes the distance to <i>v</i>. This is equivalent to finding the best approximation of <i>v</i> within <i>S</i>.</p>",
    "formula": {
      "latex": "\\[ P_S v = \\left(I - Q\\right) v \\]",
      "name": "Projection Matrix"
    },
    "intuition": "Think of orthogonal projection as a way to 'flatten' a vector onto a subspace, effectively removing any components that are not within the subspace. This is useful in many applications, such as feature engineering and dimensionality reduction.",
    "realWorldApplications": [
      "Dimensionality Reduction",
      "Feature Engineering"
    ],
    "commonMistakes": [
      "Not understanding the difference between orthogonal projection and parallel projection"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:13:01.404Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_orthogonal_projection_002",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_projection",
    "title": "Orthogonal Projection",
    "contentHtml": "<p>Given a subspace W and a vector v, orthogonal projection onto W is the best approximation of v in terms of vectors from W.</p><p>This concept is crucial in machine learning, as it's used to reduce dimensionality, perform feature selection, and improve model performance.</p>",
    "formula": "{",
    "latex": "\\[P_W = WW^T\\]\",",
    "name": "Orthogonal Projection Matrix\" },",
    "intuition": "Think of orthogonal projection like taking a photo of v from the perspective of W. The resulting image is the best representation of v in terms of vectors from W.",
    "realWorldApplications": [
      "Dimensionality reduction in PCA"
    ],
    "commonMistakes": [
      "Failing to account for the subspace when projecting"
    ],
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:13:14.612Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  },
  {
    "id": "la_con_orthogonal_projection_003",
    "subject": "linear_algebra",
    "type": "concept",
    "chapter": "inner_products_orthogonality",
    "topic": "orthogonal_projection",
    "title": "Orthogonal Projection",
    "contentHtml": "<p>Given a subspace and a vector, orthogonal projection finds the best approximation of that vector within the subspace.</p><p>This is achieved by finding the closest point in the subspace to the original vector.</p>",
    "formula": {
      "latex": "\\[ P = A (A^T A)^{-1} A^T \\]",
      "name": "Projection Matrix"
    },
    "intuition": "Think of it like taking a photo of an object from different angles. The projection matrix is like the camera settings that capture the best possible image.",
    "realWorldApplications": [
      "In machine learning, orthogonal projection is used in dimensionality reduction techniques such as PCA (Principal Component Analysis)"
    ],
    "commonMistakes": [
      "Not understanding that the projection matrix is not unique and can be different for the same subspace"
    ],
    "estimatedMinutes": 2,
    "difficulty": 3,
    "mlRelevance": "core",
    "prerequisites": [],
    "relatedCards": [],
    "nextCards": [],
    "generatedAt": "2025-12-26T13:13:28.884Z",
    "generatedBy": "llama3:latest",
    "reviewed": false,
    "version": 1
  }
]