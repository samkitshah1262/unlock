[
  {
    "id": "ai-flashattention-cuda-kernel-fusion-1",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "CUDA Kernel Fusion",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>\n    <p>A single fused kernel handles all stages: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-95\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">QK^T</script>, mask, softmax, dropout, and final multiply by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-98\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-99\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">V</script>. This eliminates repeated reads/writes between GPU HBM and SRAM that occur when separating stages.</p>\n  </li>\n  <li>\n    <p>This kernel fusion not only reduces inter-stage memory traffic but also eliminates kernel launch overhead, which is non-negligible in high-frequency, short-duration GPU operations. As noted in community discussions, <strong>launching multiple kernels individually incurs scheduling and memory synchronization costs</strong>, which FlashAttention’s fusion avoids.</p>\n  </li>\n  <li>\n    <p>Tensors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-101\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">Q, K, V</script> are partitioned into small fixed-size blocks that fit into on-chip SRAM or registers. A block of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">K, V</script> is loaded once; loops iterate over corresponding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">Q</script> blocks.</p>\n  </li>\n</ul>\n<p>A single fused kernel handles all stages: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-95\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">QK^T</script>, mask, softmax, dropout, and final multiply by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-98\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-99\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">V</script>. This eliminates repeated reads/writes between GPU HBM and SRAM that occur when separating stages.</p>\n<p>This kernel fusion not only reduces inter-stage memory traffic but also eliminates kernel launch overhead, which is non-negligible in high-frequency, short-duration GPU operations. As noted in community discussions, <strong>launching multiple kernels individually incurs scheduling and memory synchronization costs</strong>, which FlashAttention’s fusion avoids.</p>\n<p>Tensors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-101\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">Q, K, V</script> are partitioned into small fixed-size blocks that fit into on-chip SRAM or registers. A block of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">K, V</script> is loaded once; loops iterate over corresponding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">Q</script> blocks.</p>",
    "contentMarkdown": "*   A single fused kernel handles all stages: QKTQKTQK^T, mask, softmax, dropout, and final multiply by VVV. This eliminates repeated reads/writes between GPU HBM and SRAM that occur when separating stages.\n    \n*   This kernel fusion not only reduces inter-stage memory traffic but also eliminates kernel launch overhead, which is non-negligible in high-frequency, short-duration GPU operations. As noted in community discussions, **launching multiple kernels individually incurs scheduling and memory synchronization costs**, which FlashAttention’s fusion avoids.\n    \n*   Tensors Q,K,VQ,K,VQ, K, V are partitioned into small fixed-size blocks that fit into on-chip SRAM or registers. A block of K,VK,VK, V is loaded once; loops iterate over corresponding QQQ blocks.\n    \n\nA single fused kernel handles all stages: QKTQKTQK^T, mask, softmax, dropout, and final multiply by VVV. This eliminates repeated reads/writes between GPU HBM and SRAM that occur when separating stages.\n\nThis kernel fusion not only reduces inter-stage memory traffic but also eliminates kernel launch overhead, which is non-negligible in high-frequency, short-duration GPU operations. As noted in community discussions, **launching multiple kernels individually incurs scheduling and memory synchronization costs**, which FlashAttention’s fusion avoids.\n\nTensors Q,K,VQ,K,VQ, K, V are partitioned into small fixed-size blocks that fit into on-chip SRAM or registers. A block of K,VK,VK, V is loaded once; loops iterate over corresponding QQQ blocks.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "dropout"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 209,
      "contentLength": 17533
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#cuda-kernel-fusion",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-tiling-and-recomputation-strategy-2",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "Tiling and Recomputation Strategy",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>Attention calculation is expressed as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo>=</mo><mi>softmax</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-116\" style=\"width: 11.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.794em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.451em, 1009.79em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">softmax</span><span class=\"mo\" id=\"MathJax-Span-121\"></span><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"mo\" id=\"MathJax-Span-123\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-124\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1002.03em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.039em;\"><span class=\"mrow\" id=\"MathJax-Span-125\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-127\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1001.3em, 4.326em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.622em;\"><span class=\"msqrt\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0.732em;\"><span class=\"mrow\" id=\"MathJax-Span-131\"><span class=\"mi\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1000.58em, 3.388em, -999.997em); top: -4.008em; left: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.055em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1000.78em, 4.169em, -999.997em); top: -3.904em; left: 0em;\"><span style=\"font-family: STIXVariants;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.14em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.138em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-133\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo>=</mo><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math></span></span></div>\n<ul>\n  <li>\n    <p>… but the algorithm avoids materializing the full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x00D7;</mo><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>×</mo><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">N \\times N</script> matrix. Instead, it breaks the computation into tiles of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">B</script>. For each block of queries <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>Q</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-143\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"msubsup\" id=\"MathJax-Span-145\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-147\"><span class=\"mrow\" id=\"MathJax-Span-148\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Q</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">Q_{i}</script> and key-value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-158\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">K_{j}, V_{j}</script>, it computes partial attention contributions.</p>\n  </li>\n  <li>\n    <p>As described in blog explanations, these tiles are sized so they fit <strong>entirely within a single Streaming Multiprocessor’s shared memory (SRAM)</strong>. This ensures efficient compute on GPU warps and avoids uncoalesced memory accesses.</p>\n  </li>\n  <li>\n    <p>To handle numerical stability of softmax without storing all logits, it recomputes per-block max and sum for normalization. Intermediate maxima and scaling factors (like cumulative <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>m</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-163\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"msubsup\" id=\"MathJax-Span-165\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-167\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>m</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">m_i</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>l</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"msubsup\" id=\"MathJax-Span-170\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>l</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">l_i</script>) are merged across tiles as:</p>\n  </li>\n</ul>\n<p>… but the algorithm avoids materializing the full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x00D7;</mo><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>×</mo><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">N \\times N</script> matrix. Instead, it breaks the computation into tiles of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">B</script>. For each block of queries <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>Q</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-143\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"msubsup\" id=\"MathJax-Span-145\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-147\"><span class=\"mrow\" id=\"MathJax-Span-148\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Q</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">Q_{i}</script> and key-value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-158\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">K_{j}, V_{j}</script>, it computes partial attention contributions.</p>\n<p>As described in blog explanations, these tiles are sized so they fit <strong>entirely within a single Streaming Multiprocessor’s shared memory (SRAM)</strong>. This ensures efficient compute on GPU warps and avoids uncoalesced memory accesses.</p>\n<p>To handle numerical stability of softmax without storing all logits, it recomputes per-block max and sum for normalization. Intermediate maxima and scaling factors (like cumulative <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>m</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-163\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"msubsup\" id=\"MathJax-Span-165\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-167\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>m</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">m_i</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>l</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"msubsup\" id=\"MathJax-Span-170\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>l</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">l_i</script>) are merged across tiles as:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mi>j</mi></munder><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>l</mi><mi>i</mi></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mi>j</mi></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>m</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 17.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1014.85em, 3.753em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"msubsup\" id=\"MathJax-Span-175\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-179\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -3.331em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-182\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-184\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-189\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-190\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-193\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-194\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-196\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-198\"></span><span class=\"mo\" id=\"MathJax-Span-199\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-200\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-206\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-207\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 3.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><munder><mo movablelimits=\"true\" form=\"prefix\">max</mo><mi>j</mi></munder><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>l</mi><mi>i</mi></msub><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>… with streaming updates across blocks to form the final output. This limits memory to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-211\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">O(N \\cdot d)</script> instead of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-219\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-220\"><span class=\"mi\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-223\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">O(N^2)</script>.\nAccording to the Medium explanation, this <strong>recomputation is cheaper than storing full logits</strong>, thanks to the high arithmetic intensity of GPUs and the low cost of on-chip computation compared to HBM I/O.</li>\n</ul>",
    "contentMarkdown": "*   Attention calculation is expressed as:\n\nO\\=softmax(QKTd‾‾√)VO\\=softmax⁡(QKTd)V\n\n*   … but the algorithm avoids materializing the full N×NN×NN \\\\times N matrix. Instead, it breaks the computation into tiles of size BBB. For each block of queries QiQiQ\\_{i} and key-value Kj,VjKj,VjK\\_{j}, V\\_{j}, it computes partial attention contributions.\n    \n*   As described in blog explanations, these tiles are sized so they fit **entirely within a single Streaming Multiprocessor’s shared memory (SRAM)**. This ensures efficient compute on GPU warps and avoids uncoalesced memory accesses.\n    \n*   To handle numerical stability of softmax without storing all logits, it recomputes per-block max and sum for normalization. Intermediate maxima and scaling factors (like cumulative mimim\\_i, lilil\\_i) are merged across tiles as:\n    \n\n… but the algorithm avoids materializing the full N×NN×NN \\\\times N matrix. Instead, it breaks the computation into tiles of size BBB. For each block of queries QiQiQ\\_{i} and key-value Kj,VjKj,VjK\\_{j}, V\\_{j}, it computes partial attention contributions.\n\nAs described in blog explanations, these tiles are sized so they fit **entirely within a single Streaming Multiprocessor’s shared memory (SRAM)**. This ensures efficient compute on GPU warps and avoids uncoalesced memory accesses.\n\nTo handle numerical stability of softmax without storing all logits, it recomputes per-block max and sum for normalization. Intermediate maxima and scaling factors (like cumulative mimim\\_i, lilil\\_i) are merged across tiles as:\n\nmi\\=maxjsij,li\\=∑jexp(sij−mi)mi\\=maxjsij,li\\=∑jexp⁡(sij−mi)\n\n*   … with streaming updates across blocks to form the final output. This limits memory to O(N⋅d)O(N⋅d)O(N \\\\cdot d) instead of O(N2)O(N2)O(N^2). According to the Medium explanation, this **recomputation is cheaper than storing full logits**, thanks to the high arithmetic intensity of GPUs and the low cost of on-chip computation compared to HBM I/O.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 269,
      "contentLength": 44878
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#tiling-and-recomputation-strategy",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-io-analysis-3",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "I/O Analysis",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>\n    <p>Standard attention requires <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-227\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-231\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">O(N^2)</script> reads/writes to HBM (compute and softmax across full matrix). In contrast, FlashAttention performs only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"mi\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">O(N \\cdot d)</script> accesses, asymptotically optimal within constant factors for realistic SRAM sizes.</p>\n  </li>\n  <li>\n    <p>As clarified on Reddit, this does <strong>not reduce theoretical time complexity</strong>, but it <strong>dramatically reduces practical runtime</strong> by targeting the memory bottleneck—the true limiting factor on GPU workloads. One commenter summarized: <em>“It’s faster because GPUs are memory bottlenecked, not because it has better time complexity.”</em></p>\n  </li>\n</ul>\n<p>Standard attention requires <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-227\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-231\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">O(N^2)</script> reads/writes to HBM (compute and softmax across full matrix). In contrast, FlashAttention performs only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"mi\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">O(N \\cdot d)</script> accesses, asymptotically optimal within constant factors for realistic SRAM sizes.</p>\n<p>As clarified on Reddit, this does <strong>not reduce theoretical time complexity</strong>, but it <strong>dramatically reduces practical runtime</strong> by targeting the memory bottleneck—the true limiting factor on GPU workloads. One commenter summarized: <em>“It’s faster because GPUs are memory bottlenecked, not because it has better time complexity.”</em></p>",
    "contentMarkdown": "*   Standard attention requires O(N2)O(N2)O(N^2) reads/writes to HBM (compute and softmax across full matrix). In contrast, FlashAttention performs only O(N⋅d)O(N⋅d)O(N \\\\cdot d) accesses, asymptotically optimal within constant factors for realistic SRAM sizes.\n    \n*   As clarified on Reddit, this does **not reduce theoretical time complexity**, but it **dramatically reduces practical runtime** by targeting the memory bottleneck—the true limiting factor on GPU workloads. One commenter summarized: _“It’s faster because GPUs are memory bottlenecked, not because it has better time complexity.”_\n    \n\nStandard attention requires O(N2)O(N2)O(N^2) reads/writes to HBM (compute and softmax across full matrix). In contrast, FlashAttention performs only O(N⋅d)O(N⋅d)O(N \\\\cdot d) accesses, asymptotically optimal within constant factors for realistic SRAM sizes.\n\nAs clarified on Reddit, this does **not reduce theoretical time complexity**, but it **dramatically reduces practical runtime** by targeting the memory bottleneck—the true limiting factor on GPU workloads. One commenter summarized: _“It’s faster because GPUs are memory bottlenecked, not because it has better time complexity.”_",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 154,
      "contentLength": 10000
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#i/o-analysis",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-gpu-memory-hierarchy-exploitation-4",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "GPU Memory Hierarchy Exploitation",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>\n    <p>Blocks are sized so <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x00D7;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-243\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-244\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>×</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">B \\times d</script> fits in on-chip SRAM (e.g. 128–256 KB per SM). Each streaming multiprocessor processes local tiles entirely within onboard SRAM and registers before writing back to HBM.</p>\n  </li>\n  <li>\n    <p>The Medium blog emphasizes that <strong>FlashAttention’s memory access pattern is carefully optimized to be GPU-friendly</strong>: row-major loading, coalesced access, and tight loop tiling. This results in high bandwidth utilization and reduced memory stalls.</p>\n  </li>\n  <li>\n    <p>By reducing trips between SRAM and HBM, FlashAttention becomes memory-bandwidth–bound less often and thus achieves wall‑clock speedups.</p>\n  </li>\n</ul>\n<p>Blocks are sized so <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x00D7;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-243\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-244\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>×</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">B \\times d</script> fits in on-chip SRAM (e.g. 128–256 KB per SM). Each streaming multiprocessor processes local tiles entirely within onboard SRAM and registers before writing back to HBM.</p>\n<p>The Medium blog emphasizes that <strong>FlashAttention’s memory access pattern is carefully optimized to be GPU-friendly</strong>: row-major loading, coalesced access, and tight loop tiling. This results in high bandwidth utilization and reduced memory stalls.</p>\n<p>By reducing trips between SRAM and HBM, FlashAttention becomes memory-bandwidth–bound less often and thus achieves wall‑clock speedups.</p>",
    "contentMarkdown": "*   Blocks are sized so B×dB×dB \\\\times d fits in on-chip SRAM (e.g. 128–256 KB per SM). Each streaming multiprocessor processes local tiles entirely within onboard SRAM and registers before writing back to HBM.\n    \n*   The Medium blog emphasizes that **FlashAttention’s memory access pattern is carefully optimized to be GPU-friendly**: row-major loading, coalesced access, and tight loop tiling. This results in high bandwidth utilization and reduced memory stalls.\n    \n*   By reducing trips between SRAM and HBM, FlashAttention becomes memory-bandwidth–bound less often and thus achieves wall‑clock speedups.\n    \n\nBlocks are sized so B×dB×dB \\\\times d fits in on-chip SRAM (e.g. 128–256 KB per SM). Each streaming multiprocessor processes local tiles entirely within onboard SRAM and registers before writing back to HBM.\n\nThe Medium blog emphasizes that **FlashAttention’s memory access pattern is carefully optimized to be GPU-friendly**: row-major loading, coalesced access, and tight loop tiling. This results in high bandwidth utilization and reduced memory stalls.\n\nBy reducing trips between SRAM and HBM, FlashAttention becomes memory-bandwidth–bound less often and thus achieves wall‑clock speedups.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 169,
      "contentLength": 4391
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#gpu-memory-hierarchy-exploitation",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-performance-observations-5",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "Performance Observations",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>\n    <p>Benchmarks show 3× speed on GPT-2 with sequence length 1K, and 15% end‑to‑end speedup on BERT-large with seq = 512 compared to MLPerf baseline implementations.</p>\n  </li>\n  <li>\n    <p>Moreover, it enables context lengths up to 64K (e.g. Path-X and Path-256 tasks) with stronger downstream performance, which would otherwise exceed memory constraints.</p>\n  </li>\n  <li>\n    <p>Importantly, this performance improvement holds <strong>without any approximation</strong>—FlashAttention computes the exact same result as standard attention, just with dramatically lower overhead.</p>\n  </li>\n</ul>\n<p>Benchmarks show 3× speed on GPT-2 with sequence length 1K, and 15% end‑to‑end speedup on BERT-large with seq = 512 compared to MLPerf baseline implementations.</p>\n<p>Moreover, it enables context lengths up to 64K (e.g. Path-X and Path-256 tasks) with stronger downstream performance, which would otherwise exceed memory constraints.</p>\n<p>Importantly, this performance improvement holds <strong>without any approximation</strong>—FlashAttention computes the exact same result as standard attention, just with dramatically lower overhead.</p>",
    "contentMarkdown": "*   Benchmarks show 3× speed on GPT-2 with sequence length 1K, and 15% end‑to‑end speedup on BERT-large with seq = 512 compared to MLPerf baseline implementations.\n    \n*   Moreover, it enables context lengths up to 64K (e.g. Path-X and Path-256 tasks) with stronger downstream performance, which would otherwise exceed memory constraints.\n    \n*   Importantly, this performance improvement holds **without any approximation**—FlashAttention computes the exact same result as standard attention, just with dramatically lower overhead.\n    \n\nBenchmarks show 3× speed on GPT-2 with sequence length 1K, and 15% end‑to‑end speedup on BERT-large with seq = 512 compared to MLPerf baseline implementations.\n\nMoreover, it enables context lengths up to 64K (e.g. Path-X and Path-256 tasks) with stronger downstream performance, which would otherwise exceed memory constraints.\n\nImportantly, this performance improvement holds **without any approximation**—FlashAttention computes the exact same result as standard attention, just with dramatically lower overhead.",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "bert",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 141,
      "contentLength": 1161
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-observations",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-summary-6",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑1",
    "title": "Summary",
    "subtitle": "FlashAttention‑1",
    "contentHtml": "<ul>\n  <li>\n    <p>FlashAttention‑1 centers around a fused CUDA kernel combining all stages of attention.</p>\n  </li>\n  <li>\n    <p>It partitions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">Q, K, V</script> into SRAM-sized tiles, recomputes necessary normalization for softmax to avoid full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>N</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-255\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"msubsup\" id=\"MathJax-Span-257\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>N</mi><mn>2</mn></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">N^2</script> storage, and accumulates partial results efficiently.</p>\n  </li>\n  <li>\n    <p>This IO-centric design achieves lower memory traffic and faster runtime than naive or approximate alternatives, while preserving exact attention results.</p>\n  </li>\n  <li>\n    <p>Its practical optimizations—tiling, kernel fusion, recomputation, and warp-efficient softmax—make it the first attention algorithm to fully align with the GPU’s memory architecture for real-world performance gains.</p>\n  </li>\n</ul>\n<p>FlashAttention‑1 centers around a fused CUDA kernel combining all stages of attention.</p>\n<p>It partitions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">Q, K, V</script> into SRAM-sized tiles, recomputes necessary normalization for softmax to avoid full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>N</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-255\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"msubsup\" id=\"MathJax-Span-257\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>N</mi><mn>2</mn></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">N^2</script> storage, and accumulates partial results efficiently.</p>\n<p>This IO-centric design achieves lower memory traffic and faster runtime than naive or approximate alternatives, while preserving exact attention results.</p>\n<p>Its practical optimizations—tiling, kernel fusion, recomputation, and warp-efficient softmax—make it the first attention algorithm to fully align with the GPU’s memory architecture for real-world performance gains.</p>",
    "contentMarkdown": "*   FlashAttention‑1 centers around a fused CUDA kernel combining all stages of attention.\n    \n*   It partitions Q,K,VQ,K,VQ, K, V into SRAM-sized tiles, recomputes necessary normalization for softmax to avoid full N2N2N^2 storage, and accumulates partial results efficiently.\n    \n*   This IO-centric design achieves lower memory traffic and faster runtime than naive or approximate alternatives, while preserving exact attention results.\n    \n*   Its practical optimizations—tiling, kernel fusion, recomputation, and warp-efficient softmax—make it the first attention algorithm to fully align with the GPU’s memory architecture for real-world performance gains.\n    \n\nFlashAttention‑1 centers around a fused CUDA kernel combining all stages of attention.\n\nIt partitions Q,K,VQ,K,VQ, K, V into SRAM-sized tiles, recomputes necessary normalization for softmax to avoid full N2N2N^2 storage, and accumulates partial results efficiently.\n\nThis IO-centric design achieves lower memory traffic and faster runtime than naive or approximate alternatives, while preserving exact attention results.\n\nIts practical optimizations—tiling, kernel fusion, recomputation, and warp-efficient softmax—make it the first attention algorithm to fully align with the GPU’s memory architecture for real-world performance gains.",
    "order": 6,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 166,
      "contentLength": 8864
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#summary",
    "scrapedAt": "2025-12-28T11:47:47.087Z"
  },
  {
    "id": "ai-flashattention-architectural-enhancements-7",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑2",
    "title": "Architectural Enhancements",
    "subtitle": "FlashAttention‑2",
    "contentHtml": "<ul>\n  <li><strong>Fewer non‑matmul FLOPs</strong>:\n    <ul>\n      <li>FlashAttention‑2 rewrites elements of the softmax normalization to reduce non‑matrix‑multiply workloads (e.g. divisions, exponentials, masking operations), since non‑matmul FLOPs are much costlier on GPUs (up to 16× slower than Tensor-Core GEMMs).</li>\n      <li>It reorders the online softmax updates, avoiding unnecessary rescaling during intermediate blocks. Only the final output is scaled once, reducing overhead substantially.</li>\n    </ul>\n  </li>\n  <li><strong>Better parallelism across dimensions</strong>:\n    <ul>\n      <li>Unlike FlashAttention‑1, which parallelizes mainly over batch size and attention heads, FlashAttention‑2 adds parallelism over sequence length.</li>\n      <li>Each attention head’s computation can now be split across multiple thread blocks, improving occupancy even with long sequence and small batch regimes.</li>\n    </ul>\n  </li>\n  <li><strong>Work partitioning between warps</strong>\n    <ul>\n      <li>Within each thread block (e.g. 4–8 warps), FlashAttention‑2 carefully partitions tasks per warp to minimize shared memory synchronization.</li>\n      <li>This reduces intra-block communication and avoids redundant shared memory reads and writes.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>FlashAttention‑2 rewrites elements of the softmax normalization to reduce non‑matrix‑multiply workloads (e.g. divisions, exponentials, masking operations), since non‑matmul FLOPs are much costlier on GPUs (up to 16× slower than Tensor-Core GEMMs).</li>\n      <li>It reorders the online softmax updates, avoiding unnecessary rescaling during intermediate blocks. Only the final output is scaled once, reducing overhead substantially.</li>\n    </ul>\n<ul>\n      <li>Unlike FlashAttention‑1, which parallelizes mainly over batch size and attention heads, FlashAttention‑2 adds parallelism over sequence length.</li>\n      <li>Each attention head’s computation can now be split across multiple thread blocks, improving occupancy even with long sequence and small batch regimes.</li>\n    </ul>\n<ul>\n      <li>Within each thread block (e.g. 4–8 warps), FlashAttention‑2 carefully partitions tasks per warp to minimize shared memory synchronization.</li>\n      <li>This reduces intra-block communication and avoids redundant shared memory reads and writes.</li>\n    </ul>",
    "contentMarkdown": "*   **Fewer non‑matmul FLOPs**:\n    *   FlashAttention‑2 rewrites elements of the softmax normalization to reduce non‑matrix‑multiply workloads (e.g. divisions, exponentials, masking operations), since non‑matmul FLOPs are much costlier on GPUs (up to 16× slower than Tensor-Core GEMMs).\n    *   It reorders the online softmax updates, avoiding unnecessary rescaling during intermediate blocks. Only the final output is scaled once, reducing overhead substantially.\n*   **Better parallelism across dimensions**:\n    *   Unlike FlashAttention‑1, which parallelizes mainly over batch size and attention heads, FlashAttention‑2 adds parallelism over sequence length.\n    *   Each attention head’s computation can now be split across multiple thread blocks, improving occupancy even with long sequence and small batch regimes.\n*   **Work partitioning between warps**\n    *   Within each thread block (e.g. 4–8 warps), FlashAttention‑2 carefully partitions tasks per warp to minimize shared memory synchronization.\n    *   This reduces intra-block communication and avoids redundant shared memory reads and writes.\n\n*   FlashAttention‑2 rewrites elements of the softmax normalization to reduce non‑matrix‑multiply workloads (e.g. divisions, exponentials, masking operations), since non‑matmul FLOPs are much costlier on GPUs (up to 16× slower than Tensor-Core GEMMs).\n*   It reorders the online softmax updates, avoiding unnecessary rescaling during intermediate blocks. Only the final output is scaled once, reducing overhead substantially.\n\n*   Unlike FlashAttention‑1, which parallelizes mainly over batch size and attention heads, FlashAttention‑2 adds parallelism over sequence length.\n*   Each attention head’s computation can now be split across multiple thread blocks, improving occupancy even with long sequence and small batch regimes.\n\n*   Within each thread block (e.g. 4–8 warps), FlashAttention‑2 carefully partitions tasks per warp to minimize shared memory synchronization.\n*   This reduces intra-block communication and avoids redundant shared memory reads and writes.",
    "order": 7,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 270,
      "contentLength": 2367
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#architectural-enhancements",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-performance-metrics-8",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑2",
    "title": "Performance Metrics",
    "subtitle": "FlashAttention‑2",
    "contentHtml": "<ul>\n  <li>On A100 GPUs, FlashAttention‑2 achieves ~2× speedup over FlashAttention‑1, reaching effective throughput of 50–73% of theoretical peak GFLOPs/S (~230 TFLOPs/s for FP16/BF16).</li>\n  <li>During end-to-end training of GPT-style models, it sustains ~225 TFLOPs/s per A100 GPU (~72% FLOPs utilization).</li>\n  <li>Memory use remains linear in sequence length (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-260\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"mi\" id=\"MathJax-Span-262\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-265\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">O(N \\cdot d)</script>), and backward pass also benefits from reduced I/O and better tiling, giving 2–4× backward speedup over naive implementations.</li>\n</ul>",
    "contentMarkdown": "*   On A100 GPUs, FlashAttention‑2 achieves ~2× speedup over FlashAttention‑1, reaching effective throughput of 50–73% of theoretical peak GFLOPs/S (~230 TFLOPs/s for FP16/BF16).\n*   During end-to-end training of GPT-style models, it sustains ~225 TFLOPs/s per A100 GPU (~72% FLOPs utilization).\n*   Memory use remains linear in sequence length (O(N⋅d)O(N⋅d)O(N \\\\cdot d)), and backward pass also benefits from reduced I/O and better tiling, giving 2–4× backward speedup over naive implementations.",
    "order": 8,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 69,
      "contentLength": 2566
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-metrics",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-implementation-details-9",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑2",
    "title": "Implementation Details",
    "subtitle": "FlashAttention‑2",
    "contentHtml": "<ul>\n  <li><strong>Integration with CUTLASS / CuTe</strong>:\n    <ul>\n      <li>FlashAttention‑2 is implemented using NVIDIA’s CUTLASS 3.x and CuTe libraries, allowing for high-performance fused kernels built atop Tensor-Core primitives.</li>\n      <li>As a rewrite from scratch, it significantly reduces overhead compared to custom CUDA implementations used in FlashAttention‑1.</li>\n    </ul>\n  </li>\n  <li><strong>Bidirectional computation support</strong>:\n    <ul>\n      <li>Similar tiling and normalization techniques are used in the backward pass, although it involves more intermediate values inside SRAM registers and requires careful bookkeeping (e.g. storing log-sum-exp values, but not raw max and sum separately).</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>FlashAttention‑2 is implemented using NVIDIA’s CUTLASS 3.x and CuTe libraries, allowing for high-performance fused kernels built atop Tensor-Core primitives.</li>\n      <li>As a rewrite from scratch, it significantly reduces overhead compared to custom CUDA implementations used in FlashAttention‑1.</li>\n    </ul>\n<ul>\n      <li>Similar tiling and normalization techniques are used in the backward pass, although it involves more intermediate values inside SRAM registers and requires careful bookkeeping (e.g. storing log-sum-exp values, but not raw max and sum separately).</li>\n    </ul>",
    "contentMarkdown": "*   **Integration with CUTLASS / CuTe**:\n    *   FlashAttention‑2 is implemented using NVIDIA’s CUTLASS 3.x and CuTe libraries, allowing for high-performance fused kernels built atop Tensor-Core primitives.\n    *   As a rewrite from scratch, it significantly reduces overhead compared to custom CUDA implementations used in FlashAttention‑1.\n*   **Bidirectional computation support**:\n    *   Similar tiling and normalization techniques are used in the backward pass, although it involves more intermediate values inside SRAM registers and requires careful bookkeeping (e.g. storing log-sum-exp values, but not raw max and sum separately).\n\n*   FlashAttention‑2 is implemented using NVIDIA’s CUTLASS 3.x and CuTe libraries, allowing for high-performance fused kernels built atop Tensor-Core primitives.\n*   As a rewrite from scratch, it significantly reduces overhead compared to custom CUDA implementations used in FlashAttention‑1.\n\n*   Similar tiling and normalization techniques are used in the backward pass, although it involves more intermediate values inside SRAM registers and requires careful bookkeeping (e.g. storing log-sum-exp values, but not raw max and sum separately).",
    "order": 9,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 158,
      "contentLength": 1362
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#implementation-details",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-summary-10",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑2",
    "title": "Summary",
    "subtitle": "FlashAttention‑2",
    "contentHtml": "<ul>\n  <li><strong>Architectural improvements</strong>:\n    <ul>\n      <li>Reduces non-matmul operations</li>\n      <li>Parallelism expanded across sequence-length dimension</li>\n      <li>Optimized warp-level work partitioning</li>\n    </ul>\n  </li>\n  <li><strong>Performance improvements</strong>:\n    <ul>\n      <li>~2× speedup over FlashAttention‑1</li>\n      <li>~225 TFLOPs/s sustained on A100 GPUs (~72% utilization)</li>\n      <li>Enhanced backward pass performance</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Reduces non-matmul operations</li>\n      <li>Parallelism expanded across sequence-length dimension</li>\n      <li>Optimized warp-level work partitioning</li>\n    </ul>\n<ul>\n      <li>~2× speedup over FlashAttention‑1</li>\n      <li>~225 TFLOPs/s sustained on A100 GPUs (~72% utilization)</li>\n      <li>Enhanced backward pass performance</li>\n    </ul>",
    "contentMarkdown": "*   **Architectural improvements**:\n    *   Reduces non-matmul operations\n    *   Parallelism expanded across sequence-length dimension\n    *   Optimized warp-level work partitioning\n*   **Performance improvements**:\n    *   ~2× speedup over FlashAttention‑1\n    *   ~225 TFLOPs/s sustained on A100 GPUs (~72% utilization)\n    *   Enhanced backward pass performance\n\n*   Reduces non-matmul operations\n*   Parallelism expanded across sequence-length dimension\n*   Optimized warp-level work partitioning\n\n*   ~2× speedup over FlashAttention‑1\n*   ~225 TFLOPs/s sustained on A100 GPUs (~72% utilization)\n*   Enhanced backward pass performance",
    "order": 10,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 74,
      "contentLength": 870
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#summary",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-architectural-innovations-11",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑3",
    "title": "Architectural Innovations",
    "subtitle": "FlashAttention‑3",
    "contentHtml": "<ul>\n  <li><strong>Asynchrony and pipelined overlap</strong>:\n    <ul>\n      <li>The algorithm exploits hardware asynchrony on Hopper GPUs by assigning warp‑specialized roles: some warps perform matrix multiplications (GEMMs) using new WGMMA instructions, while others concurrently execute softmax and scaling, using ping‑pong scheduling between warp groups to hide memory and compute latency.</li>\n      <li>Block‑wise GEMM and softmax operations are interleaved so that while one block is undergoing matrix multiplication, a previous block is performing softmax, maximizing concurrent utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n        <ul>\n          <li>The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/FlashAttention‑3_1.jpg\" alt=\"\"></p>\n\n        <ul>\n          <li>The following figure from the paper shows 2-stage WGMMA-softmax pipelining.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/FlashAttention‑3_2.jpg\" alt=\"\"></p>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Low‑precision support (FP8 and BF16)</strong>:\n    <ul>\n      <li>FlashAttention‑3 introduces block quantization and incoherent processing to make low‑precision (FP8) usable with low error. Instead of uniform per‑tensor scaling, block‑wise quantization plus outlier handling yields accuracy that is 2.6× better (in terms of RMSE) than baseline FP8 attention implementations.</li>\n      <li>In FP16/BF16 mode, FlashAttention‑3 reaches up to ~740 TFLOPs/s (~75% of H100 peak), while in FP8 mode (e4m3 or e5m2), speeds reach ~1.2 PFLOPs/s—over 1.3× higher than FP16 throughput.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The algorithm exploits hardware asynchrony on Hopper GPUs by assigning warp‑specialized roles: some warps perform matrix multiplications (GEMMs) using new WGMMA instructions, while others concurrently execute softmax and scaling, using ping‑pong scheduling between warp groups to hide memory and compute latency.</li>\n      <li>Block‑wise GEMM and softmax operations are interleaved so that while one block is undergoing matrix multiplication, a previous block is performing softmax, maximizing concurrent utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n        <ul>\n          <li>The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/FlashAttention‑3_1.jpg\" alt=\"\"></p>\n\n        <ul>\n          <li>The following figure from the paper shows 2-stage WGMMA-softmax pipelining.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/FlashAttention‑3_2.jpg\" alt=\"\"></p>\n      </li>\n    </ul>\n<ul>\n          <li>The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.</li>\n        </ul>\n<p><img src=\"../../../images/papers/FlashAttention‑3_1.jpg\" alt=\"\"></p>\n<ul>\n          <li>The following figure from the paper shows 2-stage WGMMA-softmax pipelining.</li>\n        </ul>\n<p><img src=\"../../../images/papers/FlashAttention‑3_2.jpg\" alt=\"\"></p>\n<ul>\n      <li>FlashAttention‑3 introduces block quantization and incoherent processing to make low‑precision (FP8) usable with low error. Instead of uniform per‑tensor scaling, block‑wise quantization plus outlier handling yields accuracy that is 2.6× better (in terms of RMSE) than baseline FP8 attention implementations.</li>\n      <li>In FP16/BF16 mode, FlashAttention‑3 reaches up to ~740 TFLOPs/s (~75% of H100 peak), while in FP8 mode (e4m3 or e5m2), speeds reach ~1.2 PFLOPs/s—over 1.3× higher than FP16 throughput.</li>\n    </ul>",
    "contentMarkdown": "*   **Asynchrony and pipelined overlap**:\n    *   The algorithm exploits hardware asynchrony on Hopper GPUs by assigning warp‑specialized roles: some warps perform matrix multiplications (GEMMs) using new WGMMA instructions, while others concurrently execute softmax and scaling, using ping‑pong scheduling between warp groups to hide memory and compute latency.\n    *   Block‑wise GEMM and softmax operations are interleaved so that while one block is undergoing matrix multiplication, a previous block is performing softmax, maximizing concurrent utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n        \n        *   The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.\n        \n        ![](../../../images/papers/FlashAttention‑3_1.jpg)\n        \n        *   The following figure from the paper shows 2-stage WGMMA-softmax pipelining.\n        \n        ![](../../../images/papers/FlashAttention‑3_2.jpg)\n        \n*   **Low‑precision support (FP8 and BF16)**:\n    *   FlashAttention‑3 introduces block quantization and incoherent processing to make low‑precision (FP8) usable with low error. Instead of uniform per‑tensor scaling, block‑wise quantization plus outlier handling yields accuracy that is 2.6× better (in terms of RMSE) than baseline FP8 attention implementations.\n    *   In FP16/BF16 mode, FlashAttention‑3 reaches up to ~740 TFLOPs/s (~75% of H100 peak), while in FP8 mode (e4m3 or e5m2), speeds reach ~1.2 PFLOPs/s—over 1.3× higher than FP16 throughput.\n\n*   The algorithm exploits hardware asynchrony on Hopper GPUs by assigning warp‑specialized roles: some warps perform matrix multiplications (GEMMs) using new WGMMA instructions, while others concurrently execute softmax and scaling, using ping‑pong scheduling between warp groups to hide memory and compute latency.\n*   Block‑wise GEMM and softmax operations are interleaved so that while one block is undergoing matrix multiplication, a previous block is performing softmax, maximizing concurrent utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n    \n    *   The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.\n    \n    ![](../../../images/papers/FlashAttention‑3_1.jpg)\n    \n    *   The following figure from the paper shows 2-stage WGMMA-softmax pipelining.\n    \n    ![](../../../images/papers/FlashAttention‑3_2.jpg)\n    \n\n*   The following figure from the paper shows ping‑pong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.\n\n![](../../../images/papers/FlashAttention‑3_1.jpg)\n\n*   The following figure from the paper shows 2-stage WGMMA-softmax pipelining.\n\n![](../../../images/papers/FlashAttention‑3_2.jpg)\n\n*   FlashAttention‑3 introduces block quantization and incoherent processing to make low‑precision (FP8) usable with low error. Instead of uniform per‑tensor scaling, block‑wise quantization plus outlier handling yields accuracy that is 2.6× better (in terms of RMSE) than baseline FP8 attention implementations.\n*   In FP16/BF16 mode, FlashAttention‑3 reaches up to ~740 TFLOPs/s (~75% of H100 peak), while in FP8 mode (e4m3 or e5m2), speeds reach ~1.2 PFLOPs/s—over 1.3× higher than FP16 throughput.",
    "order": 11,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 467,
      "contentLength": 4157
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#architectural-innovations",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-performance-summary-12",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑3",
    "title": "Performance Summary",
    "subtitle": "FlashAttention‑3",
    "contentHtml": "<ul>\n  <li><strong>Forward pass</strong>:\n    <ul>\n      <li>FlashAttention‑3 delivers ~1.5–2.0× speedup over FlashAttention‑2 in FP16/BF16 mode, and ~1.5 × in the backward pass (1.5‑1.75×).</li>\n      <li>Peak throughput reported: ~840TFLOPs/s (forward) at ~85% utilization in BF16, ~1.3PFLOPs/s in FP8 mode. These numbers significantly outperform FlashAttention‑2 which tops ~230TFLOPs/s on A100.</li>\n    </ul>\n  </li>\n  <li><strong>Backward pass</strong>:\n    <ul>\n      <li>Backward-side speedups range from ~1.5× to ~1.75× over FlashAttention‑2, leveraging similar asynchrony and quantization-aware recomputation strategies.</li>\n    </ul>\n  </li>\n  <li><strong>Accuracy &amp; numerical stability</strong>:\n    <ul>\n      <li>FP16 performance matches FlashAttention‑2 accuracy, since intermediate rescaling is done in FP32.</li>\n      <li>FP8 mode achieves 2.6× lower RMSE than a standard FP8 baseline thanks to incoherent block quantization and dynamic range handling.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>FlashAttention‑3 delivers ~1.5–2.0× speedup over FlashAttention‑2 in FP16/BF16 mode, and ~1.5 × in the backward pass (1.5‑1.75×).</li>\n      <li>Peak throughput reported: ~840TFLOPs/s (forward) at ~85% utilization in BF16, ~1.3PFLOPs/s in FP8 mode. These numbers significantly outperform FlashAttention‑2 which tops ~230TFLOPs/s on A100.</li>\n    </ul>\n<ul>\n      <li>Backward-side speedups range from ~1.5× to ~1.75× over FlashAttention‑2, leveraging similar asynchrony and quantization-aware recomputation strategies.</li>\n    </ul>\n<ul>\n      <li>FP16 performance matches FlashAttention‑2 accuracy, since intermediate rescaling is done in FP32.</li>\n      <li>FP8 mode achieves 2.6× lower RMSE than a standard FP8 baseline thanks to incoherent block quantization and dynamic range handling.</li>\n    </ul>",
    "contentMarkdown": "*   **Forward pass**:\n    *   FlashAttention‑3 delivers ~1.5–2.0× speedup over FlashAttention‑2 in FP16/BF16 mode, and ~1.5 × in the backward pass (1.5‑1.75×).\n    *   Peak throughput reported: ~840TFLOPs/s (forward) at ~85% utilization in BF16, ~1.3PFLOPs/s in FP8 mode. These numbers significantly outperform FlashAttention‑2 which tops ~230TFLOPs/s on A100.\n*   **Backward pass**:\n    *   Backward-side speedups range from ~1.5× to ~1.75× over FlashAttention‑2, leveraging similar asynchrony and quantization-aware recomputation strategies.\n*   **Accuracy & numerical stability**:\n    *   FP16 performance matches FlashAttention‑2 accuracy, since intermediate rescaling is done in FP32.\n    *   FP8 mode achieves 2.6× lower RMSE than a standard FP8 baseline thanks to incoherent block quantization and dynamic range handling.\n\n*   FlashAttention‑3 delivers ~1.5–2.0× speedup over FlashAttention‑2 in FP16/BF16 mode, and ~1.5 × in the backward pass (1.5‑1.75×).\n*   Peak throughput reported: ~840TFLOPs/s (forward) at ~85% utilization in BF16, ~1.3PFLOPs/s in FP8 mode. These numbers significantly outperform FlashAttention‑2 which tops ~230TFLOPs/s on A100.\n\n*   Backward-side speedups range from ~1.5× to ~1.75× over FlashAttention‑2, leveraging similar asynchrony and quantization-aware recomputation strategies.\n\n*   FP16 performance matches FlashAttention‑2 accuracy, since intermediate rescaling is done in FP32.\n*   FP8 mode achieves 2.6× lower RMSE than a standard FP8 baseline thanks to incoherent block quantization and dynamic range handling.",
    "order": 12,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 199,
      "contentLength": 1828
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-summary",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-implementation-highlights-13",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑3",
    "title": "Implementation Highlights",
    "subtitle": "FlashAttention‑3",
    "contentHtml": "<ul>\n  <li>Uses Hopper-specific instructions: WGMMA (Warpgroup Matrix Multiply Accumulate) and TMA (Tensor Memory Accelerator) for high throughput FP16/BF16/FP8 GEMMs under fine-grained overlap scheduling.</li>\n  <li>Scheduling design decouples GEMM and softmax roles across warp‑groups with ping‑pong execution, minimizing idle cycles and synchronisation stalls.</li>\n  <li>Maintains linear memory (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">O(N \\cdot d)</script>) by streaming and recomputing per‑block normalization factors without storing full attention matrices.</li>\n</ul>",
    "contentMarkdown": "*   Uses Hopper-specific instructions: WGMMA (Warpgroup Matrix Multiply Accumulate) and TMA (Tensor Memory Accelerator) for high throughput FP16/BF16/FP8 GEMMs under fine-grained overlap scheduling.\n*   Scheduling design decouples GEMM and softmax roles across warp‑groups with ping‑pong execution, minimizing idle cycles and synchronisation stalls.\n*   Maintains linear memory (O(N⋅d)O(N⋅d)O(N \\\\cdot d)) by streaming and recomputing per‑block normalization factors without storing full attention matrices.",
    "order": 13,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 61,
      "contentLength": 2575
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#implementation-highlights",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-summary-14",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "FlashAttention‑3",
    "title": "Summary",
    "subtitle": "FlashAttention‑3",
    "contentHtml": "<ul>\n  <li>FlashAttention‑3 fully exploits Hopper hardware features (asynchrony, TMA, low‑precision support).</li>\n  <li>Achieves ~1.5–2.0x speedups over FlashAttention‑2 and up to 1.3PFLOPs/s using FP8, with much improved numerical error.</li>\n  <li>Combines warp‑specialization, pipelined overlap, and block quantization for superior architectural and performance gains.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑3 fully exploits Hopper hardware features (asynchrony, TMA, low‑precision support).\n*   Achieves ~1.5–2.0x speedups over FlashAttention‑2 and up to 1.3PFLOPs/s using FP8, with much improved numerical error.\n*   Combines warp‑specialization, pipelined overlap, and block quantization for superior architectural and performance gains.",
    "order": 14,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 42,
      "contentLength": 383
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#summary",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-overview-15",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Comparative Analysis",
    "title": "Overview",
    "subtitle": "Comparative Analysis",
    "contentHtml": "<ul>\n  <li>FlashAttention‑1 pioneered the IO‑aware fused attention kernel with tiling and streaming softmax normalization.</li>\n  <li>FlashAttention‑2 rearchitected the work distribution and reduced non‑matmul overheads.</li>\n  <li>FlashAttention‑3 adopts Hopper‑GPU asynchrony and FP8 quantization techniques to unlock peak GPU performance.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑1 pioneered the IO‑aware fused attention kernel with tiling and streaming softmax normalization.\n*   FlashAttention‑2 rearchitected the work distribution and reduced non‑matmul overheads.\n*   FlashAttention‑3 adopts Hopper‑GPU asynchrony and FP8 quantization techniques to unlock peak GPU performance.",
    "order": 15,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 38,
      "contentLength": 352
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#overview",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-architectural-differences-16",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Comparative Analysis",
    "title": "Architectural Differences",
    "subtitle": "Comparative Analysis",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>FlashAttention‑1</strong>:</p>\n\n    <ul>\n      <li>Uses a single fused CUDA kernel per head/layer combining <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-276\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-277\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-279\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">QK^T</script>, mask, softmax, dropout, and output multiplication to minimize HBM‑SRAM transfers.</li>\n      <li>Tiles Q, K, V into SRAM‑sized blocks; recomputes per‑block softmax normalization for numerical stability.</li>\n      <li>Parallelism primarily over batch and heads; limited use of sequence‑length concurrency.</li>\n      <li>I/O‐optimal design—provably requires <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-289\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">O(N \\cdot d)</script> memory traffic lower bound for practical SRAM sizes.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑2</strong>:</p>\n\n    <ul>\n      <li>Introduces parallelism across sequence‐length dimension by splitting head computations over multiple thread blocks.</li>\n      <li>Reduces non‑GEMM FLOPs by delaying softmax scaling operations—to eliminate redundant normalization across blocks.</li>\n      <li>Implemented using CUTLASS and CuTe, targeting improved occupancy and thread‑block coordination.</li>\n      <li>Enhanced warp‐group partitioning to reduce shared memory sync overhead.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑3</strong>:</p>\n\n    <ul>\n      <li>Designed for NVIDIA Hopper (H100) hardware, using warp specialization and asynchronous scheduling: some warps perform WGMMA GEMMs, others perform softmax/scaling, overlapping computation.</li>\n      <li>Pipeline GEMM and softmax per block in ping‑pong fashion across warp groups to maximize utilization of Tensor Cores and Tensor Memory Accelerator (TMA).</li>\n      <li>Introduces block-wise FP8 quantization with incoherent processing and dynamic outlier handling to minimize numerical error.</li>\n      <li>Leverages Hopper’s WGMMA and TMA instructions to sustain high throughput in both low-precision and standard FP16/BF16 modes.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>FlashAttention‑1</strong>:</p>\n<ul>\n      <li>Uses a single fused CUDA kernel per head/layer combining <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-276\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-277\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-279\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">QK^T</script>, mask, softmax, dropout, and output multiplication to minimize HBM‑SRAM transfers.</li>\n      <li>Tiles Q, K, V into SRAM‑sized blocks; recomputes per‑block softmax normalization for numerical stability.</li>\n      <li>Parallelism primarily over batch and heads; limited use of sequence‑length concurrency.</li>\n      <li>I/O‐optimal design—provably requires <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-289\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">O(N \\cdot d)</script> memory traffic lower bound for practical SRAM sizes.</li>\n    </ul>\n<p><strong>FlashAttention‑2</strong>:</p>\n<ul>\n      <li>Introduces parallelism across sequence‐length dimension by splitting head computations over multiple thread blocks.</li>\n      <li>Reduces non‑GEMM FLOPs by delaying softmax scaling operations—to eliminate redundant normalization across blocks.</li>\n      <li>Implemented using CUTLASS and CuTe, targeting improved occupancy and thread‑block coordination.</li>\n      <li>Enhanced warp‐group partitioning to reduce shared memory sync overhead.</li>\n    </ul>\n<p><strong>FlashAttention‑3</strong>:</p>\n<ul>\n      <li>Designed for NVIDIA Hopper (H100) hardware, using warp specialization and asynchronous scheduling: some warps perform WGMMA GEMMs, others perform softmax/scaling, overlapping computation.</li>\n      <li>Pipeline GEMM and softmax per block in ping‑pong fashion across warp groups to maximize utilization of Tensor Cores and Tensor Memory Accelerator (TMA).</li>\n      <li>Introduces block-wise FP8 quantization with incoherent processing and dynamic outlier handling to minimize numerical error.</li>\n      <li>Leverages Hopper’s WGMMA and TMA instructions to sustain high throughput in both low-precision and standard FP16/BF16 modes.</li>\n    </ul>",
    "contentMarkdown": "*   **FlashAttention‑1**:\n    \n    *   Uses a single fused CUDA kernel per head/layer combining QKTQKTQK^T, mask, softmax, dropout, and output multiplication to minimize HBM‑SRAM transfers.\n    *   Tiles Q, K, V into SRAM‑sized blocks; recomputes per‑block softmax normalization for numerical stability.\n    *   Parallelism primarily over batch and heads; limited use of sequence‑length concurrency.\n    *   I/O‐optimal design—provably requires O(N⋅d)O(N⋅d)O(N \\\\cdot d) memory traffic lower bound for practical SRAM sizes.\n*   **FlashAttention‑2**:\n    \n    *   Introduces parallelism across sequence‐length dimension by splitting head computations over multiple thread blocks.\n    *   Reduces non‑GEMM FLOPs by delaying softmax scaling operations—to eliminate redundant normalization across blocks.\n    *   Implemented using CUTLASS and CuTe, targeting improved occupancy and thread‑block coordination.\n    *   Enhanced warp‐group partitioning to reduce shared memory sync overhead.\n*   **FlashAttention‑3**:\n    \n    *   Designed for NVIDIA Hopper (H100) hardware, using warp specialization and asynchronous scheduling: some warps perform WGMMA GEMMs, others perform softmax/scaling, overlapping computation.\n    *   Pipeline GEMM and softmax per block in ping‑pong fashion across warp groups to maximize utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n    *   Introduces block-wise FP8 quantization with incoherent processing and dynamic outlier handling to minimize numerical error.\n    *   Leverages Hopper’s WGMMA and TMA instructions to sustain high throughput in both low-precision and standard FP16/BF16 modes.\n\n**FlashAttention‑1**:\n\n*   Uses a single fused CUDA kernel per head/layer combining QKTQKTQK^T, mask, softmax, dropout, and output multiplication to minimize HBM‑SRAM transfers.\n*   Tiles Q, K, V into SRAM‑sized blocks; recomputes per‑block softmax normalization for numerical stability.\n*   Parallelism primarily over batch and heads; limited use of sequence‑length concurrency.\n*   I/O‐optimal design—provably requires O(N⋅d)O(N⋅d)O(N \\\\cdot d) memory traffic lower bound for practical SRAM sizes.\n\n**FlashAttention‑2**:\n\n*   Introduces parallelism across sequence‐length dimension by splitting head computations over multiple thread blocks.\n*   Reduces non‑GEMM FLOPs by delaying softmax scaling operations—to eliminate redundant normalization across blocks.\n*   Implemented using CUTLASS and CuTe, targeting improved occupancy and thread‑block coordination.\n*   Enhanced warp‐group partitioning to reduce shared memory sync overhead.\n\n**FlashAttention‑3**:\n\n*   Designed for NVIDIA Hopper (H100) hardware, using warp specialization and asynchronous scheduling: some warps perform WGMMA GEMMs, others perform softmax/scaling, overlapping computation.\n*   Pipeline GEMM and softmax per block in ping‑pong fashion across warp groups to maximize utilization of Tensor Cores and Tensor Memory Accelerator (TMA).\n*   Introduces block-wise FP8 quantization with incoherent processing and dynamic outlier handling to minimize numerical error.\n*   Leverages Hopper’s WGMMA and TMA instructions to sustain high throughput in both low-precision and standard FP16/BF16 modes.",
    "order": 16,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "dropout"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 397,
      "contentLength": 11942
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#architectural-differences",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-performance-comparison-17",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Comparative Analysis",
    "title": "Performance Comparison",
    "subtitle": "Comparative Analysis",
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Version</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Target GPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Forward Speedup</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Peak Throughput</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Backward Speedup</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Numerical Accuracy (Low‑prec)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑1</td>\n<td class=\"tg-tleft-valign-first\">Ampere / A100</td>\n<td class=\"tg-tleft-valign-first\">~3× over PyTorch on GPT‑2 (seq=1K)</td>\n<td class=\"tg-tleft-valign-first\">~30–50% utilization</td>\n<td class=\"tg-tleft-valign-first\">~ similar to baseline</td>\n<td class=\"tg-tleft-valign-second\">Full FP16/BF16 accuracy; exact attention</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑2</td>\n<td class=\"tg-tleft-valign-first\">Ampere / A100</td>\n<td class=\"tg-tleft-valign-first\">~2× over v1</td>\n<td class=\"tg-tleft-valign-first\">~225TFLOPs/s (~72%)</td>\n<td class=\"tg-tleft-valign-first\">~2–4× over naive backward</td>\n<td class=\"tg-tleft-valign-second\">Same full precision accuracy</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑3</td>\n<td class=\"tg-tleft-valign-first\">Hopper / H100</td>\n<td class=\"tg-tleft-valign-first\">~1.5–2× over v2 (FP16)</td>\n<td class=\"tg-tleft-valign-first\">~740TFLOPs/s (~75% BF16); ~1.2–1.3PFLOPs/s (FP8)</td>\n<td class=\"tg-tleft-valign-first\">~1.5–1.75× over v2</td>\n<td class=\"tg-tleft-valign-second\">FP8 RMSE ~2.6× lower than baseline FP8; full precision accuracy preserved.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Version</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Target GPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Forward Speedup</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Peak Throughput</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Backward Speedup</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Numerical Accuracy (Low‑prec)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑1</td>\n<td class=\"tg-tleft-valign-first\">Ampere / A100</td>\n<td class=\"tg-tleft-valign-first\">~3× over PyTorch on GPT‑2 (seq=1K)</td>\n<td class=\"tg-tleft-valign-first\">~30–50% utilization</td>\n<td class=\"tg-tleft-valign-first\">~ similar to baseline</td>\n<td class=\"tg-tleft-valign-second\">Full FP16/BF16 accuracy; exact attention</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑2</td>\n<td class=\"tg-tleft-valign-first\">Ampere / A100</td>\n<td class=\"tg-tleft-valign-first\">~2× over v1</td>\n<td class=\"tg-tleft-valign-first\">~225TFLOPs/s (~72%)</td>\n<td class=\"tg-tleft-valign-first\">~2–4× over naive backward</td>\n<td class=\"tg-tleft-valign-second\">Same full precision accuracy</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlashAttention‑3</td>\n<td class=\"tg-tleft-valign-first\">Hopper / H100</td>\n<td class=\"tg-tleft-valign-first\">~1.5–2× over v2 (FP16)</td>\n<td class=\"tg-tleft-valign-first\">~740TFLOPs/s (~75% BF16); ~1.2–1.3PFLOPs/s (FP8)</td>\n<td class=\"tg-tleft-valign-first\">~1.5–1.75× over v2</td>\n<td class=\"tg-tleft-valign-second\">FP8 RMSE ~2.6× lower than baseline FP8; full precision accuracy preserved.</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Version**\n\n**Target GPU**\n\n**Forward Speedup**\n\n**Peak Throughput**\n\n**Backward Speedup**\n\n**Numerical Accuracy (Low‑prec)**\n\nFlashAttention‑1\n\nAmpere / A100\n\n~3× over PyTorch on GPT‑2 (seq=1K)\n\n~30–50% utilization\n\n~ similar to baseline\n\nFull FP16/BF16 accuracy; exact attention\n\nFlashAttention‑2\n\nAmpere / A100\n\n~2× over v1\n\n~225TFLOPs/s (~72%)\n\n~2–4× over naive backward\n\nSame full precision accuracy\n\nFlashAttention‑3\n\nHopper / H100\n\n~1.5–2× over v2 (FP16)\n\n~740TFLOPs/s (~75% BF16); ~1.2–1.3PFLOPs/s (FP8)\n\n~1.5–1.75× over v2\n\nFP8 RMSE ~2.6× lower than baseline FP8; full precision accuracy preserved.\n\n**Version**\n\n**Target GPU**\n\n**Forward Speedup**\n\n**Peak Throughput**\n\n**Backward Speedup**\n\n**Numerical Accuracy (Low‑prec)**\n\nFlashAttention‑1\n\nAmpere / A100\n\n~3× over PyTorch on GPT‑2 (seq=1K)\n\n~30–50% utilization\n\n~ similar to baseline\n\nFull FP16/BF16 accuracy; exact attention\n\nFlashAttention‑2\n\nAmpere / A100\n\n~2× over v1\n\n~225TFLOPs/s (~72%)\n\n~2–4× over naive backward\n\nSame full precision accuracy\n\nFlashAttention‑3\n\nHopper / H100\n\n~1.5–2× over v2 (FP16)\n\n~740TFLOPs/s (~75% BF16); ~1.2–1.3PFLOPs/s (FP8)\n\n~1.5–1.75× over v2\n\nFP8 RMSE ~2.6× lower than baseline FP8; full precision accuracy preserved.",
    "order": 17,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 154,
      "contentLength": 3471
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-comparison",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-algorithmic-io-differences-18",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Comparative Analysis",
    "title": "Algorithmic & I/O Differences",
    "subtitle": "Comparative Analysis",
    "contentHtml": "<ul>\n  <li>All versions maintain I/O-optimal behavior: FlashAttention‑1 achieves <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-290\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-291\"><span class=\"mi\" id=\"MathJax-Span-292\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">O(N \\cdot d)</script> data movement and is provably asymptotically optimal for typical SRAM sizes.</li>\n  <li>FlashAttention‑2 maintains the same I/O characteristics, but reduces extra computation overhead by reducing normalization passes.</li>\n  <li>FlashAttention‑3 retains I/O efficiency while introducing asynchronous overlap and low-precision formats to reduce HBM bandwidth use and maximize on-chip computation.</li>\n</ul>",
    "contentMarkdown": "*   All versions maintain I/O-optimal behavior: FlashAttention‑1 achieves O(N⋅d)O(N⋅d)O(N \\\\cdot d) data movement and is provably asymptotically optimal for typical SRAM sizes.\n*   FlashAttention‑2 maintains the same I/O characteristics, but reduces extra computation overhead by reducing normalization passes.\n*   FlashAttention‑3 retains I/O efficiency while introducing asynchronous overlap and low-precision formats to reduce HBM bandwidth use and maximize on-chip computation.",
    "order": 18,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 59,
      "contentLength": 2549
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#algorithmic-&-i/o-differences",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-summary-19",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Comparative Analysis",
    "title": "Summary",
    "subtitle": "Comparative Analysis",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>FlashAttention‑1</strong> introduced an I/O-aware fused GPU kernel combining <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-298\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-301\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">QK^T</script>, mask, softmax, and value multiplication. Using tiling and streaming softmax normalization, it reduces memory traffic from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-308\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">O(N^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-315\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">O(N \\cdot d)</script>, enabling exact attention with faster throughput on Ampere and earlier GPUs. Delivered ~3× speedup on GPT‑2 and ~15% on BERT‑large.</p>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑2</strong> introduced parallelism over sequence length, optimized work partitioning across warps, and reduction of non-matmul FLOPs. Achieved ~2× speedup over v1, up to ~225 TFLOPs/s and ~72% FLOPs utilization on A100 GPUs; supports head dimensions up to 256.</p>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑3</strong> exploited Hopper‑specific (H100) features—low‑precision support, warp-specialization, asynchronous pipelined GEMM-softmax execution, and block-wise FP8 quantization with incoherent processing. Achieved ~1.5–2× speedup over v2 in FP16/BF16 (~740 TFLOPs/s, ~75% utilization) and ~1.2 PFLOPs/s in FP8 mode, with ~2.6× lower numerical error than baseline FP8.</p>\n  </li>\n</ul>\n<p><strong>FlashAttention‑1</strong> introduced an I/O-aware fused GPU kernel combining <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-298\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-301\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">QK^T</script>, mask, softmax, and value multiplication. Using tiling and streaming softmax normalization, it reduces memory traffic from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-308\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">O(N^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-315\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">O(N \\cdot d)</script>, enabling exact attention with faster throughput on Ampere and earlier GPUs. Delivered ~3× speedup on GPT‑2 and ~15% on BERT‑large.</p>\n<p><strong>FlashAttention‑2</strong> introduced parallelism over sequence length, optimized work partitioning across warps, and reduction of non-matmul FLOPs. Achieved ~2× speedup over v1, up to ~225 TFLOPs/s and ~72% FLOPs utilization on A100 GPUs; supports head dimensions up to 256.</p>\n<p><strong>FlashAttention‑3</strong> exploited Hopper‑specific (H100) features—low‑precision support, warp-specialization, asynchronous pipelined GEMM-softmax execution, and block-wise FP8 quantization with incoherent processing. Achieved ~1.5–2× speedup over v2 in FP16/BF16 (~740 TFLOPs/s, ~75% utilization) and ~1.2 PFLOPs/s in FP8 mode, with ~2.6× lower numerical error than baseline FP8.</p>",
    "contentMarkdown": "*   **FlashAttention‑1** introduced an I/O-aware fused GPU kernel combining QKTQKTQK^T, mask, softmax, and value multiplication. Using tiling and streaming softmax normalization, it reduces memory traffic from O(N2)O(N2)O(N^2) to O(N⋅d)O(N⋅d)O(N \\\\cdot d), enabling exact attention with faster throughput on Ampere and earlier GPUs. Delivered ~3× speedup on GPT‑2 and ~15% on BERT‑large.\n    \n*   **FlashAttention‑2** introduced parallelism over sequence length, optimized work partitioning across warps, and reduction of non-matmul FLOPs. Achieved ~2× speedup over v1, up to ~225 TFLOPs/s and ~72% FLOPs utilization on A100 GPUs; supports head dimensions up to 256.\n    \n*   **FlashAttention‑3** exploited Hopper‑specific (H100) features—low‑precision support, warp-specialization, asynchronous pipelined GEMM-softmax execution, and block-wise FP8 quantization with incoherent processing. Achieved ~1.5–2× speedup over v2 in FP16/BF16 (~740 TFLOPs/s, ~75% utilization) and ~1.2 PFLOPs/s in FP8 mode, with ~2.6× lower numerical error than baseline FP8.\n    \n\n**FlashAttention‑1** introduced an I/O-aware fused GPU kernel combining QKTQKTQK^T, mask, softmax, and value multiplication. Using tiling and streaming softmax normalization, it reduces memory traffic from O(N2)O(N2)O(N^2) to O(N⋅d)O(N⋅d)O(N \\\\cdot d), enabling exact attention with faster throughput on Ampere and earlier GPUs. Delivered ~3× speedup on GPT‑2 and ~15% on BERT‑large.\n\n**FlashAttention‑2** introduced parallelism over sequence length, optimized work partitioning across warps, and reduction of non-matmul FLOPs. Achieved ~2× speedup over v1, up to ~225 TFLOPs/s and ~72% FLOPs utilization on A100 GPUs; supports head dimensions up to 256.\n\n**FlashAttention‑3** exploited Hopper‑specific (H100) features—low‑precision support, warp-specialization, asynchronous pipelined GEMM-softmax execution, and block-wise FP8 quantization with incoherent processing. Achieved ~1.5–2× speedup over v2 in FP16/BF16 (~740 TFLOPs/s, ~75% utilization) and ~1.2 PFLOPs/s in FP8 mode, with ~2.6× lower numerical error than baseline FP8.",
    "order": 19,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "bert",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 265,
      "contentLength": 15121
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#summary",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-accuracy-and-numerical-stability-20",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "title": "Accuracy and Numerical Stability",
    "subtitle": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "contentHtml": "<ul>\n  <li>\n    <p>All versions preserve full FP16/BF16 accuracy because intermediate normalization (softmax reduction) is computed in FP32 to avoid precision loss. None introduce approximation in the attention calculation itself.</p>\n  </li>\n  <li>\n    <p>FlashAttention‑3 extends this to low-precision FP8 operation by using block-wise quantization and incoherent processing to dramatically reduce quantization error. It achieves about 2.6× lower RMSE compared to baseline FP8 attention implementations.</p>\n  </li>\n</ul>\n<p>All versions preserve full FP16/BF16 accuracy because intermediate normalization (softmax reduction) is computed in FP32 to avoid precision loss. None introduce approximation in the attention calculation itself.</p>\n<p>FlashAttention‑3 extends this to low-precision FP8 operation by using block-wise quantization and incoherent processing to dramatically reduce quantization error. It achieves about 2.6× lower RMSE compared to baseline FP8 attention implementations.</p>",
    "contentMarkdown": "*   All versions preserve full FP16/BF16 accuracy because intermediate normalization (softmax reduction) is computed in FP32 to avoid precision loss. None introduce approximation in the attention calculation itself.\n    \n*   FlashAttention‑3 extends this to low-precision FP8 operation by using block-wise quantization and incoherent processing to dramatically reduce quantization error. It achieves about 2.6× lower RMSE compared to baseline FP8 attention implementations.\n    \n\nAll versions preserve full FP16/BF16 accuracy because intermediate normalization (softmax reduction) is computed in FP32 to avoid precision loss. None introduce approximation in the attention calculation itself.\n\nFlashAttention‑3 extends this to low-precision FP8 operation by using block-wise quantization and incoherent processing to dramatically reduce quantization error. It achieves about 2.6× lower RMSE compared to baseline FP8 attention implementations.",
    "order": 20,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 118,
      "contentLength": 998
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#accuracy-and-numerical-stability",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-practical-hardware-compatibility-21",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "title": "Practical Hardware Compatibility",
    "subtitle": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>FlashAttention‑1</strong> works on Ampere (e.g. A100), Ada, and earlier GPUs; it’s CPU-and-CUDA compatible and only requires standard CUDA/CUTLASS or Triton backends. Offers significant benefits for long-sequence attention workloads.</p>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑2</strong> supports the same GPUs but is optimized further via CUTLASS 3.x and CuTe, requiring GPU compute capabilities Ampere or later for BF16 support.</p>\n  </li>\n  <li>\n    <p><strong>FlashAttention‑3</strong> requires Hopper architecture GPUs (e.g. H100, H800) and CUDA version ≥12.3 (ideally ≥12.8). Only one version currently supports FP8 forward and partial backward.</p>\n  </li>\n</ul>\n<p><strong>FlashAttention‑1</strong> works on Ampere (e.g. A100), Ada, and earlier GPUs; it’s CPU-and-CUDA compatible and only requires standard CUDA/CUTLASS or Triton backends. Offers significant benefits for long-sequence attention workloads.</p>\n<p><strong>FlashAttention‑2</strong> supports the same GPUs but is optimized further via CUTLASS 3.x and CuTe, requiring GPU compute capabilities Ampere or later for BF16 support.</p>\n<p><strong>FlashAttention‑3</strong> requires Hopper architecture GPUs (e.g. H100, H800) and CUDA version ≥12.3 (ideally ≥12.8). Only one version currently supports FP8 forward and partial backward.</p>",
    "contentMarkdown": "*   **FlashAttention‑1** works on Ampere (e.g. A100), Ada, and earlier GPUs; it’s CPU-and-CUDA compatible and only requires standard CUDA/CUTLASS or Triton backends. Offers significant benefits for long-sequence attention workloads.\n    \n*   **FlashAttention‑2** supports the same GPUs but is optimized further via CUTLASS 3.x and CuTe, requiring GPU compute capabilities Ampere or later for BF16 support.\n    \n*   **FlashAttention‑3** requires Hopper architecture GPUs (e.g. H100, H800) and CUDA version ≥12.3 (ideally ≥12.8). Only one version currently supports FP8 forward and partial backward.\n    \n\n**FlashAttention‑1** works on Ampere (e.g. A100), Ada, and earlier GPUs; it’s CPU-and-CUDA compatible and only requires standard CUDA/CUTLASS or Triton backends. Offers significant benefits for long-sequence attention workloads.\n\n**FlashAttention‑2** supports the same GPUs but is optimized further via CUTLASS 3.x and CuTe, requiring GPU compute capabilities Ampere or later for BF16 support.\n\n**FlashAttention‑3** requires Hopper architecture GPUs (e.g. H100, H800) and CUDA version ≥12.3 (ideally ≥12.8). Only one version currently supports FP8 forward and partial backward.",
    "order": 21,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 155,
      "contentLength": 1339
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#practical-hardware-compatibility",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-integration-api-details-22",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "title": "Integration & API Details",
    "subtitle": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "contentHtml": "<ul>\n  <li>\n    <p>FlashAttention is available as a PyTorch C++/CUDA extension via <code class=\"language-plaintext highlighter-rouge\">flash_attn_interface</code> or via Triton. Typical usage replaces standard <code class=\"language-plaintext highlighter-rouge\">scaled_dot_product_attention</code> in PyTorch or in frameworks like DeepSpeed/Megatron.</p>\n  </li>\n  <li>\n    <p>For FlashAttention‑2 and 3, the library exposes optimized kernels and automatically dispatches based on GPU architecture and precision flags (e.g. FP16 vs FP8). Some pipeline frameworks like Triton or CUDA Graphs may require manual configuration for optimal low‑latency inference.</p>\n  </li>\n</ul>\n<p>FlashAttention is available as a PyTorch C++/CUDA extension via <code class=\"language-plaintext highlighter-rouge\">flash_attn_interface</code> or via Triton. Typical usage replaces standard <code class=\"language-plaintext highlighter-rouge\">scaled_dot_product_attention</code> in PyTorch or in frameworks like DeepSpeed/Megatron.</p>\n<p>For FlashAttention‑2 and 3, the library exposes optimized kernels and automatically dispatches based on GPU architecture and precision flags (e.g. FP16 vs FP8). Some pipeline frameworks like Triton or CUDA Graphs may require manual configuration for optimal low‑latency inference.</p>",
    "contentMarkdown": "*   FlashAttention is available as a PyTorch C++/CUDA extension via `flash_attn_interface` or via Triton. Typical usage replaces standard `scaled_dot_product_attention` in PyTorch or in frameworks like DeepSpeed/Megatron.\n    \n*   For FlashAttention‑2 and 3, the library exposes optimized kernels and automatically dispatches based on GPU architecture and precision flags (e.g. FP16 vs FP8). Some pipeline frameworks like Triton or CUDA Graphs may require manual configuration for optimal low‑latency inference.\n    \n\nFlashAttention is available as a PyTorch C++/CUDA extension via `flash_attn_interface` or via Triton. Typical usage replaces standard `scaled_dot_product_attention` in PyTorch or in frameworks like DeepSpeed/Megatron.\n\nFor FlashAttention‑2 and 3, the library exposes optimized kernels and automatically dispatches based on GPU architecture and precision flags (e.g. FP16 vs FP8). Some pipeline frameworks like Triton or CUDA Graphs may require manual configuration for optimal low‑latency inference.",
    "order": 22,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 130,
      "contentLength": 1298
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#integration-&-api-details",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-resource-memory-usage-23",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "title": "Resource & Memory Usage",
    "subtitle": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "contentHtml": "<ul>\n  <li>\n    <p>All versions maintain <strong>linear memory usage</strong>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">O(N \\cdot d)</script>, compared to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-328\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-329\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-332\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-334\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">O(N^2)</script> of naive attention, enabling context lengths up to 64K or more with full precision.</p>\n  </li>\n  <li>\n    <p>GPU shared memory/register usage is tightly tuned. In FlashAttention‑3, large tile sizes and async overlap are enabled by Hopper’s TMA and WGMMA, though register pressure may increase and limit maximum head-dim or batch size.</p>\n  </li>\n</ul>\n<p>All versions maintain <strong>linear memory usage</strong>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">O(N \\cdot d)</script>, compared to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-328\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-329\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-332\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-334\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">O(N^2)</script> of naive attention, enabling context lengths up to 64K or more with full precision.</p>\n<p>GPU shared memory/register usage is tightly tuned. In FlashAttention‑3, large tile sizes and async overlap are enabled by Hopper’s TMA and WGMMA, though register pressure may increase and limit maximum head-dim or batch size.</p>",
    "contentMarkdown": "*   All versions maintain **linear memory usage**, O(N⋅d)O(N⋅d)O(N \\\\cdot d), compared to O(N2)O(N2)O(N^2) of naive attention, enabling context lengths up to 64K or more with full precision.\n    \n*   GPU shared memory/register usage is tightly tuned. In FlashAttention‑3, large tile sizes and async overlap are enabled by Hopper’s TMA and WGMMA, though register pressure may increase and limit maximum head-dim or batch size.\n    \n\nAll versions maintain **linear memory usage**, O(N⋅d)O(N⋅d)O(N \\\\cdot d), compared to O(N2)O(N2)O(N^2) of naive attention, enabling context lengths up to 64K or more with full precision.\n\nGPU shared memory/register usage is tightly tuned. In FlashAttention‑3, large tile sizes and async overlap are enabled by Hopper’s TMA and WGMMA, though register pressure may increase and limit maximum head-dim or batch size.",
    "order": 23,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 122,
      "contentLength": 9612
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#resource-&-memory-usage",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-when-to-use-which-version-24",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "title": "When to Use Which Version",
    "subtitle": "Accuracy Trade-offs, Practical Considerations, and Integration Guidance",
    "contentHtml": "<ul>\n  <li>\n    <p>If you’re on Ampere-class GPUs or doing training/inference with FP16/BF16 and want a robust, well-tested solution: <strong>FlashAttention‑2</strong> is the safe and high-performance default.</p>\n  </li>\n  <li>\n    <p>If you need full compatibility with older GPUs or have simpler integration needs, <strong>FlashAttention‑1</strong> still provides excellent memory savings and speedups without hardware-specific dependencies.</p>\n  </li>\n  <li>\n    <p>If you have access to Hopper GPUs and want maximal throughput (especially with FP8), <strong>FlashAttention‑3</strong> is the best choice—but be aware of hardware and software requirements. Quantization accuracy is excellent, but backward support in FP8 may be limited in early releases.</p>\n  </li>\n</ul>\n<p>If you’re on Ampere-class GPUs or doing training/inference with FP16/BF16 and want a robust, well-tested solution: <strong>FlashAttention‑2</strong> is the safe and high-performance default.</p>\n<p>If you need full compatibility with older GPUs or have simpler integration needs, <strong>FlashAttention‑1</strong> still provides excellent memory savings and speedups without hardware-specific dependencies.</p>\n<p>If you have access to Hopper GPUs and want maximal throughput (especially with FP8), <strong>FlashAttention‑3</strong> is the best choice—but be aware of hardware and software requirements. Quantization accuracy is excellent, but backward support in FP8 may be limited in early releases.</p>",
    "contentMarkdown": "*   If you’re on Ampere-class GPUs or doing training/inference with FP16/BF16 and want a robust, well-tested solution: **FlashAttention‑2** is the safe and high-performance default.\n    \n*   If you need full compatibility with older GPUs or have simpler integration needs, **FlashAttention‑1** still provides excellent memory savings and speedups without hardware-specific dependencies.\n    \n*   If you have access to Hopper GPUs and want maximal throughput (especially with FP8), **FlashAttention‑3** is the best choice—but be aware of hardware and software requirements. Quantization accuracy is excellent, but backward support in FP8 may be limited in early releases.\n    \n\nIf you’re on Ampere-class GPUs or doing training/inference with FP16/BF16 and want a robust, well-tested solution: **FlashAttention‑2** is the safe and high-performance default.\n\nIf you need full compatibility with older GPUs or have simpler integration needs, **FlashAttention‑1** still provides excellent memory savings and speedups without hardware-specific dependencies.\n\nIf you have access to Hopper GPUs and want maximal throughput (especially with FP8), **FlashAttention‑3** is the best choice—but be aware of hardware and software requirements. Quantization accuracy is excellent, but backward support in FP8 may be limited in early releases.",
    "order": 24,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 179,
      "contentLength": 1485
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#when-to-use-which-version",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-performance-benchmarks-25",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "title": "Performance Benchmarks",
    "subtitle": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "contentHtml": "<h4 id=\"flashattention2-ampere--a100\">FlashAttention‑2 (Ampere / A100)</h4>\n<ul>\n  <li>On an NVIDIA A100 GPU, FlashAttention‑2 reaches forward‑pass throughput up to <strong>230TFLOPs/s</strong>, about <strong>50–73%</strong> of theoretical peak FP16/BF16 performance. Backward pass performance hits up to <strong>63%</strong> of peak, significantly improving on v1.\nEnd-to-end training throughput for GPT-style models reaches about <strong>225TFLOPs/s per A100 GPU</strong>, achieving roughly <strong>72% model FLOPs utilization</strong>.\nIt provides ~<strong>2× speedup over FlashAttention‑1</strong>, and up to <strong>3–9× speedup over naïve PyTorch attention</strong> in benchmarks.</li>\n</ul>\n<h4 id=\"flashattention3-hopper--h100\">FlashAttention‑3 (Hopper / H100)</h4>\n<ul>\n  <li>On NVIDIA H100 GPUs, FP16/BF16 mode hits <strong>~740TFLOPs/s</strong> (~75% utilization) and FP8 mode approaches <strong>~1.2PFLOPs/s</strong>, delivering <strong>1.5–2× speedups over FlashAttention‑2</strong>.\nFP8 operation also achieves <strong>~2.6× lower numerical error (RMSE)</strong> compared to baseline FP8 attention implementations.</li>\n</ul>\n<h4 id=\"comparative-summary\">Comparative Summary</h4>\n<ul>\n  <li>For Ampere/A100, FlashAttention‑2 delivers around <strong>2× performance gain</strong> over v1.</li>\n  <li>On Hopper/H100, FlashAttention‑3 boosts FP16 throughput by <strong>1.5–2×</strong> and FP8 performance to <strong>1.2PFLOPs/s</strong>, with high accuracy.</li>\n  <li>Across versions, attention performance grows from ~50TFLOPs/s in v1 to over <strong>1PFLOP/s in v3</strong> when using FP8.</li>\n</ul>",
    "contentMarkdown": "#### FlashAttention‑2 (Ampere / A100)\n\n*   On an NVIDIA A100 GPU, FlashAttention‑2 reaches forward‑pass throughput up to **230TFLOPs/s**, about **50–73%** of theoretical peak FP16/BF16 performance. Backward pass performance hits up to **63%** of peak, significantly improving on v1. End-to-end training throughput for GPT-style models reaches about **225TFLOPs/s per A100 GPU**, achieving roughly **72% model FLOPs utilization**. It provides ~**2× speedup over FlashAttention‑1**, and up to **3–9× speedup over naïve PyTorch attention** in benchmarks.\n\n#### FlashAttention‑3 (Hopper / H100)\n\n*   On NVIDIA H100 GPUs, FP16/BF16 mode hits **~740TFLOPs/s** (~75% utilization) and FP8 mode approaches **~1.2PFLOPs/s**, delivering **1.5–2× speedups over FlashAttention‑2**. FP8 operation also achieves **~2.6× lower numerical error (RMSE)** compared to baseline FP8 attention implementations.\n\n#### Comparative Summary\n\n*   For Ampere/A100, FlashAttention‑2 delivers around **2× performance gain** over v1.\n*   On Hopper/H100, FlashAttention‑3 boosts FP16 throughput by **1.5–2×** and FP8 performance to **1.2PFLOPs/s**, with high accuracy.\n*   Across versions, attention performance grows from ~50TFLOPs/s in v1 to over **1PFLOP/s in v3** when using FP8.",
    "order": 25,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 163,
      "contentLength": 1613
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-benchmarks",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-integration-code-examples-26",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "title": "Integration & Code Examples",
    "subtitle": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "contentHtml": "<h4 id=\"installing-flashattention-v1--v2\">Installing FlashAttention (v1 &amp; V2)</h4>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">pip <span class=\"nb\">install </span>flash-attn\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">pip <span class=\"nb\">install </span>flash-attn\n</code></pre>\n<ul>\n  <li>This provides both FlashAttention‑1 and ‑2 implementations in the official <a href=\"https://pypi.org/project/flash-attn/0.2.4/\"><code class=\"language-plaintext highlighter-rouge\">flash-attn</code></a> PyPI package (v2.x series).</li>\n</ul>\n<h4 id=\"pytorch-usage-pattern\">PyTorch Usage Pattern</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"c1\"># Inputs: Q, K, V as [batch, seq, heads, head_dim] FP16/BF16\n</span><span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout_p</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"c1\"># Inputs: Q, K, V as [batch, seq, heads, head_dim] FP16/BF16\n</span><span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout_p</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This replaces the typical <code class=\"language-plaintext highlighter-rouge\">F.scaled_dot_product_attention</code> and is generally integrated into DeepSpeed, Megatron-LM, or custom PyTorch modules.</li>\n</ul>\n<h4 id=\"flashattention3--fp8-usage\">FlashAttention‑3 / FP8 Usage</h4>\n<ul>\n  <li>As of v3 beta, FlashAttention‑3 supports FP16/BF16 forward/backward and FP8 forward on Hopper GPUs. Kernel selection will dispatch automatically if running on H100s.</li>\n  <li>When using FP8 (<code class=\"language-plaintext highlighter-rouge\">e4m3</code>, <code class=\"language-plaintext highlighter-rouge\">e5m2</code>), ensure CUDA ≥12.3 and appropriate hardware for full benefits.</li>\n</ul>",
    "contentMarkdown": "#### Installing FlashAttention (v1 & V2)\n\n![](https://aman.ai/images/copy.png)\n\n`pip install flash-attn`\n\n![](https://aman.ai/images/copy.png)\n\n`pip install flash-attn`\n\n*   This provides both FlashAttention‑1 and ‑2 implementations in the official [`flash-attn`](https://pypi.org/project/flash-attn/0.2.4/) PyPI package (v2.x series).\n\n#### PyTorch Usage Pattern\n\n![](https://aman.ai/images/copy.png)\n\n`import torch from flash_attn.flash_attn_interface import flash_attn_forward  # Inputs: Q, K, V as [batch, seq, heads, head_dim] FP16/BF16 output = flash_attn_forward(Q, K, V, causal=True, dropout_p=0.0)`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch from flash_attn.flash_attn_interface import flash_attn_forward  # Inputs: Q, K, V as [batch, seq, heads, head_dim] FP16/BF16 output = flash_attn_forward(Q, K, V, causal=True, dropout_p=0.0)`\n\n*   This replaces the typical `F.scaled_dot_product_attention` and is generally integrated into DeepSpeed, Megatron-LM, or custom PyTorch modules.\n\n#### FlashAttention‑3 / FP8 Usage\n\n*   As of v3 beta, FlashAttention‑3 supports FP16/BF16 forward/backward and FP8 forward on Hopper GPUs. Kernel selection will dispatch automatically if running on H100s.\n*   When using FP8 (`e4m3`, `e5m2`), ensure CUDA ≥12.3 and appropriate hardware for full benefits.",
    "order": 26,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "dropout"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 145,
      "contentLength": 4068
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#integration-&-code-examples",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-tuning-tips-best-practices-27",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "title": "Tuning Tips & Best Practices",
    "subtitle": "Performance Benchmarks, Code Integration Examples, and Tuning Tips",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Choose version based on GPU</strong>:</p>\n\n    <ul>\n      <li>Use <strong>FlashAttention‑2</strong> on Ampere-Class (A100 etc.) for stable high performance with FP16/BF16.</li>\n      <li>Use <strong>FlashAttention‑3</strong> on Hopper/H100 for FP8-enabled maximal throughput.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Sequence length vs head dimension tuning</strong>:\nBlock sizes are architected around head-dim and shared-memory capacity. Very small or large head dimensions might reduce efficiency due to register/shared-space constraints—especially in FlashAttention‑3.</p>\n  </li>\n  <li>\n    <p><strong>Batch size considerations</strong>:\nFor best per-GPU throughput, maintain sufficient token-level parallelism per GPU—e.g. batching multiple sequences of length ≥512 ensures high thread occupancy.</p>\n  </li>\n  <li>\n    <p><strong>Causal masking</strong>:\nBoth FlashAttention‑2 and ‑3 support causal masks. Performance remains high across masked and unmasked scenarios, with only minor overhead differences.</p>\n  </li>\n  <li>\n    <p><strong>Mixed-precision strategies</strong>:\nFor inference where FP8 is supported, use FP8 mode in FlashAttention‑3 to maximize throughput while maintaining near-FP16 accuracy. If FP8 backward is not yet stable, use BF16 for training.</p>\n  </li>\n  <li>\n    <p><strong>Library integration</strong>:\nFlashAttention auto-detects GPU architecture and dispatches the appropriate kernel. For frameworks like Triton, CUDA Graphs, or DeepSpeed, ensure FP8 pipeline is enabled manually if needed and tests pass.</p>\n  </li>\n</ul>\n<p><strong>Choose version based on GPU</strong>:</p>\n<ul>\n      <li>Use <strong>FlashAttention‑2</strong> on Ampere-Class (A100 etc.) for stable high performance with FP16/BF16.</li>\n      <li>Use <strong>FlashAttention‑3</strong> on Hopper/H100 for FP8-enabled maximal throughput.</li>\n    </ul>\n<p><strong>Sequence length vs head dimension tuning</strong>:\nBlock sizes are architected around head-dim and shared-memory capacity. Very small or large head dimensions might reduce efficiency due to register/shared-space constraints—especially in FlashAttention‑3.</p>\n<p><strong>Batch size considerations</strong>:\nFor best per-GPU throughput, maintain sufficient token-level parallelism per GPU—e.g. batching multiple sequences of length ≥512 ensures high thread occupancy.</p>\n<p><strong>Causal masking</strong>:\nBoth FlashAttention‑2 and ‑3 support causal masks. Performance remains high across masked and unmasked scenarios, with only minor overhead differences.</p>\n<p><strong>Mixed-precision strategies</strong>:\nFor inference where FP8 is supported, use FP8 mode in FlashAttention‑3 to maximize throughput while maintaining near-FP16 accuracy. If FP8 backward is not yet stable, use BF16 for training.</p>\n<p><strong>Library integration</strong>:\nFlashAttention auto-detects GPU architecture and dispatches the appropriate kernel. For frameworks like Triton, CUDA Graphs, or DeepSpeed, ensure FP8 pipeline is enabled manually if needed and tests pass.</p>",
    "contentMarkdown": "*   **Choose version based on GPU**:\n    \n    *   Use **FlashAttention‑2** on Ampere-Class (A100 etc.) for stable high performance with FP16/BF16.\n    *   Use **FlashAttention‑3** on Hopper/H100 for FP8-enabled maximal throughput.\n*   **Sequence length vs head dimension tuning**: Block sizes are architected around head-dim and shared-memory capacity. Very small or large head dimensions might reduce efficiency due to register/shared-space constraints—especially in FlashAttention‑3.\n    \n*   **Batch size considerations**: For best per-GPU throughput, maintain sufficient token-level parallelism per GPU—e.g. batching multiple sequences of length ≥512 ensures high thread occupancy.\n    \n*   **Causal masking**: Both FlashAttention‑2 and ‑3 support causal masks. Performance remains high across masked and unmasked scenarios, with only minor overhead differences.\n    \n*   **Mixed-precision strategies**: For inference where FP8 is supported, use FP8 mode in FlashAttention‑3 to maximize throughput while maintaining near-FP16 accuracy. If FP8 backward is not yet stable, use BF16 for training.\n    \n*   **Library integration**: FlashAttention auto-detects GPU architecture and dispatches the appropriate kernel. For frameworks like Triton, CUDA Graphs, or DeepSpeed, ensure FP8 pipeline is enabled manually if needed and tests pass.\n    \n\n**Choose version based on GPU**:\n\n*   Use **FlashAttention‑2** on Ampere-Class (A100 etc.) for stable high performance with FP16/BF16.\n*   Use **FlashAttention‑3** on Hopper/H100 for FP8-enabled maximal throughput.\n\n**Sequence length vs head dimension tuning**: Block sizes are architected around head-dim and shared-memory capacity. Very small or large head dimensions might reduce efficiency due to register/shared-space constraints—especially in FlashAttention‑3.\n\n**Batch size considerations**: For best per-GPU throughput, maintain sufficient token-level parallelism per GPU—e.g. batching multiple sequences of length ≥512 ensures high thread occupancy.\n\n**Causal masking**: Both FlashAttention‑2 and ‑3 support causal masks. Performance remains high across masked and unmasked scenarios, with only minor overhead differences.\n\n**Mixed-precision strategies**: For inference where FP8 is supported, use FP8 mode in FlashAttention‑3 to maximize throughput while maintaining near-FP16 accuracy. If FP8 backward is not yet stable, use BF16 for training.\n\n**Library integration**: FlashAttention auto-detects GPU architecture and dispatches the appropriate kernel. For frameworks like Triton, CUDA Graphs, or DeepSpeed, ensure FP8 pipeline is enabled manually if needed and tests pass.",
    "order": 27,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 332,
      "contentLength": 3055
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#tuning-tips-&-best-practices",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-supporting-larger-head-dimensions-28",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "title": "Supporting Larger Head Dimensions",
    "subtitle": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "contentHtml": "<ul>\n  <li>\n    <p>FlashAttention‑2 extended support to larger head dimensions (up to 256), enabling compatibility with models like GPT‑J, CodeGen, and Stable Diffusion 1.x. In practice, use of head dimensions beyond 128 is now supported and optimized.</p>\n  </li>\n  <li>\n    <p>In PyTorch, when defining a custom transformer layer:</p>\n  </li>\n</ul>\n<p>FlashAttention‑2 extended support to larger head dimensions (up to 256), enabling compatibility with models like GPT‑J, CodeGen, and Stable Diffusion 1.x. In practice, use of head dimensions beyond 128 is now supported and optimized.</p>\n<p>In PyTorch, when defining a custom transformer layer:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"n\">head_dim</span> <span class=\"o\">=</span> <span class=\"mi\">192</span>  <span class=\"c1\"># any value up to 256\n</span><span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">head_dim</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>\n\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"n\">head_dim</span> <span class=\"o\">=</span> <span class=\"mi\">192</span>  <span class=\"c1\"># any value up to 256\n</span><span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">head_dim</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>\n\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>The FlashAttention kernels automatically adjust tiling strategy internally based on <code class=\"language-plaintext highlighter-rouge\">head_dim</code>. For v2, larger dims translate into larger tile sizes to better utilize shared memory and registers, while also maintaining occupancy on Ampere/A100 GPUs.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑2 extended support to larger head dimensions (up to 256), enabling compatibility with models like GPT‑J, CodeGen, and Stable Diffusion 1.x. In practice, use of head dimensions beyond 128 is now supported and optimized.\n    \n*   In PyTorch, when defining a custom transformer layer:\n    \n\nFlashAttention‑2 extended support to larger head dimensions (up to 256), enabling compatibility with models like GPT‑J, CodeGen, and Stable Diffusion 1.x. In practice, use of head dimensions beyond 128 is now supported and optimized.\n\nIn PyTorch, when defining a custom transformer layer:\n\n![](https://aman.ai/images/copy.png)\n\n`import torch from flash_attn.flash_attn_interface import flash_attn_forward  head_dim = 192  # any value up to 256 Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16) ...  output = flash_attn_forward(Q, K, V, causal=True)`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch from flash_attn.flash_attn_interface import flash_attn_forward  head_dim = 192  # any value up to 256 Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16) ...  output = flash_attn_forward(Q, K, V, causal=True)`\n\n*   The FlashAttention kernels automatically adjust tiling strategy internally based on `head_dim`. For v2, larger dims translate into larger tile sizes to better utilize shared memory and registers, while also maintaining occupancy on Ampere/A100 GPUs.",
    "order": 28,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "transformer",
      "attention",
      "gpt"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 183,
      "contentLength": 4434
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#supporting-larger-head-dimensions",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-long-sequence-handling-eg-64k-context-29",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "title": "Long-Sequence Handling (e.g. 64K Context)",
    "subtitle": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "contentHtml": "<ul>\n  <li>\n    <p>FlashAttention is designed to scale linearly in memory with sequence length, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-336\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-337\"><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-339\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">O(N \\cdot d)</script>, enabling efficient usage even with 64K tokens.</p>\n  </li>\n  <li>\n    <p>For long-sequence inference or training:</p>\n  </li>\n</ul>\n<p>FlashAttention is designed to scale linearly in memory with sequence length, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-336\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-337\"><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-339\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">O(N \\cdot d)</script>, enabling efficient usage even with 64K tokens.</p>\n<p>For long-sequence inference or training:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"n\">seq_len</span> <span class=\"o\">=</span> <span class=\"mi\">65536</span>\n<span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">head_dim</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>\n\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"n\">seq_len</span> <span class=\"o\">=</span> <span class=\"mi\">65536</span>\n<span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">head_dim</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>\n\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>The kernel streams tiles of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-344\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-345\"><span class=\"mi\" id=\"MathJax-Span-346\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-347\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-348\"><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">K</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-350\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-351\"><span class=\"mi\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">V</script> that fit into on-chip SRAM. It recomputes block-wise normalization using max–sum streaming to avoid full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x00D7;</mo><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-353\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-354\"><span class=\"mi\" id=\"MathJax-Span-355\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-356\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>×</mo><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">N \\times N</script> matrix materialization.</li>\n  <li>This design ensures constant memory beyond per-token/load requirements—even at tens of thousands of tokens.</li>\n  <li>FlashAttention‑3 further maximizes throughput on Hopper GPUs with long sequences via asynchronous pipelining and FP8 support.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention is designed to scale linearly in memory with sequence length, O(N⋅d)O(N⋅d)O(N \\\\cdot d), enabling efficient usage even with 64K tokens.\n    \n*   For long-sequence inference or training:\n    \n\nFlashAttention is designed to scale linearly in memory with sequence length, O(N⋅d)O(N⋅d)O(N \\\\cdot d), enabling efficient usage even with 64K tokens.\n\nFor long-sequence inference or training:\n\n![](https://aman.ai/images/copy.png)\n\n`seq_len = 65536 Q = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=torch.bfloat16) ...  output = flash_attn_forward(Q, K, V, causal=True)`\n\n![](https://aman.ai/images/copy.png)\n\n`seq_len = 65536 Q = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=torch.bfloat16) ...  output = flash_attn_forward(Q, K, V, causal=True)`\n\n*   The kernel streams tiles of QQQ, KKK, and VVV that fit into on-chip SRAM. It recomputes block-wise normalization using max–sum streaming to avoid full N×NN×NN \\\\times N matrix materialization.\n*   This design ensures constant memory beyond per-token/load requirements—even at tens of thousands of tokens.\n*   FlashAttention‑3 further maximizes throughput on Hopper GPUs with long sequences via asynchronous pipelining and FP8 support.",
    "order": 29,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 154,
      "contentLength": 13246
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#long-sequence-handling-(e.g.-64k-context)",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-performance-tips-for-long-context-30",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "title": "Performance Tips for Long Context",
    "subtitle": "Code Walkthroughs for Custom Head-Dimension & Long‑Sequence Optimization",
    "contentHtml": "<ul>\n  <li>Larger tile sizes can improve throughput, but be mindful: they raise shared memory and register pressure. For sequences of 16K–64K tokens, default tile sizes are tested to balance occupancy and register use.</li>\n  <li>Validation logs from users report practical throughput of FlashAttention‑3 in FP16/BF16 approaching <strong>740 TFLOPs/s</strong> on H100 with long sequences; FP8 mode achieves <strong>~1.2 PFLOPs/s</strong> with controlled RMSE (~2.6× lower than baseline FP8).</li>\n  <li>For head-dimension tuning, stay within supported limits (≤ 256) to ensure kernel dispatch is optimized; backward support for dropout and FP8 may be constrained for <code class=\"language-plaintext highlighter-rouge\">head_dim</code>=256 on consumer GPUs.</li>\n</ul>",
    "contentMarkdown": "*   Larger tile sizes can improve throughput, but be mindful: they raise shared memory and register pressure. For sequences of 16K–64K tokens, default tile sizes are tested to balance occupancy and register use.\n*   Validation logs from users report practical throughput of FlashAttention‑3 in FP16/BF16 approaching **740 TFLOPs/s** on H100 with long sequences; FP8 mode achieves **~1.2 PFLOPs/s** with controlled RMSE (~2.6× lower than baseline FP8).\n*   For head-dimension tuning, stay within supported limits (≤ 256) to ensure kernel dispatch is optimized; backward support for dropout and FP8 may be constrained for `head_dim`\\=256 on consumer GPUs.",
    "order": 30,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "dropout"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 96,
      "contentLength": 766
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#performance-tips-for-long-context",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-softmax-normalization-via-streaming-tiling-31",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "title": "Softmax Normalization Via Streaming Tiling",
    "subtitle": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "contentHtml": "<ul>\n  <li>FlashAttention‑1 employs <em>online softmax normalization</em>, processing tiles of queries <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>Q</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"msubsup\" id=\"MathJax-Span-360\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-361\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-362\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Q</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">Q_i</script> and key‑value pair blocks <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>K</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-363\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-364\"><span class=\"msubsup\" id=\"MathJax-Span-365\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-367\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>K</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">K_j</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>V</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-368\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.89em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-369\"><span class=\"msubsup\" id=\"MathJax-Span-370\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-372\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>V</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">V_j</script> sequentially.</li>\n  <li>For each query block, the algorithm transforms <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo>=</mo><mi>softmax</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></mrow></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-373\" style=\"width: 10.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.138em, 1008.96em, 4.326em, -999.997em); top: -3.487em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-374\"><span class=\"mi\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-376\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-377\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">softmax</span><span class=\"mo\" id=\"MathJax-Span-378\"></span><span class=\"mrow\" id=\"MathJax-Span-379\"><span class=\"mo\" id=\"MathJax-Span-380\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-381\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.232em, 1001.46em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.727em;\"><span class=\"mrow\" id=\"MathJax-Span-382\"><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-384\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-386\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">⊤</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.89em, 4.273em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.466em;\"><span class=\"msqrt\" id=\"MathJax-Span-389\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0.523em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"mi\" id=\"MathJax-Span-391\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.94em, 1000.37em, 1.253em, -999.997em); top: -1.612em; left: 0.523em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.049em; border-top: 1.2px solid; width: 0.367em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -3.904em; left: 0em;\"><span><span style=\"font-size: 70.7%; font-family: STIXVariants;\">√</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.62em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.617em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-392\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.492em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo>=</mo><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">⊤</mi></mrow></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">O=\\operatorname{softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right) V</script> by iteratively computing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mi>j</mi></munder><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>&amp;#x2113;</mi><mi>i</mi></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mi>j</mi></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>m</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-394\" style=\"width: 18.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1015.42em, 3.544em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-395\"><span class=\"msubsup\" id=\"MathJax-Span-396\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-397\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-400\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 1.721em;\"><span class=\"mi\" id=\"MathJax-Span-402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-403\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-405\"><span class=\"mrow\" id=\"MathJax-Span-406\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-409\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-410\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-411\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-family: STIXGeneral-Italic;\">ℓ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-413\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-415\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-416\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"mi\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-419\"></span><span class=\"mrow\" id=\"MathJax-Span-420\"><span class=\"mo\" id=\"MathJax-Span-421\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-422\"><span class=\"msubsup\" id=\"MathJax-Span-423\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-424\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-425\"><span class=\"mrow\" id=\"MathJax-Span-426\"><span class=\"mi\" id=\"MathJax-Span-427\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-430\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-431\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-432\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-433\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><munder><mo movablelimits=\"true\" form=\"prefix\">max</mo><mi>j</mi></munder><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>ℓ</mi><mi>i</mi></msub><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">m_i=\\max _j s_{i j}, \\quad \\ell_i=\\sum_j \\exp \\left(s_{i j}-m_i\\right)</script> and updating output incrementally: streaming normalized contributions from each tile.</li>\n  <li>This scheme avoids storing the full <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x00D7;</mo><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-434\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-435\"><span class=\"mi\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-437\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>×</mo><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">N \\times N</script> logits matrix and reduces memory overhead to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-439\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-440\"><span class=\"mi\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-445\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">O(N \\cdot d)</script>, enabling contexts of up to 64K tokens. It also matches theoretical lower bounds for memory traffic given typical SRAM sizes.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑1 employs _online softmax normalization_, processing tiles of queries QiQiQ\\_i and key‑value pair blocks KjKjK\\_j, VjVjV\\_j sequentially.\n*   For each query block, the algorithm transforms O\\=softmax(QK⊤d√)VO\\=softmax⁡(QK⊤d)VO=\\\\operatorname{softmax}\\\\left(\\\\frac{Q K^{\\\\top}}{\\\\sqrt{d}}\\\\right) V by iteratively computing mi\\=maxjsij,ℓi\\=∑jexp(sij−mi)mi\\=maxjsij,ℓi\\=∑jexp⁡(sij−mi)m\\_i=\\\\max \\_j s\\_{i j}, \\\\quad \\\\ell\\_i=\\\\sum\\_j \\\\exp \\\\left(s\\_{i j}-m\\_i\\\\right) and updating output incrementally: streaming normalized contributions from each tile.\n*   This scheme avoids storing the full N×NN×NN \\\\times N logits matrix and reduces memory overhead to O(N⋅d)O(N⋅d)O(N \\\\cdot d), enabling contexts of up to 64K tokens. It also matches theoretical lower bounds for memory traffic given typical SRAM sizes.",
    "order": 31,
    "orderInChapter": 1,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 91,
      "contentLength": 24772
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#softmax-normalization-via-streaming-tiling",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-io-complexity-optimal-traffic-reduction-32",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "title": "I/O Complexity: Optimal Traffic Reduction",
    "subtitle": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "contentHtml": "<ul>\n  <li>Standard attention requires <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-447\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-448\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-450\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-451\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mn\" id=\"MathJax-Span-453\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-454\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">O(N^2)</script> reads and writes to HBM (higher-latency global memory), including full logs and softmax intermediate storage.</li>\n  <li>FlashAttention’s online approach ensures only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-455\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-458\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-459\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-460\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-461\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-462\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">O(N \\cdot d)</script> total traffic by streaming blocks that fit into SRAM, recomputing any softmax normalization factors on-the-fly rather than buffering large intermediate matrices.</li>\n  <li>FlashAttention‑2 and ‑3 retain this I/O-optimal model, further enhancing compute throughput without affecting asymptotic memory usage.</li>\n</ul>",
    "contentMarkdown": "*   Standard attention requires O(N2)O(N2)O(N^2) reads and writes to HBM (higher-latency global memory), including full logs and softmax intermediate storage.\n*   FlashAttention’s online approach ensures only O(N⋅d)O(N⋅d)O(N \\\\cdot d) total traffic by streaming blocks that fit into SRAM, recomputing any softmax normalization factors on-the-fly rather than buffering large intermediate matrices.\n*   FlashAttention‑2 and ‑3 retain this I/O-optimal model, further enhancing compute throughput without affecting asymptotic memory usage.",
    "order": 32,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 67,
      "contentLength": 4909
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#i/o-complexity:-optimal-traffic-reduction",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-flashattention2-block-parallel-hardware-tiling-fig-33",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "title": "FlashAttention‑2: Block-Parallel Hardware Tiling (Figure 1)",
    "subtitle": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "contentHtml": "<ul>\n  <li>FlashAttention‑2 arranges queries and key-value blocks in a block-parallel fashion: a block of query vectors is locally loaded per thread block, with key/value streamed block-wise to update per-query softmax state and output contributions. This parallel hardware structure eliminates sequential dependencies across blocks and supports query-level concurrency.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑2 arranges queries and key-value blocks in a block-parallel fashion: a block of query vectors is locally loaded per thread block, with key/value streamed block-wise to update per-query softmax state and output contributions. This parallel hardware structure eliminates sequential dependencies across blocks and supports query-level concurrency.",
    "order": 33,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 47,
      "contentLength": 381
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#flashattention‑2:-block-parallel-hardware-tiling-(figure-1)",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-flashattention3-pingpong-scheduling-and-overlappin-34",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "title": "FlashAttention‑3: Ping‑Pong Scheduling and Overlapping of GEMM & Softmax",
    "subtitle": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "contentHtml": "<ul>\n  <li>FlashAttention‑3 introduces <strong>ping‑pong scheduling</strong> across two or more warp-groups: while one warpgroup executes GEMM for dot-product (Tensor Core usage), another group performs softmax/exponential operations using the multifunction unit. Synchronization barriers (e.g. <code class=\"language-plaintext highlighter-rouge\">bar.sync</code>) orchestrate this overlap across iterations to maximize effective utilization of both compute units.</li>\n  <li>Additionally, within a single warpgroup, <strong>intra-warpgroup pipelining</strong> allows parts of softmax to execute concurrently with GEMM, raising throughput further. This two-stage pipelining helps push FP16 forward performance from ~570 TFLOPs/s to ~640 TFLOPs/s (ex: seq_len = 8K, head_dim = 128).</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention‑3 introduces **ping‑pong scheduling** across two or more warp-groups: while one warpgroup executes GEMM for dot-product (Tensor Core usage), another group performs softmax/exponential operations using the multifunction unit. Synchronization barriers (e.g. `bar.sync`) orchestrate this overlap across iterations to maximize effective utilization of both compute units.\n*   Additionally, within a single warpgroup, **intra-warpgroup pipelining** allows parts of softmax to execute concurrently with GEMM, raising throughput further. This two-stage pipelining helps push FP16 forward performance from ~570 TFLOPs/s to ~640 TFLOPs/s (ex: seq\\_len = 8K, head\\_dim = 128).",
    "order": 34,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 87,
      "contentLength": 790
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#flashattention‑3:-ping‑pong-scheduling-and-overlapping-of-gemm-&-softmax",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-theoretical-and-practical-impacts-35",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "title": "Theoretical and Practical Impacts",
    "subtitle": "Deep Dive Into Softmax Streaming and I/O Complexity Analysis",
    "contentHtml": "<ul>\n  <li>By aligning algorithmic tiling with GPU memory architecture, FlashAttention (all versions) achieves <strong>I/O-optimal behavior</strong>, dramatically reducing latency and memory bandwidth requirements as sequence length increases.</li>\n  <li>FlashAttention‑2’s block-parallel tiling removes sequential dependencies, improving latency and occupancy across head and batch dimensions.</li>\n  <li>FlashAttention‑3’s warp specialization and asynchronous overlap further minimize idle compute phases and merge slow non‑matmul softmax operations into periods where GEMMs are active.</li>\n</ul>",
    "contentMarkdown": "*   By aligning algorithmic tiling with GPU memory architecture, FlashAttention (all versions) achieves **I/O-optimal behavior**, dramatically reducing latency and memory bandwidth requirements as sequence length increases.\n*   FlashAttention‑2’s block-parallel tiling removes sequential dependencies, improving latency and occupancy across head and batch dimensions.\n*   FlashAttention‑3’s warp specialization and asynchronous overlap further minimize idle compute phases and merge slow non‑matmul softmax operations into periods where GEMMs are active.",
    "order": 35,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 66,
      "contentLength": 599
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#theoretical-and-practical-impacts",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-framework-support-installation-36",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Integration with Modern Frameworks and Benchmark Scripts",
    "title": "Framework Support & Installation",
    "subtitle": "Integration with Modern Frameworks and Benchmark Scripts",
    "contentHtml": "<ul>\n  <li>The <code class=\"language-plaintext highlighter-rouge\">flash-attn</code> library (v2 and above) provides seamless integration via a PyTorch C++/CUDA extension (<code class=\"language-plaintext highlighter-rouge\">flash_attn_interface</code>), compatible with PyTorch 2.2+ and GPU architectures Ampere, Ada, and Hopper.</li>\n  <li>FlashAttention‑3 requires an H100/H800 GPU and CUDA ≥ 12.3 (12.8 recommended) for full FP8 support.</li>\n</ul>",
    "contentMarkdown": "*   The `flash-attn` library (v2 and above) provides seamless integration via a PyTorch C++/CUDA extension (`flash_attn_interface`), compatible with PyTorch 2.2+ and GPU architectures Ampere, Ada, and Hopper.\n*   FlashAttention‑3 requires an H100/H800 GPU and CUDA ≥ 12.3 (12.8 recommended) for full FP8 support.",
    "order": 36,
    "orderInChapter": 1,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 43,
      "contentLength": 449
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#framework-support-&-installation",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-high-level-integration-37",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Integration with Modern Frameworks and Benchmark Scripts",
    "title": "High-Level Integration",
    "subtitle": "Integration with Modern Frameworks and Benchmark Scripts",
    "contentHtml": "<h4 id=\"pytorch-deepspeed-megatron-lm-hugging-face\">PyTorch (DeepSpeed, Megatron-LM, Hugging Face)</h4>\n<ul>\n  <li>Typical replacement code:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"c1\"># Q, K, V shaped [batch, seq_len, heads, head_dim], dtype FP16/BF16\n</span><span class=\"n\">O</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout_p</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_forward</span>\n\n<span class=\"c1\"># Q, K, V shaped [batch, seq_len, heads, head_dim], dtype FP16/BF16\n</span><span class=\"n\">O</span> <span class=\"o\">=</span> <span class=\"n\">flash_attn_forward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout_p</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>Automatically dispatches appropriate kernel version (v2 or v3) based on available GPU and precision.</li>\n  <li>DeepSpeed and Megatron-LM often integrate FlashAttention as a drop-in replacement for standard scaled dot‑product attention.</li>\n</ul>\n<h4 id=\"triton--xformers-backends\">Triton &amp; XFormers Backends</h4>\n<ul>\n  <li>Trident implementations (e.g., in Triton language) provide alternative fused kernels and can be up to <strong>1.3–1.5× slower</strong> than FlashAttention‑2 in forward pass.</li>\n  <li>xFormers and Triton vary in API compatibility; official FlashAttention‑2 implementation is recommended for max speed.</li>\n</ul>",
    "contentMarkdown": "#### PyTorch (DeepSpeed, Megatron-LM, Hugging Face)\n\n*   Typical replacement code:\n\n![](https://aman.ai/images/copy.png)\n\n`from flash_attn.flash_attn_interface import flash_attn_forward  # Q, K, V shaped [batch, seq_len, heads, head_dim], dtype FP16/BF16 O = flash_attn_forward(Q, K, V, causal=True, dropout_p=0.0)`\n\n![](https://aman.ai/images/copy.png)\n\n`from flash_attn.flash_attn_interface import flash_attn_forward  # Q, K, V shaped [batch, seq_len, heads, head_dim], dtype FP16/BF16 O = flash_attn_forward(Q, K, V, causal=True, dropout_p=0.0)`\n\n*   Automatically dispatches appropriate kernel version (v2 or v3) based on available GPU and precision.\n*   DeepSpeed and Megatron-LM often integrate FlashAttention as a drop-in replacement for standard scaled dot‑product attention.\n\n#### Triton & XFormers Backends\n\n*   Trident implementations (e.g., in Triton language) provide alternative fused kernels and can be up to **1.3–1.5× slower** than FlashAttention‑2 in forward pass.\n*   xFormers and Triton vary in API compatibility; official FlashAttention‑2 implementation is recommended for max speed.",
    "order": 37,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "dropout"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 131,
      "contentLength": 2916
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#high-level-integration",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-benchmark-script-examples-38",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Integration with Modern Frameworks and Benchmark Scripts",
    "title": "Benchmark Script Examples",
    "subtitle": "Integration with Modern Frameworks and Benchmark Scripts",
    "contentHtml": "<ul>\n  <li>Typically users benchmark with a script like:</li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">python bench_attention.py <span class=\"se\">\\</span>\n  <span class=\"nt\">--seq-len</span> 4096 <span class=\"se\">\\</span>\n  <span class=\"nt\">--batch-size</span> 8 <span class=\"se\">\\</span>\n  <span class=\"nt\">--num-heads</span> 16 <span class=\"se\">\\</span>\n  <span class=\"nt\">--head-dim</span> 128 <span class=\"se\">\\</span>\n  <span class=\"nt\">--use-causal</span> <span class=\"se\">\\</span>\n  <span class=\"nt\">--dtype</span> fp16\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">python bench_attention.py <span class=\"se\">\\</span>\n  <span class=\"nt\">--seq-len</span> 4096 <span class=\"se\">\\</span>\n  <span class=\"nt\">--batch-size</span> 8 <span class=\"se\">\\</span>\n  <span class=\"nt\">--num-heads</span> 16 <span class=\"se\">\\</span>\n  <span class=\"nt\">--head-dim</span> 128 <span class=\"se\">\\</span>\n  <span class=\"nt\">--use-causal</span> <span class=\"se\">\\</span>\n  <span class=\"nt\">--dtype</span> fp16\n</code></pre>\n<ul>\n  <li>\n    <p>Special flags such as <code class=\"language-plaintext highlighter-rouge\">--use-flash-attn</code>, or environment variables <code class=\"language-plaintext highlighter-rouge\">USE_FLASH_ATTN=1</code>, activate the FlashAttention kernel rather than default PyTorch attention.</p>\n  </li>\n  <li>\n    <p>Benchmark outputs from FlashAttention‑2 show:</p>\n\n    <ul>\n      <li>End-to-end GPT training throughput up to <strong>225 TFLOPs/s per A100 GPU</strong> (∼72% FLOPs utilization).</li>\n      <li>Forward+backward combined performance of 1.7–3.0× faster than FlashAttention‑1 and up to <strong>9× faster</strong> than PyTorch baseline depending on config, with consistent speedups across head-dimensions 64 and 128.</li>\n    </ul>\n  </li>\n  <li>\n    <p>For FlashAttention‑3 benchmarks:</p>\n\n    <ul>\n      <li>FP16/BF16 mode: ~740 TFLOPs/s (~75% utilization) on H100. FP8 mode: nearly <strong>1.2–1.3 PFLOPs/s</strong>, with quantization error ~2.6× lower than baseline FP8.</li>\n    </ul>\n  </li>\n</ul>\n<p>Special flags such as <code class=\"language-plaintext highlighter-rouge\">--use-flash-attn</code>, or environment variables <code class=\"language-plaintext highlighter-rouge\">USE_FLASH_ATTN=1</code>, activate the FlashAttention kernel rather than default PyTorch attention.</p>\n<p>Benchmark outputs from FlashAttention‑2 show:</p>\n<ul>\n      <li>End-to-end GPT training throughput up to <strong>225 TFLOPs/s per A100 GPU</strong> (∼72% FLOPs utilization).</li>\n      <li>Forward+backward combined performance of 1.7–3.0× faster than FlashAttention‑1 and up to <strong>9× faster</strong> than PyTorch baseline depending on config, with consistent speedups across head-dimensions 64 and 128.</li>\n    </ul>\n<p>For FlashAttention‑3 benchmarks:</p>\n<ul>\n      <li>FP16/BF16 mode: ~740 TFLOPs/s (~75% utilization) on H100. FP8 mode: nearly <strong>1.2–1.3 PFLOPs/s</strong>, with quantization error ~2.6× lower than baseline FP8.</li>\n    </ul>",
    "contentMarkdown": "*   Typically users benchmark with a script like:\n\n![](https://aman.ai/images/copy.png)\n\n`python bench_attention.py \\   --seq-len 4096 \\   --batch-size 8 \\   --num-heads 16 \\   --head-dim 128 \\   --use-causal \\   --dtype fp16`\n\n![](https://aman.ai/images/copy.png)\n\n`python bench_attention.py \\   --seq-len 4096 \\   --batch-size 8 \\   --num-heads 16 \\   --head-dim 128 \\   --use-causal \\   --dtype fp16`\n\n*   Special flags such as `--use-flash-attn`, or environment variables `USE_FLASH_ATTN=1`, activate the FlashAttention kernel rather than default PyTorch attention.\n    \n*   Benchmark outputs from FlashAttention‑2 show:\n    \n    *   End-to-end GPT training throughput up to **225 TFLOPs/s per A100 GPU** (∼72% FLOPs utilization).\n    *   Forward+backward combined performance of 1.7–3.0× faster than FlashAttention‑1 and up to **9× faster** than PyTorch baseline depending on config, with consistent speedups across head-dimensions 64 and 128.\n*   For FlashAttention‑3 benchmarks:\n    \n    *   FP16/BF16 mode: ~740 TFLOPs/s (~75% utilization) on H100. FP8 mode: nearly **1.2–1.3 PFLOPs/s**, with quantization error ~2.6× lower than baseline FP8.\n\nSpecial flags such as `--use-flash-attn`, or environment variables `USE_FLASH_ATTN=1`, activate the FlashAttention kernel rather than default PyTorch attention.\n\nBenchmark outputs from FlashAttention‑2 show:\n\n*   End-to-end GPT training throughput up to **225 TFLOPs/s per A100 GPU** (∼72% FLOPs utilization).\n*   Forward+backward combined performance of 1.7–3.0× faster than FlashAttention‑1 and up to **9× faster** than PyTorch baseline depending on config, with consistent speedups across head-dimensions 64 and 128.\n\nFor FlashAttention‑3 benchmarks:\n\n*   FP16/BF16 mode: ~740 TFLOPs/s (~75% utilization) on H100. FP8 mode: nearly **1.2–1.3 PFLOPs/s**, with quantization error ~2.6× lower than baseline FP8.",
    "order": 38,
    "orderInChapter": 3,
    "difficulty": 5,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "attention",
      "gpt"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 233,
      "contentLength": 3531
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#benchmark-script-examples",
    "scrapedAt": "2025-12-28T11:47:47.088Z"
  },
  {
    "id": "ai-flashattention-automated-dispatch-precision-handling-39",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Integration with Modern Frameworks and Benchmark Scripts",
    "title": "Automated Dispatch & Precision Handling",
    "subtitle": "Integration with Modern Frameworks and Benchmark Scripts",
    "contentHtml": "<ul>\n  <li>At runtime, FlashAttention inspects hardware IDs to decide between v2 (Ampere) and v3 (Hopper) kernels.</li>\n  <li>For FP8 support (FlashAttention‑3), users may need to opt into an experimental API flag (e.g. <code class=\"language-plaintext highlighter-rouge\">precision='fp8'</code>) or dependency on a nightly version.</li>\n  <li>If backward pass support in FP8 is still maturing, workflows can fallback to BF16 or FP16 for gradient computation.</li>\n</ul>",
    "contentMarkdown": "*   At runtime, FlashAttention inspects hardware IDs to decide between v2 (Ampere) and v3 (Hopper) kernels.\n*   For FP8 support (FlashAttention‑3), users may need to opt into an experimental API flag (e.g. `precision='fp8'`) or dependency on a nightly version.\n*   If backward pass support in FP8 is still maturing, workflows can fallback to BF16 or FP16 for gradient computation.",
    "order": 39,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 59,
      "contentLength": 468
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#automated-dispatch-&-precision-handling",
    "scrapedAt": "2025-12-28T11:47:47.089Z"
  },
  {
    "id": "ai-flashattention-deployment-and-inference-readiness-40",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "FlashAttention",
    "articleSlug": "flashattention",
    "chapter": "Integration with Modern Frameworks and Benchmark Scripts",
    "title": "Deployment and Inference Readiness",
    "subtitle": "Integration with Modern Frameworks and Benchmark Scripts",
    "contentHtml": "<ul>\n  <li>FlashAttention can also serve in inference contexts within frameworks like Hugging Face — kernel dispatch handles causal masking efficiently.</li>\n  <li>NVIDIA’s FlashInfer builds on FlashAttention‑3 to optimize KV-cache-aware inference, reducing inter-token latency by 29–69% compared to standard backends.</li>\n</ul>",
    "contentMarkdown": "*   FlashAttention can also serve in inference contexts within frameworks like Hugging Face — kernel dispatch handles causal masking efficiently.\n*   NVIDIA’s FlashInfer builds on FlashAttention‑3 to optimize KV-cache-aware inference, reducing inter-token latency by 29–69% compared to standard backends.",
    "order": 40,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "attention"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 39,
      "contentLength": 329
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/flashattention/#deployment-and-inference-readiness",
    "scrapedAt": "2025-12-28T11:47:47.089Z"
  }
]