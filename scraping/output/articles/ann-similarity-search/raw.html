<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • Approximate Nearest Neighbors for Similarity Search</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/ann-similarity-search/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fann-similarity-search&amp;ers=2"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWeYlydQKf3IDMd-Z-uG98bRjatJLspLIKyN-oFXKXnt373o6EjP6rPysTtjqggv3ikr_4Zqn32uygI5ZP6MDgLIq68yO3CSWX1YzjZWvYk7p3xkl8LWwn0GDcmzscYpNVMJIBBZA==?fccs=W1siQUtzUm9sODJpb1NTM1hpd1hKZDFFbDlKVDc2LXFiWTZUbS1UZkNNWG1UXy1KOHdSdUdBSWg5MENkVzRlNkJESWU4YjN1TXJpVFdZb0ZFMGpfeG9rSTlobk9jdnZmSkNTYldjYkF2YnRlTWtiMXJHM21HZmphUGFScEREMnNvOWtlY281UmFPNGJDY2pOZy15VWlBME5SZEpfYXlpeVdyTUNnPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwNDQsODYxMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9hbm4tc2ltaWxhcml0eS1zZWFyY2gvIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI5LCJmYWxzZSJdXV0"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxXhwkAzT9Al6ujB4zCIpkBELVgRxDNPtlx59AlFVq8G29Nl2kO9nJaLoMc9WSVBBF69v_rEPb_POzQY15Z3Gs0b6Rifk1YWZzZmuMb1_qn0HEvcOq4crqy1i2e2i3MP74sJcvzp9w==?fccs=W1siQUtzUm9sODJpb1NTM1hpd1hKZDFFbDlKVDc2LXFiWTZUbS1UZkNNWG1UXy1KOHdSdUdBSWg5MENkVzRlNkJESWU4YjN1TXJpVFdZb0ZFMGpfeG9rSTlobk9jdnZmSkNTYldjYkF2YnRlTWtiMXJHM21HZmphUGFScEREMnNvOWtlY281UmFPNGJDY2pOZy15VWlBME5SZEpfYXlpeVdyTUNnPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwNDQsOTg5MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5XSxudWxsLDIsbnVsbCwiZW4iXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvYW5uLXNpbWlsYXJpdHktc2VhcmNoLyIsbnVsbCxbWzgsInNDaE5INU9zYWswIl0sWzksImVuLVVTIl0sWzE5LCIyIl0sWzE3LCJbMF0iXSxbMjQsIiJdLFsyOSwiZmFsc2UiXV1d"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxVPEDw0SJSoCkJSWcIW7XRWeNxU0WYLyJrggHzw7NSFjRenrnCrm7xHF8fWUI9GROzyThgUMhtafFhe3Xab3uZYzC1FpycKUwbnHd1uHrrklkpkCUckENoRPRRkgtbdTCILDadxNA==?fccs=W1siQUtzUm9sODJpb1NTM1hpd1hKZDFFbDlKVDc2LXFiWTZUbS1UZkNNWG1UXy1KOHdSdUdBSWg5MENkVzRlNkJESWU4YjN1TXJpVFdZb0ZFMGpfeG9rSTlobk9jdnZmSkNTYldjYkF2YnRlTWtiMXJHM21HZmphUGFScEREMnNvOWtlY281UmFPNGJDY2pOZy15VWlBME5SZEpfYXlpeVdyTUNnPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwNDUsODk4MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5LDZdLG51bGwsMixudWxsLCJlbiIsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLDFdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9hbm4tc2ltaWxhcml0eS1zZWFyY2gvIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI5LCJmYWxzZSJdXV0"></script></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • Approximate Nearest Neighbors for Similarity Search</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#what-is-similarity-search" id="markdown-toc-what-is-similarity-search">What is Similarity Search?</a>    <ul>
      <li><a href="#real-world-applications" id="markdown-toc-real-world-applications">Real-World Applications</a></li>
      <li><a href="#from-exact-to-approximate-nearest-neighbor-search" id="markdown-toc-from-exact-to-approximate-nearest-neighbor-search">From Exact to Approximate Nearest Neighbor Search</a></li>
    </ul>
  </li>
  <li><a href="#approximate-nearest-neighbors-ann" id="markdown-toc-approximate-nearest-neighbors-ann">Approximate Nearest Neighbors (ANN)</a>    <ul>
      <li><a href="#role-of-ann-in-recommendation-systems" id="markdown-toc-role-of-ann-in-recommendation-systems">Role of ANN in Recommendation Systems</a></li>
    </ul>
  </li>
  <li><a href="#ann-algorithms" id="markdown-toc-ann-algorithms">ANN Algorithms</a>    <ul>
      <li><a href="#tree-based-methods" id="markdown-toc-tree-based-methods">Tree-Based Methods</a>        <ul>
          <li><a href="#kd-trees" id="markdown-toc-kd-trees">KD-Trees</a></li>
          <li><a href="#randomized-kd-forests" id="markdown-toc-randomized-kd-forests">Randomized KD-Forests</a></li>
          <li><a href="#annoys-random-projection-forest" id="markdown-toc-annoys-random-projection-forest">Annoy’s Random Projection Forest</a></li>
        </ul>
      </li>
      <li><a href="#quantization-based-methods" id="markdown-toc-quantization-based-methods">Quantization-Based Methods</a>        <ul>
          <li><a href="#product-quantization-pq" id="markdown-toc-product-quantization-pq">Product Quantization (PQ)</a></li>
          <li><a href="#optimized-product-quantization-opq" id="markdown-toc-optimized-product-quantization-opq">Optimized Product Quantization (OPQ)</a></li>
          <li><a href="#locality-sensitive-hashing-lsh" id="markdown-toc-locality-sensitive-hashing-lsh">Locality Sensitive Hashing (LSH)</a></li>
          <li><a href="#anisotropic-vector-quantization-avq" id="markdown-toc-anisotropic-vector-quantization-avq">Anisotropic Vector Quantization (AVQ)</a></li>
        </ul>
      </li>
      <li><a href="#clustering-based-methods" id="markdown-toc-clustering-based-methods">Clustering-Based Methods</a>        <ul>
          <li><a href="#inverted-file-index-ivf" id="markdown-toc-inverted-file-index-ivf">Inverted File Index (IVF)</a></li>
          <li><a href="#residual-vector-quantization-rvq" id="markdown-toc-residual-vector-quantization-rvq">Residual Vector Quantization (RVQ)</a></li>
          <li><a href="#scalable-k-means-clustering-mini-batch-k-means" id="markdown-toc-scalable-k-means-clustering-mini-batch-k-means">Scalable K-Means Clustering (Mini-Batch K-Means)</a></li>
        </ul>
      </li>
      <li><a href="#graph-based-methods" id="markdown-toc-graph-based-methods">Graph-Based Methods</a>        <ul>
          <li><a href="#navigable-small-worlds-nsw" id="markdown-toc-navigable-small-worlds-nsw">Navigable Small Worlds (NSW)</a></li>
          <li><a href="#fast-inference-for-graph-based-ann-finger" id="markdown-toc-fast-inference-for-graph-based-ann-finger">Fast Inference for Graph-Based ANN (FINGER)</a></li>
          <li><a href="#hierarchical-navigable-small-worlds-hnsw" id="markdown-toc-hierarchical-navigable-small-worlds-hnsw">Hierarchical Navigable Small Worlds (HNSW)</a></li>
        </ul>
      </li>
      <li><a href="#tabular-comparison" id="markdown-toc-tabular-comparison">Tabular Comparison</a></li>
      <li><a href="#choosing-the-right-ann-algorithm-family" id="markdown-toc-choosing-the-right-ann-algorithm-family">Choosing the Right ANN Algorithm Family</a></li>
    </ul>
  </li>
  <li><a href="#ann-libraries" id="markdown-toc-ann-libraries">ANN Libraries</a>    <ul>
      <li><a href="#faiss-facebook-ai-similarity-search" id="markdown-toc-faiss-facebook-ai-similarity-search">FAISS (Facebook AI Similarity Search)</a>        <ul>
          <li><a href="#key-features" id="markdown-toc-key-features">Key Features</a></li>
          <li><a href="#search-workflow" id="markdown-toc-search-workflow">Search Workflow</a></li>
          <li><a href="#evaluation-metrics" id="markdown-toc-evaluation-metrics">Evaluation Metrics</a></li>
          <li><a href="#pros" id="markdown-toc-pros">Pros</a></li>
          <li><a href="#cons" id="markdown-toc-cons">Cons</a></li>
        </ul>
      </li>
      <li><a href="#scann-scalable-nearest-neighbors" id="markdown-toc-scann-scalable-nearest-neighbors">ScaNN (Scalable Nearest Neighbors)</a>        <ul>
          <li><a href="#key-features-1" id="markdown-toc-key-features-1">Key Features</a></li>
          <li><a href="#anisotropic-vector-quantization-avq-1" id="markdown-toc-anisotropic-vector-quantization-avq-1">Anisotropic Vector Quantization (AVQ)</a></li>
          <li><a href="#use-case-semantic-search" id="markdown-toc-use-case-semantic-search">Use Case: Semantic Search</a></li>
          <li><a href="#implementation-considerations" id="markdown-toc-implementation-considerations">Implementation Considerations</a></li>
          <li><a href="#pros-1" id="markdown-toc-pros-1">Pros</a></li>
          <li><a href="#cons-1" id="markdown-toc-cons-1">Cons</a></li>
        </ul>
      </li>
      <li><a href="#annoy-approximate-nearest-neighbors-oh-yeah" id="markdown-toc-annoy-approximate-nearest-neighbors-oh-yeah">ANNOY (Approximate Nearest Neighbors Oh Yeah)</a>        <ul>
          <li><a href="#key-features-2" id="markdown-toc-key-features-2">Key Features</a></li>
          <li><a href="#use-case-music-recommendation" id="markdown-toc-use-case-music-recommendation">Use Case: Music Recommendation</a></li>
          <li><a href="#implementation-notes" id="markdown-toc-implementation-notes">Implementation Notes</a></li>
          <li><a href="#pros-2" id="markdown-toc-pros-2">Pros</a></li>
          <li><a href="#cons-2" id="markdown-toc-cons-2">Cons</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#comparative-analysis" id="markdown-toc-comparative-analysis">Comparative Analysis</a>    <ul>
      <li><a href="#tabular-comparison-1" id="markdown-toc-tabular-comparison-1">Tabular Comparison</a></li>
    </ul>
  </li>
  <li><a href="#ann-benchmarks" id="markdown-toc-ann-benchmarks">ANN-Benchmarks</a>    <ul>
      <li><a href="#purpose-and-scope" id="markdown-toc-purpose-and-scope">Purpose and Scope</a></li>
      <li><a href="#key-features-3" id="markdown-toc-key-features-3">Key Features</a></li>
      <li><a href="#using-ann-benchmarks" id="markdown-toc-using-ann-benchmarks">Using ANN-Benchmarks</a></li>
      <li><a href="#practical-use-cases" id="markdown-toc-practical-use-cases">Practical Use Cases</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="what-is-similarity-search">What is Similarity Search?</h2>

<ul>
  <li>Similarity search is a fundamental computational technique that enables the retrieval of items from a dataset that are most similar to a given query item. Instead of relying solely on exact matches, similarity search evaluates how closely data records resemble each other based on their semantic content. This capability underpins numerous real-world applications across diverse domains.</li>
</ul>

<h3 id="real-world-applications">Real-World Applications</h3>

<ul>
  <li>
    <p>In modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:</p>

    <ul>
      <li><strong>Image retrieval</strong>: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).</li>
      <li><strong>Document retrieval</strong>: Powering search engines that return semantically relevant documents in response to a textual query.</li>
      <li><strong>Recommendation systems</strong>: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.</li>
      <li><strong>Facial recognition and biometric systems</strong>: Matching a given face or fingerprint to a stored identity profile.</li>
      <li><strong>Medical imaging</strong>: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.</li>
    </ul>
  </li>
  <li>
    <p>These applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.</p>
  </li>
</ul>

<h3 id="from-exact-to-approximate-nearest-neighbor-search">From Exact to Approximate Nearest Neighbor Search</h3>

<ul>
  <li>
    <p>At the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.</p>
  </li>
  <li>
    <p><strong>Exact Nearest Neighbor (ENN) search</strong> involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.</p>
  </li>
  <li>
    <p><strong>Approximate Nearest Neighbor (ANN) search</strong> offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.</p>
  </li>
  <li>
    <p>This trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.</p>
  </li>
  <li>To compare and evaluate the performance of different ANN methods, practitioners often refer to standardized benchmarks such as <a href="https://ann-benchmarks.com/">ANN-Benchmarks</a>, which test algorithms across datasets and metrics.</li>
  <li>This primer will focus on the exploration of Approximate Nearest Neighbors in greater detail.</li>
</ul>

<h2 id="approximate-nearest-neighbors-ann">Approximate Nearest Neighbors (ANN)</h2>

<ul>
  <li>Approximate Nearest Neighbor (ANN) techniques are crucial for enabling fast and scalable similarity search, especially when working with large, high-dimensional datasets. These methods are designed to retrieve near-optimal neighbors for a query point without the computational overhead of exhaustively comparing against every item in the dataset. By slightly compromising on accuracy, ANN methods dramatically improve search speed and memory efficiency.</li>
</ul>

<h3 id="role-of-ann-in-recommendation-systems">Role of ANN in Recommendation Systems</h3>

<ul>
  <li>
    <p>ANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments.
To understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:</p>

    <ol>
      <li>
        <p><strong>Scalability</strong>: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.</p>
      </li>
      <li>
        <p><strong>Real-Time Recommendation</strong>: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.</p>
      </li>
      <li>
        <p><strong>Diversity and Serendipity</strong>: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.</p>
      </li>
      <li>
        <p><strong>Cold Start Handling</strong>: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.</p>
      </li>
    </ol>
  </li>
  <li>
    <p>By integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.</p>
  </li>
</ul>

<h2 id="ann-algorithms">ANN Algorithms</h2>

<ul>
  <li>ANN search is underpinned by a range of algorithmic strategies designed to support scalable, high-speed querying in high-dimensional spaces. These algorithms are broadly grouped into graph-based, tree-based, quantization-based, and clustering-based families. Each comes with specific implementation considerations, data structures, and optimization techniques.</li>
  <li>The ANN algorithms below have been sorted from simplest to most complex based on their underlying strategy types—starting with tree-based methods (e.g., KD-Tree, Annoy), followed by quantization-based methods (e.g., PQ, OPQ), clustering-based methods (e.g., IVF, RVQ), and finally graph-based methods (e.g., HNSW, FINGER) in the later rows.</li>
</ul>

<h3 id="tree-based-methods">Tree-Based Methods</h3>

<ul>
  <li>Tree-based methods form one of the earliest and most widely adopted strategies for nearest neighbor search. They operate by recursively dividing the vector space using hyperplanes (such as axis-aligned or randomly oriented splits), resulting in hierarchical structures like binary trees or forests. During querying, these structures enable the algorithm to efficiently prune large portions of the search space, reducing the number of distance computations required.</li>
  <li>
    <p>In real-world scenarios, tree-based methods are frequently used in settings where:</p>

    <ul>
      <li><strong>Dimensionality is moderate</strong> (typically under 100 dimensions), and datasets are either static or change infrequently.</li>
      <li><strong>Speed and interpretability are critical</strong>, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.</li>
      <li><strong>Low-latency and real-time processing is required</strong>, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.</li>
      <li><strong>Embedded or resource-constrained environments</strong> are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.</li>
    </ul>
  </li>
  <li>Tree-based methods also serve as foundational components in hybrid pipelines—often combined with graph-based or quantization-based modules to accelerate coarse filtering before refined re-ranking stages.</li>
  <li>Despite their limitations in high-dimensional contexts, their speed, simplicity, and practical effectiveness make them a valuable tool in many production-grade systems.</li>
</ul>

<h4 id="kd-trees">KD-Trees</h4>

<ul>
  <li>
    <p><strong>Partition Strategy</strong>: Recursive binary splits along dimensions with maximum variance.</p>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Traverses down the tree to a leaf.</li>
      <li>Backtracking is used to check sibling branches when necessary.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation</strong>:</p>

    <ul>
      <li>Fast for dimensions &lt; 30.</li>
      <li>Can use priority queues to simulate best-first search.</li>
      <li>Common in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Simple and interpretable structure.</li>
      <li>Low memory overhead.</li>
      <li>Fast for low-dimensional, static datasets.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Performance degrades rapidly in high-dimensional spaces.</li>
      <li>Not suitable for dynamic datasets or frequent updates.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>KD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.</li>
    </ul>
  </li>
</ul>

<h4 id="randomized-kd-forests">Randomized KD-Forests</h4>

<ul>
  <li>
    <p><strong>Multiple KD-Trees</strong>: Built with randomized split choices or axis shuffling.</p>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Queries traverse multiple trees in parallel.</li>
      <li>Results from all trees are merged.</li>
    </ul>
  </li>
  <li>
    <p><strong>Parameters</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">n_trees</code>: Number of trees to build (impacts accuracy and index size).</li>
      <li><code class="language-plaintext highlighter-rouge">leaf_size</code>: Minimum number of items in leaf nodes.</li>
      <li><code class="language-plaintext highlighter-rouge">search_checks</code>: Max nodes visited during querying.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Improves robustness over standard KD-trees.</li>
      <li>Offers tunable accuracy-speed trade-offs.</li>
      <li>Better suited to moderate dimensionality (up to ~100 dimensions).</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Higher memory footprint due to multiple trees.</li>
      <li>Requires careful tuning to avoid overfitting or redundancy.</li>
      <li>Performance still degrades in high-dimensional regimes.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>Randomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.</li>
    </ul>
  </li>
</ul>

<h4 id="annoys-random-projection-forest">Annoy’s Random Projection Forest</h4>

<ul>
  <li>
    <p><strong>Partition Strategy</strong>:</p>

    <ul>
      <li>Randomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.</li>
    </ul>
  </li>
  <li>
    <p><strong>Index</strong>:</p>

    <ul>
      <li>A forest of such trees is constructed.</li>
      <li>Each tree introduces different hyperplane splits, improving diversity.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Each tree yields a list of candidate points.</li>
      <li>Combined and sorted using brute-force on small candidate sets.</li>
    </ul>
  </li>
  <li>
    <p><strong>File-based Indexing</strong>:</p>

    <ul>
      <li>Indexes saved to disk as static files and memory-mapped at query time.</li>
      <li>Efficient for multi-process access and low RAM environments.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Memory-efficient via on-disk index loading.</li>
      <li>Supports shared access across processes.</li>
      <li>Simple to implement and tune (<code class="language-plaintext highlighter-rouge">n_trees</code>, <code class="language-plaintext highlighter-rouge">search_k</code>).</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Index is static—no support for incremental updates.</li>
      <li>Lacks GPU and batch processing support.</li>
      <li>Recall suffers at high dimensionality or when optimal tree coverage is hard to achieve.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>Annoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.</li>
    </ul>
  </li>
</ul>

<h3 id="quantization-based-methods">Quantization-Based Methods</h3>

<ul>
  <li>
    <p>Quantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.</p>
  </li>
  <li>
    <p>These methods are widely used in industry for:</p>

    <ul>
      <li>Large-scale image/video/text retrieval where space and speed are at a premium.</li>
      <li>Real-time inference pipelines that need rapid ranking of semantically similar results.</li>
      <li>Hybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.</li>
      <li>Scenarios that require deployment on limited-memory devices or optimized GPU environments.</li>
    </ul>
  </li>
  <li>
    <p>Despite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.</p>
  </li>
</ul>

<h4 id="product-quantization-pq">Product Quantization (PQ)</h4>

<ul>
  <li>
    <p><strong>Overview</strong>:</p>

    <ul>
      <li>Vector <code class="language-plaintext highlighter-rouge">x</code> is split into <code class="language-plaintext highlighter-rouge">m</code> sub-vectors.</li>
      <li>Each sub-vector is quantized into one of <code class="language-plaintext highlighter-rouge">k</code> centroids (learned using k-means).</li>
      <li>Final code is a tuple of centroid indices.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Precompute a distance table for each query sub-vector and all subspace centroids.</li>
      <li>Final distance is the sum of subspace distances from lookup tables.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation</strong>:</p>

    <ul>
      <li>In FAISS, PQ is implemented with SIMD intrinsics and fused operations.</li>
      <li>Typically used with IVF (Inverted File Index) for additional speed-up.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Excellent compression with minimal storage cost.</li>
      <li>Efficient for both CPU and GPU architectures.</li>
      <li>Fast distance computation using lookups, no full vector arithmetic needed.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Sensitive to data distribution; uniform quantization can lose detail.</li>
      <li>May degrade recall for fine-grained queries or tightly clustered data.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>PQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.</li>
    </ul>
  </li>
</ul>

<h4 id="optimized-product-quantization-opq">Optimized Product Quantization (OPQ)</h4>

<ul>
  <li>
    <p><strong>Extension to PQ</strong>:</p>

    <ul>
      <li>Learn a rotation matrix to decorrelate input vectors before PQ encoding.</li>
      <li>Reduces quantization loss, improving accuracy.</li>
    </ul>
  </li>
  <li>
    <p><strong>Training</strong>:</p>

    <ul>
      <li>Alternates between PCA-like optimization and quantization.</li>
      <li>Computationally heavier but yields significant accuracy improvements.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Lower distortion than basic PQ, especially for high-dimensional, correlated data.</li>
      <li>Flexible, modular integration with other indexing structures (e.g., IVF).</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Slower training and more complex parameter tuning.</li>
      <li>Requires retraining if the data distribution shifts significantly.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>OPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.</li>
    </ul>
  </li>
</ul>

<h4 id="locality-sensitive-hashing-lsh">Locality Sensitive Hashing (LSH)</h4>

<ul>
  <li>
    <p><strong>Hashing Scheme</strong>:</p>

    <ul>
      <li>Family of hash functions ensures that similar vectors hash to the same bucket with high probability.</li>
      <li>For cosine similarity: random hyperplane hash functions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Multi-Probe LSH</strong>:</p>

    <ul>
      <li>During query, multiple nearby buckets are probed to improve recall.</li>
    </ul>
  </li>
  <li>
    <p><strong>Challenges</strong>:</p>

    <ul>
      <li>Works best for low to moderate dimensions.</li>
      <li>Often memory-intensive and suffers in high-density regions of vector space.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Extremely fast sublinear search; constant-time hash lookups.</li>
      <li>Simple to implement with theoretical performance guarantees.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Poor recall for complex, high-dimensional datasets.</li>
      <li>High memory usage for large numbers of hash tables.</li>
      <li>Tuning hash functions and probing depth is non-trivial.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>LSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.</li>
    </ul>
  </li>
</ul>

<h4 id="anisotropic-vector-quantization-avq">Anisotropic Vector Quantization (AVQ)</h4>

<ul>
  <li>
    <p><strong>Used in ScaNN</strong>:</p>

    <ul>
      <li>Unlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.</li>
      <li>Each centroid has an anisotropic (elliptical) region rather than spherical.</li>
    </ul>
  </li>
  <li>
    <p><strong>Index Construction</strong>:</p>

    <ul>
      <li>Uses SVD-like analysis to deform Voronoi cells.</li>
      <li>Quantization boundaries are optimized to better reflect real data spread.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Improves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Adapts well to non-uniform data densities.</li>
      <li>Superior recall in semantic search applications compared to isotropic quantizers.</li>
      <li>Well-suited to modern ML-generated embeddings.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>More complex to construct and train than standard PQ.</li>
      <li>Not yet widely supported outside of ScaNN.</li>
      <li>Longer index build time due to centroid deformation.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>AVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.</li>
    </ul>
  </li>
</ul>

<h3 id="clustering-based-methods">Clustering-Based Methods</h3>

<ul>
  <li>Clustering-based approaches are a foundational class of ANN algorithms that partition the dataset into discrete groups or clusters using unsupervised learning techniques, most commonly k-means. These clusters serve as coarse partitions of the search space, enabling fast and scalable nearest neighbor search by reducing the number of direct comparisons required at query time.</li>
  <li>
    <p>Clustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation.  Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:</p>

    <ul>
      <li><strong>FAISS IVF-PQ</strong>: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.</li>
      <li><strong>ScaNN</strong>: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.</li>
      <li><strong>Two-Stage Retrieval Systems</strong>: Use clustering to shortlist candidates and exact scoring for ranking.</li>
    </ul>
  </li>
</ul>

<h4 id="inverted-file-index-ivf">Inverted File Index (IVF)</h4>

<ul>
  <li>
    <p><strong>Partition Strategy</strong>:</p>

    <ul>
      <li>The dataset is partitioned using <em>k-means clustering</em> into <code class="language-plaintext highlighter-rouge">nlist</code> coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>At query time, the query vector is compared to all centroids, and the top <code class="language-plaintext highlighter-rouge">nprobe</code> closest centroids are selected.</li>
      <li>Only the vectors in the selected clusters are searched, significantly narrowing down the search space.</li>
    </ul>
  </li>
  <li>
    <p><strong>Index Construction</strong>:</p>

    <ul>
      <li>A training phase precedes indexing, where representative centroids are learned from a sample of the dataset.</li>
      <li>Vectors are then indexed into inverted lists corresponding to their nearest centroid.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation</strong>:</p>

    <ul>
      <li>IVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Scalable to billions of vectors.</li>
      <li>Fast query execution due to aggressive pruning.</li>
      <li>Integrates well with quantization and re-ranking modules.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Requires careful tuning of <code class="language-plaintext highlighter-rouge">nlist</code> (cluster count) and <code class="language-plaintext highlighter-rouge">nprobe</code> (clusters to scan at query time).</li>
      <li>Clustering quality impacts recall and precision.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>Powers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in <strong>FAISS IVF-PQ</strong> and hybrid indexing pipelines.</li>
    </ul>
  </li>
</ul>

<h4 id="residual-vector-quantization-rvq">Residual Vector Quantization (RVQ)</h4>

<ul>
  <li>
    <p><strong>Overview</strong>:</p>

    <ul>
      <li>RVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.</li>
      <li>This hierarchical structure allows for progressively finer approximations of the original vector.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Queries are encoded using the same residual structure and matched against compound codes generated during indexing.</li>
      <li>Suitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.</li>
    </ul>
  </li>
  <li>
    <p><strong>Index Construction</strong>:</p>

    <ul>
      <li>Involves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.</li>
      <li>Typically implemented in vector search systems that require higher recall than basic PQ can offer.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Delivers better accuracy than single-pass quantization.</li>
      <li>Supports fast computation with table lookups and SIMD-accelerated operations.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>More complex to train and tune.</li>
      <li>Increased query-time latency due to deeper decoding stages.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>Used in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.</li>
    </ul>
  </li>
</ul>

<h4 id="scalable-k-means-clustering-mini-batch-k-means">Scalable K-Means Clustering (Mini-Batch K-Means)</h4>

<ul>
  <li>
    <p><strong>Purpose</strong>:</p>

    <ul>
      <li>Designed to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.</li>
      <li>Enables efficient, online construction of cluster centroids without processing the entire dataset at once.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search Integration</strong>:</p>

    <ul>
      <li>Frequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.</li>
    </ul>
  </li>
  <li>
    <p><strong>Training Efficiency</strong>:</p>

    <ul>
      <li>Orders of magnitude faster than traditional k-means for large datasets.</li>
      <li>Supports continual training or incremental centroid updates.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Suitable for massive datasets that exceed memory constraints.</li>
      <li>Faster training with minimal accuracy loss compared to full-batch methods.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Quality of clusters depends on batch size and sampling strategy.</li>
      <li>Can converge to suboptimal centroids in non-uniform or complex vector distributions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>Ideal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.</li>
    </ul>
  </li>
</ul>

<h3 id="graph-based-methods">Graph-Based Methods</h3>

<ul>
  <li>
    <p>Graph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through <strong>greedy traversal</strong>, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.</p>
  </li>
  <li>
    <p>These algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:</p>

    <ul>
      <li><strong>Excellent accuracy-speed trade-offs</strong>, particularly at high recall thresholds.</li>
      <li><strong>Support for dynamic updates</strong>, which is crucial for evolving datasets.</li>
      <li><strong>High empirical performance</strong> across embedding types (text, image, video, audio).</li>
      <li><strong>Adoption in commercial-grade vector databases and search systems</strong>, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.</li>
    </ul>
  </li>
  <li>
    <p>Applications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.</p>
  </li>
</ul>

<h4 id="navigable-small-worlds-nsw">Navigable Small Worlds (NSW)</h4>

<ul>
  <li>
    <p>NSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.</p>
  </li>
  <li>
    <p><strong>Data Structure</strong>: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.</p>
  </li>
  <li>
    <p><strong>Construction</strong>:</p>

    <ul>
      <li>Nodes are added one by one using randomized greedy strategies.</li>
      <li>For each new node, a set of connections is formed to existing nodes that are closest in terms of distance.</li>
      <li>Edge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Begins at a randomly selected node.</li>
      <li>Follows a greedy walk, moving to the neighbor closest to the query.</li>
      <li>Terminates once no neighbor is closer than the current node—reaching a local optimum.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Straightforward to implement and debug.</li>
      <li>Requires minimal configuration—fewer hyperparameters than hierarchical models.</li>
      <li>Lower memory usage than multilayer graphs.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>No hierarchical navigation; search can be slower or less reliable.</li>
      <li>Convergence to the global nearest neighbor is not guaranteed.</li>
      <li>Less suited for very large datasets or high-recall applications.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>NSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.</li>
    </ul>
  </li>
</ul>

<h4 id="fast-inference-for-graph-based-ann-finger">Fast Inference for Graph-Based ANN (FINGER)</h4>

<ul>
  <li>
    <p>FINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.</p>
  </li>
  <li>
    <p><strong>Algorithmic Enhancement</strong>:</p>

    <ul>
      <li>FINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.</li>
    </ul>
  </li>
  <li>
    <p><strong>Technique</strong>:</p>

    <ul>
      <li>Decomposes vectors into components (projections and residuals).</li>
      <li>Estimates distances using angular relationships and dot products.</li>
      <li>Avoids costly full-vector comparisons for less promising candidates.</li>
    </ul>
  </li>
  <li>
    <p><strong>Integration</strong>:</p>

    <ul>
      <li>Applied exclusively at query time.</li>
      <li>Does not alter the underlying graph; can be used on top of existing indexes.</li>
      <li>Particularly beneficial for large graphs with dense connectivity.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Reduces search latency by 20–60% in practice.</li>
      <li>Easy to integrate without retraining or rebuilding the index.</li>
      <li>Compatible with both simple (NSW) and hierarchical (HNSW) graphs.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Performance gains vary with data characteristics (e.g., vector distribution).</li>
      <li>Adds complexity to the search logic.</li>
      <li>May slightly reduce recall if approximation is too aggressive.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>FINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.</li>
    </ul>
  </li>
</ul>

<h4 id="hierarchical-navigable-small-worlds-hnsw">Hierarchical Navigable Small Worlds (HNSW)</h4>

<ul>
  <li>
    <p>HNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.</p>
  </li>
  <li>
    <p><strong>Data Structure</strong>:</p>

    <ul>
      <li>A multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.</li>
      <li>Nodes appear at multiple levels, with decreasing density as the level increases.</li>
    </ul>
  </li>
  <li>
    <p><strong>Construction</strong>:</p>

    <ul>
      <li>Each point is assigned a random maximum layer.</li>
      <li>Insertions start at the top and descend layer by layer, connecting to the closest nodes at each level.</li>
      <li>Parameters like <code class="language-plaintext highlighter-rouge">M</code> (max edges per node) and <code class="language-plaintext highlighter-rouge">efConstruction</code> control connectivity.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search</strong>:</p>

    <ul>
      <li>Starts from the topmost layer using a greedy search.</li>
      <li>Progressively moves down levels, narrowing the search to increasingly local areas.</li>
      <li><code class="language-plaintext highlighter-rouge">efSearch</code> determines the number of nodes visited during querying.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation Notes</strong>:</p>

    <ul>
      <li>Well-optimized in libraries such as NMSLIB, FAISS, and hnswlib.</li>
      <li>Supports dynamic insertions and deletions, making it suitable for production systems.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Achieves high recall at low latency.</li>
      <li>Scales efficiently to millions of high-dimensional vectors.</li>
      <li>Tunable for different performance requirements.</li>
      <li>Robust in both static and dynamic settings.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Higher memory usage due to multilayer structure.</li>
      <li>Longer index build times, especially with large datasets.</li>
      <li>Requires hyperparameter tuning for optimal performance.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use Case</strong>:</p>

    <ul>
      <li>HNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.</li>
    </ul>
  </li>
</ul>

<h3 id="tabular-comparison">Tabular Comparison</h3>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Algorithm</strong></th>
<th class="tg-hcenter-valign-first"><strong>Data Structure</strong></th>
<th class="tg-hcenter-valign-first"><strong>Construction Strategy</strong></th>
<th class="tg-hcenter-valign-first"><strong>Search Strategy</strong></th>
<th class="tg-hcenter-valign-first"><strong>Key Strengths</strong></th>
<th class="tg-hcenter-valign-second"><strong>Limitations</strong></th>
</tr>
</thead>
<tbody>

<tr style="border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);">
<td colspan="6" align="center"><strong>Tree-Based Methods</strong></td>
</tr>

<tr>
<td class="tg-tleft-valign-first">KD-Tree</td>
<td class="tg-tleft-valign-first">Binary tree with axis-aligned splits</td>
<td class="tg-tleft-valign-first">Recursive partitioning along max-variance axes</td>
<td class="tg-tleft-valign-first">Tree traversal with backtracking</td>
<td class="tg-tleft-valign-first">Fast for low dimensions (&lt;30), simple structure</td>
<td class="tg-tleft-valign-second">Degrades in high-dimensional spaces</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Randomized KD-Forest</td>
<td class="tg-tleft-valign-first">Multiple KD-Trees with random splits</td>
<td class="tg-tleft-valign-first">Parallel tree building with randomized axes</td>
<td class="tg-tleft-valign-first">Aggregate candidates from all trees</td>
<td class="tg-tleft-valign-first">Improved recall over single KD-tree</td>
<td class="tg-tleft-valign-second">Higher memory and tuning complexity</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Annoy Forest</td>
<td class="tg-tleft-valign-first">Forest of binary trees via random projections</td>
<td class="tg-tleft-valign-first">Random hyperplane splits</td>
<td class="tg-tleft-valign-first">Tree-wise candidate extraction with brute-force refinement</td>
<td class="tg-tleft-valign-first">Memory-mapped files, low RAM use, fast lookup</td>
<td class="tg-tleft-valign-second">Static index, no GPU/batch support</td>
</tr>

<tr style="border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);">
<td colspan="6" align="center"><strong>Quantization-Based Methods</strong></td>
</tr>

<tr>
<td class="tg-tleft-valign-first">Product Quantization (PQ)</td>
<td class="tg-tleft-valign-first">Codebooks for each subspace of vector</td>
<td class="tg-tleft-valign-first">K-means on vector subspaces</td>
<td class="tg-tleft-valign-first">Lookup-table based distance approximation</td>
<td class="tg-tleft-valign-first">Compact codes, efficient on CPUs/GPUs</td>
<td class="tg-tleft-valign-second">Accuracy loss due to quantization</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Optimized PQ (OPQ)</td>
<td class="tg-tleft-valign-first">Rotated subspace codebooks</td>
<td class="tg-tleft-valign-first">Learned rotation + PQ training</td>
<td class="tg-tleft-valign-first">Same as PQ, with lower quantization error</td>
<td class="tg-tleft-valign-first">Better accuracy than PQ, widely supported</td>
<td class="tg-tleft-valign-second">Heavier training, complex tuning</td>
</tr>

<tr style="border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);">
<td colspan="6" align="center"><strong>Clustering-Based Methods</strong></td>
</tr>

<tr>
<td class="tg-tleft-valign-first">Inverted File Index (IVF)</td>
<td class="tg-tleft-valign-first">Cluster centroids with inverted lists</td>
<td class="tg-tleft-valign-first">k-means clustering for coarse partitioning</td>
<td class="tg-tleft-valign-first">Select nearest clusters and search within them</td>
<td class="tg-tleft-valign-first">Scalable to billions of vectors, integrates with quantization</td>
<td class="tg-tleft-valign-second">Clustering quality impacts recall and precision</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Residual Vector Quantization (RVQ)</td>
<td class="tg-tleft-valign-first">Hierarchical residual codebooks</td>
<td class="tg-tleft-valign-first">Recursive quantization of residuals</td>
<td class="tg-tleft-valign-first">Multi-stage decoding and comparison</td>
<td class="tg-tleft-valign-first">Higher recall than single-level quantizers</td>
<td class="tg-tleft-valign-second">Increased query latency, complex training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Mini-Batch K-Means</td>
<td class="tg-tleft-valign-first">Incrementally updated cluster centroids</td>
<td class="tg-tleft-valign-first">Iterative mini-batch updates on sampled data</td>
<td class="tg-tleft-valign-first">Used to assign points or create IVF partitions</td>
<td class="tg-tleft-valign-first">Efficient training on large datasets, supports streaming data</td>
<td class="tg-tleft-valign-second">Cluster quality depends on batch configuration</td>
</tr>

<tr style="border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);">
<td colspan="6" align="center"><strong>Graph-Based Methods</strong></td>
</tr>

<tr>
<td class="tg-tleft-valign-first">NSW</td>
<td class="tg-tleft-valign-first">Single-layer navigable small-world graph</td>
<td class="tg-tleft-valign-first">Randomized edge insertion based on proximity</td>
<td class="tg-tleft-valign-first">Greedy walk guided by local nearest neighbors</td>
<td class="tg-tleft-valign-first">Lightweight, easy to implement, lower memory</td>
<td class="tg-tleft-valign-second">No hierarchy, slower convergence, reduced recall in complex data</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">FINGER</td>
<td class="tg-tleft-valign-first">Augmentation over navigable graph</td>
<td class="tg-tleft-valign-first">No structural changes; integrates into search phase</td>
<td class="tg-tleft-valign-first">Approximate distance estimation via vector projections</td>
<td class="tg-tleft-valign-first">Accelerates search in existing graphs, reduces computation by 20–60%</td>
<td class="tg-tleft-valign-second">Dependent on vector distribution, adds search-time complexity</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">HNSW</td>
<td class="tg-tleft-valign-first">Multilayer navigable proximity graph</td>
<td class="tg-tleft-valign-first">Greedy layer-wise insertion with long- and short-range links</td>
<td class="tg-tleft-valign-first">Hierarchical greedy search with priority queue</td>
<td class="tg-tleft-valign-first">High recall, dynamic updates, excellent accuracy-speed trade-off</td>
<td class="tg-tleft-valign-second">High memory usage, longer build time, parameter tuning required</td>
</tr>

</tbody>
</table>
</div>

<h3 id="choosing-the-right-ann-algorithm-family">Choosing the Right ANN Algorithm Family</h3>

<ul>
  <li>
    <p>Here’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:</p>

    <ul>
      <li>
        <p><strong>Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)</strong>: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.</p>
      </li>
      <li>
        <p><strong>Quantization-Based Methods (PQ, OPQ, LSH, AVQ)</strong>: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.</p>
      </li>
      <li>
        <p><strong>Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)</strong>: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.</p>
      </li>
      <li>
        <p><strong>Graph-Based Methods (NSW, FINGER, HNSW)</strong>: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Choose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.</p>
  </li>
</ul>

<h2 id="ann-libraries">ANN Libraries</h2>

<ul>
  <li>While ANN algorithms provide the foundational theory for efficient similarity search, their widespread use in real-world applications is made possible by powerful open-source libraries. These libraries abstract away the low-level implementation details and offer optimized, production-ready APIs for indexing, querying, and evaluating high-dimensional data.</li>
  <li>Below, we examine three leading libraries: <strong>FAISS</strong>, <strong>ScaNN</strong>, and <strong>ANNOY</strong>.</li>
</ul>

<h3 id="faiss-facebook-ai-similarity-search">FAISS (Facebook AI Similarity Search)</h3>

<ul>
  <li>FAISS is a highly optimized C++ library with Python bindings, developed by Facebook AI Research to support efficient similarity search over large-scale datasets of dense vectors. It is widely used in both academic research and industrial systems.</li>
</ul>

<h4 id="key-features">Key Features</h4>

<ul>
  <li>
    <p><strong>Multiple Indexing Strategies</strong>:</p>

    <ul>
      <li><em>Flat Index</em>: Exhaustive search for exact results.</li>
      <li><em>Inverted File Index (IVF)</em>: Partitions data using k-means centroids; search occurs in the closest partitions.</li>
      <li><em>Product Quantization (PQ)</em> and <em>Optimized PQ (OPQ)</em>: Compresses vectors into compact codes.</li>
      <li><em>IVF-PQ/IVF-OPQ</em>: Combines inverted indexing with quantization for speed-accuracy trade-offs.</li>
      <li><em>HNSW</em>: Integrated for graph-based search.</li>
    </ul>
  </li>
  <li>
    <p><strong>GPU Acceleration</strong>:</p>

    <ul>
      <li>FAISS provides CUDA-based implementations for key indexing methods.</li>
      <li>Enables real-time search on billion-scale datasets.</li>
    </ul>
  </li>
  <li>
    <p><strong>Scalability</strong>:</p>

    <ul>
      <li>Supports distributed indexing using sharding.</li>
      <li>Can operate on datasets that do not fit into RAM using memory-mapped files.</li>
    </ul>
  </li>
</ul>

<h4 id="search-workflow">Search Workflow</h4>

<ol>
  <li><strong>Training</strong>: For PQ or IVF, the index must first be trained on a representative sample.</li>
  <li><strong>Indexing</strong>: Vectors are added to the index using <code class="language-plaintext highlighter-rouge">.add()</code> or <code class="language-plaintext highlighter-rouge">.add_with_ids()</code>.</li>
  <li><strong>Querying</strong>: The <code class="language-plaintext highlighter-rouge">.search()</code> method returns the k nearest neighbors for each query.</li>
  <li><strong>Tuning</strong>: Parameters like <code class="language-plaintext highlighter-rouge">nlist</code> (number of clusters), <code class="language-plaintext highlighter-rouge">nprobe</code> (number of partitions to search), and PQ code size significantly affect accuracy and speed.</li>
</ol>

<h4 id="evaluation-metrics">Evaluation Metrics</h4>

<ul>
  <li><strong>Speed</strong>: Number of queries per second or average query latency.</li>
  <li><strong>Memory Usage</strong>: Especially critical when using PQ or GPU mode.</li>
  <li><strong>Accuracy</strong>: Measured using recall metrics (e.g., recall\@1, recall\@10).</li>
</ul>

<h4 id="pros">Pros</h4>

<ul>
  <li>High flexibility and modular design.</li>
  <li>GPU acceleration with multi-threaded CPU support.</li>
  <li>Rich suite of index types and hybrid approaches.</li>
</ul>

<h4 id="cons">Cons</h4>

<ul>
  <li>Can be complex to configure optimally.</li>
  <li>GPU integration requires familiarity with CUDA.</li>
  <li>No built-in support for dynamic (online) index updates; best suited for batch ingestion.</li>
</ul>

<p><img src="/primers/ai/assets/similarity-search/faiss.webp" alt=""></p>

<h3 id="scann-scalable-nearest-neighbors">ScaNN (Scalable Nearest Neighbors)</h3>

<ul>
  <li>ScaNN is an efficient similarity search library developed by Google Research, tailored for Maximum Inner Product Search (MIPS) and high-accuracy ANN in embedding-based retrieval systems.</li>
</ul>

<h4 id="key-features-1">Key Features</h4>

<ul>
  <li>
    <p><strong>Hybrid Indexing Architecture</strong>:</p>

    <ul>
      <li><strong>Partitioning Tree</strong>: Efficiently narrows the candidate set using k-means clustering or partition trees.</li>
      <li><strong>Asymmetric Distance Computation (ADC)</strong>: For fast dot-product or Euclidean similarity.</li>
      <li><strong>Anisotropic Vector Quantization (AVQ)</strong>: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.</li>
    </ul>
  </li>
</ul>

<h4 id="anisotropic-vector-quantization-avq-1">Anisotropic Vector Quantization (AVQ)</h4>

<ul>
  <li>
    <p>Anisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.</p>
  </li>
  <li>
    <p><strong>Quantization Grid</strong>:</p>

    <ul>
      <li>AVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.</li>
    </ul>
  </li>
  <li>
    <p><strong>Anisotropic Adjustment</strong>:</p>

    <ul>
      <li>Unlike standard product quantization, AVQ adapts the <strong>shape and size</strong> of each grid cell to reflect the <strong>local density and orientation</strong> of the data.</li>
      <li>This is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.</li>
    </ul>
  </li>
  <li>
    <p><strong>Indexing Phase</strong>:</p>

    <ul>
      <li>During index construction, vectors are assigned to their nearest quantization cell using modified centroids.</li>
      <li>This mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.</li>
    </ul>
  </li>
  <li>
    <p><strong>Search Phase</strong>:</p>

    <ul>
      <li>For a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.</li>
      <li>These candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.</li>
    </ul>
  </li>
  <li>
    <p><strong>Performance Benefits</strong>:</p>

    <ul>
      <li>AVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.</li>
      <li>On benchmarks like <a href="http://ann-benchmarks.com/glove-100-angular_10_angular.html">glove-100-angular</a>, ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).</li>
      <li>This quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.</li>
    </ul>
  </li>
</ul>

<p><img src="/primers/ai/assets/similarity-search/scann1.jpg" alt=""></p>

<h4 id="use-case-semantic-search">Use Case: Semantic Search</h4>

<ul>
  <li>ScaNN excels in use cases where embeddings are generated from text, images, or product metadata using deep learning models. Its design is aligned with the two-tower model, where both queries and items are independently embedded into a shared vector space.</li>
</ul>

<h4 id="implementation-considerations">Implementation Considerations</h4>

<ul>
  <li><strong>Training</strong>: Required for quantization and partitioning.</li>
  <li><strong>Search Configuration</strong>: Parameters such as the number of clusters, quantization depth, and number of re-ranked results need tuning.</li>
  <li><strong>Deployment</strong>: Can be used with TensorFlow Serving or directly integrated into a backend service.</li>
  <li>ScaNN is open-source and available via <a href="https://github.com/google-research/google-research/tree/master/scann">GitHub</a>. It can be installed via Pip and supports both TensorFlow and NumPy inputs.</li>
</ul>

<h4 id="pros-1">Pros</h4>

<ul>
  <li>High accuracy and throughput for MIPS.</li>
  <li>Strong performance on semantically rich embeddings.</li>
  <li>Open-source and actively maintained.</li>
</ul>

<h4 id="cons-1">Cons</h4>

<ul>
  <li>Less mature ecosystem than FAISS.</li>
  <li>GPU support is limited.</li>
  <li>Initial index build time can be high.</li>
</ul>

<p><img src="/primers/ai/assets/similarity-search/scann.gif" alt="">
<img src="/primers/ai/assets/similarity-search/scann1.jpg" alt=""></p>

<h3 id="annoy-approximate-nearest-neighbors-oh-yeah">ANNOY (Approximate Nearest Neighbors Oh Yeah)</h3>

<ul>
  <li>Developed by Spotify, ANNOY is a lightweight and efficient C++ library with Python bindings for performing fast similarity search using random projection forests. It is particularly suitable for static datasets and read-heavy workloads.</li>
</ul>

<h4 id="key-features-2">Key Features</h4>

<ul>
  <li>
    <p><strong>Indexing Method</strong>:</p>

    <ul>
      <li>Constructs multiple binary trees using random hyperplane splits.</li>
      <li>Each tree represents a different partitioning of the space.</li>
    </ul>
  </li>
  <li>
    <p><strong>Memory Mapping</strong>:</p>

    <ul>
      <li>Indexes are written to disk and memory-mapped at query time.</li>
      <li>Enables extremely fast lookups with minimal RAM usage.</li>
    </ul>
  </li>
  <li>
    <p><strong>Runtime Parameters</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">n_trees</code>: Affects accuracy and index size. More trees = better recall.</li>
      <li><code class="language-plaintext highlighter-rouge">search_k</code>: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.</li>
    </ul>
  </li>
</ul>

<h4 id="use-case-music-recommendation">Use Case: Music Recommendation</h4>

<ul>
  <li>Spotify uses ANNOY for fast retrieval of similar songs, playlists, or users based on vector representations of user behavior or audio features.</li>
</ul>

<h4 id="implementation-notes">Implementation Notes</h4>

<ul>
  <li>Static indexing: Does not support dynamic insertions (though <code class="language-plaintext highlighter-rouge">annoy2</code> aims to address this).</li>
  <li>No GPU support.</li>
  <li>No native batch processing interface; needs custom implementation for throughput optimization.</li>
</ul>

<h4 id="pros-2">Pros</h4>

<ul>
  <li>Simple to use and highly portable.</li>
  <li>Memory-efficient due to disk-based index.</li>
  <li>Suitable for multi-process environments.</li>
</ul>

<h4 id="cons-2">Cons</h4>

<ul>
  <li>Limited support for dynamic updates.</li>
  <li>Slower than FAISS or ScaNN for very large datasets.</li>
  <li>No GPU acceleration or quantization techniques.</li>
</ul>

<p><img src="/primers/ai/assets/similarity-search/annoy.jpg" alt=""></p>

<ul>
  <li>These libraries encapsulate the most widely adopted implementations of ANN techniques in industry today. Each has a different performance profile and is better suited to specific use cases:
    <ul>
      <li><strong>FAISS</strong>: Ideal for large-scale, high-performance search with GPU support.</li>
      <li><strong>ScaNN</strong>: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.</li>
      <li><strong>ANNOY</strong>: Lightweight and efficient for static, read-optimized systems.</li>
    </ul>
  </li>
</ul>

<h2 id="comparative-analysis">Comparative Analysis</h2>

<ul>
  <li>
    <p>This section offers a comparison that highlights the fundamental trade-offs between speed, accuracy, memory efficiency, hardware requirements, and integration complexity across major ANN methods and libraries. The most appropriate method depends on factors such as:</p>
  </li>
  <li>
    <p><strong>Dataset Characteristics</strong>:</p>

    <ul>
      <li>Size (thousands to billions of vectors)</li>
      <li>Dimensionality (sparse vs. dense vectors, 32–1000+ dimensions)</li>
      <li>Distribution (uniform vs. clustered or anisotropic)</li>
      <li>Update frequency (static vs. dynamic/real-time ingestion)</li>
    </ul>
  </li>
  <li>
    <p><strong>Performance Needs</strong>:</p>

    <ul>
      <li>Latency requirements (real-time systems vs. batch recommendations)</li>
      <li>Throughput (queries per second in multi-user environments)</li>
      <li>Index build time and preprocessing overhead</li>
    </ul>
  </li>
  <li>
    <p><strong>Resource Constraints</strong>:</p>

    <ul>
      <li>Available memory (RAM vs. disk)</li>
      <li>Compute acceleration (CPU-only vs. GPU-accelerated systems)</li>
      <li>Infrastructure for deployment (cloud-native vs. embedded use cases)</li>
    </ul>
  </li>
  <li>
    <p><strong>Accuracy Tolerance</strong>:</p>

    <ul>
      <li>Use cases that demand precise retrieval vs. tolerable approximation</li>
      <li>Top-k recall metrics, precision trade-offs</li>
    </ul>
  </li>
</ul>

<h3 id="tabular-comparison-1">Tabular Comparison</h3>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Library</strong></th>
<th class="tg-hcenter-valign-first"><strong>Definition/Functioning</strong></th>
<th class="tg-hcenter-valign-first"><strong>Pros</strong></th>
<th class="tg-hcenter-valign-second"><strong>Cons</strong></th>
</tr>
</thead>
<tbody>

<tr>
<td class="tg-tleft-valign-first"><strong>FAISS</strong></td>
<td class="tg-tleft-valign-first">Modular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.</td>
<td class="tg-tleft-valign-first">
<ul>
<li>Handles billion-scale datasets efficiently with GPU and SIMD acceleration</li>
<li>Supports both L2 and inner product distance functions</li>
<li>Highly modular: custom pipelines can be built by combining indexing strategies</li>
<li>Supports on-disk indexing, IVF quantization, and parallel queries</li>
</ul>
</td>
<td class="tg-tleft-valign-second">
<ul>
<li>No native support for dynamic indexing (most indexes are immutable)</li>
<li>Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise</li>
<li>Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)</li>
</ul>
</td>
</tr>

<tr>
<td class="tg-tleft-valign-first"><strong>ScaNN</strong></td>
<td class="tg-tleft-valign-first">Google's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.</td>
<td class="tg-tleft-valign-first">
<ul>
<li>Designed for embedding-based search (e.g., NLP, recommendation systems)</li>
<li>Highly accurate top-k results in inner product space (MIPS)</li>
<li>Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines</li>
<li>Significantly faster than LSH or naive MIPS approaches</li>
</ul>
</td>
<td class="tg-tleft-valign-second">
<ul>
<li>No official GPU acceleration (primarily CPU-based)</li>
<li>Less flexible than FAISS for custom index architectures</li>
<li>Longer index construction time due to partitioning and quantization training</li>
</ul>
</td>
</tr>

<tr>
<td class="tg-tleft-valign-first"><strong>ANNOY</strong></td>
<td class="tg-tleft-valign-first">Tree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.</td>
<td class="tg-tleft-valign-first">
<ul>
<li>Index files can be memory-mapped and shared across processes</li>
<li>Supports very large datasets on low-memory systems (e.g., embedded devices)</li>
<li>Simple API and fast to build indexes for static data</li>
<li>Ideal for batch processing and edge deployments</li>
</ul>
</td>
<td class="tg-tleft-valign-second">
<ul>
<li>Cannot update index after construction (no dynamic insertions)</li>
<li>Not designed for GPU acceleration or vector quantization</li>
<li>Recall suffers for very high-dimensional data (&gt;200D)</li>
<li>No native batch-query optimization; needs manual parallelization</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>

<ul>
  <li>
    <p>This comparison should guide you toward the best ANN system for your specific technical needs:</p>

    <ul>
      <li>Choose <strong>FAISS</strong> for large-scale indexing with advanced GPU acceleration and tight performance tuning.</li>
      <li>Use <strong>ScaNN</strong> if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.</li>
      <li>Opt for <strong>ANNOY</strong> when working with static datasets in constrained memory environments or needing file-based deployment.</li>
      <li>Prefer <strong>HNSW</strong> when low latency and high recall are critical, and memory usage is acceptable.</li>
      <li>Consider <strong>FINGER</strong> as a low-overhead performance enhancer if you already employ graph-based indexes.</li>
    </ul>
  </li>
</ul>

<h2 id="ann-benchmarks"><a href="https://ann-benchmarks.com/">ANN-Benchmarks</a></h2>

<ul>
  <li><a href="https://ann-benchmarks.com/">ANN-Benchmarks</a> is an open-source benchmarking platform that provides a standardized and reproducible environment for evaluating the performance of approximate nearest neighbor algorithms. It enables researchers and practitioners to assess trade-offs between speed, accuracy, and memory usage across various libraries and algorithmic approaches.</li>
</ul>

<h3 id="purpose-and-scope">Purpose and Scope</h3>

<ul>
  <li>The benchmark suite is designed to answer key questions such as:
    <ul>
      <li>Which ANN method offers the best speed/accuracy trade-off for a particular type of data?</li>
      <li>How do different libraries perform under the same distance metric?</li>
      <li>What is the relative performance of quantization vs. graph-based indexing on benchmark datasets?</li>
    </ul>
  </li>
</ul>

<h3 id="key-features-3">Key Features</h3>

<ul>
  <li>
    <p><strong>Common Datasets</strong>:</p>

    <ul>
      <li>Includes real-world and synthetic datasets like <code class="language-plaintext highlighter-rouge">SIFT</code>, <code class="language-plaintext highlighter-rouge">GloVe</code>, <code class="language-plaintext highlighter-rouge">Fashion-MNIST</code>, and <code class="language-plaintext highlighter-rouge">Deep Image Descriptors</code>.</li>
      <li>Each dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.</li>
    </ul>
  </li>
  <li>
    <p><strong>Distance Metrics</strong>:</p>

    <ul>
      <li>Supports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).</li>
    </ul>
  </li>
  <li>
    <p><strong>Evaluation Metrics</strong>:</p>

    <ul>
      <li><strong>Recall@<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 0.622em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.519em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1000.52em, 2.327em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">k</script></strong>: Measures the fraction of true nearest neighbors found in the top-k results.</li>
      <li><strong>QPS (Queries per second)</strong>: Throughput of the method, showing speed under load.</li>
      <li><strong>Index size and build time</strong>: Assesses memory footprint and pre-processing requirements.</li>
    </ul>
  </li>
  <li>
    <p><strong>Leaderboard Interface</strong>:</p>

    <ul>
      <li>Live comparison of algorithmic results across datasets.</li>
      <li>Interactive plots showing recall vs. query time, making it easy to identify optimal configurations.</li>
    </ul>
  </li>
</ul>

<h3 id="using-ann-benchmarks">Using ANN-Benchmarks</h3>

<p>The benchmarks can be run locally via Docker and Python, allowing users to test their own ANN implementations or configurations. It supports integrating custom methods into the benchmarking pipeline, which makes it a valuable research tool.</p>

<p><img src="../../../images/read/ANN-Benchmarks.jpg" alt=""></p>

<h3 id="practical-use-cases">Practical Use Cases</h3>

<ul>
  <li><strong>Model Selection</strong>: Choose the best ANN library for your specific task and hardware constraints.</li>
  <li><strong>Algorithm Tuning</strong>: Understand how hyperparameters like <code class="language-plaintext highlighter-rouge">nlist</code>, <code class="language-plaintext highlighter-rouge">nprobe</code>, or <code class="language-plaintext highlighter-rouge">search_k</code> affect real-world performance.</li>
  <li><strong>Regression Testing</strong>: Evaluate how updates to ANN methods impact speed or recall.</li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li>Jegou, Hervé, et al., <a href="https://engineering.fb.com/2017/03/29/data-infrastructure/FAISS-a-library-for-efficient-similarity-search/">FAISS: A library for efficient similarity search</a>, Engineering at Meta, 29 March 2017.</li>
  <li>Erik Bernhardsson, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Babenko_Efficient_Indexing_of_CVPR_2016_paper.pdf">Efficient Indexing of Billion-Scale datasets of deep descriptors</a>, 1 October 2015.</li>
  <li><a href="https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html">Announcing ScaNN: Efficient Vector Similarity Search</a></li>
  <li><a href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html">Nearest neighbors and vector models – part 2 – algorithms and data structures</a></li>
  <li><a href="https://www.amazon.science/blog/more-efficient-approximate-nearest-neighbor-search">More-efficient approximate nearest-neighbor search</a></li>
</ul>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code0"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code0">@article{VatsChadha2020DistilledApproximateNearestNeighbors,
  title   = {Approximate Nearest Neighbors -- Similarity Search},
  author  = {Chadha, Aman and Jain, Vinija},
  journal = {Distilled AI},
  year    = {2022},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895470&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fann-similarity-search%2F&amp;pra=5&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766923044247&amp;bpp=1&amp;bdt=30&amp;idt=7&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=777413978894&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31096042%2C95366178%2C95376242%2C95376583%2C95378749%2C95379212&amp;oid=2&amp;pvsid=921925264202153&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=33792&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=10" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe src="https://www.google.com/recaptcha/api2/aframe" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>