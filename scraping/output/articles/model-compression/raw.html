<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • Model Compression for On-Device AI</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/model-compression/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fmodel-compression&amp;ers=2"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWWkJRJ9rYdL2TORRPoNZopFeD0ol_Xefs-pDFXCyq064_oPsF4HmtUdk7pugv9zLNEZHhLdxMyqkmTyYqFnXtIcsXVpScM62PqgU8ECstRfls2fcK9BQ8MntBZVkKFyDK15r9GtA==?fccs=W1siQUtzUm9sX2NuaWFPNjBoZEc0Y2J6WHJrdzhXNEZvTDZEaHdlTExCbU44NE9LU3NpN0NOT2xZSXpETFg2SlRqenRVR2NTNnlBVWVGOGR4aklJdWJ0NFdJUTJZWXpGMDUzRF92bjNPRGJ3VVBQRFFPMjR3UjVMNkZpb1ZNTUJMSTFaWjJTUjEwNmVpUU9hcG5fS0xmZk15b1JveWN6NExnVFJRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5NDgsNTkxMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9tb2RlbC1jb21wcmVzc2lvbi8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbOTUzNDAyNTIsOTUzNDAyNTQsOTUzNzk4MjNdXSJdLFsyOSwiZmFsc2UiXV1d"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxU3VnF48OjZxaE7m_ddT5qJAQ7b6t3zerE0xDXZtx6yuNAdvQUgv8pSW1oy8UF16mt7o1kKs6tenRxA6sYuxnsf4nQKR1tp7jjTSGoSth76L80coY8ds55_IuOGh4_Gr0CdIdNDng==?fccs=W1siQUtzUm9sX2NuaWFPNjBoZEc0Y2J6WHJrdzhXNEZvTDZEaHdlTExCbU44NE9LU3NpN0NOT2xZSXpETFg2SlRqenRVR2NTNnlBVWVGOGR4aklJdWJ0NFdJUTJZWXpGMDUzRF92bjNPRGJ3VVBQRFFPMjR3UjVMNkZpb1ZNTUJMSTFaWjJTUjEwNmVpUU9hcG5fS0xmZk15b1JveWN6NExnVFJRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5NDgsNzk0MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5XSxudWxsLDIsbnVsbCwiZW4iXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvbW9kZWwtY29tcHJlc3Npb24vIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI1LCJbWzk1MzQwMjUyLDk1MzQwMjU0LDk1Mzc5ODIzXV0iXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWwwI-rRfLR8RE9SHmeDOpI--LXgs8S7ghWj4_Lxf4s2fDd0mbTJ-OsmpYHHXG7Vr3blIvazZmpmzkPECASs6cARyPr0276vQEd3X4LvzcATcAYaDXpBhLEUpKGHOcsGwQiAYxZYg==?fccs=W1siQUtzUm9sX2NuaWFPNjBoZEc0Y2J6WHJrdzhXNEZvTDZEaHdlTExCbU44NE9LU3NpN0NOT2xZSXpETFg2SlRqenRVR2NTNnlBVWVGOGR4aklJdWJ0NFdJUTJZWXpGMDUzRF92bjNPRGJ3VVBQRFFPMjR3UjVMNkZpb1ZNTUJMSTFaWjJTUjEwNmVpUU9hcG5fS0xmZk15b1JveWN6NExnVFJRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5NDksNjQ4MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5LDZdLG51bGwsMixudWxsLCJlbiIsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLDFdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9tb2RlbC1jb21wcmVzc2lvbi8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbOTUzNDAyNTIsOTUzNDAyNTQsOTUzNzk4MjNdXSJdLFsyOSwiZmFsc2UiXV1d"></script></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • Model Compression for On-Device AI</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#quantization" id="markdown-toc-quantization">Quantization</a>    <ul>
      <li><a href="#background-precision" id="markdown-toc-background-precision">Background: Precision</a>        <ul>
          <li><a href="#ieee-754-floating-point-standard" id="markdown-toc-ieee-754-floating-point-standard">IEEE 754 Floating Point Standard</a></li>
          <li><a href="#half-precision-float16" id="markdown-toc-half-precision-float16">Half-Precision (<code class="language-plaintext Highlighter-rouge">float16</code>)</a></li>
          <li><a href="#single-precision-float32" id="markdown-toc-single-precision-float32">Single-Precision (<code class="language-plaintext Highlighter-rouge">float32</code>)</a></li>
          <li><a href="#double-precision-float64" id="markdown-toc-double-precision-float64">Double-Precision (<code class="language-plaintext Highlighter-rouge">float64</code>)</a></li>
          <li><a href="#brain-floating-point-bfloat16" id="markdown-toc-brain-floating-point-bfloat16">Brain Floating Point (<code class="language-plaintext Highlighter-rouge">bfloat16</code>)</a></li>
          <li><a href="#gputpu-considerations" id="markdown-toc-gputpu-considerations">GPU/TPU Considerations</a></li>
          <li><a href="#comparative-summary" id="markdown-toc-comparative-summary">Comparative Summary</a></li>
        </ul>
      </li>
      <li><a href="#background-matrix-multiplication-in-gpus" id="markdown-toc-background-matrix-multiplication-in-gpus">Background: Matrix Multiplication in GPUs</a>        <ul>
          <li><a href="#under-the-hood" id="markdown-toc-under-the-hood">Under-the-hood</a></li>
          <li><a href="#how-tensor-cores-work" id="markdown-toc-how-tensor-cores-work">How Tensor Cores Work</a></li>
        </ul>
      </li>
      <li><a href="#definition" id="markdown-toc-definition">Definition</a></li>
      <li><a href="#types-of-quantization" id="markdown-toc-types-of-quantization">Types of Quantization</a>        <ul>
          <li><a href="#integer-quantization" id="markdown-toc-integer-quantization">Integer Quantization</a></li>
          <li><a href="#non-linear-integer-quantization-methods" id="markdown-toc-non-linear-integer-quantization-methods">Non-Linear Integer Quantization Methods</a></li>
          <li><a href="#floating-point-quantization" id="markdown-toc-floating-point-quantization">Floating-Point Quantization</a></li>
        </ul>
      </li>
      <li><a href="#dequantization-considerations" id="markdown-toc-dequantization-considerations">Dequantization Considerations</a></li>
      <li><a href="#quantization-workflows" id="markdown-toc-quantization-workflows">Quantization Workflows</a></li>
      <li><a href="#benefits-and-limitations" id="markdown-toc-benefits-and-limitations">Benefits and Limitations</a></li>
      <li><a href="#mitigation-strategies" id="markdown-toc-mitigation-strategies">Mitigation Strategies</a></li>
      <li><a href="#weights-vs-activation-quantization" id="markdown-toc-weights-vs-activation-quantization">Weights vs. Activation Quantization</a></li>
      <li><a href="#quantization-with-pytorch" id="markdown-toc-quantization-with-pytorch">Quantization with PyTorch</a>        <ul>
          <li><a href="#dynamic--runtime-quantization" id="markdown-toc-dynamic--runtime-quantization">Dynamic / Runtime Quantization</a>            <ul>
              <li><a href="#dynamic-quantization-vs-post-training-quantization" id="markdown-toc-dynamic-quantization-vs-post-training-quantization">Dynamic Quantization vs. Post-Training Quantization</a></li>
              <li><a href="#example-workflow" id="markdown-toc-example-workflow">Example Workflow</a>                <ul>
                  <li><a href="#workflow-explanation" id="markdown-toc-workflow-explanation">Workflow Explanation</a></li>
                  <li><a href="#typical-benefits" id="markdown-toc-typical-benefits">Typical Benefits</a></li>
                  <li><a href="#notes--tradeoffs" id="markdown-toc-notes--tradeoffs">Notes &amp; Trade‑offs</a></li>
                  <li><a href="#further-reading" id="markdown-toc-further-reading">Further Reading</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><a href="#post-training-quantization" id="markdown-toc-post-training-quantization">Post-Training Quantization</a>            <ul>
              <li><a href="#post-training-quantization-vs-dynamic-quantization" id="markdown-toc-post-training-quantization-vs-dynamic-quantization">Post-Training Quantization vs. Dynamic Quantization</a></li>
              <li><a href="#example-workflow-1" id="markdown-toc-example-workflow-1">Example Workflow</a>                <ul>
                  <li><a href="#workflow-explanation-1" id="markdown-toc-workflow-explanation-1">Workflow Explanation</a></li>
                  <li><a href="#typical-benefits-1" id="markdown-toc-typical-benefits-1">Typical Benefits</a></li>
                  <li><a href="#notes--tradeoffs-1" id="markdown-toc-notes--tradeoffs-1">Notes &amp; Trade‑offs</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><a href="#quantizationaware-training-qat" id="markdown-toc-quantizationaware-training-qat">Quantization‑aware Training (QAT)</a>            <ul>
              <li><a href="#quantizationaware-training-vs-post-training-quantization-vs-dynamic-quantization" id="markdown-toc-quantizationaware-training-vs-post-training-quantization-vs-dynamic-quantization">Quantization‑aware Training vs. Post-Training Quantization vs. Dynamic Quantization</a></li>
              <li><a href="#example-workflow-2" id="markdown-toc-example-workflow-2">Example Workflow</a>                <ul>
                  <li><a href="#workflow-explanation-2" id="markdown-toc-workflow-explanation-2">Workflow Explanation</a></li>
                  <li><a href="#typical-benefits-2" id="markdown-toc-typical-benefits-2">Typical Benefits</a></li>
                  <li><a href="#notes--tradeoffs-2" id="markdown-toc-notes--tradeoffs-2">Notes &amp; Trade‑offs</a></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#comparative-analysis" id="markdown-toc-comparative-analysis">Comparative Analysis</a></li>
      <li><a href="#compute-vs-memory-bottlenecks" id="markdown-toc-compute-vs-memory-bottlenecks">Compute vs. Memory Bottlenecks</a></li>
      <li><a href="#modern-quantization-techniques" id="markdown-toc-modern-quantization-techniques">Modern Quantization Techniques</a>        <ul>
          <li><a href="#gptq-quantization-with-second-order-error-compensation" id="markdown-toc-gptq-quantization-with-second-order-error-compensation">GPTQ: Quantization with Second-Order Error Compensation</a>            <ul>
              <li><a href="#process" id="markdown-toc-process">Process</a></li>
              <li><a href="#pros" id="markdown-toc-pros">Pros</a></li>
              <li><a href="#cons" id="markdown-toc-cons">Cons</a></li>
            </ul>
          </li>
          <li><a href="#smoothquant" id="markdown-toc-smoothquant">SmoothQuant</a>            <ul>
              <li><a href="#process-1" id="markdown-toc-process-1">Process</a></li>
              <li><a href="#pros-1" id="markdown-toc-pros-1">Pros</a></li>
              <li><a href="#cons-1" id="markdown-toc-cons-1">Cons</a></li>
            </ul>
          </li>
          <li><a href="#activation-aware-weight-quantization-awq" id="markdown-toc-activation-aware-weight-quantization-awq">Activation-Aware Weight Quantization (AWQ)</a>            <ul>
              <li><a href="#process-2" id="markdown-toc-process-2">Process</a></li>
              <li><a href="#pros-2" id="markdown-toc-pros-2">Pros</a></li>
              <li><a href="#cons-2" id="markdown-toc-cons-2">Cons</a></li>
            </ul>
          </li>
          <li><a href="#gguf-quantization-legacy-kquants-iquants" id="markdown-toc-gguf-quantization-legacy-kquants-iquants">GGUF Quantization (Legacy, K‑Quants, I‑Quants)</a>            <ul>
              <li><a href="#quantization-types" id="markdown-toc-quantization-types">Quantization Types</a></li>
              <li><a href="#gguf-file-layout-and-execution" id="markdown-toc-gguf-file-layout-and-execution">GGUF File Layout and Execution</a></li>
              <li><a href="#importance-matrix-imatrix" id="markdown-toc-importance-matrix-imatrix">Importance Matrix (Imatrix)</a></li>
              <li><a href="#pros-3" id="markdown-toc-pros-3">Pros</a></li>
              <li><a href="#cons-3" id="markdown-toc-cons-3">Cons</a></li>
            </ul>
          </li>
          <li><a href="#aweq-activationweight-equalization" id="markdown-toc-aweq-activationweight-equalization">AWEQ: Activation‑Weight Equalization</a>            <ul>
              <li><a href="#motivation" id="markdown-toc-motivation">Motivation</a></li>
              <li><a href="#process-implementation-overview" id="markdown-toc-process-implementation-overview">Process (Implementation Overview)</a></li>
              <li><a href="#pros-4" id="markdown-toc-pros-4">Pros</a></li>
              <li><a href="#cons-4" id="markdown-toc-cons-4">Cons</a></li>
            </ul>
          </li>
          <li><a href="#exl2-quantization" id="markdown-toc-exl2-quantization">EXL2 Quantization</a>            <ul>
              <li><a href="#process-3" id="markdown-toc-process-3">Process</a></li>
              <li><a href="#pros-5" id="markdown-toc-pros-5">Pros</a></li>
              <li><a href="#cons-5" id="markdown-toc-cons-5">Cons</a></li>
            </ul>
          </li>
          <li><a href="#spinquant" id="markdown-toc-spinquant">SpinQuant</a>            <ul>
              <li><a href="#process-4" id="markdown-toc-process-4">Process</a></li>
              <li><a href="#pros-6" id="markdown-toc-pros-6">Pros</a></li>
              <li><a href="#cons-6" id="markdown-toc-cons-6">Cons</a></li>
            </ul>
          </li>
          <li><a href="#fptquant" id="markdown-toc-fptquant">FPTQuant</a>            <ul>
              <li><a href="#process-5" id="markdown-toc-process-5">Process</a></li>
              <li><a href="#pros-7" id="markdown-toc-pros-7">Pros</a></li>
              <li><a href="#cons-7" id="markdown-toc-cons-7">Cons</a></li>
            </ul>
          </li>
          <li><a href="#palettization-weight-clustering" id="markdown-toc-palettization-weight-clustering">Palettization (Weight Clustering)</a>            <ul>
              <li><a href="#process-6" id="markdown-toc-process-6">Process</a></li>
              <li><a href="#integration-in-workflows" id="markdown-toc-integration-in-workflows">Integration in Workflows</a></li>
              <li><a href="#when-to-use-palettization" id="markdown-toc-when-to-use-palettization">When to Use Palettization</a></li>
              <li><a href="#pros-8" id="markdown-toc-pros-8">Pros</a></li>
              <li><a href="#cons-8" id="markdown-toc-cons-8">Cons</a></li>
            </ul>
          </li>
          <li><a href="#what-to-use-when" id="markdown-toc-what-to-use-when">What to Use When?</a></li>
          <li><a href="#comparative-analysis-1" id="markdown-toc-comparative-analysis-1">Comparative Analysis</a></li>
        </ul>
      </li>
      <li><a href="#multimodal-quantization" id="markdown-toc-multimodal-quantization">Multimodal Quantization</a>        <ul>
          <li><a href="#why-vlm-quantization-is-more-complex" id="markdown-toc-why-vlm-quantization-is-more-complex">Why VLM Quantization is More Complex</a></li>
          <li><a href="#quantizing-the-visual-backbone" id="markdown-toc-quantizing-the-visual-backbone">Quantizing the Visual Backbone</a></li>
          <li><a href="#quantizing-the-language-backbone" id="markdown-toc-quantizing-the-language-backbone">Quantizing the Language Backbone</a></li>
          <li><a href="#cross-modal-projection-and-fusion-layer-quantization" id="markdown-toc-cross-modal-projection-and-fusion-layer-quantization">Cross-Modal Projection and Fusion Layer Quantization</a></li>
          <li><a href="#quantization-aware-training-qat-in-vlms" id="markdown-toc-quantization-aware-training-qat-in-vlms">Quantization-Aware Training (QAT) in VLMs</a></li>
          <li><a href="#calibration-and-evaluation-in-vlms" id="markdown-toc-calibration-and-evaluation-in-vlms">Calibration and Evaluation in VLMs</a></li>
          <li><a href="#hybrid-and-mixed-precision-quantization" id="markdown-toc-hybrid-and-mixed-precision-quantization">Hybrid and Mixed-Precision Quantization</a></li>
          <li><a href="#tooling-support" id="markdown-toc-tooling-support">Tooling Support</a></li>
          <li><a href="#comparative-analysis-of-llms-vs-vlm-quantization" id="markdown-toc-comparative-analysis-of-llms-vs-vlm-quantization">Comparative Analysis of LLMs vs. VLM Quantization</a></li>
        </ul>
      </li>
      <li><a href="#device-and-operator-support-across-frameworks" id="markdown-toc-device-and-operator-support-across-frameworks">Device and Operator Support Across Frameworks</a>        <ul>
          <li><a href="#pytorch" id="markdown-toc-pytorch">PyTorch</a>            <ul>
              <li><a href="#integration-in-torchvision" id="markdown-toc-integration-in-torchvision">Integration in <code class="language-plaintext Highlighter-rouge">torchvision</code></a></li>
              <li><a href="#resources" id="markdown-toc-resources">Resources</a></li>
            </ul>
          </li>
          <li><a href="#tensorflow" id="markdown-toc-tensorflow">TensorFlow</a>            <ul>
              <li><a href="#integration-in-tfkerasapplications" id="markdown-toc-integration-in-tfkerasapplications">Integration in <code class="language-plaintext Highlighter-rouge">tf.keras.applications</code></a></li>
              <li><a href="#resources-1" id="markdown-toc-resources-1">Resources</a></li>
            </ul>
          </li>
          <li><a href="#coreml" id="markdown-toc-coreml">CoreML</a>            <ul>
              <li><a href="#integration-with-pytorch-and-tensorflow-models" id="markdown-toc-integration-with-pytorch-and-tensorflow-models">Integration with PyTorch and TensorFlow Models</a></li>
              <li><a href="#resources-2" id="markdown-toc-resources-2">Resources</a></li>
            </ul>
          </li>
          <li><a href="#jax" id="markdown-toc-jax">JAX</a>            <ul>
              <li><a href="#integration-examples" id="markdown-toc-integration-examples">Integration Examples</a></li>
              <li><a href="#resources-3" id="markdown-toc-resources-3">Resources</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#choosing-the-right-quantization-approach" id="markdown-toc-choosing-the-right-quantization-approach">Choosing the Right Quantization Approach</a></li>
      <li><a href="#performance-results" id="markdown-toc-performance-results">Performance Results</a></li>
      <li><a href="#accuracy-results" id="markdown-toc-accuracy-results">Accuracy Results</a>        <ul>
          <li><a href="#computer-vision-model-accuracy" id="markdown-toc-computer-vision-model-accuracy">Computer Vision Model Accuracy</a></li>
          <li><a href="#speech-and-nlp-model-accuracy" id="markdown-toc-speech-and-nlp-model-accuracy">Speech and NLP Model Accuracy</a></li>
        </ul>
      </li>
      <li><a href="#popular-quantization-libraries" id="markdown-toc-popular-quantization-libraries">Popular Quantization Libraries</a>        <ul>
          <li><a href="#bitsandbytes" id="markdown-toc-bitsandbytes">BitsAndBytes</a></li>
          <li><a href="#hugging-face-optimum" id="markdown-toc-hugging-face-optimum">Hugging Face Optimum</a></li>
          <li><a href="#onnx-runtime-quantization" id="markdown-toc-onnx-runtime-quantization">ONNX Runtime Quantization</a></li>
          <li><a href="#nvidia-tensorrt--tensorrt-llm" id="markdown-toc-nvidia-tensorrt--tensorrt-llm">NVIDIA TensorRT / TensorRT-LLM</a></li>
          <li><a href="#intel-neural-compressor-inc" id="markdown-toc-intel-neural-compressor-inc">Intel Neural Compressor (INC)</a></li>
          <li><a href="#openvino-tooling" id="markdown-toc-openvino-tooling">OpenVINO Tooling</a></li>
          <li><a href="#tensorflow-lite" id="markdown-toc-tensorflow-lite">TensorFlow Lite</a></li>
          <li><a href="#apple-core-ml-tools" id="markdown-toc-apple-core-ml-tools">Apple Core ML Tools</a></li>
          <li><a href="#llm-specific-quantizers-gptq-awq-and-their-python-toolkits" id="markdown-toc-llm-specific-quantizers-gptq-awq-and-their-python-toolkits">LLM-Specific Quantizers (GPTQ, AWQ) and Their Python Toolkits</a></li>
          <li><a href="#rules-of-thumb-for-choosing-a-library" id="markdown-toc-rules-of-thumb-for-choosing-a-library">Rules of Thumb for Choosing a Library</a></li>
          <li><a href="#implementation-notes" id="markdown-toc-implementation-notes">Implementation Notes</a></li>
        </ul>
      </li>
      <li><a href="#how-far-can-quantization-be-pushed" id="markdown-toc-how-far-can-quantization-be-pushed">How Far Can Quantization be Pushed?</a></li>
      <li><a href="#further-reading-1" id="markdown-toc-further-reading-1">Further Reading</a></li>
    </ul>
  </li>
  <li><a href="#knowledge-distillation" id="markdown-toc-knowledge-distillation">Knowledge Distillation</a>    <ul>
      <li><a href="#mechanism" id="markdown-toc-mechanism">Mechanism</a></li>
      <li><a href="#types-of-knowledge-distillation" id="markdown-toc-types-of-knowledge-distillation">Types of Knowledge Distillation</a>        <ul>
          <li><a href="#response-based-distillation" id="markdown-toc-response-based-distillation">Response-Based Distillation</a></li>
          <li><a href="#feature-based-distillation" id="markdown-toc-feature-based-distillation">Feature-Based Distillation</a></li>
          <li><a href="#relation-based-distillation" id="markdown-toc-relation-based-distillation">Relation-Based Distillation</a></li>
        </ul>
      </li>
      <li><a href="#distillation-modes" id="markdown-toc-distillation-modes">Distillation Modes</a>        <ul>
          <li><a href="#offline-distillation" id="markdown-toc-offline-distillation">Offline Distillation</a></li>
          <li><a href="#online-distillation" id="markdown-toc-online-distillation">Online Distillation</a></li>
          <li><a href="#self-distillation" id="markdown-toc-self-distillation">Self-Distillation</a></li>
        </ul>
      </li>
      <li><a href="#why-use-knowledge-distillation-instead-of-training-small-models-from-scratch" id="markdown-toc-why-use-knowledge-distillation-instead-of-training-small-models-from-scratch">Why Use Knowledge Distillation Instead of Training Small Models from Scratch?</a></li>
      <li><a href="#why-knowledge-distillation-works" id="markdown-toc-why-knowledge-distillation-works">Why Knowledge Distillation Works</a></li>
      <li><a href="#distillation-in-practice" id="markdown-toc-distillation-in-practice">Distillation in Practice</a></li>
      <li><a href="#reverse-distillation" id="markdown-toc-reverse-distillation">Reverse Distillation</a></li>
      <li><a href="#weak-supervision-via-distillation" id="markdown-toc-weak-supervision-via-distillation">Weak Supervision Via Distillation</a></li>
      <li><a href="#compute-vs-memory-bottlenecks-1" id="markdown-toc-compute-vs-memory-bottlenecks-1">Compute vs. Memory Bottlenecks</a></li>
      <li><a href="#limitations-and-challenges" id="markdown-toc-limitations-and-challenges">Limitations and Challenges</a></li>
    </ul>
  </li>
  <li><a href="#model-pruning" id="markdown-toc-model-pruning">Model Pruning</a>    <ul>
      <li><a href="#formal-definition" id="markdown-toc-formal-definition">Formal Definition</a></li>
      <li><a href="#rationale-and-theoretical-motivation" id="markdown-toc-rationale-and-theoretical-motivation">Rationale and Theoretical Motivation</a></li>
      <li><a href="#types-of-pruning" id="markdown-toc-types-of-pruning">Types of Pruning</a>        <ul>
          <li><a href="#unstructured-pruning" id="markdown-toc-unstructured-pruning">Unstructured Pruning</a></li>
          <li><a href="#structured-pruning" id="markdown-toc-structured-pruning">Structured Pruning</a></li>
        </ul>
      </li>
      <li><a href="#pruning-workflow" id="markdown-toc-pruning-workflow">Pruning Workflow</a>        <ul>
          <li><a href="#step-1-train-the-full-model" id="markdown-toc-step-1-train-the-full-model">Step 1: Train the Full Model</a></li>
          <li><a href="#step-2-apply-pruning-mask" id="markdown-toc-step-2-apply-pruning-mask">Step 2: Apply Pruning Mask</a></li>
          <li><a href="#step-3-fine-tune-the-pruned-model" id="markdown-toc-step-3-fine-tune-the-pruned-model">Step 3: Fine-Tune the Pruned Model</a></li>
        </ul>
      </li>
      <li><a href="#compute-vs-memory-bottlenecks-2" id="markdown-toc-compute-vs-memory-bottlenecks-2">Compute vs. Memory Bottlenecks</a></li>
      <li><a href="#practical-considerations" id="markdown-toc-practical-considerations">Practical Considerations</a>        <ul>
          <li><a href="#target-sparsity" id="markdown-toc-target-sparsity">Target Sparsity</a></li>
          <li><a href="#compatibility" id="markdown-toc-compatibility">Compatibility</a></li>
          <li><a href="#deployment-readiness" id="markdown-toc-deployment-readiness">Deployment Readiness</a></li>
        </ul>
      </li>
      <li><a href="#comparative-analysis-2" id="markdown-toc-comparative-analysis-2">Comparative Analysis</a></li>
      <li><a href="#implementing-pruning-in-pytorch-and-tensorflow" id="markdown-toc-implementing-pruning-in-pytorch-and-tensorflow">Implementing Pruning in PyTorch and TensorFlow</a>        <ul>
          <li><a href="#pytorch-pruning" id="markdown-toc-pytorch-pruning">PyTorch Pruning</a></li>
          <li><a href="#tensorflow-pruning" id="markdown-toc-tensorflow-pruning">TensorFlow Pruning</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#mixed-precision-training" id="markdown-toc-mixed-precision-training">Mixed Precision Training</a>    <ul>
      <li><a href="#overview-1" id="markdown-toc-overview-1">Overview</a></li>
      <li><a href="#how-mixed-precision-training-works" id="markdown-toc-how-mixed-precision-training-works">How Mixed Precision Training Works</a>        <ul>
          <li><a href="#how-pytorch-automatic-mixed-precision-works" id="markdown-toc-how-pytorch-automatic-mixed-precision-works">How PyTorch Automatic Mixed Precision Works</a>            <ul>
              <li><a href="#overview-of-amp-components" id="markdown-toc-overview-of-amp-components">Overview of AMP Components</a></li>
              <li><a href="#practical-implementation-in-a-training-loop" id="markdown-toc-practical-implementation-in-a-training-loop">Practical Implementation in a Training Loop</a></li>
              <li><a href="#loss-and-gradient-scaling-with-gradscaler" id="markdown-toc-loss-and-gradient-scaling-with-gradscaler">Loss and Gradient Scaling with <code class="language-plaintext Highlighter-rouge">GradScaler</code></a>                <ul>
                  <li><a href="#dynamic-scaling-with-exponential-backoff" id="markdown-toc-dynamic-scaling-with-exponential-backoff">Dynamic Scaling with Exponential Backoff</a></li>
                </ul>
              </li>
              <li><a href="#automatic-precision-casting-with-the-autocast-context-manager" id="markdown-toc-automatic-precision-casting-with-the-autocast-context-manager">Automatic Precision Casting with the <code class="language-plaintext Highlighter-rouge">autocast</code> Context Manager</a></li>
              <li><a href="#using-amp-with-multiple-gpus" id="markdown-toc-using-amp-with-multiple-gpus">Using AMP with Multiple GPUs</a></li>
              <li><a href="#memory-considerations" id="markdown-toc-memory-considerations">Memory Considerations</a></li>
              <li><a href="#further-reading-2" id="markdown-toc-further-reading-2">Further Reading</a></li>
            </ul>
          </li>
          <li><a href="#how-tensorflow-automatic-mixed-precision-works" id="markdown-toc-how-tensorflow-automatic-mixed-precision-works">How TensorFlow Automatic Mixed Precision Works</a>            <ul>
              <li><a href="#conceptual-overview" id="markdown-toc-conceptual-overview">Conceptual Overview</a></li>
              <li><a href="#practical-implementation-in-a-training-pipeline" id="markdown-toc-practical-implementation-in-a-training-pipeline">Practical Implementation in a Training Pipeline</a></li>
              <li><a href="#performance-benchmarks" id="markdown-toc-performance-benchmarks">Performance Benchmarks</a></li>
              <li><a href="#key-takeaways" id="markdown-toc-key-takeaways">Key Takeaways</a></li>
              <li><a href="#recommendations" id="markdown-toc-recommendations">Recommendations</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#low-rank-decomposition--adaptation" id="markdown-toc-low-rank-decomposition--adaptation">Low-Rank Decomposition &amp; Adaptation</a>    <ul>
      <li><a href="#overview-2" id="markdown-toc-overview-2">Overview</a></li>
      <li><a href="#formal-definition-1" id="markdown-toc-formal-definition-1">Formal Definition</a></li>
      <li><a href="#concept" id="markdown-toc-concept">Concept</a></li>
      <li><a href="#low-rank-correction-for-quantization" id="markdown-toc-low-rank-correction-for-quantization">Low-Rank Correction for Quantization</a></li>
      <li><a href="#quantized-low-rank-adaptation-techniques" id="markdown-toc-quantized-low-rank-adaptation-techniques">Quantized Low-Rank Adaptation Techniques</a>        <ul>
          <li><a href="#lqlora-quantized--low-rank-adaptation" id="markdown-toc-lqlora-quantized--low-rank-adaptation">LQ‑LoRA: Quantized + Low-Rank Adaptation</a>            <ul>
              <li><a href="#overview-3" id="markdown-toc-overview-3">Overview</a></li>
              <li><a href="#key-properties" id="markdown-toc-key-properties">Key Properties</a></li>
              <li><a href="#pros-9" id="markdown-toc-pros-9">Pros</a></li>
              <li><a href="#cons-9" id="markdown-toc-cons-9">Cons</a></li>
            </ul>
          </li>
          <li><a href="#qlora-quantized-lora-with-4-bit-base-model" id="markdown-toc-qlora-quantized-lora-with-4-bit-base-model">QLoRA: Quantized LoRA with 4-bit Base Model</a>            <ul>
              <li><a href="#overview-4" id="markdown-toc-overview-4">Overview</a></li>
              <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
              <li><a href="#pros-10" id="markdown-toc-pros-10">Pros</a></li>
              <li><a href="#cons-10" id="markdown-toc-cons-10">Cons</a></li>
            </ul>
          </li>
          <li><a href="#qa-lora-quantization-aware-lora" id="markdown-toc-qa-lora-quantization-aware-lora">QA-LoRA: Quantization-Aware LoRA</a>            <ul>
              <li><a href="#overview-5" id="markdown-toc-overview-5">Overview</a></li>
              <li><a href="#architecture-1" id="markdown-toc-architecture-1">Architecture</a></li>
              <li><a href="#key-features" id="markdown-toc-key-features">Key Features</a></li>
              <li><a href="#pros-11" id="markdown-toc-pros-11">Pros</a></li>
              <li><a href="#cons-11" id="markdown-toc-cons-11">Cons</a></li>
            </ul>
          </li>
          <li><a href="#comparative-analysis-3" id="markdown-toc-comparative-analysis-3">Comparative Analysis</a></li>
        </ul>
      </li>
      <li><a href="#pros--cons" id="markdown-toc-pros--cons">Pros &amp; Cons</a></li>
      <li><a href="#comparison--use-cases" id="markdown-toc-comparison--use-cases">Comparison &amp; Use Cases</a></li>
      <li><a href="#key-takeaways-1" id="markdown-toc-key-takeaways-1">Key Takeaways</a></li>
    </ul>
  </li>
  <li><a href="#lightweight-model-design" id="markdown-toc-lightweight-model-design">Lightweight Model Design</a>    <ul>
      <li><a href="#principles-of-lightweight-design" id="markdown-toc-principles-of-lightweight-design">Principles of Lightweight Design</a></li>
      <li><a href="#design-methodologies" id="markdown-toc-design-methodologies">Design Methodologies</a></li>
      <li><a href="#representative-architectures" id="markdown-toc-representative-architectures">Representative Architectures</a></li>
      <li><a href="#when-to-use-lightweight-models" id="markdown-toc-when-to-use-lightweight-models">When to Use Lightweight Models</a></li>
    </ul>
  </li>
  <li><a href="#what-to-use-when-1" id="markdown-toc-what-to-use-when-1">What to Use When?</a></li>
  <li><a href="#combining-model-compression-techniques" id="markdown-toc-combining-model-compression-techniques">Combining Model Compression Techniques</a></li>
  <li><a href="#further-reading-3" id="markdown-toc-further-reading-3">Further Reading</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="background">Background</h2>

<ul>
  <li>
    <p>Modern generative models often contain between 100 billion to 1 trillion parameters. Since each parameter (as a 32-bit float) consumes 4 bytes, the memory footprint can scale from 400 GB to over 4 TB. This makes them prohibitively large for deployment on edge devices, where memory and compute resources are highly constrained. Furthermore, deploying machine learning models directly on edge devices such as smartphones, tablets, or embedded systems offers key advantages in privacy, latency, and user experience. On-device processing ensures that data remains local to the device, significantly reducing the risk of exposure from data transmission or centralized storage breaches. This is particularly critical for applications in computer vision and conversational AI, where interactions often involve personal or sensitive information.</p>
  </li>
  <li>
    <p>However, the computational and memory demands of modern machine learning models—especially large language models—pose a major barrier to efficient on-device deployment. These models are typically too large and resource-intensive to run in real-time on devices with limited hardware capabilities. As models continue to scale in size and complexity, challenges such as increased latency, power consumption, and hardware constraints become even more pronounced.</p>
  </li>
  <li>
    <p>To address these limitations, a wide array of model compression and optimization techniques has been developed. These include model quantization (static, dynamic, and quantization-aware training), structured and unstructured pruning, knowledge distillation (response-based, feature-based, relation-based), low-rank decomposition, activation-aware quantization, operator fusion, and mixed precision training. Advanced post-training quantization methods such as AWQ, SmoothQuant, SpinQuant, AWEQ, and FPTQuant further push the boundaries of efficient model deployment.</p>
  </li>
  <li>
    <p>Collectively, these techniques aim to reduce model size, lower computational complexity, and accelerate inference—without significantly compromising accuracy. By enabling smaller, faster, and more power-efficient models, these strategies make it increasingly feasible to run advanced AI applications directly on user devices, supporting both better privacy and smoother user experiences.</p>
  </li>
</ul>

<h2 id="overview">Overview</h2>

<ul>
  <li>
    <p>To enable on-device AI, a wide range of model compression techniques have been developed. Below, we visually and conceptually summarize the five core strategies widely used in industry and research.</p>
  </li>
  <li><strong>Model Quantization</strong>
    <ul>
      <li>
        <p>Quantization reduces the precision of weights and activations, typically from 32-bit floats to 8-bit integers, yielding up to a 4× reduction in model size and significant speed-ups using optimized kernels.</p>

        <ul>
          <li><em>Post-training quantization</em> applies precision reduction directly on a trained model and may include heuristic corrections (e.g., bias correction, per-channel scaling).</li>
          <li><em>Quantization-aware training (QAT)</em> simulates quantization noise during training, allowing the model to adapt and maintain higher accuracy under reduced precision.</li>
          <li>Advanced quantization strategies like AWQ, SmoothQuant, and AWEQ refine post-training quantization by adjusting scaling factors or reweighting attention layers.</li>
          <li>A detailed discourse on this topic is available in the <a href="#quantization">Quantization</a> section.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Knowledge Distillation</strong>
    <ul>
      <li>
        <p>Knowledge distillation trains a compact <em>student model</em> to mimic a larger <em>teacher model</em>.</p>

        <ul>
          <li><em>Response-based distillation</em> focuses on matching the output logits or probabilities.</li>
          <li><em>Feature-based distillation</em> aligns intermediate representations between teacher and student.</li>
          <li><em>Relation-based distillation</em> preserves inter-feature dependencies (e.g., attention maps).</li>
          <li>Distillation is often combined with other techniques (e.g., quantization or pruning) to maximize efficiency.</li>
          <li>A detailed discourse on this topic is available in the <a href="#knowledge-distillation">Knowledge Distillation</a> section.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Model Pruning</strong>
    <ul>
      <li>
        <p>Pruning reduces model size by removing weights that have minimal impact on overall performance.</p>

        <ul>
          <li><em>Unstructured pruning</em> eliminates individual weights based on their magnitude (e.g., L1/L2 norm) or gradient impact (e.g., first or second derivative of the loss).</li>
          <li><em>Structured pruning</em> goes further by removing entire neurons, filters, or layers, which can directly lead to faster inference on hardware accelerators.</li>
          <li>In practice, pruning is often iterative: prune, retrain, evaluate, and repeat to recover performance loss.</li>
          <li>A detailed discourse on this topic is available in the <a href="#model-pruning">Model Pruning</a> section.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Low-Rank Decomposition</strong>
    <ul>
      <li>
        <p>Many neural networks, especially transformer-based architectures, contain large weight matrices that can be approximated as a product of smaller matrices.</p>

        <ul>
          <li>For example, an N×N matrix can often be replaced by two N×k matrices (with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;&amp;lt;&amp;lt;&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.02em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">&lt;<span style="font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;">&lt;</span></span><span class="mi" id="MathJax-Span-5" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>&lt;&lt;</mo><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">k << N</script>), reducing space complexity from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-7"><span class="mi" id="MathJax-Span-8" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-9" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-10"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-11" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.784em;"><span class="mn" id="MathJax-Span-12" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-13" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">O(N^2)</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-14" style="width: 3.961em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1003.28em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-15"><span class="mi" id="MathJax-Span-16" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-17" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-18" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-19" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mi" id="MathJax-Span-20" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>⋅</mo><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">O(N \cdot k</script>).</li>
          <li>Methods like SVD (singular value decomposition) or CP decomposition are used here.</li>
          <li>In practice, fine-tuning the decomposed model is essential to restore original performance.</li>
          <li>A detailed discourse on this topic is available in the <a href="#low-rank-decomposition--adaptation">Low-Rank Decomposition</a> section.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Lightweight Model Design</strong>
    <ul>
      <li>
        <p>Rather than compressing an existing model, lightweight design focuses on creating efficient architectures from the ground up.</p>

        <ul>
          <li>Examples include MobileBERT, DistilBERT, TinyBERT, and ConvNeXt-T, which use smaller embedding sizes, depth-wise separable convolutions, fewer transformer blocks, or other architectural efficiencies.</li>
          <li>Empirical design choices are often guided by NAS (Neural Architecture Search) or latency-aware loss functions.</li>
          <li>A detailed discourse on this topic is available in the <a href="#lightweight-model-design">Lightweight Model Design</a> section.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The illustration below <a href="http://theaiedge.io/">(source)</a> summarizes how these different compression methods contribute to reducing model size and enabling efficient deployment across platforms, including on-device scenarios.</li>
</ul>

<p><img src="/primers/ai/assets/model-compression/comp.jpeg" alt="Model Compression Techniques"></p>

<h2 id="quantization">Quantization</h2>

<h3 id="background-precision">Background: Precision</h3>

<ul>
  <li>
    <p>Before diving into quantization, it’s essential to understand precision—specifically, how computers represent decimal numbers like <code class="language-plaintext highlighter-rouge">1.0151</code> or <code class="language-plaintext highlighter-rouge">566132.8</code>. Since we can conceive of infinitely precise values (like π), but only have limited space in memory, there’s a fundamental trade-off between <strong>precision</strong> (how many significant digits can be stored) and <strong>size</strong> (how many bits are used to represent the number).</p>
  </li>
  <li>
    <p>In computer engineering, these values are stored as <strong>floating point numbers</strong>, governed by the <a href="#ieee-754-floating-point-standard">IEEE 754 floating point standard</a>. This specification defines how bits are allocated to represent the <strong>sign</strong>, <strong>exponent</strong>, and <strong>mantissa</strong> (also called the <strong>significand</strong>, which holds the meaningful digits).</p>
  </li>
  <li>
    <p>Floating point formats vary by their bit-width, and each level of precision has a different rounding error margin:</p>

    <ul>
      <li><strong>Double-precision (<code class="language-plaintext highlighter-rouge">fp64</code>)</strong> – 64 bits, max rounding error of approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;52&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-21" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-22"><span class="msubsup" id="MathJax-Span-23"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-24" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-25"><span class="mrow" id="MathJax-Span-26"><span class="mo" id="MathJax-Span-27" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-28" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">52</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>52</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-4">2^{-52}</script>.</li>
      <li><strong>Single-precision (<code class="language-plaintext highlighter-rouge">float32</code>)</strong> – 32 bits, max rounding error of approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;23&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-29" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-30"><span class="msubsup" id="MathJax-Span-31"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-32" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-33"><span class="mrow" id="MathJax-Span-34"><span class="mo" id="MathJax-Span-35" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-36" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">23</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>23</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-5">2^{-23}</script>.</li>
      <li><strong>Half-precision (<code class="language-plaintext highlighter-rouge">float16</code>)</strong> – 16 bits, max rounding error of approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-37" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-38"><span class="msubsup" id="MathJax-Span-39"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-40" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-41"><span class="mrow" id="MathJax-Span-42"><span class="mo" id="MathJax-Span-43" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-44" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">10</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>10</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-6">2^{-10}</script>.</li>
    </ul>
  </li>
  <li>
    <p>For a deeper exploration, check out the <a href="https://www.youtube.com/watch?v=zguLmgYWhM0">PyCon 2019 talk: “Floats are Friends: making the most of IEEE754.00000000000000002”</a>.</p>
  </li>
  <li>
    <p>In practice:</p>

    <ul>
      <li><strong>Python</strong> defaults to using <code class="language-plaintext highlighter-rouge">fp64</code> for its <code class="language-plaintext highlighter-rouge">float</code> type.</li>
      <li><strong>PyTorch</strong>, which is optimized for performance and memory efficiency, defaults to <code class="language-plaintext highlighter-rouge">float32</code>.</li>
    </ul>
  </li>
  <li>
    <p>Understanding these formats is crucial when moving on to concepts like <strong>mixed precision training</strong>, where models leverage different floating point types to balance performance and accuracy.</p>
  </li>
</ul>

<h4 id="ieee-754-floating-point-standard">IEEE 754 Floating Point Standard</h4>

<ul>
  <li>
    <p>The IEEE 754 standard defines the binary representation of floating point numbers used in nearly all modern hardware and programming environments. A floating-point number is composed of three parts:</p>

    <ul>
      <li><strong>Sign bit (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-45" style="width: 0.726em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.571em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1000.57em, 2.327em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-46"><span class="mi" id="MathJax-Span-47" style="font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi></math></span></span><script type="math/tex" id="MathJax-Element-7">S</script>)</strong>: 1 bit indicating positive or negative.</li>
      <li><strong>Exponent (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-48" style="width: 0.777em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-49"><span class="mi" id="MathJax-Span-50" style="font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi></math></span></span><script type="math/tex" id="MathJax-Element-8">E</script>)</strong>: Encodes the range (scale) of the number.</li>
      <li><strong>Mantissa or significand (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-51" style="width: 1.139em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.932em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1000.93em, 2.327em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-52"><span class="mi" id="MathJax-Span-53" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-9">M</script>)</strong>: Encodes the precision.</li>
    </ul>
  </li>
  <li>
    <p>The general representation for a binary floating-point number is:</p>
  </li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtext&gt;value&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1.&lt;/mn&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mtext&gt;bias&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-54" style="width: 14.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1012.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-55"><span class="mtext" id="MathJax-Span-56" style="font-family: STIXGeneral-Regular;">value</span><span class="mo" id="MathJax-Span-57" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-58" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mo" id="MathJax-Span-59" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-60" style="font-family: STIXGeneral-Regular;">1</span><span class="msubsup" id="MathJax-Span-61"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-62" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-63" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-64" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-65" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1.</span><span class="mi" id="MathJax-Span-66" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-67" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-68" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-69" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-70"><span class="mrow" id="MathJax-Span-71"><span class="mi" id="MathJax-Span-72" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-73" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mtext" id="MathJax-Span-74" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">bias</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>value</mtext><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><mi>M</mi><mo>×</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>E</mi><mo>−</mo><mtext>bias</mtext></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-10">\text{value} = (-1)^S \times 1.M \times 2^{E - \text{bias}}</script>

<ul>
  <li>
    <p>Each format—half, single, and double—allocates a different number of bits to these components, balancing precision and range against memory usage and compute requirements.</p>
  </li>
  <li>
    <p>The following figure <a href="https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/">(source)</a> shows the IEEE 754 standard formats for floating-point numbers, illustrating the bitwise layout of the signed bit, exponent, and significand across double (64-bit), single (32-bit), and half (16-bit) precision representations.</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/ieee_formats.webp" alt=""></p>

<h4 id="half-precision-float16">Half-Precision (<code class="language-plaintext Highlighter-rouge">float16</code>)</h4>

<ul>
  <li><strong>Bit layout</strong>: 1 sign bit, 5 exponent bits, 10 mantissa/significand bits.</li>
  <li><strong>Total bits</strong>: 16</li>
  <li><strong>Exponent bias</strong>: 15</li>
  <li><strong>Dynamic range</strong>: Approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-75" style="width: 4.273em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1003.54em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-76"><span class="mn" id="MathJax-Span-77" style="font-family: STIXGeneral-Regular;">6</span><span class="mo" id="MathJax-Span-78" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-79" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-80" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-81"><span class="mrow" id="MathJax-Span-82"><span class="mo" id="MathJax-Span-83" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-84" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">5</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-11">6 \times 10^{-5}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;6.5&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-85" style="width: 4.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1003.75em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-86"><span class="mn" id="MathJax-Span-87" style="font-family: STIXGeneral-Regular;">6.5</span><span class="mo" id="MathJax-Span-88" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-89" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-90" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="mn" id="MathJax-Span-91" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6.5</mn><mo>×</mo><msup><mn>10</mn><mn>4</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-12">6.5 \times 10^4</script></li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mtext&gt;value&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;float16&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1.&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;15&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-92" style="width: 17.294em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.378em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1014.38em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-93"><span class="msubsup" id="MathJax-Span-94"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mtext" id="MathJax-Span-95" style="font-family: STIXGeneral-Regular;">value</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 2.19em;"><span class="texatom" id="MathJax-Span-96"><span class="mrow" id="MathJax-Span-97"><span class="mtext" id="MathJax-Span-98" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">float16</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-99" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-100" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mo" id="MathJax-Span-101" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-102" style="font-family: STIXGeneral-Regular;">1</span><span class="msubsup" id="MathJax-Span-103"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-104" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-105" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-106" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-107" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1.</span><span class="msubsup" id="MathJax-Span-108"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-109" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-110"><span class="mrow" id="MathJax-Span-111"><span class="mn" id="MathJax-Span-112" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">10</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-113" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-114" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-115" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-116"><span class="mrow" id="MathJax-Span-117"><span class="mi" id="MathJax-Span-118" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-119" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-120" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">15</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mtext>value</mtext><mrow class="MJX-TeXAtom-ORD"><mtext>float16</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class="MJX-TeXAtom-ORD"><mn>10</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>E</mi><mo>−</mo><mn>15</mn></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-13">\text{value}_{\text{float16}} = (-1)^S \times 1.M_{10} \times 2^{E - 15}</script>

<ul>
  <li>
    <p>Half-precision is mostly used during inference, especially in low-power or memory-constrained environments such as mobile devices or embedded hardware. It offers limited range and precision, and is generally <em>not</em> suitable for training deep networks unless special care is taken (e.g., using loss scaling or <code class="language-plaintext highlighter-rouge">float32</code> accumulations).</p>
  </li>
  <li>
    <p><strong>GPU Considerations</strong>:</p>

    <ul>
      <li>Many GPUs, like NVIDIA’s Volta, Turing, Ampere, Hopper, Blackwell, include specialized hardware units called <strong>Tensor Cores</strong> optimized for <code class="language-plaintext highlighter-rouge">float16</code> operations.</li>
      <li><code class="language-plaintext highlighter-rouge">float16</code> can be processed at higher throughput than <code class="language-plaintext highlighter-rouge">float32</code>, enabling significant speedups for matrix multiplications during inference.</li>
      <li>Often paired with Mixed-Precision Training (MPT), where activations and weights are stored in <code class="language-plaintext highlighter-rouge">float16</code>, but gradients are accumulated in <code class="language-plaintext highlighter-rouge">float32</code>.</li>
    </ul>
  </li>
</ul>

<h4 id="single-precision-float32">Single-Precision (<code class="language-plaintext Highlighter-rouge">float32</code>)</h4>

<ul>
  <li><strong>Bit layout</strong>: 1 sign bit, 8 exponent bits, 23 mantissa bits.</li>
  <li><strong>Total bits</strong>: 32</li>
  <li><strong>Exponent bias</strong>: 127</li>
  <li><strong>Dynamic range</strong>: Approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.4&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;45&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-121" style="width: 5.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.638em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.64em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-122"><span class="mn" id="MathJax-Span-123" style="font-family: STIXGeneral-Regular;">1.4</span><span class="mo" id="MathJax-Span-124" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-125" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-126" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-127"><span class="mrow" id="MathJax-Span-128"><span class="mo" id="MathJax-Span-129" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-130" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">45</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.4</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>45</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-14">1.4 \times 10^{-45}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;3.4&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-131" style="width: 4.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-132"><span class="mn" id="MathJax-Span-133" style="font-family: STIXGeneral-Regular;">3.4</span><span class="mo" id="MathJax-Span-134" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-135" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-136" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-137"><span class="mrow" id="MathJax-Span-138"><span class="mn" id="MathJax-Span-139" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3.4</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-15">3.4 \times 10^{38}</script></li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mtext&gt;value&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;float32&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1.&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;23&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;127&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-140" style="width: 17.659em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.69em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1014.69em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-141"><span class="msubsup" id="MathJax-Span-142"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mtext" id="MathJax-Span-143" style="font-family: STIXGeneral-Regular;">value</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 2.19em;"><span class="texatom" id="MathJax-Span-144"><span class="mrow" id="MathJax-Span-145"><span class="mtext" id="MathJax-Span-146" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">float32</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-147" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-148" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mo" id="MathJax-Span-149" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-150" style="font-family: STIXGeneral-Regular;">1</span><span class="msubsup" id="MathJax-Span-151"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-152" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-153" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-154" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-155" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1.</span><span class="msubsup" id="MathJax-Span-156"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-157" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-158"><span class="mrow" id="MathJax-Span-159"><span class="mn" id="MathJax-Span-160" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">23</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-161" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-162" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-163" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-164"><span class="mrow" id="MathJax-Span-165"><span class="mi" id="MathJax-Span-166" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-167" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-168" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">127</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mtext>value</mtext><mrow class="MJX-TeXAtom-ORD"><mtext>float32</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class="MJX-TeXAtom-ORD"><mn>23</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>E</mi><mo>−</mo><mn>127</mn></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-16">\text{value}_{\text{float32}} = (-1)^S \times 1.M_{23} \times 2^{E - 127}</script>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">float32</code> is the default numerical format for most deep learning frameworks and hardware. It provides a good balance between precision and range, making it robust for both training and inference.</p>
  </li>
  <li>
    <p><strong>GPU Considerations</strong>:</p>

    <ul>
      <li>Supported natively on all modern GPUs.</li>
      <li>Most general-purpose ALUs (Arithmetic Logic Units) on the GPU are designed to process <code class="language-plaintext highlighter-rouge">float32</code> efficiently.</li>
      <li>Slower than <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code> in terms of throughput and power usage but more accurate.</li>
    </ul>
  </li>
</ul>

<h4 id="double-precision-float64">Double-Precision (<code class="language-plaintext Highlighter-rouge">float64</code>)</h4>

<ul>
  <li><strong>Bit layout</strong>: 1 sign bit, 11 exponent bits, 52 mantissa bits.</li>
  <li><strong>Total bits</strong>: 64</li>
  <li><strong>Exponent bias</strong>: 1023</li>
  <li><strong>Dynamic range</strong>: Approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;324&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-169" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.22em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-170"><span class="mn" id="MathJax-Span-171" style="font-family: STIXGeneral-Regular;">5</span><span class="mo" id="MathJax-Span-172" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-173" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-174" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-175"><span class="mrow" id="MathJax-Span-176"><span class="mo" id="MathJax-Span-177" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-178" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">324</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>324</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-17">5 \times 10^{-324}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.8&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;308&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-179" style="width: 5.419em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.482em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.48em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-180"><span class="mn" id="MathJax-Span-181" style="font-family: STIXGeneral-Regular;">1.8</span><span class="mo" id="MathJax-Span-182" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-183" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-184" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-185"><span class="mrow" id="MathJax-Span-186"><span class="mn" id="MathJax-Span-187" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">308</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.8</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>308</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-18">1.8 \times 10^{308}</script></li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mtext&gt;value&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;float64&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1.&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;52&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1023&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-188" style="width: 18.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.055em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1015.05em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-189"><span class="msubsup" id="MathJax-Span-190"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mtext" id="MathJax-Span-191" style="font-family: STIXGeneral-Regular;">value</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 2.19em;"><span class="texatom" id="MathJax-Span-192"><span class="mrow" id="MathJax-Span-193"><span class="mtext" id="MathJax-Span-194" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">float64</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-195" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-196" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mo" id="MathJax-Span-197" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-198" style="font-family: STIXGeneral-Regular;">1</span><span class="msubsup" id="MathJax-Span-199"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-200" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-201" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-202" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-203" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1.</span><span class="msubsup" id="MathJax-Span-204"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-205" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-206"><span class="mrow" id="MathJax-Span-207"><span class="mn" id="MathJax-Span-208" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">52</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-209" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-210" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-211" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-212"><span class="mrow" id="MathJax-Span-213"><span class="mi" id="MathJax-Span-214" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-215" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-216" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1023</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mtext>value</mtext><mrow class="MJX-TeXAtom-ORD"><mtext>float64</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class="MJX-TeXAtom-ORD"><mn>52</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>E</mi><mo>−</mo><mn>1023</mn></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-19">\text{value}_{\text{float64}} = (-1)^S \times 1.M_{52} \times 2^{E - 1023}</script>

<ul>
  <li><code class="language-plaintext highlighter-rouge">float64</code> is typically used in scientific computing, numerical simulations, and applications requiring high precision. It is <strong>rarely used</strong> in deep learning because its benefits are minimal for most ML tasks, while the compute and memory costs are high.</li>
</ul>

<h4 id="brain-floating-point-bfloat16">Brain Floating Point (<code class="language-plaintext Highlighter-rouge">bfloat16</code>)</h4>

<ul>
  <li><strong>Bit layout</strong>: 1 sign bit, 8 exponent bits, 7 mantissa bits</li>
  <li><strong>Total bits</strong>: 16</li>
  <li><strong>Exponent bias</strong>: 127 (same as <code class="language-plaintext highlighter-rouge">float32</code>)</li>
  <li><strong>Dynamic range</strong>: Approximately <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-217" style="width: 5.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.638em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.64em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-218"><span class="mn" id="MathJax-Span-219" style="font-family: STIXGeneral-Regular;">1.2</span><span class="mo" id="MathJax-Span-220" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-221" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-222" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-223"><span class="mrow" id="MathJax-Span-224"><span class="mo" id="MathJax-Span-225" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-226" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.2</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-20">1.2 \times 10^{-38}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;3.4&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-227" style="width: 4.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-228"><span class="mn" id="MathJax-Span-229" style="font-family: STIXGeneral-Regular;">3.4</span><span class="mo" id="MathJax-Span-230" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-231" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-232" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-233"><span class="mrow" id="MathJax-Span-234"><span class="mn" id="MathJax-Span-235" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3.4</mn><mo>×</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-21">3.4 \times 10^{38}</script></li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mtext&gt;value&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;bfloat16&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1.&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;127&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-236" style="width: 17.659em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.69em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1014.69em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-237"><span class="msubsup" id="MathJax-Span-238"><span style="display: inline-block; position: relative; width: 4.586em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mtext" id="MathJax-Span-239" style="font-family: STIXGeneral-Regular;">value</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 2.19em;"><span class="texatom" id="MathJax-Span-240"><span class="mrow" id="MathJax-Span-241"><span class="mtext" id="MathJax-Span-242" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">bfloat16</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-243" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-244" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mo" id="MathJax-Span-245" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-246" style="font-family: STIXGeneral-Regular;">1</span><span class="msubsup" id="MathJax-Span-247"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-248" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-249" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-250" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-251" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1.</span><span class="msubsup" id="MathJax-Span-252"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-253" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-254"><span class="mrow" id="MathJax-Span-255"><span class="mn" id="MathJax-Span-256" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">7</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-257" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="msubsup" id="MathJax-Span-258" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-259" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-260"><span class="mrow" id="MathJax-Span-261"><span class="mi" id="MathJax-Span-262" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-263" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-264" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">127</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mtext>value</mtext><mrow class="MJX-TeXAtom-ORD"><mtext>bfloat16</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class="MJX-TeXAtom-ORD"><mn>7</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>E</mi><mo>−</mo><mn>127</mn></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-22">\text{value}_{\text{bfloat16}} = (-1)^S \times 1.M_{7} \times 2^{E - 127}</script>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">bfloat16</code> (Brain Floating Point 16) was introduced by Google for training deep neural networks. Unlike <code class="language-plaintext highlighter-rouge">float16</code>, which reduces both exponent and mantissa bits, <code class="language-plaintext highlighter-rouge">bfloat16</code> keeps the <strong>same exponent width as <code class="language-plaintext highlighter-rouge">float32</code></strong> (8 bits) but reduces the mantissa to 7 bits.</p>
  </li>
  <li>
    <p>This design retains the <strong>dynamic range</strong> of <code class="language-plaintext highlighter-rouge">float32</code>, which makes it far more robust to underflow/overflow issues during training compared to <code class="language-plaintext highlighter-rouge">float16</code>.</p>
  </li>
  <li>
    <p>However, the <strong>precision is reduced</strong>, since fewer mantissa bits mean fewer significant digits are preserved. Despite this, it performs well in practice for training large models.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">bfloat16</code> is ideal for training tasks where:</p>

    <ul>
      <li>High dynamic range is important</li>
      <li>Some loss of precision can be tolerated (e.g., in early layers or gradients)</li>
      <li>Lower memory and compute overhead is desired compared to <code class="language-plaintext highlighter-rouge">float32</code></li>
    </ul>
  </li>
  <li>
    <p>It is commonly used in mixed-precision training, often with accumulations in <code class="language-plaintext highlighter-rouge">float32</code> to improve numerical stability.</p>
  </li>
  <li>
    <p><strong>Use cases</strong>:</p>

    <ul>
      <li>Large-scale model training (e.g., LLMs)</li>
      <li>TPUs (Google Cloud), and newer GPUs from NVIDIA, AMD, and Intel that support native <code class="language-plaintext highlighter-rouge">bfloat16</code> ops</li>
    </ul>
  </li>
</ul>

<h4 id="gputpu-considerations">GPU/TPU Considerations</h4>

<ul>
  <li>
    <p><strong>float16</strong>:</p>

    <ul>
      <li>Supported by specialized hardware units (Tensor Cores) in NVIDIA Volta, Turing, Ampere, Hopper, and Blackwell architectures.</li>
      <li>Offers higher throughput and lower memory usage, especially during inference.</li>
      <li>Typically used in mixed-precision training with <code class="language-plaintext highlighter-rouge">float32</code> accumulations to maintain stability.</li>
    </ul>
  </li>
  <li>
    <p><strong>bfloat16</strong>:</p>

    <ul>
      <li>Natively supported on Google TPUs, NVIDIA Ampere and newer (e.g., A100, H100), Intel Habana Gaudi accelerators, and select AMD GPUs.</li>
      <li>Enables <strong>high dynamic range</strong> similar to <code class="language-plaintext highlighter-rouge">float32</code> while halving memory usage.</li>
      <li>Increasingly adopted in training large models where <code class="language-plaintext highlighter-rouge">float16</code> may encounter stability issues.</li>
      <li>Like <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code> is also used in mixed-precision training pipelines.</li>
    </ul>
  </li>
  <li>
    <p><strong>float32</strong>:</p>

    <ul>
      <li>Universally supported across all GPU architectures.</li>
      <li>Offers the best balance between range and precision.</li>
      <li>Slower and more memory-intensive compared to <code class="language-plaintext highlighter-rouge">float16</code> and <code class="language-plaintext highlighter-rouge">bfloat16</code>.</li>
    </ul>
  </li>
  <li>
    <p><strong>float64</strong>:</p>

    <ul>
      <li>Rare in deep learning; primarily used in scientific computing.</li>
      <li>Most GPUs support it at much lower throughput.</li>
      <li>Often omitted entirely from inference workloads due to cost.</li>
    </ul>
  </li>
</ul>

<h4 id="comparative-summary">Comparative Summary</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Format</strong></th>
<th class="tg-hcenter-valign-first"><strong>Bits</strong></th>
<th class="tg-hcenter-valign-first"><strong>Exponent Bits</strong></th>
<th class="tg-hcenter-valign-first"><strong>Mantissa Bits</strong></th>
<th class="tg-hcenter-valign-first"><strong>Bias</strong></th>
<th class="tg-hcenter-valign-first"><strong>Range (Approx.)</strong></th>
<th class="tg-hcenter-valign-second"><strong>GPU Usage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">float16</td>
<td class="tg-tleft-valign-first">16</td>
<td class="tg-tleft-valign-first">5</td>
<td class="tg-tleft-valign-first">10</td>
<td class="tg-tleft-valign-first">15</td>
<td class="tg-tleft-valign-first"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-265" style="width: 2.324em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.908em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1001.91em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-266"><span class="msubsup" id="MathJax-Span-267"><span style="display: inline-block; position: relative; width: 1.908em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-268" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-269"><span class="mrow" id="MathJax-Span-270"><span class="mo" id="MathJax-Span-271" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-272" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">5</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-23">10^{-5}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-273" style="width: 1.729em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.432em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1001.43em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-274"><span class="msubsup" id="MathJax-Span-275"><span style="display: inline-block; position: relative; width: 1.432em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-276" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-277"><span class="mrow" id="MathJax-Span-278"><span class="mn" id="MathJax-Span-279" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">4</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>4</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-24">10^{4}</script></td>
<td class="tg-tleft-valign-second">Fast inference, mixed precision</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">bfloat16</td>
<td class="tg-tleft-valign-first">16</td>
<td class="tg-tleft-valign-first">8</td>
<td class="tg-tleft-valign-first">7</td>
<td class="tg-tleft-valign-first">127</td>
<td class="tg-tleft-valign-first"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-280" style="width: 2.741em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-281"><span class="msubsup" id="MathJax-Span-282"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-283" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-284"><span class="mrow" id="MathJax-Span-285"><span class="mo" id="MathJax-Span-286" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-287" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-25">10^{-38}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-288" style="width: 2.146em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-289"><span class="msubsup" id="MathJax-Span-290"><span style="display: inline-block; position: relative; width: 1.789em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-291" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-292"><span class="mrow" id="MathJax-Span-293"><span class="mn" id="MathJax-Span-294" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-26">10^{38}</script></td>
<td class="tg-tleft-valign-second">Training + inference, high range</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">float32</td>
<td class="tg-tleft-valign-first">32</td>
<td class="tg-tleft-valign-first">8</td>
<td class="tg-tleft-valign-first">23</td>
<td class="tg-tleft-valign-first">127</td>
<td class="tg-tleft-valign-first"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;45&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-295" style="width: 2.741em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-296"><span class="msubsup" id="MathJax-Span-297"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-298" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-299"><span class="mrow" id="MathJax-Span-300"><span class="mo" id="MathJax-Span-301" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-302" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">45</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>45</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-27">10^{-45}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-303" style="width: 2.146em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-304"><span class="msubsup" id="MathJax-Span-305"><span style="display: inline-block; position: relative; width: 1.789em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-306" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-307"><span class="mrow" id="MathJax-Span-308"><span class="mn" id="MathJax-Span-309" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">38</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>38</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-28">10^{38}</script></td>
<td class="tg-tleft-valign-second">Default for training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">float64</td>
<td class="tg-tleft-valign-first">64</td>
<td class="tg-tleft-valign-first">11</td>
<td class="tg-tleft-valign-first">52</td>
<td class="tg-tleft-valign-first">1023</td>
<td class="tg-tleft-valign-first"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;324&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-310" style="width: 3.158em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.622em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1002.62em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-311"><span class="msubsup" id="MathJax-Span-312"><span style="display: inline-block; position: relative; width: 2.622em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-313" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-314"><span class="mrow" id="MathJax-Span-315"><span class="mo" id="MathJax-Span-316" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-317" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">324</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>324</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-29">10^{-324}</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;308&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-318" style="width: 2.622em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.146em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.134em, 1002.15em, 2.384em, -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-319"><span class="msubsup" id="MathJax-Span-320"><span style="display: inline-block; position: relative; width: 2.146em; height: 0px;"><span style="position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;"><span class="mn" id="MathJax-Span-321" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 1.015em;"><span class="texatom" id="MathJax-Span-322"><span class="mrow" id="MathJax-Span-323"><span class="mn" id="MathJax-Span-324" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">308</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>308</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-30">10^{308}</script></td>
<td class="tg-tleft-valign-second">Rare in ML, slow on GPU</td>
</tr>
</tbody>
</table>
</div>

<ul>
  <li>This foundation in floating point formats prepares us to understand quantization—where bit-widths are reduced even further (e.g., 8-bit, 4-bit, or binary)—to achieve efficient computation with minimal loss in model performance.</li>
</ul>

<h3 id="background-matrix-multiplication-in-gpus">Background: Matrix Multiplication in GPUs</h3>

<ul>
  <li>
    <p>Efficient matrix multiplication is at the heart of modern deep learning acceleration on GPUs. This section provides a high-level view of how matrix-matrix multiplications are implemented and optimized on GPU hardware, with special focus on tiling, Tensor Cores, and the performance implications of quantization.</p>
  </li>
  <li>
    <p>Matrix multiplications, especially General Matrix Multiplications (GEMMs), are a core computational primitive in deep learning workloads. Whether in fully connected layers, convolutions (via <code class="language-plaintext highlighter-rouge">im2col</code>), or attention mechanisms, these operations are executed billions of times during training and inference. As such, optimizing GEMM performance is essential for efficient neural network execution, particularly on GPUs.</p>
  </li>
  <li>
    <p>To execute GEMMs efficiently, GPUs partition the output matrix into <em>tiles</em>. Each tile corresponds to a submatrix of the result and is computed by a thread block. The GPU steps through the input matrices along the shared dimension (K) in tiles, performing multiply-accumulate operations and writing the results into the corresponding tile of the output matrix. The illustration below <a href="http://theaiedge.io/">(source)</a> shows the tiled outer product approach to GEMMs.</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/tiled_outer_product_gemm_placeholder.jpg" alt="Tiled outer product approach to GEMMs"></p>

<ul>
  <li>
    <p>Thread blocks are mapped to the GPU’s <strong>streaming multiprocessors (SMs)</strong>, the fundamental compute units that execute instructions in parallel. Each SM can process one or more thread blocks concurrently, depending on the available resources and occupancy.</p>
  </li>
  <li>
    <p>Performance in GPU matrix multiplication is often bounded by one of two factors:</p>

    <ul>
      <li><strong>Compute (math) bound</strong>: When the arithmetic intensity (FLOPs per byte) is high enough that math operations dominate runtime.</li>
      <li><strong>Memory bound</strong>: When the operation requires frequent memory access compared to math operations, limiting throughput.</li>
    </ul>
  </li>
  <li>
    <p>Whether a given GEMM is compute- or memory-bound depends on the matrix dimensions (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-31-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-325" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.44em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-326"><span class="mi" id="MathJax-Span-327" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-328" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-329" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-330" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-331" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi></math></span></span><script type="math/tex" id="MathJax-Element-31">M, N, K</script>) and the hardware’s characteristics. For example, matrix-vector products (where either <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-32-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-332" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-333"><span class="mi" id="MathJax-Span-334" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-32">M</script> = 1 or <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-335" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-336"><span class="mi" id="MathJax-Span-337" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-33">N</script> = 1) are typically memory-bound due to their low arithmetic intensity.</p>
  </li>
  <li>
    <p>Modern NVIDIA GPUs include specialized hardware units called <strong>Tensor Cores</strong>, which are designed to accelerate GEMMs involving low-precision data types such as <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code>, and <code class="language-plaintext highlighter-rouge">int8</code>. Tensor Cores perform small matrix multiplications in parallel and require that the matrices’ dimensions align with certain multiples (e.g., 8 for <code class="language-plaintext highlighter-rouge">float16</code>, 16 for <code class="language-plaintext highlighter-rouge">int8</code>) to achieve peak performance. For instance, on Ampere and newer architectures like Hopper or Blackwell, aligning dimensions to larger multiples (e.g., 64 or 128 elements) often yields even better throughput.</p>
  </li>
  <li>
    <p>Matrix dimensions that are not aligned to tile sizes lead to <strong>tile quantization</strong>, where some tiles carry less useful work, reducing efficiency. Similarly, if the total number of tiles is not an even multiple of the number of GPU SMs, <strong>wave quantization</strong> can cause underutilization. Both effects can significantly degrade performance despite identical algorithmic complexity.</p>
  </li>
  <li>
    <p>To address this, libraries like cuBLAS employ heuristics or benchmarking to select optimal tile sizes, balancing between tile reuse (large tiles) and parallelism (many small tiles). Larger tiles tend to be more efficient due to better data reuse, but may reduce parallel occupancy on smaller problems.</p>
  </li>
  <li>
    <p>In summary, matrix multiplication performance on GPUs is a delicate balance between compute, memory bandwidth, and architecture-aware tiling strategies. Quantization not only affects data representation but also interacts intricately with the underlying matrix multiplication engine and GPU efficiency.</p>
  </li>
</ul>

<h4 id="under-the-hood">Under-the-hood</h4>

<ul>
  <li>Modern GPUs are capable of performing numerical computations more efficiently using 16-bit or 8-bit formats—such as <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code>, and the emerging <code class="language-plaintext highlighter-rouge">float8</code>, <code class="language-plaintext highlighter-rouge">float6</code>, and <code class="language-plaintext highlighter-rouge">float4</code>—with minimal loss in model performance. Mixed precision training strategically leverages these lower-precision formats to accelerate computation and reduce memory consumption, while preserving high-precision (<code class="language-plaintext highlighter-rouge">float32</code>) for numerically sensitive variables and operations to ensure convergence and model integrity (cf. numerical stability in the section on <a href="#overview">Mixed Precision Overview</a>).</li>
  <li>NVIDIA’s GPUs, from Volta onward, offer specialized hardware units known as <strong>Tensor Cores</strong>. These units are optimized for dense matrix operations and drastically improve throughput when leveraging reduced-precision data types. For developers using PyTorch, the <a href="https://pytorch.org/docs/stable/amp.html"><code class="language-plaintext highlighter-rouge">torch.cuda.amp</code></a> module offers automatic mixed-precision training functionality, simplifying adoption with minimal code edits. This automates casting, loss scaling, and fallback to high precision where necessary, ensuring both performance and stability during training.</li>
</ul>

<h4 id="how-tensor-cores-work">How Tensor Cores Work</h4>

<ul>
  <li>
    <p>Tensor Cores serve as specialized hardware designed to accelerate matrix multiplications—critical operations in forward and backward neural network passes. A standard Tensor Core can perform operations such as multiply-and-accumulate on small tiles of data (e.g., 4×4) in reduced-precision formats (e.g., <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code>), or more recent mixed-precision variants using integer types.</p>
  </li>
  <li>
    <p>Crucially, if model tensors remain in <code class="language-plaintext highlighter-rouge">float32</code> in the absence of mixed-precision data handling, the Tensor Cores remain unused and the GPU fails to attain its full performance potential. Enabling automatic mixed precision is therefore essential to utilize these units effectively.</p>
  </li>
  <li>
    <p>In summary, as NVIDIA’s GPU microarchitectures have progressed from Volta through Blackwell, Tensor Cores have become increasingly versatile—offering a widening array of lower-precision formats and hardware optimizations. To fully exploit their capabilities, developers must adopt mixed precision training frameworks (such as AMP), ensuring that compute and memory resources are used optimally while preserving model fidelity.</p>
  </li>
  <li>
    <p>Tensor Core architectures have evolved across NVIDIA’s GPU microarchitectures:</p>

    <ul>
      <li><strong>Volta</strong> introduced first-generation Tensor Cores, supporting <code class="language-plaintext highlighter-rouge">float16</code> matrix-multiply-accumulate (MMA) fused operations.</li>
      <li><strong>Turing</strong> brought second-generation Tensor Cores, adding support for <code class="language-plaintext highlighter-rouge">int8</code> and <code class="language-plaintext highlighter-rouge">int4</code> operations, as well as warp-level synchronous MMA primitives and early AI applications such as DLSS.</li>
      <li><strong>Hopper</strong> (e.g., H100) features fourth-generation Tensor Cores with native <code class="language-plaintext highlighter-rouge">float8</code> precision in the Transformer Engine, yielding up to 4× faster training on models such as GPT‑3 (175B) compared to previous Tensor Core models.</li>
      <li><strong>Blackwell</strong> advances to fifth‑generation Tensor Cores, introducing support for sub‑byte floating-point formats including <code class="language-plaintext highlighter-rouge">float4</code> and FP6, alongside <code class="language-plaintext highlighter-rouge">float8</code>/<code class="language-plaintext highlighter-rouge">bfloat16</code>/<code class="language-plaintext highlighter-rouge">float16</code> and <code class="language-plaintext highlighter-rouge">int8</code> support. These “Ultra Tensor Cores” incorporate micro‑tensor scaling techniques to fine-tune performance and accuracy—doubling attention-layer throughput and increasing AI FLOPs by 1.5× compared to earlier Blackwell versions.</li>
    </ul>
  </li>
  <li>
    <p>The following figure (<a href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">source</a>) depicts the fundamental computational pattern executed by a Tensor Core—a fused MMA on small matrix tiles, typically of size 4×4 in early architectures. In this operation, two input matrices (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-34-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-338" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-339"><span class="mi" id="MathJax-Span-340" style="font-family: STIXGeneral-Italic;">A</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span><script type="math/tex" id="MathJax-Element-34">A</script> and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-35-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-341" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-342"><span class="mi" id="MathJax-Span-343" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-35">B</script>), stored in a reduced-precision format such as <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code>, are multiplied together. The results of these element-wise multiplications are then summed and accumulated directly into a third matrix (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-36-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-344" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-345"><span class="mi" id="MathJax-Span-346" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></span></span><script type="math/tex" id="MathJax-Element-36">C</script>), which may be stored in either <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code>, <code class="language-plaintext highlighter-rouge">float32</code>, or, in newer architectures, <code class="language-plaintext highlighter-rouge">float8</code> or <code class="language-plaintext highlighter-rouge">float4</code>. This fusion of multiplication and accumulation into a single hardware instruction eliminates the need to store intermediate results in memory, drastically reducing memory bandwidth requirements and increasing throughput. Larger GEMM (General Matrix Multiply) operations are implemented by tiling them into many such MMA operations executed in parallel across the GPU’s Tensor Cores.</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/tensor.avif" alt=""></p>

<h3 id="definition">Definition</h3>

<ul>
  <li>Quantization in the context of deep learning refers to the process of reducing the numerical precision of a model’s parameters (weights) and/or intermediate computations (activations). Typically, models are trained and stored using 32-bit floating-point (<code class="language-plaintext highlighter-rouge">float32</code>) precision. Quantization replaces these high-precision values with lower-precision representations—such as 16-bit floating point (<code class="language-plaintext highlighter-rouge">float16</code>), 8-bit integer (<code class="language-plaintext highlighter-rouge">int8</code>), 4-bit integer, or binary formats in more extreme scenarios. The primary goals are to reduce model size, improve memory and compute efficiency, and accelerate inference—particularly on hardware that supports low-precision arithmetic.</li>
  <li>The primary goal of quantization is to enhance inference speed. In contrast, as will be discussed in the section on <a href="#mixed-precision-training">Mixed Precision Training</a>, the goal of Automatic Mixed Precision (AMP) is to reduce training time. Quantization is effective, in part, because modern neural networks are typically highly over-parameterized and exhibit robustness to minor numerical perturbations. With appropriate calibration and suitable tools, lower-precision representations can approximate the full-precision model closely enough for practical deployment.</li>
</ul>

<h3 id="types-of-quantization">Types of Quantization</h3>

<ul>
  <li>
    <p>There are two general categories of quantization:</p>

    <ul>
      <li>
        <p><strong>Floating-point quantization</strong>: This reduces the bit-width of floating-point values—for example, converting from <code class="language-plaintext highlighter-rouge">float32</code> to <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code>. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).</p>
      </li>
      <li>
        <p><strong>Integer quantization</strong>: This maps floating-point values to fixed-point integer representations (e.g., <code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">uint8</code>). This type requires an additional transformation using <strong>scale</strong> and <strong>zero-point</strong> to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="integer-quantization">Integer Quantization</h4>

<ul>
  <li>
    <p>Integer quantization is typically implemented as a learned linear transformation (i.e., linear mapping) defined by two parameters: <strong>scale</strong> and <strong>zero-point</strong>.</p>

    <ul>
      <li>
        <p>The <strong>scale</strong> is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.</p>
      </li>
      <li>
        <p>The <strong>zero-point</strong> is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.</p>

        <ul>
          <li>The forward quantization formula (<code class="language-plaintext highlighter-rouge">float</code> to <code class="language-plaintext highlighter-rouge">int</code>) is:</li>
        </ul>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code0"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code0">q = round(x / scale) + zero_point
</code></pre></div>        </div>

        <ul>
          <li>The reverse dequantization formula (<code class="language-plaintext highlighter-rouge">int</code> to <code class="language-plaintext highlighter-rouge">float</code>) is:</li>
        </ul>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code1"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code1">x = scale * (q - zero_point)
</code></pre></div>        </div>
      </li>
      <li>
        <p>As an example:</p>

        <ul>
          <li>Suppose we want to quantize floating-point values in the range <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-37-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1.0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1.0&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-347" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-348"><span class="mo" id="MathJax-Span-349" style="font-family: STIXGeneral-Regular;">[</span><span class="mo" id="MathJax-Span-350" style="font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-351" style="font-family: STIXGeneral-Regular;">1.0</span><span class="mo" id="MathJax-Span-352" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-353" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">1.0</span><span class="mo" id="MathJax-Span-354" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mo>−</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-37">[-1.0, 1.0]</script> to 8-bit unsigned integers (<code class="language-plaintext highlighter-rouge">uint8</code>, range 0–255). The mapping would look like:</li>
        </ul>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code2"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code2">scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784
zero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128
</code></pre></div>        </div>

        <ul>
          <li>This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="non-linear-integer-quantization-methods">Non-Linear Integer Quantization Methods</h4>

<ul>
  <li>
    <p>While linear quantization using scale and zero‑point is the most common, <strong>non‑linear quantization methods</strong> are also employed in integer quantization to better match real-world data distributions. These methods typically apply to <strong>integer quantization</strong>, as they redefine how integer values map to real numbers:</p>

    <ul>
      <li>
        <p><strong>Logarithmic quantization</strong> uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is <strong>not</strong> relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.</p>
      </li>
      <li>
        <p><strong>K-means or cluster-based quantization</strong> groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.</p>
      </li>
      <li>
        <p><strong>Learned transformations</strong>, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to <strong>integer quantization</strong> of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.</p>
      </li>
      <li>
        <p>In summary, <strong>non-linear quantization techniques are relevant for integer quantization workflows</strong>, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-355" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-356"><span class="mo" id="MathJax-Span-357" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-38">\rightarrow</script> float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="floating-point-quantization">Floating-Point Quantization</h4>

<ul>
  <li>Floating-point quantization is implemented by truncating or rounding the mantissa and exponent fields in IEEE754 representation—e.g. conversion from <code class="language-plaintext highlighter-rouge">float32</code> to <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code>—preserving the format structure but reducing bit-width. This form of quantization (i.e., bit‑width reduction) is <strong>non-linear</strong> in effect because the quantization steps vary by exponent range: numbers near zero have finer granularity than large values due to the floating-point exponent scaling.</li>
  <li>This approach aligns with a well-known model in signal quantization theory often referred to as the compressor–quantizer–expander model. In this framework, the exponential scaling of floating-point numbers acts as a compressor that non-linearly maps real values into a domain where uniform quantization (truncation of mantissa bits) is applied. The quantizer then discretizes the mantissa (hidden quantizer), and the expander step reconstructs the approximate value from the compressed representation. This structure enables efficient representation of a wide dynamic range with relatively coarse quantization, especially benefiting smaller values close to zero.</li>
  <li>Common APIs include casting methods like <code class="language-plaintext highlighter-rouge">model.half()</code> in PyTorch and PyTorch’s support for <code class="language-plaintext highlighter-rouge">float16</code> static quantization configs (e.g. <code class="language-plaintext highlighter-rouge">float16_static_qconfig</code>). Floating-point quantization halves memory footprint with minimal accuracy loss on GPU inference platforms.</li>
</ul>

<h3 id="dequantization-considerations">Dequantization Considerations</h3>

<ul>
  <li><strong>Dequantization is not always needed during inference</strong>, and its necessity depends on the type of quantization and the underlying hardware. In <strong>integer-only quantization pipelines</strong>—commonly used for inference on mobile CPUs or edge devices—computations are performed entirely in the integer domain (e.g., <code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">uint8</code>), and <strong>dequantization is typically only applied at the final stage</strong>, such as for logits or output activations. This avoids floating-point operations altogether during inference.
    <ul>
      <li>However, in <strong>hybrid quantization workflows</strong>, where some layers are quantized (e.g., to <code class="language-plaintext highlighter-rouge">int8</code>) and others remain in higher precision (<code class="language-plaintext highlighter-rouge">float32</code> or <code class="language-plaintext highlighter-rouge">float16</code>), <strong>intermediate dequantization is required</strong> at the layer boundaries to enable compatibility between quantized and non-quantized components. This is common in models that cannot fully tolerate quantization across all layers due to accuracy degradation or unsupported ops.</li>
      <li>In contrast, when quantizing to <strong>lower-precision floating-point formats</strong> like <code class="language-plaintext highlighter-rouge">float16</code>, <strong>dequantization is not needed at all</strong>, because these formats are still natively supported by GPU hardware. For example, <strong>NVIDIA Tensor Cores</strong> are optimized for <code class="language-plaintext highlighter-rouge">float16</code> (and <code class="language-plaintext highlighter-rouge">bfloat16</code>) matrix operations, so models using <code class="language-plaintext highlighter-rouge">float16</code> quantization can be executed directly end-to-end without converting back to <code class="language-plaintext highlighter-rouge">float32</code>. All computations remain in low-precision floating-point format, maintaining performance while avoiding the complexity of dequantization logic entirely.</li>
    </ul>
  </li>
</ul>

<h3 id="quantization-workflows">Quantization Workflows</h3>

<ul>
  <li>
    <p>There are three main workflows/approaches to apply quantization:</p>

    <ul>
      <li>
        <p><strong>Dynamic / Runtime Quantization</strong>: This method quantizes <strong>model weights statically</strong> (e.g. to <code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">float16</code>), while <strong>activations remain in full precision until runtime</strong>, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code>, recurrent layers (<code class="language-plaintext highlighter-rouge">nn.LSTM</code>, <code class="language-plaintext highlighter-rouge">nn.GRU</code>), and transformers. In PyTorch, this is implemented via the function <a href="https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic"><code class="language-plaintext highlighter-rouge">quantize_dynamic</code></a>, for example:</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code3"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code3"><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model_float32</span><span class="p">,</span>
    <span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">},</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">qint8</span>
<span class="p">)</span>
</code></pre></div>        </div>

        <ul>
          <li>With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.</li>
          <li>Look up the <a href="#dynamic--runtime-quantization">Dynamic / Runtime Quantization</a> section for a detailed discourse on this topic.</li>
        </ul>
      </li>
      <li>
        <p><strong>Post-Training Quantization (PTQ)</strong>: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate <code class="language-plaintext highlighter-rouge">scale</code> and <code class="language-plaintext highlighter-rouge">zero-point</code> values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.</p>

        <ul>
          <li>From the TensorFlow <a href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training">post-training quantization</a> documentation:</li>
        </ul>

        <blockquote>
          <p>We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.</p>
        </blockquote>

        <ul>
          <li>This reflects hardware preferences: <code class="language-plaintext highlighter-rouge">float16</code> enables faster matrix multiplications on GPU accelerators like Tensor Cores, while <code class="language-plaintext highlighter-rouge">int8</code> is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).</li>
          <li>Look up the <a href="#post-training-quantization">Post-Training Quantization</a> section for a detailed discourse on this topic.</li>
        </ul>
      </li>
      <li>
        <p><strong>Quantization-Aware Training (QAT)</strong>: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., <code class="language-plaintext highlighter-rouge">FakeQuantize</code> in PyTorch or <code class="language-plaintext highlighter-rouge">tf.quantization.fake_quant_with_min_max_vars</code> in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full <code class="language-plaintext highlighter-rouge">float32</code> precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.</p>
        <ul>
          <li>Look up the <a href="#quantizationaware-training-qat">Quantization‑aware Training (QAT)</a> section for a detailed discourse on this topic.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="benefits-and-limitations">Benefits and Limitations</h3>

<ul>
  <li>Quantization can lead to significant reductions in efficiency, both in terms of memory footprint and computational load. For example, converting <code class="language-plaintext highlighter-rouge">float32</code> weights to <code class="language-plaintext highlighter-rouge">int8</code> reduces storage requirements by 75%, and on supported hardware, can improve inference speed by 2× to 4×. These benefits are amplified on edge devices with limited memory, power, and compute such as mobile phones, IoT sensors, embedded processors, and accelerators that support low-precision execution.</li>
</ul>

<blockquote>
  <p>While quantization offers compelling advantages, it can also suffer from practical limitations. Quantization might not work uniformly well across all architectures or layers since some operators (powering these layers and architectures) might not be supported in quantized form on all hardware targets. Put simply, some hardware can lack native support for certain quantized operators, since they typically have to be implemented individually. Operators like group convolutions, custom layers, normalization approaches (say LayerNorm), etc. may fall back to <code class="language-plaintext highlighter-rouge">float32</code> or require custom low-level kernels, potentially limiting compatibility or efficiency on target platforms. Lastly, layers with small value ranges, heavy outliers, or complex nonlinear interactions may require higher precision (e.g., <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">float32</code>) to avoid accuracy degradation.</p>
</blockquote>

<h3 id="mitigation-strategies">Mitigation Strategies</h3>

<ul>
  <li>
    <p>Selective strategies like <strong>per-channel</strong>, <strong>per-group</strong>, <strong>per-layer</strong>, <strong>per-tensor</strong>, and <strong>mixed-precision quantization</strong> are commonly used to mitigate limitations such as accuracy loss, hardware incompatibilities, and uneven value distributions across layers.</p>
  </li>
  <li>
    <p><strong>Per-channel quantization</strong> (also referred to as <em>channel-wise quantization</em>): This approach assigns separate scale and zero-point values to each output channel of a weight tensor. It is particularly effective in convolutional and linear layers where each output channel (or filter) may have significantly different weight distributions. By capturing channel-wise variations in magnitude, it provides better quantization accuracy, especially in vision models like <strong>ResNet</strong>, <strong>MobileNet</strong>, and <strong>EfficientNet</strong>, as well as transformer-based architectures such as <strong>BERT</strong> and <strong>GPT</strong>. PyTorch implements this using the <code class="language-plaintext highlighter-rouge">torch.per_channel_affine</code> quantization scheme.</p>
  </li>
  <li>
    <p><strong>Per-group quantization</strong> (also referred to as <em>group-wise quantization</em>): A compromise between per-tensor and per-channel quantization. Channels are divided into groups, with each group sharing quantization parameters. This reduces the overhead of storing separate scale/zero-point values for every channel, while still preserving more distributional information than per-tensor quantization. It is particularly useful in resource-constrained deployment scenarios where memory and compute costs must be balanced against accuracy. Though not always exposed via high-level APIs in frameworks like PyTorch, this strategy is supported in certain hardware accelerators and vendor-specific toolchains (e.g., Qualcomm, MediaTek, Xilinx).</p>
  </li>
  <li>
    <p><strong>Per-layer quantization</strong> (also referred to as <em>layer-wise quantization</em>): This applies the same quantization parameters (scale and zero-point) across an entire layer’s output or weight tensor. For example, all outputs of a linear layer or all weights of a convolution kernel are quantized using a single shared set of parameters. This method is computationally efficient and requires minimal additional metadata, making it widely used in low-resource settings or for fast prototyping. However, it often leads to higher quantization error in layers with highly varied internal distributions.</p>
  </li>
  <li>
    <p><strong>Per-tensor quantization</strong> (also referred to as <em>tensor-wise quantization</em>): A special case of per-layer quantization where quantization is applied uniformly across an entire tensor (typically weight or activation). A single scale and zero-point are calculated for the full tensor, regardless of dimensionality or channel boundaries. This is the simplest and most lightweight quantization method, requiring minimal bookkeeping and fast execution. While effective for layers with narrow and uniform value ranges, it can result in significant information loss when used on tensors with wide dynamic range or uneven channel statistics. This method is often the default in early-stage quantization workflows or on hardware that does not support fine-grained schemes.</p>
  </li>
  <li>
    <p><strong>Mixed-precision quantization</strong>: Instead of applying uniform quantization across all model layers, mixed-precision quantization selectively retains higher-precision (e.g., <code class="language-plaintext highlighter-rouge">float32</code> or <code class="language-plaintext highlighter-rouge">float16</code>) computation in layers that are sensitive to quantization noise—such as attention heads, layer normalization, or output classifiers. Other layers, particularly early convolution blocks or MLPs, can be safely quantized to <code class="language-plaintext highlighter-rouge">int8</code> or lower. This approach enables developers to achieve a favorable trade-off between accuracy and efficiency. Mixed-precision is supported by most modern inference engines including <strong>TensorRT</strong>, <strong>TVM</strong>, <strong>XNNPACK</strong>, and <strong>PyTorch FX Graph Mode Quantization</strong>.</p>
  </li>
</ul>

<h3 id="weights-vs-activation-quantization">Weights vs. Activation Quantization</h3>

<ul>
  <li>
    <p>Why they’re not the same: Weights and activations have fundamentally different roles and constraints during quantization.</p>

    <ul>
      <li>
        <p>Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as <em>per‑channel quantization</em> or <em>non‑uniform quantization</em>, and values may be precomputed and stored in compact low‑precision formats like <code class="language-plaintext highlighter-rouge">int8</code>, <code class="language-plaintext highlighter-rouge">int4</code>, or binary.</p>
      </li>
      <li>
        <p>Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:</p>

        <ul>
          <li>
            <p><strong>Static Activation Quantization</strong>: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as <a href="https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html"><code class="language-plaintext highlighter-rouge">MinMaxObserver</code></a> or histogram‑based observers.</p>
          </li>
          <li>
            <p><strong>Dynamic Activation Quantization</strong>: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Handling outliers: Outliers in activation or weight distributions can drastically degrade quantization quality by skewing range and reducing effective precision. Several mitigation strategies include:</p>

    <ul>
      <li>
        <p><strong>Percentile‑based Clipping</strong>: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.</p>
      </li>
      <li>
        <p><strong>Per‑Channel Quantization</strong>: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via <a href="https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html">PerChannelMinMaxObserver</a> or similar observers using <a href="https://pytorch.org/docs/stable/quantization.html"><code class="language-plaintext highlighter-rouge">torch.per_channel_affine</code></a> schemes. Core functions include <a href="https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html"><code class="language-plaintext highlighter-rouge">torch.quantize_per_channel</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html"><code class="language-plaintext highlighter-rouge">torch.fake_quantize_per_channel_affine</code></a> to simulate quantization.</p>

        <ul>
          <li>
            <p>Per‑channel quantization is especially effective in:</p>

            <ul>
              <li>Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.</li>
              <li>Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Learned Scale Factors</strong>: Methods like Learned Step Size Quantization (LSQ), introduced in the paper <a href="https://arxiv.org/abs/1902.08153">“Learned Step Size Quantization”</a> by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Advanced Weight Handling:</p>

    <ul>
      <li>
        <p><strong>Per‑Group Quantization</strong>: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.</p>
      </li>
      <li>
        <p><strong>Activation‑Aware Weight Quantization (AWQ)</strong>: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.</p>
      </li>
      <li>
        <p><strong>Zero‑Point Optimization</strong>: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In practice, weight and activation quantization are applied jointly but with distinct parameter sets and calibration workflows. Modern toolkits support fine-grained configuration, including:</p>

    <ul>
      <li>PyTorch’s <a href="https://pytorch.org/docs/stable/quantization.html">torch.quantization</a> and <a href="https://pytorch.org/docs/stable/quantization-support.html">torch.ao.quantization</a> modules;</li>
      <li>TensorFlow Model Optimization Toolkit (<a href="https://www.tensorflow.org/model_optimization/guide/quantization">quantization guide</a>) supporting calibration APIs and quantization-aware training;</li>
      <li>NVIDIA TensorRT, which enables layer-wise quantization, quantization-aware training, and PTQ via its <a href="https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html">TensorRT SDK documentation</a> and the Torch‑TensorRT Model Optimizer;</li>
      <li>Intel Neural Compressor, an open‑source framework offering post‑training static, dynamic, and quantization‑aware training workflows for PyTorch and TensorFlow (<a href="https://intel.github.io/neural-compressor/latest/docs/source/quantization.html">Intel Neural Compressor documentation</a>).</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Effective quantization requires balancing statistical rigor, hardware compatibility, and architecture sensitivity. Activations require runtime awareness, while weights benefit from static optimization—and both may leverage learned or adaptive scaling to maintain fidelity in low‑bit regimes.</p>
</blockquote>

<h3 id="quantization-with-pytorch">Quantization with PyTorch</h3>

<ul>
  <li>
    <p>Quantization in PyTorch enables the execution of computations and memory accesses with reduced-precision data types, typically <code class="language-plaintext highlighter-rouge">int8</code>, leading to improvements in model efficiency, inference speed, and memory footprint. PyTorch provides comprehensive support for quantization, starting from version 1.3, through an API that integrates seamlessly with the existing eager execution model.</p>
  </li>
  <li>
    <p><strong>Quantized Tensor Representation</strong></p>

    <ul>
      <li>
        <p>PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically <code class="language-plaintext highlighter-rouge">int8</code>, and sometimes <code class="language-plaintext highlighter-rouge">float16</code>). These tensors can be operated on via quantized kernels available under <code class="language-plaintext highlighter-rouge">torch.nn.quantized</code> and <code class="language-plaintext highlighter-rouge">torch.nn.quantized.dynamic</code>. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.</p>
      </li>
      <li>
        <p>Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form <code class="language-plaintext highlighter-rouge">round(x / scale) + zero_point</code>, enabling a linear mapping between floating point and integer domains.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization Backends</strong></p>

    <ul>
      <li>PyTorch leverages optimized backend libraries to execute quantized operations efficiently. FBGEMM (Facebook’s GEMM library) is optimized for server environments (x86 CPUs), while QNNPACK is designed for mobile and embedded environments. These are analogous to BLAS/MKL libraries in floating-point computation and are integrated automatically based on the target deployment platform.</li>
    </ul>
  </li>
  <li>
    <p><strong>Numerical Stability and Mixed Precision</strong></p>

    <ul>
      <li>One challenge in quantization is maintaining numerical stability, particularly for operations involving accumulation or exponentiation. To address this, PyTorch supports mixed-precision training and inference using the <code class="language-plaintext highlighter-rouge">torch.cuda.amp</code> module. AMP (Automatic Mixed Precision) allows portions of the model to be cast to <code class="language-plaintext highlighter-rouge">torch.float16</code> while retaining <code class="language-plaintext highlighter-rouge">torch.float32</code> for operations requiring higher precision, improving performance with minimal loss of accuracy. Although initially introduced for CUDA GPUs, mixed-precision techniques are distinct from quantization but can be complementary in certain scenarios.</li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization Techniques in PyTorch</strong></p>

    <ul>
      <li>
        <p>PyTorch provides three primary quantization workflows under the <code class="language-plaintext highlighter-rouge">torch.quantization</code> namespace, often referred to collectively as “eager mode quantization”:</p>
      </li>
      <li><strong>Dynamic Quantization</strong>:
        <ul>
          <li>
            <p>Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).</p>
          </li>
          <li>
            <p>Example:</p>
          </li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code4"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code4"><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">quantize_dynamic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">qint8</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Post-Training Quantization</strong>:
        <ul>
          <li>
            <p>Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.</p>
          </li>
          <li>
            <p>Example sequence:</p>
          </li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code5"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code5"><span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Run calibration with representative data
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Quantization-Aware Training (QAT)</strong>:
        <ul>
          <li>
            <p>This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.</p>
          </li>
          <li>
            <p>Example sequence:</p>
          </li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code6"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code6"><span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Train model
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Operator and Layer Coverage</strong></p>

    <ul>
      <li>Quantization support varies by method. Dynamic quantization supports layers like Linear and RNNs, while static and QAT methods support a broader set including Conv, ReLU, BatchNorm (via fusion), and more. FX Graph Mode Quantization (a newer, graph-level approach not covered here) further expands operator support and streamlines workflows.</li>
    </ul>
  </li>
  <li>
    <p>For additional guidance and end-to-end examples, refer to the official PyTorch blog: <a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">Introduction to Quantization on PyTorch</a>.</p>
  </li>
</ul>

<h4 id="dynamic--runtime-quantization">Dynamic / Runtime Quantization</h4>

<ul>
  <li>
    <p><a href="https://pytorch.org/docs/stable/quantization.html#dynamic-quantization">Dynamic quantization</a> is one of the most simple quantization techniques in PyTorch, particularly suitable for models where most computation occurs in linear layers—such as transformer models (e.g. BERT) or recurrent networks (e.g. LSTM)—because these operations are dominated by matrix multiplications, which benefit significantly from <code class="language-plaintext highlighter-rouge">int8</code> acceleration without requiring quantized convolutions.</p>
  </li>
  <li>
    <p>In dynamic quantization, model weights are converted from 32-bit floating point (<code class="language-plaintext highlighter-rouge">float32</code>) to a lower precision format such as <code class="language-plaintext highlighter-rouge">int8</code> and are permanently stored in this quantized form. Activations, however, remain in <code class="language-plaintext highlighter-rouge">float32</code> format until runtime. At inference time, these activations are dynamically quantized to <code class="language-plaintext highlighter-rouge">int8</code> immediately before the corresponding computation (i.e., matrix multiplication or linear operation) is executed. After the operation, the result is stored back in <code class="language-plaintext highlighter-rouge">float32</code>. This hybrid approach enables significant performance gains—such as reduced latency and memory usage—while maintaining reasonable model accuracy.</p>
  </li>
</ul>

<blockquote>
  <p>The aim of dynamic quantization is thus to save compute through faster arithmetic, rather than primarily to reduce storage needs.</p>
</blockquote>

<ul>
  <li>
    <p>Unlike static quantization or quantization-aware training (QAT), dynamic quantization requires no calibration dataset or retraining. This makes it ideal when representative data is unavailable or ease of deployment is paramount.</p>
  </li>
  <li>
    <p>Quantization parameters (scale and zero-point) for activations are determined dynamically at each invocation based on the input data range, while weights use fixed scale and zero-point values computed ahead of time. As such, since only the model weights are quantized ahead of time, while activations remain in <code class="language-plaintext highlighter-rouge">float32</code> and are quantized dynamically at runtime based on input data, dynamic quantization is often referred to as a data-free or weight-only quantization method during preparation.</p>
  </li>
  <li>
    <p>PyTorch provides the API <a href="https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html"><code class="language-plaintext highlighter-rouge">torch.ao.quantization.quantize_dynamic(...)</code></a> for applying dynamic quantization:</p>

    <ul>
      <li>A model (<code class="language-plaintext highlighter-rouge">torch.nn.Module</code>)</li>
      <li>A specification of target layer types or names (commonly <code class="language-plaintext highlighter-rouge">{nn.Linear, nn.LSTM}</code>)</li>
      <li>A target dtype (e.g., <code class="language-plaintext highlighter-rouge">torch.qint8</code>).</li>
    </ul>
  </li>
  <li>
    <p>Only supported layer types are quantized—primarily <code class="language-plaintext highlighter-rouge">nn.Linear</code> and RNN variants; convolutional layers (e.g., <code class="language-plaintext highlighter-rouge">nn.Conv2d</code>) are not supported by dynamic quantization.</p>
  </li>
  <li>
    <p>This approach is particularly effective for transformer and RNN models, where inference throughput is limited by memory-bound weight matrices. For example, quantizing BERT with dynamic quantization often yields up to 4× reduction in model size and measurable speedups in CPU inference latency.</p>
  </li>
</ul>

<h5 id="dynamic-quantization-vs-post-training-quantization">Dynamic Quantization vs. Post-Training Quantization</h5>

<ul>
  <li>Unlike <a href="#post-training-static-quantization">Post-Training Quantization</a>, dynamic quantization does not use minmax observers or any calibration mechanism. Activation ranges are computed dynamically at runtime based on actual input data, so no observers are required during model preparation.</li>
</ul>

<h5 id="example-workflow">Example Workflow</h5>

<ul>
  <li>Below is an example illustrating a typical dynamic quantization workflow:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code7"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code7"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="n">quantize_dynamic</span>

<span class="c1"># Assume `model` is a pretrained floating‑point nn.Module, in eval mode
</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Apply dynamic quantization to Linear and LSTM layers
</span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="p">{</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">},</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">qint8</span>
<span class="p">)</span>

<span class="c1"># Run inference: activations will be quantized at runtime
</span><span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
</code></pre></div></div>

<h6 id="workflow-explanation">Workflow Explanation</h6>

<ul>
  <li><code class="language-plaintext highlighter-rouge">model.eval()</code> ensures deterministic behavior during quantization.</li>
  <li><code class="language-plaintext highlighter-rouge">quantize_dynamic(...)</code> replaces supported layers with their dynamic quantized implementations.</li>
  <li>Activations remain in <code class="language-plaintext highlighter-rouge">float32</code> until needed.</li>
  <li>At runtime, activations are quantized to <code class="language-plaintext highlighter-rouge">int8</code> on-the-fly, and computations are performed with <code class="language-plaintext highlighter-rouge">int8</code> weights and mixed precision accumulators.</li>
  <li>After the operation, results return to <code class="language-plaintext highlighter-rouge">float32</code>.</li>
</ul>

<h6 id="typical-benefits">Typical Benefits</h6>

<ul>
  <li>Model size reduced by ~75%, thanks to <code class="language-plaintext highlighter-rouge">int8</code> weights.</li>
  <li>Latency improvements, especially on CPU-bound operations.</li>
  <li>No calibration or fine-tuning required.</li>
</ul>

<h6 id="notes--tradeoffs">Notes &amp; Trade‑offs</h6>

<ul>
  <li>Dynamic quantization does not support convolution or custom layers unless manually wrapped.</li>
  <li>Dynamic quantization handles input distributions that vary widely more gracefully than static quantization, which uses fixed calibration ranges.</li>
  <li>For CNN models or workloads where activations must also be quantized ahead of time, static quantization or QAT may yield better performance and accuracy.</li>
</ul>

<h6 id="further-reading">Further Reading</h6>

<ul>
  <li>A comprehensive end-to-end tutorial for dynamic quantization on BERT is available <a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">here</a>.</li>
  <li>For a more general example and advanced usage guide, see the <a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html">dynamic quantization tutorial</a>.</li>
  <li>The full API documentation for <code class="language-plaintext highlighter-rouge">torch.quantization.quantize_dynamic</code> is available <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic">here</a>.</li>
</ul>

<h4 id="post-training-quantization">Post-Training Quantization</h4>

<ul>
  <li>
    <p>Post-Training Quantization (PTQ) is a technique in PyTorch that enables the conversion of a model’s weights and activations from floating-point (typically <code class="language-plaintext highlighter-rouge">float32</code>) to 8-bit integers (<code class="language-plaintext highlighter-rouge">int8</code>), significantly improving inference efficiency in terms of speed and memory usage. This method is particularly well-suited for deployment scenarios on both server and edge devices, where latency and resource constraints are critical.</p>
  </li>
  <li>
    <p>To facilitate this process, PyTorch inserts special modules known as <em>observers</em> into the model. These modules capture the activation ranges at various points in the network. Once sufficient data has been passed through the model during calibration, the observers record min-max values or histograms (depending on the observer type), which are then used during quantization.</p>
  </li>
  <li>
    <p>A key benefit of static quantization is that it allows quantized values to be passed between operations directly, eliminating the need for costly float-to-int and int-to-float conversions at each layer. This optimization significantly reduces runtime overhead and enables end-to-end execution in <code class="language-plaintext highlighter-rouge">int8</code>.</p>
  </li>
  <li>
    <p>PyTorch also supports several advanced features to further improve the effectiveness of static quantization:</p>

    <ul>
      <li>
        <p><strong>Observers</strong>:</p>

        <ul>
          <li>Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver"><code class="language-plaintext highlighter-rouge">MinMaxObserver</code></a> and <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver"><code class="language-plaintext highlighter-rouge">HistogramObserver</code></a>, and users can register them via the model’s <code class="language-plaintext highlighter-rouge">qconfig</code>.</li>
          <li>Observers are inserted using <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare"><code class="language-plaintext highlighter-rouge">torch.quantization.prepare</code></a>.</li>
        </ul>
      </li>
      <li>
        <p><strong>Operator Fusion</strong>:</p>

        <ul>
          <li>PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.</li>
          <li>Modules can be fused using <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules"><code class="language-plaintext highlighter-rouge">torch.quantization.fuse_modules</code></a>.</li>
        </ul>
      </li>
      <li>
        <p><strong>Per-Channel Weight Quantization</strong>:</p>

        <ul>
          <li>Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.</li>
          <li>Final conversion to the quantized model is done using <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert"><code class="language-plaintext highlighter-rouge">torch.quantization.convert</code></a>.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="post-training-quantization-vs-dynamic-quantization">Post-Training Quantization vs. Dynamic Quantization</h5>

<ul>
  <li>Unlike <a href="#dynamic--runtime-quantization">dynamic quantization</a>, which quantizes activations on-the-fly during inference, static quantization requires an additional calibration step. This calibration involves running representative data through the model to collect statistics on the distribution of activations. These statistics guide the quantization process by determining appropriate scaling factors and zero points for each tensor.</li>
  <li>Put simply, in PTQ, while weights are quantized ahead of time, activations are quantized using calibration data collected via observers, enabling fully quantized inference across all layers.</li>
</ul>

<h5 id="example-workflow-1">Example Workflow</h5>

<ul>
  <li>Below is an example illustrating a typical PTQ workflow:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code8"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code8"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.quantization</span>

<span class="c1"># Step 1: Define or load the model
</span><span class="n">model</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># assume a pre-trained model is loaded
</span>
<span class="c1"># Step 2: Set the quantization configuration
# Choose backend depending on target device
</span><span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s">'qnnpack'</span><span class="p">)</span>  <span class="c1"># for ARM/mobile
# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # for x86/server
</span>
<span class="c1"># Step 3: Fuse modules (e.g., Conv + BN + ReLU)
</span><span class="n">model_fused</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="s">'conv'</span><span class="p">,</span> <span class="s">'bn'</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">]])</span>

<span class="c1"># Step 4: Insert observer modules
</span><span class="n">model_prepared</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model_fused</span><span class="p">)</span>

<span class="c1"># Step 5: Calibrate the model with representative data
# For example, run a few batches of real or synthetic inputs
</span><span class="n">model_prepared</span><span class="p">(</span><span class="n">example_batch</span><span class="p">)</span>

<span class="c1"># Step 6: Convert to a quantized model
</span><span class="n">model_quantized</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>This static quantization pipeline can yield 2× to 4× speedups in inference latency and a 4× reduction in model size, with minimal degradation in accuracy when calibrated effectively.</li>
</ul>

<h6 id="workflow-explanation-1">Workflow Explanation</h6>

<ul>
  <li>
    <p>After the example workflow, here is a breakdown of each step and its purpose:</p>

    <ul>
      <li><strong>Model Preparation</strong>: The model must be in eval mode (<code class="language-plaintext highlighter-rouge">model.eval()</code>) so that observers and quantization stubs function deterministically. Depending on the backend, <code class="language-plaintext highlighter-rouge">model.qconfig = torch.ao.quantization.get_default_qconfig('x86')</code> or <code class="language-plaintext highlighter-rouge">'qnnpack'</code> sets the appropriate quantization configuration.</li>
      <li><strong>Operator Fusion</strong>: Use <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules"><code class="language-plaintext highlighter-rouge">torch.quantization.fuse_modules</code></a> to merge modules like Conv‑BatchNorm‑ReLU into a single fused operator. This improves numerical stability and reduces redundant quant‑dequant steps.</li>
      <li><strong>Observer Insertion</strong>: Invoke <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare"><code class="language-plaintext highlighter-rouge">torch.quantization.prepare</code></a> to automatically insert observer modules (e.g., <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver"><code class="language-plaintext highlighter-rouge">MinMaxObserver</code></a>). These record activation statistics during the calibration phase.</li>
      <li><strong>Calibration</strong>: Run representative real-world input data through the prepared model to collect min/max or histogram statistics via observers. Approximately 100‑200 mini‑batches often suffice for good calibration.</li>
      <li><strong>Conversion to Quantized Model</strong>: Use <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert"><code class="language-plaintext highlighter-rouge">torch.quantization.convert</code></a> to replace observed layers with quantized counterparts, applying pre-determined scales and zero points. The resulting model executes end‑to‑end in <code class="language-plaintext highlighter-rouge">int8</code> arithmetic.</li>
    </ul>
  </li>
</ul>

<h6 id="typical-benefits-1">Typical Benefits</h6>

<ul>
  <li>Model size is typically reduced by ≈4× (since <code class="language-plaintext highlighter-rouge">int8</code> requires only 1 byte per parameter instead of 4) and memory bandwidth requirements drop significantly.</li>
  <li>Inference latency improves—often 2× to 4× faster than float32—by eliminating repeated float‑int conversions and enabling optimized integer kernels on CPU and mobile.</li>
  <li>Enables uniform quantized execution across the network, which improves cache locality and enables hardware acceleration on supported platforms.</li>
</ul>

<h6 id="notes--tradeoffs-1">Notes &amp; Trade‑offs</h6>

<ul>
  <li>Requires a representative calibration dataset. If the input distribution drifts significantly, fixed quantization ranges may degrade accuracy over time.</li>
  <li>Slight accuracy loss compared to floating‑point baseline—although typically small (~1‑2%)—especially on highly non-linear or sensitive models. For critical accuracy use‑cases, Quantization Aware Training may be more suitable.</li>
  <li>Not all operators are supported for eager/static quantization. While convolution, linear, and RNN layers are supported, custom or unsupported layers may need manual handling or fallbacks. Per-channel quantization support is available, but requires proper qconfig settings.</li>
  <li>The quantization workflow in PyTorch uses either Eager Mode or FX Graph Mode. FX mode can automate fusion and support functional operators, but may require model refactoring. Eager Mode offers more manual control but with limited operator coverage.</li>
</ul>

<h4 id="quantizationaware-training-qat">Quantization‑aware Training (QAT)</h4>

<ul>
  <li>
    <p>QAT is the most accurate among PyTorch’s three quantization techniques for static quantization. With QAT, all weights and activations are subject to “fake quantization” during both forward and backward passes: values are rounded to simulate <code class="language-plaintext highlighter-rouge">int8</code> quantization, while computations remain in floating‑point. Consequently, weight updates occur with full awareness that the model will eventually operate in <code class="language-plaintext highlighter-rouge">int8</code>. As a result, models trained with QAT generally achieve higher post‑quantization accuracy than those produced by post‑training quantization or dynamic quantization.</p>
  </li>
  <li>
    <p>The principle is straightforward: the training process is informed about the ultimate quantized inference format. During training, activations and weights are rounded appropriately, so gradient flow reflects the quantization effects. However, the backpropagation itself—the gradient descent—is executed using full‑precision arithmetic.</p>
  </li>
  <li>
    <p>After QAT training and conversion, the final model stores both weights and activations in the <code class="language-plaintext highlighter-rouge">int8</code> quantized format, making it suitable for efficient inference on quantization-compatible hardware.</p>
  </li>
  <li>
    <p>To implement QAT in PyTorch’s eager‑mode workflow, one typically follows these steps:</p>

    <ol>
      <li>Fuse suitable modules (e.g. Conv+ReLU, Conv+BatchNorm) via <code class="language-plaintext highlighter-rouge">torch.quantization.fuse_modules</code>.</li>
      <li>Insert <code class="language-plaintext highlighter-rouge">QuantStub</code> and <code class="language-plaintext highlighter-rouge">DeQuantStub</code> modules to manage quantization boundaries.</li>
      <li>Assign <code class="language-plaintext highlighter-rouge">.qconfig</code> to modules—e.g. via <code class="language-plaintext highlighter-rouge">torch.quantization.get_default_qat_qconfig('fbgemm')</code> or <code class="language-plaintext highlighter-rouge">'qnnpack'</code>.</li>
      <li>Prepare the model using <code class="language-plaintext highlighter-rouge">torch.ao.quantization.prepare_qat()</code> or <code class="language-plaintext highlighter-rouge">torch.quantization.prepare_qat()</code>.</li>
      <li>Train or fine‑tune the model in training mode.</li>
      <li>After training, apply <code class="language-plaintext highlighter-rouge">torch.ao.quantization.convert()</code> or <code class="language-plaintext highlighter-rouge">torch.quantization.convert()</code> to produce the fully quantized <code class="language-plaintext highlighter-rouge">int8</code> model.</li>
    </ol>
  </li>
  <li>
    <p>A code snippet in PyTorch invoking QAT:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code9"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code9"><span class="n">qat_model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">qat_model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># train or fine‑tune qat_model ...
</span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">qat_model</span><span class="p">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>The fake quantization modules, which simulate the effects of quantization during both forward and backward passes, internally use observers (e.g., <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver"><code class="language-plaintext highlighter-rouge">MinMaxObserver</code></a> or <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver"><code class="language-plaintext highlighter-rouge">HistogramObserver</code></a>) to track activation and weight ranges during training. These fake quantization modules, which are inserted during training, are typically replaced with real quantized operators in the converted model.</li>
</ul>

<h5 id="quantizationaware-training-vs-post-training-quantization-vs-dynamic-quantization">Quantization‑aware Training vs. Post-Training Quantization vs. Dynamic Quantization</h5>

<ul>
  <li>Note that unlike <a href="#dynamic--runtime-quantization">dynamic quantization</a> (which only quantizes weights statically and activations on-the-fly during inference), QAT simulates quantization for both weights and activations during training. This allows the model to learn parameters that are robust to quantization-induced errors introduced at inference time. Put simply, it allows the parameters to adapt to quantization noise during inference and typically results in significantly better accuracy, especially for models with activation-sensitive layers such as convolutional networks.</li>
  <li>Furthermore, unlike <a href="#post-training-quantization">post-training quantization</a>, QAT does not require a separate calibration phase after training. Instead, it uses the observer modules during training itself to learn and track the necessary quantization parameters, effectively integrating calibration into the training loop.</li>
</ul>

<h5 id="example-workflow-2">Example Workflow</h5>

<ul>
  <li>This sub‑section illustrates a complete workflow for applying static QAT to a convolutional neural network (e.g. ResNet18):</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code10"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code10"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.quantization</span>
<span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">QuantStub</span><span class="p">,</span> <span class="n">DeQuantStub</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">DeQuantStub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 1. Load pre‑trained or float‑trained model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># 2. Fuse conv, bn, relu
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="s">'conv'</span><span class="p">,</span><span class="s">'bn'</span><span class="p">,</span><span class="s">'relu'</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 3. Attach QAT config
</span><span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>

<span class="c1"># 4. Prepare QAT
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 5. Fine‑tune QAT model
</span><span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1"># run training loop for several epochs ...
</span>
<span class="c1"># 6. Convert to quantized model
</span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># 7. Evaluate quantized_model for accuracy and inference performance
</span></code></pre></div></div>

<h6 id="workflow-explanation-2">Workflow Explanation</h6>

<ul>
  <li>This workflow enforces quantization effects during training by simulating rounding and clamping via fake quantization modules. <code class="language-plaintext highlighter-rouge">QuantStub</code> and <code class="language-plaintext highlighter-rouge">DeQuantStub</code> demarcate where data transitions between float and quantized domains. <code class="language-plaintext highlighter-rouge">qconfig</code> controls observer placement and quantization schemes (e.g. symmetric vs affine, per‑tensor vs per‑channel).</li>
  <li>Fake quantization is active during training, guiding the network to adapt to the constraints of int8 inference arithmetic. Only after fine‑tuning does <code class="language-plaintext highlighter-rouge">convert()</code> replace fake quant modules with actual quantized operators for efficient <code class="language-plaintext highlighter-rouge">int8</code> inference execution.</li>
</ul>

<h6 id="typical-benefits-2">Typical Benefits</h6>

<ul>
  <li>Higher quantized accuracy than post‑training static or dynamic quantization, often reducing performance degradation to minimal levels.</li>
  <li>Improved robustness to quantization noise, particularly important for convolutional networks and vision models.</li>
  <li>Retains compression benefits: reduced model size (≈25% of float model) and faster inference on hardware optimized for <code class="language-plaintext highlighter-rouge">int8</code>.</li>
</ul>

<h6 id="notes--tradeoffs-2">Notes &amp; Trade‑offs</h6>

<ul>
  <li>QAT requires additional training or fine‑tuning, increasing overall development time.</li>
  <li>Careful scheduling is needed: a small learning rate is recommended to avoid instability introduced by straight‑through estimator (STE) approximations.</li>
  <li>Model preparation steps such as layer fusion and correct placement of quant stubs are critical. Missing fusions can degrade accuracy.</li>
  <li>Not all operators or model architectures are fully quantization‑aware; some require manual adaptation.</li>
  <li>The quantized model behavior may differ subtly from the fake‑quant version: as reported, the output of a real quantized model may diverge slightly from fake‑quant during testing on toy models.</li>
</ul>

<h3 id="comparative-analysis">Comparative Analysis</h3>

<ul>
  <li>Here is a detailed comparative analysis of Dynamic Quantization, PTQ, and QAT:</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Aspect</strong></th>
<th class="tg-hcenter-valign-first"><strong>Dynamic / Runtime Quantization</strong></th>
<th class="tg-hcenter-valign-first"><strong>Post-Training Quantization (PTQ)</strong></th>
<th class="tg-hcenter-valign-second"><strong>Quantization-Aware Training (QAT)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Primary Use Case</td>
<td class="tg-tleft-valign-first">Fast, easy quantization of models with primarily linear operations (e.g. LSTM, BERT)</td>
<td class="tg-tleft-valign-first">Quantization of convolutional or more complex models with moderate accuracy tradeoff</td>
<td class="tg-tleft-valign-second">High-accuracy quantization for models sensitive to quantization (e.g. CNNs, object detectors)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Requires Retraining?</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-second">Yes</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Requires Calibration Data?</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-first">Yes (to collect activation statistics)</td>
<td class="tg-tleft-valign-second">No separate calibration; statistics are collected during training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">When Activations Are Quantized</td>
<td class="tg-tleft-valign-first">At runtime (dynamically before each operation)</td>
<td class="tg-tleft-valign-first">Statically using observer statistics from calibration</td>
<td class="tg-tleft-valign-second">During training using fake quantization modules</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Quantization of Weights</td>
<td class="tg-tleft-valign-first">Done ahead of time (static)</td>
<td class="tg-tleft-valign-first">Done ahead of time (static)</td>
<td class="tg-tleft-valign-second">Simulated during training, finalized during conversion</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Quantization of Activations</td>
<td class="tg-tleft-valign-first">Dynamically quantized at inference time</td>
<td class="tg-tleft-valign-first">Statically quantized using calibration ranges</td>
<td class="tg-tleft-valign-second">Simulated via fake quantization during training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Typical Accuracy</td>
<td class="tg-tleft-valign-first">Moderate loss (acceptable for linear-dominant models)</td>
<td class="tg-tleft-valign-first">Slight to moderate loss</td>
<td class="tg-tleft-valign-second">Minimal loss; best accuracy among all methods</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Complexity of Setup</td>
<td class="tg-tleft-valign-first">Very low</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-second">High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Computation Format During Training</td>
<td class="tg-tleft-valign-first">Full float32</td>
<td class="tg-tleft-valign-first">Full float32</td>
<td class="tg-tleft-valign-second">Simulated int8 via fake quantization in float32</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Final Inference Format</td>
<td class="tg-tleft-valign-first">int8 weights, float32 activations outside ops</td>
<td class="tg-tleft-valign-first">int8 weights and activations</td>
<td class="tg-tleft-valign-second">int8 weights and activations</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Deployment Readiness</td>
<td class="tg-tleft-valign-first">Easy and quick to apply, suitable for rapid deployment</td>
<td class="tg-tleft-valign-first">Requires calibration workflow</td>
<td class="tg-tleft-valign-second">Requires full training pipeline</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Main Benefit</td>
<td class="tg-tleft-valign-first">Faster inference via int8 compute; no data or retraining needed</td>
<td class="tg-tleft-valign-first">Reduced latency and memory with moderate setup</td>
<td class="tg-tleft-valign-second">Maximum accuracy with full int8 inference</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Target Operators</td>
<td class="tg-tleft-valign-first">Mostly linear layers (e.g. <code>nn.Linear</code>, <code>nn.LSTM</code>)</td>
<td class="tg-tleft-valign-first">Broad operator support with fused modules (e.g. <code>Conv+ReLU</code>)</td>
<td class="tg-tleft-valign-second">Full model coverage with operator fusion and training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Memory Footprint Reduction</td>
<td class="tg-tleft-valign-first">Partial (activations still float32)</td>
<td class="tg-tleft-valign-first">Full (weights and activations in int8)</td>
<td class="tg-tleft-valign-second">Full (weights and activations in int8)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Primary Optimization Goal</td>
<td class="tg-tleft-valign-first">Compute efficiency (faster matmuls), leading to latency savings</td>
<td class="tg-tleft-valign-first">Latency and memory savings</td>
<td class="tg-tleft-valign-second">Accuracy preservation under quantization</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Example Usage</td>
<td class="tg-tleft-valign-first"><code>torch.quantization.quantize_dynamic</code></td>
<td class="tg-tleft-valign-first"><code>prepare</code>, <code>calibrate</code>, <code>convert</code></td>
<td class="tg-tleft-valign-second"><code>prepare_qat</code>, <code>train</code>, <code>convert</code></td>
</tr>
</tbody>
</table>
</div>

<h3 id="compute-vs-memory-bottlenecks">Compute vs. Memory Bottlenecks</h3>

<ul>
  <li>
    <p>Deep learning performance is typically constrained by one of two primary bottlenecks: compute (arithmetic throughput) or memory (bandwidth and capacity). The balance between them depends on the hardware architecture, the model’s structure, and the numerical precision used.</p>
  </li>
  <li>
    <p><strong>Compute-Bound Workloads</strong>:</p>

    <ul>
      <li>
        <p>A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:</p>

        <ul>
          <li>Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).</li>
          <li>Dense layers and convolution layers with large channel counts and large kernel sizes.</li>
          <li>Transformer attention mechanisms with large batch sizes or long sequence lengths.</li>
        </ul>
      </li>
      <li>
        <p>In compute-bound scenarios, lowering the precision of operands (e.g., <code class="language-plaintext highlighter-rouge">float32</code> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-39-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-358" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-359"><span class="mo" id="MathJax-Span-360" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-39">\rightarrow</script> <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">int8</code>) allows hardware to execute more operations per clock cycle. For example:</p>

        <ul>
          <li>NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code> GEMMs compared to <code class="language-plaintext highlighter-rouge">float32</code>.</li>
          <li>Integer accelerators (e.g., <code class="language-plaintext highlighter-rouge">int8</code> SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.</li>
        </ul>
      </li>
      <li>
        <p>By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Memory-Bound Workloads</strong>:</p>

    <ul>
      <li>
        <p>A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:</p>

        <ul>
          <li>The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.</li>
          <li>Batch sizes are small, reducing the amount of computation per data load.</li>
          <li>Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.</li>
        </ul>
      </li>
      <li>
        <p>Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:</p>

        <ul>
          <li><strong>Reducing memory footprint:</strong> Lower precision reduces the size of weights and activations (e.g., <code class="language-plaintext highlighter-rouge">float32</code> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-40-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-361" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-362"><span class="mo" id="MathJax-Span-363" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-40">\rightarrow</script> <code class="language-plaintext highlighter-rouge">int8</code> cuts memory use by 75%).</li>
          <li><strong>Improving cache locality:</strong> More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.</li>
          <li><strong>Increasing effective bandwidth:</strong> Smaller data transfers mean more elements can be moved per memory transaction.</li>
        </ul>
      </li>
      <li>
        <p>On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Mixed Bottleneck Scenarios</strong>:</p>

    <ul>
      <li>
        <p>Many real-world models contain both compute-bound and memory-bound regions:</p>

        <ul>
          <li>Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.</li>
          <li>Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.</li>
          <li>Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.</li>
        </ul>
      </li>
      <li>
        <p>In such cases, <strong>mixed-precision quantization</strong> can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Where Quantization Delivers the Most Impact</strong>:</p>

    <ul>
      <li>
        <p>Quantization is most impactful when:</p>

        <ul>
          <li>The model runs on hardware with specialized low-precision units (Tensor Cores, <code class="language-plaintext highlighter-rouge">int8</code> MAC units, <code class="language-plaintext highlighter-rouge">float8</code> engines).</li>
          <li>Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).</li>
          <li>Model size exceeds cache capacity, leading to frequent DRAM access.</li>
          <li>Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>In summary, quantization addresses <strong>compute bottlenecks</strong> by enabling more operations per cycle and <strong>memory bottlenecks</strong> by reducing data transfer volume and improving cache utilization. Understanding which bottleneck dominates for a given layer or model is key to selecting the right quantization strategy.</p>
  </li>
</ul>

<h3 id="modern-quantization-techniques">Modern Quantization Techniques</h3>

<h4 id="gptq-quantization-with-second-order-error-compensation">GPTQ: Quantization with Second-Order Error Compensation</h4>

<ul>
  <li>
    <p>Introduced in <a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> by Frantar et al. (2023), GPTQ is a high-accuracy post-training quantization (PTQ) method tailored for large-scale transformers. Unlike round-to-nearest schemes, GPTQ minimizes the quantization error using approximate second-order information derived from the Hessian of the loss. It quantizes weights in a layer-wise fashion while updating unquantized weights to compensate for introduced error, achieving efficient <code class="language-plaintext highlighter-rouge">int3/4</code> quantization of models as large as OPT-175B or BLOOM-176B without finetuning. Practical implementations are available through <a href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a> and <a href="https://github.com/IST-DASLab/gptq">LLM.int4</a>.</p>
  </li>
  <li>
    <p>GPTQ significantly outperforms simple rounding methods by preserving perplexity under low-bit quantization. Notably, it is one of the few techniques that scales to 100B+ parameter models using modest compute (e.g., a single A100 GPU). While it focuses on weight-only quantization, activation quantization can be layered on top via orthogonal techniques such as SmoothQuant.</p>
  </li>
</ul>

<h5 id="process">Process</h5>

<ol>
  <li>
    <p><strong>Layer-Wise Quantization Objective</strong></p>

    <ul>
      <li>
        <p>For a linear layer with weight matrix <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-41-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-364" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-365"><span class="mi" id="MathJax-Span-366" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-41">W</script> and input activations <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-42-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-367" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-368"><span class="mi" id="MathJax-Span-369" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></span></span><script type="math/tex" id="MathJax-Element-42">X</script>, GPTQ minimizes the reconstruction error after quantization:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-43-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x005E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x005E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msubsup&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-370" style="width: 9.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1007.55em, 3.336em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-371"><span class="munderover" id="MathJax-Span-372"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-373" style="font-family: STIXGeneral-Regular;">min</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.128em, 1000.63em, 4.273em, -999.997em); top: -3.122em; left: 0.471em;"><span class="texatom" id="MathJax-Span-374"><span class="mrow" id="MathJax-Span-375"><span class="texatom" id="MathJax-Span-376"><span class="mrow" id="MathJax-Span-377"><span class="munderover" id="MathJax-Span-378"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-379" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.216em; left: 0.263em;"><span style="height: 0em; vertical-align: 0em; width: 0.263em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-380" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">̂&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.154em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-381" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">‖</span><span class="mi" id="MathJax-Span-382" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-383" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-384" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="texatom" id="MathJax-Span-385" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-386"><span class="munderover" id="MathJax-Span-387"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-388" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.268em; left: 0.367em;"><span style="height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-389" style="font-family: STIXGeneral-Regular;">̂&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span class="mi" id="MathJax-Span-390" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-391"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-392" style="font-family: STIXGeneral-Regular;">‖</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;"><span class="mn" id="MathJax-Span-393" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;"><span class="mn" id="MathJax-Span-394" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.503em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>W</mi><mo stretchy="false">^</mo></mover></mrow></mrow></munder><mo fence="false" stretchy="false">‖</mo><mi>W</mi><mi>X</mi><mo>−</mo><mrow class="MJX-TeXAtom-ORD"><mover><mi>W</mi><mo stretchy="false">^</mo></mover></mrow><mi>X</mi><msubsup><mo fence="false" stretchy="false">‖</mo><mn>2</mn><mn>2</mn></msubsup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-43">\min_{\hat{W}} \| WX - \hat{W}X \|_2^2</script>
      </li>
      <li>
        <p>Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Approximate Second-Order Weight Selection (OBQ Foundation)</strong></p>

    <ul>
      <li>
        <p>GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-44-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-395" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.1em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-396"><span class="msubsup" id="MathJax-Span-397"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-398" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-399" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mi>q</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-44">w_q</script> to quantize by minimizing its induced error, scaled by its Hessian diagonal element:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-45-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;arg&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mtext&gt;quant&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mi&gt;&amp;#x03B4;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mtext&gt;quant&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-400" style="width: 32.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 27.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.419em, 1027.03em, 3.388em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-401"><span class="msubsup" id="MathJax-Span-402"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-403" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-404" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-405" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-406" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">arg</span><span class="mo" id="MathJax-Span-407"></span><span class="mo" id="MathJax-Span-408" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">min</span><span class="mfrac" id="MathJax-Span-409" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 7.503em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.023em, 1007.4em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.695em;"><span class="mrow" id="MathJax-Span-410"><span class="mo" id="MathJax-Span-411" style="font-family: STIXGeneral-Regular;">(</span><span class="mtext" id="MathJax-Span-412" style="font-family: STIXGeneral-Regular;">quant</span><span class="mo" id="MathJax-Span-413" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-414"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-415" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-416" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-417" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-418" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="msubsup" id="MathJax-Span-419" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-420" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-421" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-422"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-423" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.315em;"><span class="mn" id="MathJax-Span-424" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;"><span class="mrow" id="MathJax-Span-425"><span class="mo" id="MathJax-Span-426" style="font-family: STIXGeneral-Regular;">[</span><span class="msubsup" id="MathJax-Span-427"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-428" style="font-family: STIXGeneral-Italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.32em; left: 0.836em;"><span class="texatom" id="MathJax-Span-429"><span class="mrow" id="MathJax-Span-430"><span class="mo" id="MathJax-Span-431" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-432" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-433"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-434" style="font-family: STIXGeneral-Regular;">]</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.315em;"><span class="texatom" id="MathJax-Span-435"><span class="mrow" id="MathJax-Span-436"><span class="mi" id="MathJax-Span-437" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span class="mi" id="MathJax-Span-438" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1007.5em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.503em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-439" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-440" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-441" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">δ</span><span class="mo" id="MathJax-Span-442" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-443" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">−</span><span class="mfrac" id="MathJax-Span-444"><span style="display: inline-block; position: relative; width: 6.409em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.18em, 1006.25em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.122em;"><span class="mrow" id="MathJax-Span-445"><span class="msubsup" id="MathJax-Span-446"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-447" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-448" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-449" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mtext" id="MathJax-Span-450" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">quant</span><span class="mo" id="MathJax-Span-451" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-452"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-453" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-454" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-455" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;"><span class="mrow" id="MathJax-Span-456"><span class="mo" id="MathJax-Span-457" style="font-family: STIXGeneral-Regular;">[</span><span class="msubsup" id="MathJax-Span-458"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-459" style="font-family: STIXGeneral-Italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.32em; left: 0.836em;"><span class="texatom" id="MathJax-Span-460"><span class="mrow" id="MathJax-Span-461"><span class="mo" id="MathJax-Span-462" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-463" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-464"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-465" style="font-family: STIXGeneral-Regular;">]</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.315em;"><span class="texatom" id="MathJax-Span-466"><span class="mrow" id="MathJax-Span-467"><span class="mi" id="MathJax-Span-468" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span class="mi" id="MathJax-Span-469" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1006.41em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.409em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-470" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-471"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-472" style="font-family: STIXGeneral-Italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.836em;"><span class="texatom" id="MathJax-Span-473"><span class="mrow" id="MathJax-Span-474"><span class="mo" id="MathJax-Span-475" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-476" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-477"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-478" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.315em;"><span class="texatom" id="MathJax-Span-479"><span class="mrow" id="MathJax-Span-480"><span class="mo" id="MathJax-Span-481" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">:</span><span class="mo" id="MathJax-Span-482" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-483" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.309em; border-left: 0px solid; width: 0px; height: 3.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>w</mi><mi>q</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><mo movablelimits="true" form="prefix">min</mo><mfrac><mrow><mo stretchy="false">(</mo><mtext>quant</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>w</mi><mi>q</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><mo stretchy="false">[</mo><msup><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy="false">]</mo><mrow class="MJX-TeXAtom-ORD"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo>,</mo><mspace width="1em"></mspace><mi>δ</mi><mo>=</mo><mo>−</mo><mfrac><mrow><msub><mi>w</mi><mi>q</mi></msub><mo>−</mo><mtext>quant</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">[</mo><msup><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy="false">]</mo><mrow class="MJX-TeXAtom-ORD"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo stretchy="false">(</mo><msup><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo>:</mo><mo>,</mo><mi>q</mi></mrow></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-45">w_q = \arg\min \frac{(\text{quant}(w_q) - w_q)^2}{[H^{-1}]_{qq}}, \quad \delta = -\frac{w_q - \text{quant}(w_q)}{[H^{-1}]_{qq}} (H^{-1})_{:,q}</script>
      </li>
      <li>
        <p>The inverse Hessian <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-46-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BB;&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-484" style="width: 10.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.117em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1009.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-485"><span class="msubsup" id="MathJax-Span-486"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-487" style="font-family: STIXGeneral-Italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.836em;"><span class="texatom" id="MathJax-Span-488"><span class="mrow" id="MathJax-Span-489"><span class="mo" id="MathJax-Span-490" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-491" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-492" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-493" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mn" id="MathJax-Span-494" style="font-family: STIXGeneral-Regular;">2</span><span class="mi" id="MathJax-Span-495" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-496"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-497" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="mi" id="MathJax-Span-498" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">⊤</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-499" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-500" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">λ</span><span class="mi" id="MathJax-Span-501" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-502"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-503" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.315em;"><span class="texatom" id="MathJax-Span-504"><span class="mrow" id="MathJax-Span-505"><span class="mo" id="MathJax-Span-506" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-507" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant="normal">⊤</mi></msup><mo>+</mo><mi>λ</mi><mi>I</mi><msup><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-46">H^{-1} = (2X X^\top + \lambda I)^{-1}</script> captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Blockwise Column Quantization with Shared Hessian</strong></p>

    <ul>
      <li>
        <p>GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.</p>
      </li>
      <li>
        <p>This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-47-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;row&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;col&lt;/mtext&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-508" style="width: 6.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.42em, 2.711em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-509"><span class="mi" id="MathJax-Span-510" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-511" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-512"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-513" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-514"><span class="mrow" id="MathJax-Span-515"><span class="mtext" id="MathJax-Span-516" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">row</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-517" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-518" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-519" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;"><span class="mn" id="MathJax-Span-520" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;"><span class="texatom" id="MathJax-Span-521"><span class="mrow" id="MathJax-Span-522"><span class="mtext" id="MathJax-Span-523" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">col</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-524" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msub><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-47">O(d_{\text{row}} \cdot d_{\text{col}}^3)</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-48-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;row&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;col&lt;/mtext&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;col&lt;/mtext&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-525" style="width: 11.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.794em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1009.74em, 2.711em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-526"><span class="mi" id="MathJax-Span-527" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-528" style="font-family: STIXGeneral-Regular;">(</span><span class="mo" id="MathJax-Span-529" style="font-family: STIXGeneral-Regular;">max</span><span class="mo" id="MathJax-Span-530" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-531"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-532" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-533"><span class="mrow" id="MathJax-Span-534"><span class="mtext" id="MathJax-Span-535" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">row</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-536" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-537" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-538" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;"><span class="mn" id="MathJax-Span-539" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;"><span class="texatom" id="MathJax-Span-540"><span class="mrow" id="MathJax-Span-541"><span class="mtext" id="MathJax-Span-542" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">col</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-543" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-544" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-545" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;"><span class="mn" id="MathJax-Span-546" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;"><span class="texatom" id="MathJax-Span-547"><span class="mrow" id="MathJax-Span-548"><span class="mtext" id="MathJax-Span-549" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">col</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-550" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-551" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><msub><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-48">O(\max(d_{\text{row}} \cdot d_{\text{col}}^2, d_{\text{col}}^3))</script>.</p>
      </li>
      <li>
        <p>The following figure (<a href="https://arxiv.org/abs/2210.17323">source</a>) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.</p>
      </li>
    </ul>

    <p><img src="../assets/model-compression/GPTQ.jpg" alt="GPTQ Figure 2 - Quantization Procedure"></p>
  </li>
  <li>
    <p><strong>Lazy Batch Updates for GPU Efficiency</strong></p>

    <ul>
      <li>To alleviate the memory-bandwidth bottleneck of GPU kernels, GPTQ processes batches of columns (e.g., 128) before updating the full weight matrix.</li>
      <li>This “lazy update” scheme postpones weight and Hessian modifications until a full block has been processed, improving throughput and parallelism.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cholesky-Based Inversion for Numerical Stability</strong></p>

    <ul>
      <li>
        <p>To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-49-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mtext&gt;computed once per block&lt;/mtext&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-552" style="width: 19.69em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1016.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-553"><span class="msubsup" id="MathJax-Span-554"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-555" style="font-family: STIXGeneral-Italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.836em;"><span class="texatom" id="MathJax-Span-556"><span class="mrow" id="MathJax-Span-557"><span class="mo" id="MathJax-Span-558" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-559" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-560" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-561" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msubsup" id="MathJax-Span-562"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-563" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.628em;"><span class="mi" id="MathJax-Span-564" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">⊤</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-565" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-566" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-567" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">computed once per block</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi mathvariant="normal">⊤</mi></msup><mo>,</mo><mspace width="1em"></mspace><mtext>computed once per block</mtext></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-49">H^{-1} = L L^\top, \quad \text{computed once per block}</script>
      </li>
      <li>
        <p>Combined with dampening (adding <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-50-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BB;&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-568" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.89em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-569"><span class="mi" id="MathJax-Span-570" style="font-family: STIXGeneral-Italic;">λ</span><span class="mi" id="MathJax-Span-571" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi><mi>I</mi></math></span></span><script type="math/tex" id="MathJax-Element-50">\lambda I</script> to the Hessian), this ensures stability across very large models (e.g., &gt;100B parameters).</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization Scheme</strong></p>

    <ul>
      <li>GPTQ supports asymmetric per-row quantization with <code class="language-plaintext highlighter-rouge">int4</code> or <code class="language-plaintext highlighter-rouge">int3</code> bitwidths.</li>
      <li>
        <p>The quantization grid is fixed via min/max values per row, and weights are quantized using:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-51-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x005E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mtext&gt;Round&lt;/mtext&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-572" style="width: 16.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(2.346em, 1013.6em, 4.794em, -999.997em); top: -3.799em; left: 0em;"><span class="mrow" id="MathJax-Span-573"><span class="msubsup" id="MathJax-Span-574"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-575"><span class="mrow" id="MathJax-Span-576"><span class="munderover" id="MathJax-Span-577"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-578" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.159em;"><span style="height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-579" style="font-family: STIXGeneral-Regular;">̂&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-580"><span class="mrow" id="MathJax-Span-581"><span class="mi" id="MathJax-Span-582" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span class="mi" id="MathJax-Span-583" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">j<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-584" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-585" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-586" style="font-family: STIXGeneral-Regular;">Δ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-587" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-588" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mtext" id="MathJax-Span-589" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">Round</span><span class="mrow" id="MathJax-Span-590" style="padding-left: 0.211em;"><span class="mo" id="MathJax-Span-591" style="vertical-align: -0.466em;"><span><span style="font-size: 111%; font-family: STIXSizeTwoSym;">(</span></span></span><span class="mfrac" id="MathJax-Span-592"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.284em, 1003.02em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -1.508em;"><span class="mrow" id="MathJax-Span-593"><span class="msubsup" id="MathJax-Span-594"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-595" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-596"><span class="mrow" id="MathJax-Span-597"><span class="mi" id="MathJax-Span-598" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span class="mi" id="MathJax-Span-599" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">j<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-600" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="msubsup" id="MathJax-Span-601" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-602" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-603" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.518em;"><span class="msubsup" id="MathJax-Span-604"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-605" style="font-family: STIXGeneral-Regular;">Δ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-606" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-607" style="vertical-align: -0.466em;"><span><span style="font-size: 111%; font-family: STIXSizeTwoSym;">)</span></span></span></span><span class="mo" id="MathJax-Span-608" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="msubsup" id="MathJax-Span-609" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-610" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-611" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.805em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.691em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub><mo>⋅</mo><mtext>Round</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-51">\hat{w}_{ij} = \Delta_i \cdot \text{Round} \left( \frac{w_{ij} - z_i}{\Delta_i} \right) + z_i</script>

        <ul>
          <li>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-52-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-612" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-613"><span class="msubsup" id="MathJax-Span-614"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-615" style="font-family: STIXGeneral-Regular;">Δ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-616" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-52">\Delta_i</script> is the scale and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-53-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-617" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-618"><span class="msubsup" id="MathJax-Span-619"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-620" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-621" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-53">z_i</script> the zero point for row <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-54-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-622" style="width: 0.315em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-623"><span class="mi" id="MathJax-Span-624" style="font-family: STIXGeneral-Italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-54">i</script>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation &amp; Runtime</strong></p>

    <ul>
      <li>Full quantization of a 175B parameter model (OPT-175B or BLOOM-176B) takes ~4 GPU hours on an NVIDIA A100 using 128 calibration samples from C4.</li>
      <li>Quantization is applied layer-wise with minimal memory overhead by reloading and processing one transformer block at a time.</li>
      <li>Models quantized with GPTQ can run on a single GPU, achieving up to 4.5× speedups over <code class="language-plaintext highlighter-rouge">float16</code> baselines.</li>
    </ul>
  </li>
</ol>

<h5 id="pros">Pros</h5>

<ul>
  <li><strong>Highly accurate</strong>: Maintains perplexity within 0.03 of <code class="language-plaintext highlighter-rouge">float16</code> for OPT-175B (<code class="language-plaintext highlighter-rouge">int4</code>) and tolerable degradation at <code class="language-plaintext highlighter-rouge">int3</code>.</li>
  <li><strong>Scalable</strong>: Efficient enough to quantize 100B+ parameter models using a single GPU.</li>
  <li><strong>Hardware-efficient</strong>: Enables deployment of massive LLMs on consumer-grade GPUs (e.g., RTX 3090).</li>
  <li><strong>Open-source tooling</strong>: Supported by AutoGPTQ, Hugging Face Transformers, and integrations with <code class="language-plaintext highlighter-rouge">load_in_4bit=True</code>.</li>
</ul>

<h5 id="cons">Cons</h5>

<ul>
  <li><strong>Weight-only</strong>: Does not include activation quantization; activation memory remains in <code class="language-plaintext highlighter-rouge">float16</code> unless combined with other methods.</li>
  <li><strong>Nontrivial math</strong>: Relies on Hessian approximations and matrix inversions, which may complicate custom implementation or adaptation.</li>
  <li><strong>Challenging for non-standard layers</strong>: Works best with standard linear layers; adaptation for fused or exotic architectures may require modification.</li>
</ul>

<h4 id="smoothquant">SmoothQuant</h4>

<ul>
  <li>Introduced in <a href="https://arxiv.org/abs/2211.10438">SmoothQuant: Accurate and Efficient Post‑Training Quantization for Large Language Models</a> by Xiao et al. (2022), SmoothQuant enables uniform 8-bit quantization for both weights and activations (W8A8) in LLMs by balancing the quantization difficulty between them. It allows high-accuracy post-training quantization (PTQ) on transformer architectures without requiring fine-tuning. A high-level explanation is also available via <a href="https://leimao.github.io/blog/SmoothQuant-LLM-Quantization/">Lei Mao’s Log Book</a>.</li>
</ul>

<h5 id="process-1">Process</h5>

<ol>
  <li>
    <p><strong>Analyze Activation Outliers</strong>:
Activation tensors in LLMs often have long-tailed distributions, leading to a high dynamic range. These outliers cause large quantization errors when mapping to low-bit formats like <code class="language-plaintext highlighter-rouge">int8</code>.</p>
  </li>
  <li>
    <p><strong>Offline Scaling of Input and Weights</strong>:
To reduce activation outliers, SmoothQuant proposes to pre-scale input activations and inversely scale the associated weight matrices before quantization. This is done by computing per-channel maximum absolute values of activation tensors and applying the following scaling transformation:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display"><span class="MathJax MathJax_FullWidth" id="MathJax-Element-55-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-625" style="width: 100%; display: inline-block; min-width: 7.711em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(2.867em, 1006.36em, 6.253em, -999.997em); top: -4.112em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-626"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(2.763em, 1004.48em, 4.846em, -999.997em); top: -4.008em; left: 50%; margin-left: -2.237em;"><span class="msubsup" id="MathJax-Span-627"><span style="display: inline-block; position: relative; width: 2.346em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-628" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="texatom" id="MathJax-Span-629"><span class="mrow" id="MathJax-Span-630"><span class="mi" id="MathJax-Span-631" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">s</span><span class="mi" id="MathJax-Span-632" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span class="mi" id="MathJax-Span-633" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">a</span><span class="mi" id="MathJax-Span-634" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-635" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-636" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-637" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mfrac" id="MathJax-Span-638" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-639" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-640" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.18em, 1006.36em, 4.326em, -999.997em); top: -2.185em; left: 50%; margin-left: -3.174em;"><span class="mspace" id="MathJax-Span-641" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-642"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-643" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-644"><span class="mrow" id="MathJax-Span-645"><span class="mi" id="MathJax-Span-646" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">s</span><span class="mi" id="MathJax-Span-647" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span class="mi" id="MathJax-Span-648" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">a</span><span class="mi" id="MathJax-Span-649" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-650" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-651" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-652" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-653" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-654" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">∗</span><span class="mi" id="MathJax-Span-655" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.117em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -2.434em; border-left: 0px solid; width: 0px; height: 3.878em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mspace linebreak="newline"></mspace><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>W</mi><mo>∗</mo><mi>s</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-55">x_{scaled} = \frac{x}{s} \\
W_{scaled} = W * s</script>

    <ul>
      <li>Here, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-56-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-656" style="width: 0.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-657"><span class="mi" id="MathJax-Span-658" style="font-family: STIXGeneral-Italic;">s</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></span></span><script type="math/tex" id="MathJax-Element-56">s</script> is the scaling factor (per input channel), <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-57-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-659" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-660"><span class="mi" id="MathJax-Span-661" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-57">x</script> is the activation input, and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-58-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-662" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-663"><span class="mi" id="MathJax-Span-664" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-58">W</script> is the weight matrix.</li>
      <li>
        <p>This transformation preserves the original linear operation because:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-59-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;@&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;@&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-665" style="width: 10.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.065em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.992em, 1009.01em, 3.076em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-666"><span class="mi" id="MathJax-Span-667" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-668"><span class="mrow" id="MathJax-Span-669"><span class="mo" id="MathJax-Span-670" style="font-family: STIXGeneral-Regular;">@</span></span></span><span class="mi" id="MathJax-Span-671" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-672" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mo" id="MathJax-Span-673" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mfrac" id="MathJax-Span-674"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-675" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-676" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-677" style="font-family: STIXGeneral-Regular;">)</span><span class="texatom" id="MathJax-Span-678"><span class="mrow" id="MathJax-Span-679"><span class="mo" id="MathJax-Span-680" style="font-family: STIXGeneral-Regular;">@</span></span></span><span class="mo" id="MathJax-Span-681" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-682" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-683" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">∗</span><span class="mi" id="MathJax-Span-684" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">s</span><span class="mo" id="MathJax-Span-685" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo>@</mo></mrow><mi>W</mi><mo>≈</mo><mo stretchy="false">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo>@</mo></mrow><mo stretchy="false">(</mo><mi>W</mi><mo>∗</mo><mi>s</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-59">x @ W ≈ (\frac{x}{s}) @ (W * s)</script>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Quantize Scaled Tensors</strong>:
Apply standard post-training quantization (e.g., <code class="language-plaintext highlighter-rouge">torch.quantize_per_tensor</code>) on the scaled weight and activation tensors using uniform <code class="language-plaintext highlighter-rouge">int8</code> quantization.</p>
  </li>
  <li>
    <p><strong>No Runtime Overhead</strong>:
The inverse scaling factors <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-60-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-686" style="width: 0.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-687"><span class="mi" id="MathJax-Span-688" style="font-family: STIXGeneral-Italic;">s</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></span></span><script type="math/tex" id="MathJax-Element-60">s</script> are folded into the preceding layers during offline preprocessing. At inference, the quantized model does not require additional computation to reverse the scaling—hence, preserving speed.</p>
  </li>
</ol>

<h5 id="pros-1">Pros</h5>

<ul>
  <li>Training-free and calibration-light—requires only a few batches of representative data for statistics.</li>
  <li>Allows fully static W8A8 quantization for transformers, which were previously hard to quantize due to activation outliers.</li>
  <li>Compatible with major LLMs like Llama, OPT, BLOOM, and GLM.</li>
  <li>Hardware-friendly: <code class="language-plaintext highlighter-rouge">int8</code> inference is highly optimized on modern CPUs (e.g., VNNI, AMX) and GPUs.</li>
  <li>
    <p>Achieves substantial efficiency improvements:</p>

    <ul>
      <li>~2× reduction in memory footprint.</li>
      <li>~1.5× to 2× speedup on supported backends.</li>
      <li>&lt;0.5% accuracy loss on common NLP benchmarks.</li>
    </ul>
  </li>
</ul>

<h5 id="cons-1">Cons</h5>

<ul>
  <li>Limited to 8-bit formats—does not address extreme quantization (e.g., 4-bit or binary).</li>
  <li>Effectiveness depends on the distribution of activations. Improper scaling (e.g., due to poor calibration data) may still degrade performance.</li>
  <li>Static scale determination can be fragile in models with dynamic context (e.g., prompts of variable length).</li>
</ul>

<h4 id="activation-aware-weight-quantization-awq">Activation-Aware Weight Quantization (AWQ)</h4>

<ul>
  <li>Introduced in <a href="https://arxiv.org/abs/2306.00978">AWQ: Activation‑aware Weight Quantization for LLM Compression and Acceleration</a> by Lin et al. (2024), AWQ is a post‑training, weight‑only quantization technique tailored for LLMs. It identifies and protects <em>salient channels</em>—those with large activations—via per‑channel scaling derived from calibration, enabling accurate <strong><code class="language-plaintext highlighter-rouge">int4</code>/<code class="language-plaintext highlighter-rouge">int3</code></strong> quantization without retraining. Reference implementation (AutoAWQ and CUDA kernels) is available on <a href="https://github.com/mit-han-lab/llm-awq">GitHub</a>.</li>
  <li>AWQ offers a highly practical and accurate low-bit, weight-only quantization path. By using activation statistics to protect the most critical channels, it achieves near full-precision accuracy under <code class="language-plaintext highlighter-rouge">int4/3</code> quantization without training. It retains the efficiency of group-wise kernels for deployment while minimizing model size and speeding up inference for LLMs suited to edge and GPU environments.</li>
</ul>

<h5 id="process-2">Process</h5>

<ol>
  <li>
    <p><strong>Calibration Pass</strong></p>

    <ul>
      <li>Run a <em>small calibration dataset</em> through the unquantized model to gather per-channel activation statistics, typically the expected absolute value <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-61-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-689" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1002.45em, 2.711em, -999.997em); top: -2.341em; left: 0em;"><span class="mrow" id="MathJax-Span-690"><span class="texatom" id="MathJax-Span-691"><span class="mrow" id="MathJax-Span-692"><span class="mi" id="MathJax-Span-693" style="font-family: STIXGeneral-Regular;">𝔼</span></span></span><span class="mo" id="MathJax-Span-694" style="font-family: STIXGeneral-Regular;">[</span><span class="mo" id="MathJax-Span-695" style="font-family: STIXGeneral-Regular;">‖</span><span class="msubsup" id="MathJax-Span-696"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-697" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-698" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-699" style="font-family: STIXGeneral-Regular;">‖</span><span class="mo" id="MathJax-Span-700" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.346em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mo stretchy="false">[</mo><mo fence="false" stretchy="false">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="false" stretchy="false">‖</mo><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-61">\mathbb{E}[\|x_i\|]</script> of input to each weight channel <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-62-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-701" style="width: 0.315em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-702"><span class="mi" id="MathJax-Span-703" style="font-family: STIXGeneral-Italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-62">i</script>.</li>
    </ul>
  </li>
  <li>
    <p><strong>Group-Wise Weight Quantization Baseline</strong></p>

    <ul>
      <li>Use group size <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-63-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-704" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-705"><span class="mi" id="MathJax-Span-706" style="font-family: STIXGeneral-Italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-63">G</script> (e.g. 32 channels) to quantize weights <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-64-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-707" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.878em, 1000.73em, 2.659em, -999.997em); top: -2.497em; left: 0em;"><span class="mrow" id="MathJax-Span-708"><span class="texatom" id="MathJax-Span-709"><span class="mrow" id="MathJax-Span-710"><span class="mi" id="MathJax-Span-711" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.503em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">w</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-64">\mathbf{w}</script> via a uniform symmetric scheme:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-65-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mtext&gt;Round&lt;/mtext&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-712" style="width: 21.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 17.815em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.732em, 1017.82em, 3.388em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-713"><span class="mi" id="MathJax-Span-714" style="font-family: STIXGeneral-Italic;">Q</span><span class="mo" id="MathJax-Span-715" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-716" style="font-family: STIXGeneral-Italic;">w</span><span class="mo" id="MathJax-Span-717" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-718" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-719" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">Δ</span><span class="mo" id="MathJax-Span-720" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mtext" id="MathJax-Span-721" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">Round</span><span class="mrow" id="MathJax-Span-722" style="padding-left: 0.211em;"><span class="mo" id="MathJax-Span-723" style="font-family: STIXGeneral-Regular;">(</span><span class="mrow" id="MathJax-Span-724"><span class="mi" id="MathJax-Span-725" style="font-family: STIXGeneral-Italic;">w</span><span class="texatom" id="MathJax-Span-726"><span class="mrow" id="MathJax-Span-727"><span class="mo" id="MathJax-Span-728" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mi" id="MathJax-Span-729" style="font-family: STIXGeneral-Regular;">Δ</span></span><span class="mo" id="MathJax-Span-730" style="font-family: STIXGeneral-Regular;">)</span></span><span class="mo" id="MathJax-Span-731" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">,</span><span class="mspace" id="MathJax-Span-732" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-733" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">Δ</span><span class="mo" id="MathJax-Span-734" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mfrac" id="MathJax-Span-735" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.18em, 1003.13em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.612em;"><span class="mrow" id="MathJax-Span-736"><span class="mo" id="MathJax-Span-737" style="font-family: STIXGeneral-Regular;">max</span><span class="texatom" id="MathJax-Span-738" style="padding-left: 0.211em;"><span class="mrow" id="MathJax-Span-739"><span class="mo" id="MathJax-Span-740" style="font-family: STIXVariants;">|</span></span></span><span class="mi" id="MathJax-Span-741" style="font-family: STIXGeneral-Italic;">w</span><span class="texatom" id="MathJax-Span-742"><span class="mrow" id="MathJax-Span-743"><span class="mo" id="MathJax-Span-744" style="font-family: STIXVariants;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.971em, 1003.39em, 4.326em, -999.997em); top: -3.174em; left: 50%; margin-left: -1.716em;"><span class="mrow" id="MathJax-Span-745"><span class="msubsup" id="MathJax-Span-746"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-747" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-748"><span class="mrow" id="MathJax-Span-749"><span class="mi" id="MathJax-Span-750" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">b</span><span class="mo" id="MathJax-Span-751" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-752" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-753" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mn" id="MathJax-Span-754" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1003.6em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.596em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.941em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Q</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">Δ</mi><mo>⋅</mo><mtext>Round</mtext><mrow><mo>(</mo><mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi mathvariant="normal">Δ</mi></mrow><mo>)</mo></mrow><mo>,</mo><mspace width="1em"></mspace><mi mathvariant="normal">Δ</mi><mo>=</mo><mfrac><mrow><mo movablelimits="true" form="prefix">max</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>b</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-65">Q(w) = \Delta \cdot \text{Round}\left(w / \Delta\right), \quad \Delta = \frac{\max |w|}{2^{b-1}-1}</script>

    <ul>
      <li>The quantization error in group-wise quantization is proportional to input activation magnitude rather than weight magnitude alone.</li>
    </ul>
  </li>
  <li>
    <p><strong>Compute Activation-Based Scaling Factors</strong></p>

    <ul>
      <li>Let <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-66-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-755" style="width: 6.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1005.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-756"><span class="msubsup" id="MathJax-Span-757"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-758" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.89em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;"><span class="texatom" id="MathJax-Span-759"><span class="mrow" id="MathJax-Span-760"><span class="mo" id="MathJax-Span-761" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-762" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-763" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-764" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-765" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-766" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-767"><span class="mrow" id="MathJax-Span-768"><span class="mi" id="MathJax-Span-769" style="font-family: STIXGeneral-Regular;">𝔼</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.628em;"><span class="mi" id="MathJax-Span-770" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-771" style="font-family: STIXGeneral-Regular;">[</span><span class="mo" id="MathJax-Span-772" style="font-family: STIXGeneral-Regular;">‖</span><span class="msubsup" id="MathJax-Span-773"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-774" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-775" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-776" style="font-family: STIXGeneral-Regular;">‖</span><span class="mo" id="MathJax-Span-777" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mi>i</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mi>x</mi></msub><mo stretchy="false">[</mo><mo fence="false" stretchy="false">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="false" stretchy="false">‖</mo><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-66">s_i^{(x)} = \mathbb{E}_x [\|x_i\|]</script> represent average per-channel activation magnitude.</li>
      <li>For scaling exponent <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-67-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-778" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1003.86em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-779"><span class="mi" id="MathJax-Span-780" style="font-family: STIXGeneral-Italic;">α</span><span class="mo" id="MathJax-Span-781" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="mo" id="MathJax-Span-782" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">[</span><span class="mn" id="MathJax-Span-783" style="font-family: STIXGeneral-Regular;">0</span><span class="mo" id="MathJax-Span-784" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-785" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">1</span><span class="mo" id="MathJax-Span-786" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-67">\alpha \in [0,1]</script>, define:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-68-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-787" style="width: 5.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1004.85em, 2.659em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-788"><span class="msubsup" id="MathJax-Span-789"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-790" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;"><span class="texatom" id="MathJax-Span-791"><span class="mrow" id="MathJax-Span-792"><span class="mo" id="MathJax-Span-793" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-794" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span class="mo" id="MathJax-Span-795" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-796" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-797" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-798" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="msubsup" id="MathJax-Span-799"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-800" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.89em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;"><span class="texatom" id="MathJax-Span-801"><span class="mrow" id="MathJax-Span-802"><span class="mo" id="MathJax-Span-803" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-804" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-805" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-806" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-807"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-808" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.315em;"><span class="mi" id="MathJax-Span-809" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>s</mi><mi>i</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false">(</mo><msubsup><mi>s</mi><mi>i</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msubsup><msup><mo stretchy="false">)</mo><mi>α</mi></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-68">s_i^{(\alpha)} = (s_i^{(x)})^\alpha</script>

    <ul>
      <li>Use a <strong>small grid search</strong> over <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-69-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-810" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-811"><span class="mi" id="MathJax-Span-812" style="font-family: STIXGeneral-Italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-69">\alpha</script> to minimize an approximate MSE:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-70-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo symmetric=&quot;true&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-OPEN&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;(&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-CLOSE&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mo symmetric=&quot;true&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-813" style="width: 18.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.211em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.628em, 1015.21em, 2.919em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-814"><span class="msubsup" id="MathJax-Span-815"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-816"><span class="mrow" id="MathJax-Span-817"><span class="mi" id="MathJax-Span-818" style="font-family: STIXGeneral-Regular;">𝔼</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.628em;"><span class="mi" id="MathJax-Span-819" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-820"><span style="display: inline-block; position: relative; width: 14.221em; height: 0px;"><span style="position: absolute; clip: rect(2.711em, 1013.65em, 4.794em, -999.997em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-821"><span class="mo" id="MathJax-Span-822" style="vertical-align: 1.148em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: STIXGeneral-Regular; top: -2.445em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: STIXGeneral-Regular; position: absolute; top: -2.862em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-823"><span class="mi" id="MathJax-Span-824" style="font-family: STIXGeneral-Italic;">Q</span><span class="mo" id="MathJax-Span-825" style="font-family: STIXGeneral-Regular;">(</span><span class="texatom" id="MathJax-Span-826"><span class="mrow" id="MathJax-Span-827"><span class="mi" id="MathJax-Span-828" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span class="mo" id="MathJax-Span-829" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="texatom" id="MathJax-Span-830" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-831"><span class="mi" id="MathJax-Span-832" style="font-family: STIXGeneral-Regular;">d</span><span class="mi" id="MathJax-Span-833" style="font-family: STIXGeneral-Regular;">i</span><span class="mi" id="MathJax-Span-834" style="font-family: STIXGeneral-Regular;">a</span><span class="mi" id="MathJax-Span-835" style="font-family: STIXGeneral-Regular;">g</span></span></span><span class="mo" id="MathJax-Span-836" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-837"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-838" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.367em;"><span class="texatom" id="MathJax-Span-839"><span class="mrow" id="MathJax-Span-840"><span class="mo" id="MathJax-Span-841" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-842" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span class="mo" id="MathJax-Span-843" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-844" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-845" style="font-family: STIXGeneral-Regular;">)</span><span class="texatom" id="MathJax-Span-846"><span class="mrow" id="MathJax-Span-847"><span class="mo" id="MathJax-Span-848" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">(</span></span></span></span></span><span class="msubsup" id="MathJax-Span-849"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-850" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.367em;"><span class="texatom" id="MathJax-Span-851"><span class="mrow" id="MathJax-Span-852"><span class="mo" id="MathJax-Span-853" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-854" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span class="mo" id="MathJax-Span-855" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-856" style=""><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(2.919em, 1000.37em, 4.586em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-857" style=""><span class="mrow" id="MathJax-Span-858"><span class="mo" id="MathJax-Span-859" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.685em; left: 0.523em;"><span class="texatom" id="MathJax-Span-860"><span class="mrow" id="MathJax-Span-861"><span class="mo" id="MathJax-Span-862" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-863" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-864" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-865" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="texatom" id="MathJax-Span-866" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-867"><span class="mi" id="MathJax-Span-868" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span class="mi" id="MathJax-Span-869" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span class="mo" id="MathJax-Span-870" style="vertical-align: 1.148em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: STIXGeneral-Regular; top: -2.445em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: STIXGeneral-Regular; position: absolute; top: -2.862em; left: 0em;">‖<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.841em; left: 13.753em;"><span class="mn" id="MathJax-Span-871" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.809em; border-left: 0px solid; width: 0px; height: 2.503em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mi>x</mi></msub><msup><mrow><mo symmetric="true">‖</mo><mrow><mi>Q</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">w</mi></mrow><mo>⋅</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><msup><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-OPEN"><mo maxsize="1.2em" minsize="1.2em">(</mo></mrow><msup><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></msup><msup><mrow class="MJX-TeXAtom-CLOSE"><mo maxsize="1.2em" minsize="1.2em">)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mi>x</mi><mo>−</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">w</mi></mrow><mi>x</mi></mrow><mo symmetric="true">‖</mo></mrow><mn>2</mn></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-70">\mathbb{E}_x \left\lVert Q(\mathbf{w} \cdot \mathrm{diag}(s^{(\alpha)}))\bigl(s^{(\alpha)}\bigr)^{-1}x - \mathbf{w}x \right\rVert^2</script>

    <ul>
      <li>Choose the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-71-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-872" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.99em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-873"><span class="msubsup" id="MathJax-Span-874"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-875" style="font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.576em;"><span class="mo" id="MathJax-Span-876" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>α</mi><mo>∗</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-71">\alpha^*</script> that yields the lowest simulated error (no backprop required).</li>
    </ul>
  </li>
  <li>
    <p><strong>Scale, Quantize, and Fuse</strong></p>

    <ul>
      <li>Transform weights and activations as:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-72-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mtext&gt;such that&lt;/mtext&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-877" style="width: 24.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 20.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1020.78em, 2.711em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-878"><span class="msubsup" id="MathJax-Span-879"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-880"><span class="mrow" id="MathJax-Span-881"><span class="munderover" id="MathJax-Span-882"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-883" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.159em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-884" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-885" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-886" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-887" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-888" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-889" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-890" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-891" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-892" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.367em;"><span class="mo" id="MathJax-Span-893" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-894" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-895" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-896" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-897" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-898"><span class="mrow" id="MathJax-Span-899"><span class="munderover" id="MathJax-Span-900"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-901" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.107em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-902" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-903" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-904" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-905" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-906" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-907" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="texatom" id="MathJax-Span-908"><span class="mrow" id="MathJax-Span-909"><span class="mo" id="MathJax-Span-910" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="msubsup" id="MathJax-Span-911"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-912" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.367em;"><span class="mo" id="MathJax-Span-913" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-914" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-915" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-916" style="font-family: STIXGeneral-Regular;">such that</span><span class="mspace" id="MathJax-Span-917" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-918"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-919"><span class="mrow" id="MathJax-Span-920"><span class="munderover" id="MathJax-Span-921"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-922" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.159em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-923" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.68em;"><span class="mi" id="MathJax-Span-924" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">⊤</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="texatom" id="MathJax-Span-925"><span class="mrow" id="MathJax-Span-926"><span class="munderover" id="MathJax-Span-927"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-928" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.107em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-929" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-930" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-931" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-932" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.68em;"><span class="mi" id="MathJax-Span-933" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">⊤</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-934" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>w</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><msubsup><mi>s</mi><mi>i</mi><mo>∗</mo></msubsup><mo>,</mo><mspace width="1em"></mspace><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msubsup><mi>s</mi><mi>i</mi><mo>∗</mo></msubsup><mspace width="1em"></mspace><mtext>such that</mtext><mspace width="1em"></mspace><msup><mrow class="MJX-TeXAtom-ORD"><mover><mi>w</mi><mo stretchy="false">~</mo></mover></mrow><mi mathvariant="normal">⊤</mi></msup><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">~</mo></mover></mrow><mo>=</mo><msup><mi>w</mi><mi mathvariant="normal">⊤</mi></msup><mi>x</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-72">\tilde w_i = w_i \cdot s_i^*, \quad \tilde x_i = x_i / s_i^* \quad \text{such that} \quad \tilde w^\top \tilde x = w^\top x</script>

    <ul>
      <li>Apply <strong><code class="language-plaintext highlighter-rouge">int4</code> or <code class="language-plaintext highlighter-rouge">int3</code> group-wise quantization</strong> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-73-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-935" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-936"><span class="texatom" id="MathJax-Span-937"><span class="mrow" id="MathJax-Span-938"><span class="munderover" id="MathJax-Span-939"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-940" style="font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.159em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-941" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>w</mi><mo stretchy="false">~</mo></mover></mrow></math></span></span><script type="math/tex" id="MathJax-Element-73">\tilde w</script> using a single scale per group.</li>
      <li>Fuse the activation scaling <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-74-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-942" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1001.3em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-943"><span class="msubsup" id="MathJax-Span-944"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-945" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;"><span class="texatom" id="MathJax-Span-946"><span class="mrow" id="MathJax-Span-947"><span class="mo" id="MathJax-Span-948" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-949" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;"><span class="mi" id="MathJax-Span-950" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mi>i</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-74">s_i^{-1}</script> directly into the preceding layer normalization or linear layer, avoiding runtime rescaling and preserving inference speed by leveraging existing CUDA kernels from weight-only quantization libraries.</li>
    </ul>
  </li>
  <li>
    <p><strong>Deployment Optimization</strong></p>

    <ul>
      <li>AWQ pairs with <strong>TinyChat</strong>, an inference engine optimized for 4-bit weight-only transformers with kernel fusion and reorder-free dequantization.</li>
      <li>Uses <strong>platform‑aware weight packing</strong> to maximize throughput on GPUs (observed ~3× speedup over Hugging Face <code class="language-plaintext highlighter-rouge">float16</code> with negligible accuracy drop).</li>
    </ul>
  </li>
</ol>

<h5 id="pros-2">Pros</h5>

<ul>
  <li><strong>Salient-channel preservation</strong>: By scaling up high-activation channels, AWQ protects the most influential weights using only ~1% additive precision, significantly reducing quantization error.</li>
  <li><strong>Training‑less</strong>: Requires no finetuning or backpropagation—calibration and closed-form scaling search are sufficient, preserving generalization across domains including instruction-tuned and multi-modal LMs.</li>
  <li><strong>Hardware‑efficient</strong>: Retains group-wise quantization kernels; activation rescaling is fused into existing linear or layer-norm layers, maintaining inference latency and memory efficiency.</li>
</ul>

<h5 id="cons-2">Cons</h5>

<ul>
  <li><strong>Calibration dependency</strong>: Requires representative activation samples and search over <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-75-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-951" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-952"><span class="mi" id="MathJax-Span-953" style="font-family: STIXGeneral-Italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-75">\alpha</script>, which adds one preprocessing pass but no training.</li>
  <li><strong>Limited activation reduction</strong>: Activations are not quantized (typically kept in <code class="language-plaintext highlighter-rouge">float16</code>), so runtime memory use is not halved.</li>
  <li><strong>Architecture constraints</strong>: Fusion of scaling into preceding layernorm assumes alignment between weight input channels and layernorm channels; may require adaptation for custom architectures.</li>
</ul>

<h4 id="gguf-quantization-legacy-kquants-iquants">GGUF Quantization (Legacy, K‑Quants, I‑Quants)</h4>

<ul>
  <li>
    <p>GGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors and was developed by <a href="https://github.com/ggerganov">@ggerganov</a>, who also created <code class="language-plaintext highlighter-rouge">llama.cpp</code>, a widely-used C/C++ LLM inference framework. Models trained in PyTorch or other frameworks can be quantized and converted to GGUF using community tools for deployment on low-resource hardware or CPU-only systems. A detailed summary of GGUF is available in <a href="https://www.reddit.com/r/LocalLlama/comments/1ba55rj/overview_of_gguf_quantization_methods/">this</a> Reddit post.</p>
  </li>
  <li>
    <p>The GGUF file stores tensors and metadata in a compact and readable format that supports a range of quantization methods including legacy quants, K-quants, and I-quants. Quantization blocks encode 256 weights each, along with minimal overhead (e.g., scale, zero-point, or LUT references), and are decoded efficiently during inference using architecture-specific kernels. The format also supports optional importance matrices and tokenizers directly embedded into the file, eliminating external dependencies during inference.</p>
  </li>
  <li>
    <p>The following figure (<a href="https://huggingface.co/docs/hub/en/gguf">source</a>) illustrates the internal structure of a GGUF model file, including the tensor and metadata layout:</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/GGUF.jpg" alt="GGUF structure"></p>

<h5 id="quantization-types">Quantization Types</h5>

<ol>
  <li>
    <p><strong>Legacy Quants (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-76-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-954" style="width: 7.441em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.149em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1006.1em, 2.534em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-955"><span class="mi" id="MathJax-Span-956" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-957"><span style="display: inline-block; position: relative; width: 0.932em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-958" style="font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="mn" id="MathJax-Span-959" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">0</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-960" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-961" style="font-family: STIXGeneral-Italic; padding-left: 0.209em;">Q</span><span class="msubsup" id="MathJax-Span-962"><span style="display: inline-block; position: relative; width: 0.932em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-963" style="font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="mn" id="MathJax-Span-964" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-965" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-966" style="font-family: STIXGeneral-Italic; padding-left: 0.209em;">Q</span><span class="msubsup" id="MathJax-Span-967"><span style="display: inline-block; position: relative; width: 0.932em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-968" style="font-family: STIXGeneral-Regular;">8</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="mn" id="MathJax-Span-969" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">0</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-970" style="font-family: STIXGeneral-Regular;">,</span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mn>4</mn><mn>0</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>4</mn><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-76">Q4_0, Q4_1, Q8_0,</script> etc.)</strong></p>

    <ul>
      <li>Basic block-based quantization where each 256-weight block is encoded with 4 or 8 bits per weight and one (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-77-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-971" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-972"><span class="mi" id="MathJax-Span-973" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-974"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-975" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mn" id="MathJax-Span-976" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">0</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mi>x</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-77">Qx_0</script>) or two (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-78-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-977" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-978"><span class="mi" id="MathJax-Span-979" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-980"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-981" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mn" id="MathJax-Span-982" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mi>x</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-78">Qx_1</script>) constants for scaling/offset.</li>
      <li>Simple bit-unpacking operations (bit shift, AND, multiply) make these formats highly efficient for older hardware and platforms without vector acceleration.</li>
    </ul>
  </li>
  <li>
    <p><strong>K-Quants (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-79-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-983" style="width: 6.149em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.065em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1005.01em, 2.585em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-984"><span class="mi" id="MathJax-Span-985" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-986"><span style="display: inline-block; position: relative; width: 1.346em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-987" style="font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="texatom" id="MathJax-Span-988"><span class="mrow" id="MathJax-Span-989"><span class="msubsup" id="MathJax-Span-990"><span style="display: inline-block; position: relative; width: 0.777em; height: 0px;"><span style="position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mi" id="MathJax-Span-991" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.871em; left: 0.467em;"><span class="texatom" id="MathJax-Span-992"><span class="mrow" id="MathJax-Span-993"><span class="mi" id="MathJax-Span-994" style="font-size: 50%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-995" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-996" style="font-family: STIXGeneral-Italic; padding-left: 0.209em;">Q</span><span class="msubsup" id="MathJax-Span-997"><span style="display: inline-block; position: relative; width: 1.552em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-998" style="font-family: STIXGeneral-Regular;">5</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="texatom" id="MathJax-Span-999"><span class="mrow" id="MathJax-Span-1000"><span class="msubsup" id="MathJax-Span-1001"><span style="display: inline-block; position: relative; width: 0.984em; height: 0px;"><span style="position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mi" id="MathJax-Span-1002" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.871em; left: 0.467em;"><span class="texatom" id="MathJax-Span-1003"><span class="mrow" id="MathJax-Span-1004"><span class="mi" id="MathJax-Span-1005" style="font-size: 50%; font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-1006" style="font-family: STIXGeneral-Regular;">,</span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mrow class="MJX-TeXAtom-ORD"><mi>S</mi></mrow></msub></mrow></msub><mo>,</mo><mi>Q</mi><msub><mn>5</mn><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-79">Q3_{K_{S}}, Q5_{K_{M}},</script> etc.)</strong></p>

    <ul>
      <li>Smarter allocation of bits across layers or weight blocks, guided by internal heuristics or optional importance matrices.</li>
      <li>Uses combinations of quantization levels in different layers (XS, S, M), optimizing performance-quality tradeoff.</li>
      <li>Maintains speed advantages of legacy quants while improving model fidelity and reducing quantization noise.</li>
    </ul>
  </li>
  <li>
    <p><strong>I-Quants (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-80-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1007" style="width: 10.54em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.68em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.346em, 1008.63em, 2.534em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-1008"><span class="mi" id="MathJax-Span-1009" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1010" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1011"><span style="display: inline-block; position: relative; width: 1.862em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-1012" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="texatom" id="MathJax-Span-1013"><span class="mrow" id="MathJax-Span-1014"><span class="mi" id="MathJax-Span-1015" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1016" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1017" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-1018" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-1019" style="font-family: STIXGeneral-Italic; padding-left: 0.209em;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1020" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1021"><span style="display: inline-block; position: relative; width: 0.932em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-1022" style="font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="mi" id="MathJax-Span-1023" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-1024" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-1025" style="font-family: STIXGeneral-Italic; padding-left: 0.209em;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1026" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1027"><span style="display: inline-block; position: relative; width: 1.397em; height: 0px;"><span style="position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;"><span class="mn" id="MathJax-Span-1028" style="font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span><span style="position: absolute; top: -3.82em; left: 0.519em;"><span class="texatom" id="MathJax-Span-1029"><span class="mrow" id="MathJax-Span-1030"><span class="mi" id="MathJax-Span-1031" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-1032" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.98em;"></span></span></span></span><span class="mo" id="MathJax-Span-1033" style="font-family: STIXGeneral-Regular;">,</span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>3</mn><mi>S</mi></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-80">IQ2_{XXS}, IQ3_S, IQ4_{XS},</script> etc.)</strong></p>

    <ul>
      <li>Advanced block-wise quantization using ideas from QuIP; includes lookup tables to store additional decoding values.</li>
      <li>Allows lower bpw (2-4) while preserving model quality, especially useful for extremely memory-constrained inference.</li>
      <li>Lookup-based dequantization introduces more compute overhead and can cause performance regressions on CPU-bound hardware.</li>
    </ul>
  </li>
</ol>

<h5 id="gguf-file-layout-and-execution">GGUF File Layout and Execution</h5>

<ul>
  <li>Each GGUF file begins with a magic header and version indicator (<code class="language-plaintext highlighter-rouge">0x47 0x47 0x55 0x46</code> for “GGUF”, currently version 3), followed by two 64-bit integers: the number of tensors and number of metadata key-value pairs.</li>
  <li>Tensor definitions include name, shape, type, and byte offset. Supported quant types include formats like <code class="language-plaintext highlighter-rouge">GGML_TYPE_Q2_K</code>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-81-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1034" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1035"><span class="mi" id="MathJax-Span-1036" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1037"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1038" style="font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-1039" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mn>3</mn><mi>K</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-81">Q3_K</script>, or <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-82-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1040" style="width: 3.023em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1041"><span class="mi" id="MathJax-Span-1042" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1043" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1044"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1045" style="font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1046"><span class="mrow" id="MathJax-Span-1047"><span class="mi" id="MathJax-Span-1048" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1049" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-82">IQ4_{XS}</script>.</li>
  <li>Metadata stores tokenizer info, architecture name, context length, and any quantization parameters used during export.</li>
  <li>Tensors are read from disk at inference time via offset pointers—enabling partial loading or memory-mapped inference.</li>
</ul>

<h5 id="importance-matrix-imatrix">Importance Matrix (Imatrix)</h5>

<ul>
  <li>An optional matrix that prioritizes preserving accuracy in weights deemed most significant based on a calibration pass.</li>
  <li>Can be used with K-quants and legacy quants, not exclusive to I-quants.</li>
  <li>Stored directly in the GGUF metadata and silently improves quantization quality with no inference-time cost.</li>
</ul>

<h5 id="pros-3">Pros</h5>

<ul>
  <li><strong>Efficient deployment format</strong>: GGUF enables fast loading, lightweight inference, and portable packaging across platforms.</li>
  <li><strong>Flexible quant schemes</strong>: From legacy-friendly <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-83-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1050" style="width: 2.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.67em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1051"><span class="mi" id="MathJax-Span-1052" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1053"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1054" style="font-family: STIXGeneral-Regular;">8</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="mn" id="MathJax-Span-1055" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">0</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-83">Q8_0</script> to ultra-compressed <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-84-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1056" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1057"><span class="mi" id="MathJax-Span-1058" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1059" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1060"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1061" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1062"><span class="mrow" id="MathJax-Span-1063"><span class="mi" id="MathJax-Span-1064" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1065" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1066" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-84">IQ2_{XXS}</script>, GGUF supports a wide range of bit-widths and precision tradeoffs.</li>
  <li><strong>All-in-one packaging</strong>: Tokenizers, metadata, and importance matrices are embedded—no need for external configuration files.</li>
  <li><strong>Community driven</strong>: Supported natively by <code class="language-plaintext highlighter-rouge">llama.cpp</code> and increasingly integrated with Hugging Face tools and runners.</li>
</ul>

<h5 id="cons-3">Cons</h5>

<ul>
  <li><strong>Hardware-specific behavior</strong>: Some quant schemes (especially I-quants) may perform suboptimally on older CPUs or non-VNNI hardware.</li>
  <li><strong>Naming ambiguity</strong>: Quantization method and imatrix usage are not always visible in the filename; may require manual inspection or re-quantization.</li>
  <li><strong>Rapid evolution</strong>: Format and tooling are evolving quickly—older GGUF models may need conversion to newer versions.</li>
</ul>

<h4 id="aweq-activationweight-equalization">AWEQ: Activation‑Weight Equalization</h4>

<ul>
  <li>Introduced in <a href="https://arxiv.org/abs/2311.01305">AWEQ: Post‑Training Quantization with Activation‑Weight Equalization for LLMs</a> (Nov 2023) by Li et al., AWEQ is a training‑free post‑training quantization technique designed to facilitate both ultra‑low‑bit and 8‑bit weight+activation (e.g., W8A8) quantization in large language models such as Llama and OPT. It works by shifting quantization hardness from activations to weights to reduce error.</li>
  <li>AWEQ effectively balances activation and weight ranges channel-wise via <strong>per-channel equalization</strong> followed by <strong>uniform quantization</strong>, incorporating <strong>bias correction</strong> to reduce residual errors. It achieves significantly improved quantization accuracy—especially for W8A8 floating‑point alternatives—without any training or runtime slow-down, making it an excellent choice for production deployments requiring both efficiency and fidelity.</li>
</ul>

<h5 id="motivation">Motivation</h5>

<ul>
  <li>Large‑scale LLM activations often exhibit <strong>long‑tailed per‑channel distributions</strong> with large outliers, making activation quantization challenging even at 8 bits. AWEQ addresses this by balancing activation and weight ranges so that differences in range (and therefore quantization difficulty) are harmonized channel‑wise, reducing wastage in the quantization grid and improving uniform quantization performance.</li>
</ul>

<h5 id="process-implementation-overview">Process (Implementation Overview)</h5>

<ol>
  <li>
    <p><strong>Channel Range Analysis</strong></p>

    <ul>
      <li>Run forward passes over a small calibration dataset to compute <strong>per‑channel activation range</strong> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-85-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1067" style="width: 12.919em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1010.68em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1068"><span class="mi" id="MathJax-Span-1069" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1070" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1071" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-1072"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-1073" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.315em;"><span class="mi" id="MathJax-Span-1074" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1075" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-1076" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">max</span><span class="mo" id="MathJax-Span-1077" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1078"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1079" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.628em;"><span class="mi" id="MathJax-Span-1080" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1081" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1082" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mo" id="MathJax-Span-1083" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">min</span><span class="mo" id="MathJax-Span-1084" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1085"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1086" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.628em;"><span class="mi" id="MathJax-Span-1087" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1088" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo stretchy="false">(</mo><mi>X</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mo movablelimits="true" form="prefix">min</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-85">r(X)_i = \max(X_i) - \min(X_i)</script> and <strong>weight range</strong> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-86-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1089" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.29em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1090"><span class="mi" id="MathJax-Span-1091" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1092" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1093" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-1094"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-1095" style="font-family: STIXGeneral-Regular;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.315em;"><span class="mi" id="MathJax-Span-1096" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-86">r(W)_i</script> for each linear or attention block tensor. Range refers to max minus min values across all elements in that channel.</li>
    </ul>
  </li>
  <li>
    <p><strong>Equalization Factor Computation</strong></p>

    <ul>
      <li>
        <p>Compute a scale vector <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-87-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1097" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1002.92em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1098"><span class="mi" id="MathJax-Span-1099" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-1100" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1101" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1102"><span class="mrow" id="MathJax-Span-1103"><span class="mi" id="MathJax-Span-1104" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="mi" id="MathJax-Span-1105" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>C</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-87">s \in \mathbb{R}^C</script> to equalize ranges via:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-88-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1106" style="width: 12.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.107em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.732em, 1010.11em, 3.232em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1107"><span class="msubsup" id="MathJax-Span-1108"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1109"><span class="mrow" id="MathJax-Span-1110"><span class="munderover" id="MathJax-Span-1111"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1112" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.268em; left: 0.211em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1113" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-1114" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1115" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mfrac" id="MathJax-Span-1116" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.414em;"><span class="msubsup" id="MathJax-Span-1117"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1118" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.628em;"><span class="mi" id="MathJax-Span-1119" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.44em, 1000.68em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.31em;"><span class="msubsup" id="MathJax-Span-1120"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1121" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-1122" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.99em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.992em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1123" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-1124" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-1125" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1126"><span class="mrow" id="MathJax-Span-1127"><span class="munderover" id="MathJax-Span-1128"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1129" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.268em; left: 0.367em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1130" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.888em;"><span class="mi" id="MathJax-Span-1131" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1132" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-1133" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1134" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-1135" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1136" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-1137" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1138" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="mi" id="MathJax-Span-1139" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.753em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>X</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><msub><mi>s</mi><mi>i</mi></msub></mfrac><mo>,</mo><mspace width="1em"></mspace><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>W</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>W</mi><mi>i</mi></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-88">\tilde{X}_i = \frac{X_i}{s_i}, \quad \tilde{W}_i = s_i \cdot W_i</script>

        <ul>
          <li>The objective is typically set such that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-89-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1140" style="width: 6.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1005.32em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1141"><span class="mi" id="MathJax-Span-1142" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1143" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1144"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1145"><span class="mrow" id="MathJax-Span-1146"><span class="munderover" id="MathJax-Span-1147"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1148" style="font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.268em; left: 0.211em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1149" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-1150" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1151" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1152" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1153" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1154" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1155"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1156"><span class="mrow" id="MathJax-Span-1157"><span class="munderover" id="MathJax-Span-1158"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1159" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.268em; left: 0.367em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1160" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.888em;"><span class="mi" id="MathJax-Span-1161" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1162" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>X</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>W</mi><mo stretchy="false">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-89">r(\tilde{X}_i) = r(\tilde{W}_i)</script> for all <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-90-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1163" style="width: 0.315em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1164"><span class="mi" id="MathJax-Span-1165" style="font-family: STIXGeneral-Italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-90">i</script>, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Tensor Scaling (Fusion)</strong></p>

    <ul>
      <li>Apply channel‑wise scaling at the input boundaries of transformer modules—such as prior to self‑attention key/value/data and FFN layers.</li>
      <li>Merge activation scaling into preceding layers (e.g., LayerNorm or Linear) to <strong>eliminate runtime overhead</strong>. For example, transform internal <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-91-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2190;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1166" style="width: 7.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1006.62em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1167"><span class="mi" id="MathJax-Span-1168" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1169" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">←</span><span class="texatom" id="MathJax-Span-1170" style="padding-left: 0.315em;"><span class="mrow" id="MathJax-Span-1171"><span class="mi" id="MathJax-Span-1172" style="font-family: STIXGeneral-Regular;">d</span><span class="mi" id="MathJax-Span-1173" style="font-family: STIXGeneral-Regular;">i</span><span class="mi" id="MathJax-Span-1174" style="font-family: STIXGeneral-Regular;">a</span><span class="mi" id="MathJax-Span-1175" style="font-family: STIXGeneral-Regular;">g</span></span></span><span class="mo" id="MathJax-Span-1176" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1177" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-1178" style="font-family: STIXGeneral-Regular;">)</span><span class="mspace" id="MathJax-Span-1179" style="height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-1180" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mo stretchy="false">←</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mspace width="thinmathspace"></mspace><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-91">W \leftarrow \mathrm{diag}(s) \, W</script>; hence, activations use quantizable ranges without additional scaling logic.</li>
    </ul>
  </li>
  <li>
    <p><strong>Uniform Quantization</strong></p>

    <ul>
      <li>Quantize the equalized tensors using <strong>per‑tensor uniform affine quantization</strong> (e.g., 4‑bit or 8‑bit symmetric). Activation quantization thresholds can be fused with the input block for efficient inference.</li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization Bias Correction (BC)</strong></p>

    <ul>
      <li>Because quantization after scaling and symmetric clipping can introduce a bias <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-92-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1181" style="width: 5.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1004.9em, 2.659em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1182"><span class="mi" id="MathJax-Span-1183" style="font-family: STIXGeneral-Italic;">ϵ</span><span class="mo" id="MathJax-Span-1184" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-1185" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1186" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="mi" id="MathJax-Span-1187" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1188" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-1189" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>=</mo><msub><mi>W</mi><mi>f</mi></msub><mo>−</mo><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-92">\epsilon = W_f - W</script>, AWEQ applies post‑hoc <strong>bias correction</strong>:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-93-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1190" style="width: 8.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1006.62em, 2.711em, -999.997em); top: -2.341em; left: 0em;"><span class="mrow" id="MathJax-Span-1191"><span class="texatom" id="MathJax-Span-1192"><span class="mrow" id="MathJax-Span-1193"><span class="munderover" id="MathJax-Span-1194"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1195" style="font-family: STIXGeneral-Italic;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.06em; left: 0.055em;"><span style="height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1196" style="font-family: STIXGeneral-Regular;">̃&nbsp;<span style="height: 0em; vertical-align: 0em; margin-left: -0.258em;"></span></span><span style="display: inline-block; overflow: hidden; height: 1px; width: 0em;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1197" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-1198" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1199" style="font-family: STIXGeneral-Italic;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-1200" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1201" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-1202" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">ϵ</span><span class="mo" id="MathJax-Span-1203" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="texatom" id="MathJax-Span-1204" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-1205"><span class="mi" id="MathJax-Span-1206" style="font-family: STIXGeneral-Regular;">𝔼</span></span></span><span class="mo" id="MathJax-Span-1207" style="font-family: STIXGeneral-Regular;">[</span><span class="mi" id="MathJax-Span-1208" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1209" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.346em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">~</mo></mover></mrow><mo>=</mo><msub><mi>y</mi><mi>e</mi></msub><mo>−</mo><mi>ϵ</mi><mo>⋅</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-93">\tilde{y} = y_e - \epsilon \cdot \mathbb{E}[x]</script>

    <ul>
      <li>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-94-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1210" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.67em, 2.659em, -999.997em); top: -2.341em; left: 0em;"><span class="mrow" id="MathJax-Span-1211"><span class="texatom" id="MathJax-Span-1212"><span class="mrow" id="MathJax-Span-1213"><span class="mi" id="MathJax-Span-1214" style="font-family: STIXGeneral-Regular;">𝔼</span></span></span><span class="mo" id="MathJax-Span-1215" style="font-family: STIXGeneral-Regular;">[</span><span class="mi" id="MathJax-Span-1216" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1217" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.346em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-94">\mathbb{E}[x]</script> is estimated over calibration data, and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-95-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1218" style="width: 6.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.68em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1219"><span class="msubsup" id="MathJax-Span-1220"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1221" style="font-family: STIXGeneral-Italic;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-1222" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1223" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mo" id="MathJax-Span-1224" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">(</span><span class="mi" id="MathJax-Span-1225" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1226" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-1227" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">ϵ</span><span class="mo" id="MathJax-Span-1228" style="font-family: STIXGeneral-Regular;">)</span><span class="mi" id="MathJax-Span-1229" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mi>e</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi>W</mi><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-95">y_e = (W + \epsilon)x</script>. This corrects the expected error per layer without changing runtime performance, enhancing stability in deep LLMs without BatchNorm.</li>
    </ul>
  </li>
</ol>

<h5 id="pros-4">Pros</h5>

<ul>
  <li><strong>Training‑free</strong>, with no need for quantization-aware training or gradient-based fine-tuning.</li>
  <li>Supports both <strong>W8A8 activation quantization</strong> and <strong>ultra‑low-bit weight-only quantization</strong>, including <code class="language-plaintext highlighter-rouge">int/4</code> with robust performance.</li>
  <li><strong>Hardware-friendly</strong>, as it avoids dynamic scaling during inference; changes are statically fused before deployment.</li>
  <li>Demonstrates <strong>best-in-class accuracy</strong> on tasks such as zero‑shot Llama 2 7B evaluation (e.g., average: 70.38% over PIQA, HellaSwag, WinoGrande, ARC‑e—all within &lt;0.01 absolute from <code class="language-plaintext highlighter-rouge">float16</code>).</li>
</ul>

<h5 id="cons-4">Cons</h5>

<ul>
  <li>Requires <strong>representative calibration data</strong> to compute statistics and activation range profiles.</li>
  <li><strong>Per‑tensor quantization</strong> may not perform as well as per‑channel for certain weight distributions, though the equalization helps mitigate this.</li>
  <li>Slight overhead in computing equalization factors and bias expectations at quantization time (calibration phase only).</li>
</ul>

<h4 id="exl2-quantization">EXL2 Quantization</h4>

<ul>
  <li>
    <p><a href="https://github.com/turboderp-org/exllamav2#exl2-quantization">ExLlamaV2</a>, commonly known as EXL2, is a flexible, weight-only quantization scheme developed specifically for local inference of large language models on consumer GPUs. It supports mixed-precision quantization with bit-widths from 2 to 8 bits, and can dynamically allocate precision per weight group to optimize model accuracy at a target average bitrate. This makes it suitable for extreme compression of LLMs such as Llama2-70B, enabling execution on GPUs with as little as 24 GB VRAM.</p>
  </li>
  <li>
    <p>EXL2 builds upon the GPTQ framework but introduces finer-grained control over quantization allocation, using an error-minimization strategy driven by calibration data. Unlike uniform quantization, EXL2 allows important weights to retain higher precision while compressing less critical ones more aggressively. This is implemented without significant performance penalties due to tight integration with ExLlama’s inference engine and CUDA backend.</p>
  </li>
</ul>

<h5 id="process-3">Process</h5>

<ol>
  <li>
    <p><strong>Calibration and Error Evaluation</strong>:</p>

    <ul>
      <li>Begin by passing a small calibration dataset through the model to obtain representative statistics.</li>
      <li>For each linear layer weight matrix, the EXL2 pipeline quantizes the matrix multiple times using different bit-width configurations (e.g., 2, 3, 4, 5, 6, or 8 bits).</li>
      <li>After each quantization trial, compute the quantization error between the original and quantized matrix multiplied by the calibration input. The maximum per-layer error across all trials is tracked.</li>
    </ul>
  </li>
  <li>
    <p><strong>Bitrate-Constrained Optimization</strong>:</p>

    <ul>
      <li>A greedy or grid-based search selects the bit-width assignment that minimizes the <strong>maximum layer-wise error</strong> while satisfying a user-defined <strong>average bitrate target</strong> (e.g., 2.55 bits per weight).</li>
      <li>This allows for <strong>non-uniform quantization within each matrix</strong>, so important rows or columns (typically corresponding to high-magnitude weights or activations) may receive higher precision.</li>
    </ul>
  </li>
  <li>
    <p><strong>Mixed-Bit Packing and Storage Format</strong>:</p>

    <ul>
      <li>Each matrix is stored in a compact format supporting mixed-bit representation. A metadata structure encodes the bit-width used for each group.</li>
      <li>Group size is typically fixed (e.g., 64 or 128), enabling compatibility with blockwise CUDA kernels.</li>
      <li>The storage layout ensures efficient memory access and can be interpreted directly by ExLlama’s fast inference kernels.</li>
    </ul>
  </li>
  <li>
    <p><strong>Inference Support</strong>:</p>

    <ul>
      <li>At runtime, ExLlamaV2 uses custom CUDA kernels capable of unpacking and computing with mixed-bit quantized weights.</li>
      <li>There is no need for runtime dequantization to full precision—matmul and sampling are done directly on quantized values.</li>
      <li>The system also supports <strong>act-order remapping</strong>, allowing reordering of weight matrices to preserve activation alignment in grouped attention layers, which is important for compatibility with GQA architectures and inference speed.</li>
    </ul>
  </li>
  <li>
    <p><strong>Conversion Pipeline</strong>:</p>

    <ul>
      <li>
        <p>A command-line <a href="https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization">script</a> is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:</p>

        <ul>
          <li>Automatic bit-width search using calibration data.</li>
          <li>Weight remapping and act-order alignment.</li>
          <li>Storage into a compact format suitable for ExLlamaV2.</li>
        </ul>
      </li>
      <li>
        <p>Conversion is computationally intensive, especially for large models, but only needs to be done once.</p>
      </li>
    </ul>
  </li>
</ol>

<h5 id="pros-5">Pros</h5>

<ul>
  <li><strong>Extreme compression</strong>: Achieves ultra-low bitrates (e.g., 2.5–3.0 bpw) without retraining, enabling 70B models to run on 24 GB GPUs.</li>
  <li><strong>Layer-aware precision allocation</strong>: Allocates bits where they matter most, reducing perceptual degradation in output quality.</li>
  <li><strong>Performance-friendly</strong>: Designed for fast execution with minimal overhead through mixed-bit CUDA kernels.</li>
  <li><strong>Flexible deployment</strong>: Supports a range of bitrates and model sizes, allowing trade-offs between quality and performance.</li>
</ul>

<h5 id="cons-5">Cons</h5>

<ul>
  <li><strong>Complex conversion</strong>: Requires full model calibration, multiple quantization trials per matrix, and custom tooling.</li>
  <li><strong>Conversion time</strong>: Large models (13B–70B) take significant time to convert due to exhaustive per-layer bit-width search.</li>
  <li><strong>Inference compatibility</strong>: Requires ExLlamaV2 backend for proper kernel execution; not compatible with standard PyTorch or ONNX runtimes.</li>
</ul>

<h4 id="spinquant">SpinQuant</h4>

<ul>
  <li>Introduced in <a href="https://arxiv.org/abs/2405.16406">SpinQuant: LLM Quantization with Learned Rotations</a> by Liu et al. (2024), SpinQuant reduces quantization error by applying <strong>learned orthonormal rotations</strong> to weights, activations, and KV-cache blocks. These rotations normalize tensor distributions, mitigate outliers, and enable accurate <strong>W4A4KV4</strong> quantization. The implementation is available on <a href="https://github.com/facebookresearch/SpinQuant">GitHub</a>.</li>
</ul>

<h5 id="process-4">Process</h5>

<ol>
  <li>
    <p><strong>Parameterize rotation matrices</strong>:</p>

    <ul>
      <li>SpinQuant uses blockwise <strong>orthonormal rotation matrices</strong> initialized using Hadamard, shortcut, or random orthonormal bases.</li>
      <li>These rotations are applied to groups of weights, activations, and KV-cache blocks (e.g., <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-96-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1230" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1231"><span class="mi" id="MathJax-Span-1232" style="font-family: STIXGeneral-Italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-96">Q</script>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-97-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1233" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1234"><span class="mi" id="MathJax-Span-1235" style="font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></span></span><script type="math/tex" id="MathJax-Element-97">K</script>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-98-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1236" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1237"><span class="mi" id="MathJax-Span-1238" style="font-family: STIXGeneral-Italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math></span></span><script type="math/tex" id="MathJax-Element-98">V</script> matrices or FFN weights), where outliers may exist.</li>
    </ul>
  </li>
  <li>
    <p><strong>Optimize via Cayley-SGD on the Stiefel manifold</strong>:</p>

    <ul>
      <li>A small calibration set is passed through a simulated W4A4KV4 pipeline.</li>
      <li>Quantization error (e.g., MSE or KL divergence) between full-precision and quantized outputs is computed.</li>
      <li>Gradients are backpropagated through the rotation parameters using <strong>Cayley-SGD</strong>, a method that maintains orthonormality constraints by optimizing directly on the Stiefel manifold.</li>
    </ul>
  </li>
  <li>
    <p><strong>Fold optimized rotations into model weights</strong>:</p>

    <ul>
      <li>Once optimized, the learned rotation matrices are <strong>fused into the model weights and biases</strong> (e.g., replacing <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-99-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1239" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1240"><span class="mi" id="MathJax-Span-1241" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-99">W</script> with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-100-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1242" style="width: 3.388em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.815em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1002.82em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1243"><span class="msubsup" id="MathJax-Span-1244"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1245" style="font-family: STIXGeneral-Italic;">R</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.628em;"><span class="mi" id="MathJax-Span-1246" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-1247" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1248" style="font-family: STIXGeneral-Italic;">R</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mi>T</mi></msup><mi>W</mi><mi>R</mi></math></span></span><script type="math/tex" id="MathJax-Element-100">R^T W R</script>) during preprocessing.</li>
      <li>This ensures that no extra computation is introduced at inference time—quantization is performed on the already rotated tensors.</li>
    </ul>
  </li>
  <li>
    <p><strong>Apply W4A4KV4 quantization</strong>:</p>

    <ul>
      <li>Post-rotation, weights, activations, and KV-cache blocks are quantized to 4-bit using standard uniform quantization schemes.</li>
      <li>The rotations have distributed the influence of large-magnitude outliers, allowing for a tighter and more efficient quantization range.</li>
    </ul>
  </li>
</ol>

<h5 id="pros-6">Pros</h5>

<ul>
  <li>
    <p><strong>Outlier mitigation via distribution normalization</strong>:</p>

    <ul>
      <li>Rotations “smear” large-magnitude values across dimensions, redistributing peak energies that would otherwise dominate quantization bins.</li>
      <li>This normalization significantly reduces the impact of extreme values and improves low-bit quantization fidelity.</li>
    </ul>
  </li>
  <li>
    <p><strong>Accuracy preservation</strong>:</p>

    <ul>
      <li>Achieves within <strong>~2.9 points</strong> of full precision on Llama 2 (7B) zero-shot tasks.</li>
      <li>Outperforms existing techniques like AWQ, SmoothQuant, and QuaRot by 19–45% in accuracy retention.</li>
    </ul>
  </li>
  <li>
    <p><strong>No runtime overhead</strong>:</p>

    <ul>
      <li>Unlike some quantization techniques that add inference complexity, SpinQuant’s learned rotations are merged offline.</li>
      <li>At inference, the model behaves identically to a standard quantized model, with no additional compute.</li>
    </ul>
  </li>
</ul>

<h5 id="cons-6">Cons</h5>

<ul>
  <li>
    <p><strong>Involves optimization and calibration</strong>:</p>

    <ul>
      <li>Cayley-SGD optimization introduces computational overhead during preprocessing.</li>
      <li>Requires a small validation set to simulate quantization and compute gradients.</li>
    </ul>
  </li>
  <li>
    <p><strong>Preprocessing complexity</strong>:</p>

    <ul>
      <li>Folding rotations into model weights adds engineering complexity, especially when targeting hardware deployment.</li>
      <li>Though folded offline, the rotated tensors may have slightly increased numerical range, requiring careful scale selection.</li>
    </ul>
  </li>
  <li>
    <p><strong>Larger intermediate tensors</strong>:</p>

    <ul>
      <li>While inference cost remains low, merged rotated weights can increase storage slightly due to loss of weight sparsity or alignment.</li>
    </ul>
  </li>
</ul>

<h4 id="fptquant">FPTQuant</h4>

<ul>
  <li>Introduced in <a href="https://arxiv.org/abs/2506.04985">FPTQuant: 4-bit Function‑Preserving Transforms for Transformer PTQ</a> by Pan et al. (2025), Function-Preserving Transforms Quantization (FPTQuant) reshapes transformer activations before quantization to preserve function.</li>
  <li>A complementary overview is available in <a href="https://leimao.github.io/blog/FPTQuant-LLM-Quantization/">Lei Mao’s Log Book</a>.</li>
</ul>

<h5 id="process-5">Process</h5>

<ol>
  <li>
    <p><strong>Function-Preserving Activation Transforms</strong>:</p>

    <ul>
      <li>
        <p>FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:</p>

        <ul>
          <li><strong>Logarithmic transforms</strong>: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).</li>
          <li><strong>Affine or exponential transforms</strong>: Normalize activations without changing the computation graph logic.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Merging Transforms into Weights</strong>:</p>

    <ul>
      <li>Since these transforms are invertible, the effect can be canceled out by adjusting the downstream linear weights.</li>
      <li>
        <p>Specifically:</p>

        <ul>
          <li>Let <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-101-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1249" style="width: 4.586em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1250"><span class="mi" id="MathJax-Span-1251" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1252" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">→</span><span class="mi" id="MathJax-Span-1253" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1254" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1255" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1256" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo stretchy="false">→</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-101">x \rightarrow f(x)</script> be the transform applied to activations.</li>
          <li>Then <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-102-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1257" style="width: 9.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1007.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1258"><span class="mi" id="MathJax-Span-1259" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1260" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1261" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">→</span><span class="mi" id="MathJax-Span-1262" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="msubsup" id="MathJax-Span-1263"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1264" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1265"><span class="mrow" id="MathJax-Span-1266"><span class="mo" id="MathJax-Span-1267" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-1268" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1269" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1270" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1271" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1272" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1273" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1274" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mi>x</mi><mo stretchy="false">→</mo><mi>W</mi><msup><mi>f</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-102">Wx \rightarrow Wf^{-1}(f(x))</script> ensures the output remains unchanged.</li>
          <li>FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization Step</strong>:</p>

    <ul>
      <li>With the dynamic range compressed, 4-bit symmetric per-channel quantization is applied to the adjusted weights using PTQ methods.</li>
      <li>Activations are not explicitly quantized, but their transformed form is compatible with W4A16 runtimes (e.g., vLLM) where only weights are quantized.</li>
    </ul>
  </li>
  <li>
    <p><strong>No Runtime Penalty</strong>:</p>

    <ul>
      <li>All transforms are resolved offline and merged into weights.</li>
      <li>The runtime model is a standard quantized model with no extra ops introduced during inference.</li>
    </ul>
  </li>
</ol>

<h5 id="pros-7">Pros</h5>

<ul>
  <li>Fully <strong>training-free</strong>, <strong>invertible</strong>, and <strong>architecture-agnostic</strong>.</li>
  <li>Achieves <strong><code class="language-plaintext highlighter-rouge">int4</code> weight-only quantization</strong> with minimal or no accuracy loss, by preserving function through mathematically exact transformation.</li>
  <li>Compatible with <strong>W4A16 systems</strong> like vLLM, delivering significant memory and latency improvements without major architectural rework.</li>
</ul>

<h5 id="cons-7">Cons</h5>

<ul>
  <li>Primarily targets <strong>weights</strong>—does not quantize activations directly, limiting total memory benefits.</li>
  <li>Still an <strong>emerging method</strong>—performance and generalization are under ongoing validation across LLM variants (e.g., Mistral, Gemma).</li>
  <li>May require per-layer transform tuning based on architecture layout (e.g., attention vs MLP blocks).</li>
</ul>

<h4 id="palettization-weight-clustering">Palettization (Weight Clustering)</h4>

<ul>
  <li><strong>Palettization</strong>, also known as <strong>weight clustering</strong>, is a quantization scheme that replaces full‑precision weights with low‑bit indices into a small lookup table (LUT). Each weight value is approximated by the nearest centroid in the LUT, enabling efficient storage and retrieval.</li>
</ul>

<h5 id="process-6">Process</h5>

<ol>
  <li><strong>Clustering</strong>: Collect all float‑format weights for a layer (or group of layers), then run <strong>k-means clustering</strong> to derive a set of centroids (typically, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-103-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1275" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1276"><span class="msubsup" id="MathJax-Span-1277"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1278" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1279"><span class="mrow" id="MathJax-Span-1280"><span class="mi" id="MathJax-Span-1281" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-103">2^{n}</script> entries for n‑bit palettization).</li>
  <li><strong>Index Mapping</strong>: Each weight is replaced with an integer index pointing to its closest centroid in the LUT. The original full‑precision value is no longer stored.</li>
  <li><strong>Granularity Options</strong>:
    <ul>
      <li><strong>Per‑tensor granularity</strong>: A single LUT for the entire weight tensor.</li>
      <li><strong>Per‑group‑channel granularity</strong>: The tensor is divided into groups of channels (defined by <code class="language-plaintext highlighter-rouge">group_size</code>), each with its own LUT—offering a better accuracy/compression trade-off.</li>
    </ul>
  </li>
  <li><strong>Optional Vector Clustering</strong> (<code class="language-plaintext highlighter-rouge">cluster_dim &gt; 1</code>): Enables multi-dimensional centroids by clustering weight vectors instead of scalars, improving approximation quality for some architectures.</li>
  <li><strong>Post‑processing</strong>: Optionally quantize LUT centroids themselves to a lower precision (e.g. <code class="language-plaintext highlighter-rouge">int8</code>) for additional compression.</li>
</ol>

<h5 id="integration-in-workflows">Integration in Workflows</h5>

<ul>
  <li>Available via Apple’s <code class="language-plaintext highlighter-rouge">coremltools.optimize.torch.palettization</code> API, which injects <strong>FakePalettize</strong> layers into the model for <strong>palettization-aware training (PAT)</strong>.</li>
  <li>During training, k‑means clustering is applied online, and the LUT and indices are learned through gradient steps. After convergence, the <code class="language-plaintext highlighter-rouge">finalize()</code> call folds LUTs and indices into permanent quantized weights.</li>
</ul>

<h5 id="when-to-use-palettization">When to Use Palettization</h5>

<ul>
  <li><strong>Memory-critical deployment</strong>: Edge devices or mobile apps where weight storage is the bottleneck.</li>
  <li><strong>Aggressive compression</strong>: Scenarios requiring sub‑4‑bit representation.</li>
  <li><strong>Architecture flexibility</strong>: Works with both CNNs and transformers when standard affine quantization struggles.</li>
  <li><strong>Fine‑tunable deployment targets</strong>: Fine-tuning after palettization enables high accuracy while still achieving significant compression ratios.</li>
</ul>

<h5 id="pros-8">Pros</h5>

<ul>
  <li><strong>Extreme compression:</strong> Supports ultra-low bit‑width representations (e.g. 2–4 bits) for weights.</li>
  <li><strong>Memory savings:</strong> Offers major memory savings—each weight becomes a small index instead of a float.</li>
  <li><strong>Vector clustering:</strong> Multidimensional centroids can preserve structure in weight matrices.</li>
  <li><strong>Flexible granularity:</strong> Per-tensor, per-group, or vector-level control enables tailored compression vs. accuracy trade-offs.</li>
  <li><strong>Compatible with fine‑tuning:</strong> Compatible via PAT, allowing retention of accuracy through fine‑tuning post-clustering.</li>
</ul>

<h5 id="cons-8">Cons</h5>

<ul>
  <li>Requires additional training or fine‑tuning steps (PAT) to compensate for quantization error.</li>
  <li>Clustering and LUT management adds complexity to both training and inference pipelines. In other words, introduces LUT metadata and integer-to-centroid lookup logic in inference.</li>
  <li>Larger runtime overhead than standard affine quantization, especially with per-channel or per-group palettization which increases storage overhead for multiple LUTs and adds runtime logic to look up indices.</li>
  <li>Less intuitive and more complex to implement than simple scale-based quantization.</li>
</ul>

<h4 id="what-to-use-when">What to Use When?</h4>

<ul>
  <li>
    <p>Selecting the right method depends on deployment goals, model architecture, available compute, and desired trade-offs between accuracy, speed, and memory. Below is a structured guide to help determine <strong>what to use when</strong>.</p>
  </li>
  <li>
    <p><strong>For Ultra-Low Bit Weight-Only Quantization (<code class="language-plaintext highlighter-rouge">int3</code>/<code class="language-plaintext highlighter-rouge">int4</code>) with No Accuracy Drop</strong>:</p>

    <ul>
      <li>
        <p><em>Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant</em></p>
      </li>
      <li><strong>AWQ</strong>: Best for <strong>fast deployment</strong> of LLMs (e.g., Llama) on <strong>edge GPUs</strong> or <strong>low-latency inference</strong> with prebuilt CUDA kernels. No training or tuning required, and integrates well with TinyChat or similar runtimes.</li>
      <li><strong>FPTQuant</strong>: Ideal when you need <strong>function-preserving <code class="language-plaintext highlighter-rouge">int4</code> compression</strong> with no runtime penalty and full compatibility with transformer architectures. Use for <strong>W4A16</strong> deployment in platforms like <strong>vLLM</strong>.</li>
      <li><strong>GPTQ</strong>: Recommended for <strong>very large models (13B–175B)</strong> where preserving perplexity is critical. Use when quantization accuracy is a priority and you’re comfortable with modest compute during conversion.</li>
      <li><strong>EXL2</strong>: Choose when running <strong>massive models (e.g., Llama2-70B)</strong> on <strong>consumer GPUs</strong>. Offers the <strong>best compression-speed balance</strong> via dynamic bit allocation and works well with ExLlama.</li>
      <li><strong>SpinQuant</strong>: Select when targeting <strong>4-bit quantization of both weights and activations</strong> while retaining high accuracy (e.g., for academic or performance-sensitive production use). Best for <strong>W4A4KV4</strong> targets with pre-deployment compute budget.</li>
    </ul>
  </li>
  <li>
    <p><strong>For Full W8A8 Quantization (Weight + Activation)</strong>:</p>

    <ul>
      <li>
        <p><em>Use: SmoothQuant, AWEQ</em></p>
      </li>
      <li><strong>SmoothQuant</strong>: The best choice for <strong>training-free, full <code class="language-plaintext highlighter-rouge">int8</code> quantization</strong> with minimal setup. Choose for <strong>static quantization</strong> pipelines on <strong>NLP models</strong> like Llama, OPT, or BLOOM where CPU or GPU <code class="language-plaintext highlighter-rouge">int8</code> inference is desired.</li>
      <li><strong>AWEQ</strong>: Prefer this over SmoothQuant when you need <strong>better accuracy</strong> with <strong>activation-weight balance</strong>, especially for models that are hard to quantize (e.g., with long-tailed distributions). Supports both <strong>W8A8 and ultra-low-bit variants</strong>, and requires <strong>no fine-tuning</strong>.</li>
    </ul>
  </li>
  <li>
    <p><strong>For Mixed-Precision or Variable-Bitrate Quantization</strong>:</p>

    <ul>
      <li>
        <p><em>Use: EXL2, GGUF (K-Quants, I-Quants)</em></p>
      </li>
      <li><strong>EXL2</strong>: Use when deploying models in <strong>memory-constrained</strong> environments but still wanting to preserve <strong>key model behavior</strong> via <strong>bit allocation per group</strong>. Especially useful for <strong>interactive LLMs</strong> on laptops or desktops.</li>
      <li><strong>GGUF (K/I-Quants)</strong>: Ideal for <strong>offline, file-efficient packaging</strong> and <strong>CPU or mobile inference</strong> with tooling like <code class="language-plaintext highlighter-rouge">llama.cpp</code>. Offers a trade-off between compatibility and compression via <strong>predefined quant profiles</strong> (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-104-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1282" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1283"><span class="mi" id="MathJax-Span-1284" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1285"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1286" style="font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1287"><span class="mrow" id="MathJax-Span-1288"><span class="msubsup" id="MathJax-Span-1289"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1290" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1291"><span class="mrow" id="MathJax-Span-1292"><span class="mi" id="MathJax-Span-1293" style="font-size: 50%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><msub><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mrow class="MJX-TeXAtom-ORD"><mi>S</mi></mrow></msub></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-104">Q3_{K_{S}}</script>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-105-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msub&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1294" style="width: 3.023em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1295"><span class="mi" id="MathJax-Span-1296" style="font-family: STIXGeneral-Italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1297" style="font-family: STIXGeneral-Italic;">Q</span><span class="msubsup" id="MathJax-Span-1298"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1299" style="font-family: STIXGeneral-Regular;">4</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1300"><span class="mrow" id="MathJax-Span-1301"><span class="mi" id="MathJax-Span-1302" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1303" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-105">IQ4_{XS}</script>, etc.).</li>
    </ul>
  </li>
  <li>
    <p><strong>For Extreme Compression with Customization or Training Support</strong>:</p>

    <ul>
      <li>
        <p><em>Use: Palettization (Weight Clustering)</em></p>
      </li>
      <li>Use palettization when <strong>maximum compression</strong> is needed and <strong>some fine-tuning is acceptable</strong>. Ideal for <strong>mobile deployment</strong> or <strong>experimental architectures</strong> where LUTs and centroid representation can drastically reduce size.</li>
      <li>Choose <strong>vector clustering</strong> when structure preservation in weight matrices matters (e.g., vision-transformer hybrids or customized transformer blocks).</li>
    </ul>
  </li>
  <li>
    <p><strong>For Legacy or Format-Constrained Inference</strong>:</p>

    <ul>
      <li>
        <p><em>Use: GGUF (Legacy Quants)</em></p>
      </li>
      <li>Best suited for <strong>lightweight, portable LLM inference</strong> on CPU or embedded hardware via <code class="language-plaintext highlighter-rouge">llama.cpp</code>.</li>
      <li>Use when you need <strong>fast loading</strong>, <strong>offline conversion</strong>, and <strong>minimal dependencies</strong>, especially for local LLMs or desktop chatbots.</li>
    </ul>
  </li>
  <li>
    <p><strong>If Activation Quantization Is Not Required (Weight-Only Models)</strong>:</p>

    <ul>
      <li>
        <p><em>Use: AWQ, GPTQ, FPTQuant, EXL2</em></p>
      </li>
      <li>These methods focus on <strong><code class="language-plaintext highlighter-rouge">int3/4</code> weight-only quantization</strong> without modifying activations (typically left in <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code>).</li>
      <li>Use these when <strong>inference memory is not your bottleneck</strong>, and you want the best <strong>latency-to-accuracy</strong> trade-off without model retraining.</li>
    </ul>
  </li>
  <li>
    <p><strong>Summary Decision Matrix</strong>:</p>
  </li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Goal / Constraint</strong></th>
<th class="tg-hcenter-valign-second"><strong>Recommended Method(s)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Fastest low-bit inference (no training)</td>
<td class="tg-tleft-valign-second">AWQ, FPTQuant</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Best W4A4 quantization accuracy</td>
<td class="tg-tleft-valign-second">SpinQuant</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">W8A8 quant with balanced scaling</td>
<td class="tg-tleft-valign-second">SmoothQuant, AWEQ</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Largest models on smallest VRAM</td>
<td class="tg-tleft-valign-second">EXL2, GPTQ</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Mixed precision with dynamic control</td>
<td class="tg-tleft-valign-second">EXL2, GGUF (K/I-Quants)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">CPU/mobile inference with format support</td>
<td class="tg-tleft-valign-second">GGUF</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Edge deployment with low compute budget</td>
<td class="tg-tleft-valign-second">AWQ, GGUF (Legacy)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Extreme compression + retraining allowed</td>
<td class="tg-tleft-valign-second">Palettization</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Activation-aware optimizations</td>
<td class="tg-tleft-valign-second">SpinQuant, SmoothQuant, AWEQ</td>
</tr>
</tbody>
</table>
</div>

<h4 id="comparative-analysis-1">Comparative Analysis</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Method</strong></th>
<th class="tg-hcenter-valign-first"><strong>Type</strong></th>
<th class="tg-hcenter-valign-first"><strong>What Quantized</strong></th>
<th class="tg-hcenter-valign-first"><strong>Bits</strong></th>
<th class="tg-hcenter-valign-first"><strong>Training-Free?</strong></th>
<th class="tg-hcenter-valign-first"><strong>Key Innovation</strong></th>
<th class="tg-hcenter-valign-first"><strong>Accuracy Retention</strong></th>
<th class="tg-hcenter-valign-second"><strong>Introduced</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Uniform Quantization</td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights ± Activations</td>
<td class="tg-tleft-valign-first">4–8 bit</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Simple affine mapping, per-tensor or per-channel</td>
<td class="tg-tleft-valign-first">Good for smooth distributions (~2pt drop)</td>
<td class="tg-tleft-valign-second">(Fundamental baseline)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2210.17323">GPTQ</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">INT3/4 (also 2-bit)</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Second-order Hessian-based error compensation per-layer</td>
<td class="tg-tleft-valign-first">Highly accurate (perplexity within ~0.03 of <code>float16</code>)</td>
<td class="tg-tleft-valign-second">Oct 2022</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2211.10438">SmoothQuant</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights + Activations</td>
<td class="tg-tleft-valign-first">W8A8</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Scaling activation/weights to balance quantization difficulty</td>
<td class="tg-tleft-valign-first">Very high (&lt;0.5 % loss)</td>
<td class="tg-tleft-valign-second">Nov 2022</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2306.00978">AWQ</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">W4</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Activation-aware per-channel scaling via calibration</td>
<td class="tg-tleft-valign-first">High (&gt;<code>float16</code>)</td>
<td class="tg-tleft-valign-second">Jun 2023</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://github.com/ggml-org/llama.cpp">GGUF</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">2–8 bit (block-based)</td>
<td class="tg-tleft-valign-first">yes (quantized offline)</td>
<td class="tg-tleft-valign-first">Flexible binary format with various schemes, importance matrices embedded</td>
<td class="tg-tleft-valign-first">Varies by scheme; efficient loading</td>
<td class="tg-tleft-valign-second">Aug 2023 (as part of `llama.cpp`)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2311.01305">AWEQ</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights + Activations</td>
<td class="tg-tleft-valign-first">W4 or W8A8</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Activation-weight equalization + bias correction</td>
<td class="tg-tleft-valign-first">Best-in-class (within &lt;0.01 absolute from <code>float16</code>)</td>
<td class="tg-tleft-valign-second">Nov 2023</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://github.com/chu-tianxiang/exl2-for-all?utm_source=chatgpt.com">EXL2 (ExLlamaV2)</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization (dynamic allocation)</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">Mixed: 2–8 bit</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">GPTQ-based mixed-bit allocations per layer via error minimization</td>
<td class="tg-tleft-valign-first">Very high; Llama2-70B runs on 24 GB GPU</td>
<td class="tg-tleft-valign-second">Nov 2023</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2405.16406">SpinQuant</a></td>
<td class="tg-tleft-valign-first">Quantization Aware Training (calibration + optimization)</td>
<td class="tg-tleft-valign-first">Weights + Activations + KV-cache</td>
<td class="tg-tleft-valign-first">W4A4KV4</td>
<td class="tg-tleft-valign-first">no (requires calibration + optimization)</td>
<td class="tg-tleft-valign-first">Learned orthonormal rotations to normalize distributions</td>
<td class="tg-tleft-valign-first">Within ~2.9 pt of FP</td>
<td class="tg-tleft-valign-second">May 2024</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://arxiv.org/abs/2506.04985">FPTQuant</a></td>
<td class="tg-tleft-valign-first">Post Training Quantization</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">W4 (weight-only)</td>
<td class="tg-tleft-valign-first">yes</td>
<td class="tg-tleft-valign-first">Function-preserving invertible transforms to activations, merged into weights</td>
<td class="tg-tleft-valign-first">Excellent (minimal loss)</td>
<td class="tg-tleft-valign-second">Jun 2025</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><a href="https://apple.github.io/coremltools/docs-guides/source/opt-palettization-overview.html">Palettization</a></td>
<td class="tg-tleft-valign-first">Quantization Aware (Training or Fine-tuning)</td>
<td class="tg-tleft-valign-first">Weights only</td>
<td class="tg-tleft-valign-first">2–4 bit (LUT index)</td>
<td class="tg-tleft-valign-first">no (requires fine-tuning or PAT)</td>
<td class="tg-tleft-valign-first">Weight clustering via k-means with optional vector or group granularity</td>
<td class="tg-tleft-valign-first">High (with PAT); moderate otherwise</td>
<td class="tg-tleft-valign-second">2024</td>
</tr>
</tbody>
</table>
</div>

<h3 id="multimodal-quantization">Multimodal Quantization</h3>

<ul>
  <li>Quantizing multimodal LLMs—especially Vision-Language Models (VLMs) such as BLIP-2, LLaVA, or Flamingo—presents unique complexities not encountered in traditional text-only LLMs. These models process and fuse inputs from disparate modalities (e.g., images and text), resulting in heterogeneous model architectures and dynamic activation distributions that resist uniform quantization techniques.</li>
  <li>The inherent heterogeneity in architecture, distribution, and task metrics makes naive post-training quantization insufficient for production-grade deployment. Mixed-precision and QAT remain the most promising paths forward, especially when combined with robust calibration data and modality-aware loss functions. As VLMs become more prevalent in edge AI and on-device inference, a new generation of quantization-aware toolchains will be essential to unlock their full potential.</li>
</ul>

<h4 id="why-vlm-quantization-is-more-complex">Why VLM Quantization is More Complex</h4>

<ul>
  <li>
    <p>Multimodal models are composed of at least two distinct processing pipelines—one for each modality (e.g., image and text)—and often a third for cross-modal alignment or fusion. This architectural heterogeneity introduces the following challenges:</p>

    <ul>
      <li><strong>Diverse tensor statistics</strong>: Vision and language inputs yield activations with vastly different distributions and dynamic ranges, making uniform quantization impractical across modalities.</li>
      <li><strong>Cross-modal attention sensitivity</strong>: Cross-attention layers that fuse modalities are especially fragile to precision loss, as they are responsible for preserving semantic alignment between vision and language.</li>
      <li><strong>Embedding alignment</strong>: Vision embeddings (e.g., image patches from ViTs) and text embeddings must remain aligned for effective fusion. Quantization artifacts can easily disrupt this shared embedding space.</li>
      <li><strong>Lack of inductive biases</strong>: Unlike CNNs, which offer natural robustness to quantization via spatial weight sharing and locality, ViTs and transformers often rely more heavily on learned long-range dependencies, which are easily degraded by quantization noise.</li>
      <li><strong>Multi-objective optimization</strong>: A VLM may be used across many tasks (e.g., captioning, VQA, grounding), requiring quantized models to generalize well across domains, not just on language metrics like perplexity.</li>
    </ul>
  </li>
  <li>
    <p>To address these challenges, quantization of multimodal models typically involves hybrid and adaptive strategies as described below.</p>
  </li>
</ul>

<h4 id="quantizing-the-visual-backbone">Quantizing the Visual Backbone</h4>

<ol>
  <li>
    <p><strong>CNN-based encoders (e.g., ResNet, EfficientNet)</strong>:</p>

    <ul>
      <li>CNNs are relatively robust to quantization, and standard per-channel <code class="language-plaintext highlighter-rouge">int8</code> quantization (as used in MobileNet) can often be applied.</li>
      <li>Pre-trained encoders may be frozen and quantized independently of the rest of the model.</li>
    </ul>
  </li>
  <li>
    <p><strong>Vision Transformers (ViTs)</strong>:</p>

    <ul>
      <li>More sensitive to quantization due to their reliance on attention mechanisms and lack of inductive biases.</li>
      <li>Key operations such as softmax and positional embeddings are particularly fragile.</li>
      <li>Attention maps are harder to compress as they carry spatial relevance crucial for image understanding.</li>
    </ul>

    <p><strong>Best practices:</strong></p>

    <ul>
      <li>Use per-head or per-channel quantization for attention weights.</li>
      <li>Apply post-training quantization (PTQ) carefully, or use quantization-aware training (QAT) for attention-heavy layers.</li>
      <li>Maintain FP16 or BF16 precision in early layers or attention blocks if task-critical.</li>
    </ul>
  </li>
</ol>

<h4 id="quantizing-the-language-backbone">Quantizing the Language Backbone</h4>

<ul>
  <li>
    <p>Language processing in VLMs is typically transformer-based (e.g., LLaMA, T5, GPT-style decoders). The quantization techniques here are more mature and generally follow:</p>

    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">int4</code> quantization</strong> using post-training methods (e.g., GPTQ, AWQ, SmoothQuant).</li>
      <li><strong>Per-group or per-channel quantization</strong> for MLPs and attention blocks.</li>
      <li><strong>Mixed-precision inference</strong>, especially keeping attention output or layer norms in FP16 when accuracy is crucial.</li>
    </ul>
  </li>
</ul>

<h4 id="cross-modal-projection-and-fusion-layer-quantization">Cross-Modal Projection and Fusion Layer Quantization</h4>

<ul>
  <li>
    <p>This is the most critical and fragile component in VLMs. The fusion modules align visual and textual embeddings into a shared latent space.</p>
  </li>
  <li><strong>Cross-attention layers</strong> are highly sensitive to quantization because they match image regions with textual tokens. Errors here degrade the entire model’s reasoning ability.</li>
  <li>
    <p><strong>Query Transformers (Q-formers)</strong>, as used in BLIP-2, process image features into language-style prompts. Quantization here must preserve alignment fidelity.</p>
  </li>
  <li>
    <p><strong>Strategies:</strong></p>

    <ul>
      <li>Retain cross-modal fusion (specifically, projection and/or cross-attention layers) in FP16 or use higher-precision <code class="language-plaintext highlighter-rouge">int8</code>.</li>
      <li>Apply QAT to cross-modal components to preserve alignment under quantization-induced rounding and clipping.</li>
      <li>Use per-tensor calibration based on multimodal datasets to balance activations across modalities.</li>
    </ul>
  </li>
</ul>

<h4 id="quantization-aware-training-qat-in-vlms">Quantization-Aware Training (QAT) in VLMs</h4>

<ul>
  <li>
    <p>Due to sensitivity in fusion and vision branches, QAT is often <em>required</em> for VLMs, unlike in pure-text LLMs where PTQ often suffices.</p>
  </li>
  <li>During QAT, fake quantization layers simulate precision loss during both forward and backward passes.</li>
  <li>
    <p>Loss functions may include:</p>

    <ul>
      <li><strong>Cross-modal alignment loss</strong> (cosine similarity of vision/text embeddings)</li>
      <li><strong>Task-specific loss</strong> (e.g., VQA classification loss)</li>
      <li><strong>KL divergence or logit-matching</strong> between full-precision and quantized models</li>
    </ul>
  </li>
  <li>
    <p>Progressive QAT approaches are sometimes used:</p>

    <ul>
      <li>Freeze the vision encoder</li>
      <li>Apply quantization noise gradually to fusion layers</li>
      <li>Fine-tune using diverse tasks to preserve robustness</li>
    </ul>
  </li>
</ul>

<h4 id="calibration-and-evaluation-in-vlms">Calibration and Evaluation in VLMs</h4>

<ul>
  <li>
    <p>Calibration datasets for VLMs must reflect the multimodal distribution. Suitable datasets include:</p>

    <ul>
      <li>MS-COCO (captioning)</li>
      <li>VQAv2 (VQA)</li>
      <li>GQA, VizWiz, RefCOCO (referring expression comprehension)</li>
    </ul>
  </li>
  <li>
    <p>Metrics to evaluate quantized VLMs differ from those used in text-only LLMs:</p>

    <ul>
      <li><strong>Captioning:</strong> CIDEr, BLEU, METEOR</li>
      <li><strong>VQA:</strong> answer accuracy</li>
      <li><strong>Localization:</strong> IoU and precision</li>
    </ul>
  </li>
  <li>
    <p>These tasks are affected differently by quantization; degradation in fusion modules typically leads to sharper accuracy drops than in text-only settings.</p>
  </li>
</ul>

<h4 id="hybrid-and-mixed-precision-quantization">Hybrid and Mixed-Precision Quantization</h4>

<ul>
  <li>Given the disparity in sensitivity across components:
    <ul>
      <li>Use <strong><code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">int4</code></strong> for robust modules (e.g., MLPs, FFNs)</li>
      <li>Use <strong>FP16 or BF16</strong> for:
        <ul>
          <li>Cross-modal projections</li>
          <li>Final projection</li>
          <li>Optionally, try attention output and embedding normalization layers</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Mixed-precision kernels</strong> in deployment frameworks (e.g., TensorRT, OpenVINO) allow selective high-precision execution.</li>
</ul>

<h4 id="tooling-support">Tooling Support</h4>

<ul>
  <li>
    <p>Quantization tooling for VLMs is still developing. While text-only LLMs enjoy mature and well-integrated support in libraries like <strong>GPTQ</strong> and <strong>AWQ</strong>, multimodal quantization often requires combining custom workflows with early-stage tooling.</p>
  </li>
  <li>
    <p>Current options include:</p>

    <ul>
      <li>
        <p><strong><a href="https://developer.nvidia.com/tensorrt">TensorRT</a></strong>: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for <code class="language-plaintext highlighter-rouge">int8</code> and FP16. Requires ONNX export and hardware-specific calibration.</p>
      </li>
      <li>
        <p><strong><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">OpenVINO</a></strong>: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.</p>
      </li>
      <li>
        <p><strong><a href="https://huggingface.co/docs/optimum/overview">Hugging Face Optimum</a></strong>: A library bridging Transformers with hardware backends. Supports quantization workflows with <strong>Intel Neural Compressor</strong> and <strong>ONNX Runtime</strong> for vision+language models.</p>
      </li>
      <li>
        <p><strong><a href="https://intel.github.io/neural-compressor/">Intel Neural Compressor</a></strong>: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.</p>
      </li>
      <li>
        <p><strong><a href="https://github.com/haotian-liu/LLaVA">LLaVA Quantized Forks</a></strong>: Some forks of LLaVA apply <strong>GPTQ</strong>-style <code class="language-plaintext highlighter-rouge">int4</code> quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.</p>
      </li>
      <li>
        <p><strong><a href="https://github.com/salesforce/LAVIS">BLIP-2 Quantized Implementations</a></strong> (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.</p>
      </li>
      <li>
        <p>Emerging research tools are adapting <strong><a href="https://github.com/IST-DASLab/gptq">GPTQ</a></strong> and <strong><a href="https://github.com/mit-han-lab/llm-awq">AWQ</a></strong> to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Note: As of now, there is <strong>no unified framework</strong> for full-stack VLM quantization (vision encoder, fusion, and decoder) akin to what exists for LLaMA or GPT models. Most implementations involve manually freezing or partially quantizing the model.</p>

<h4 id="comparative-analysis-of-llms-vs-vlm-quantization">Comparative Analysis of LLMs vs. VLM Quantization</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Component</strong></th>
<th class="tg-hcenter-valign-first"><strong>Text LLM (e.g., GPT)</strong></th>
<th class="tg-hcenter-valign-second"><strong>VLM (e.g., BLIP-2, LLaVA)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Architecture</td>
<td class="tg-tleft-valign-first">Uniform transformer</td>
<td class="tg-tleft-valign-second">Heterogeneous (ViT + LLM + cross-attn)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Sensitivity to quant.</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-second">High (esp. fusion layers)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Common quant methods</td>
<td class="tg-tleft-valign-first">PTQ, QAT, GPTQ, AWQ</td>
<td class="tg-tleft-valign-second">Mixed-precision, QAT, PTQ (limited)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Vision encoder</td>
<td class="tg-tleft-valign-first">N/A</td>
<td class="tg-tleft-valign-second">CNN/ViT, sensitive to <code>int4</code>/<code>int8</code></td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Cross-modal fusion</td>
<td class="tg-tleft-valign-first">N/A</td>
<td class="tg-tleft-valign-second">Needs higher precision</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Evaluation metric</td>
<td class="tg-tleft-valign-first">Perplexity</td>
<td class="tg-tleft-valign-second">Task-specific (e.g., VQA accuracy)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Tooling maturity</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-second">Low to medium</td>
</tr>
</tbody>
</table>
</div>

<h3 id="device-and-operator-support-across-frameworks">Device and Operator Support Across Frameworks</h3>

<h4 id="pytorch">PyTorch</h4>

<ul>
  <li>
    <p>Quantization in PyTorch is supported for a limited subset of operators, and the availability of these operators depends on the specific quantization approach being employed—dynamic, PTQ, or QAT. The list of supported quantized operators is not exhaustive and evolves with newer PyTorch releases. For an up-to-date reference, consult the official PyTorch quantization documentation <a href="https://pytorch.org/docs/stable/quantization.html">here</a>.</p>
  </li>
  <li>
    <p>The implementation of quantization in PyTorch is backend-dependent, meaning that both the quantization configuration (which defines how tensors are quantized) and the set of quantized kernels (which define how arithmetic is performed on quantized tensors) vary based on the target hardware. Currently, PyTorch provides official support for quantized inference only on CPUs, specifically for x86 and ARM architectures. These are supported via two primary backends:</p>

    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">fbgemm</code></strong>: Optimized for server-class x86 CPUs.</li>
      <li><strong><code class="language-plaintext highlighter-rouge">qnnpack</code></strong>: Designed for mobile ARM CPUs.</li>
    </ul>
  </li>
  <li>
    <p>The backend must be explicitly set to ensure compatibility between the model’s quantized representation and the runtime kernels, as shown in the example below:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code11"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code11"><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">backend</span> <span class="o">=</span> <span class="s">'fbgemm'</span>  <span class="c1"># 'qnnpack' for ARM/mobile inference
</span><span class="n">my_model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>

<span class="c1"># Prepare and convert model
# Set the backend on which the quantized kernels need to be run
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">backend</span>

<span class="c1"># Continue with model preparation and conversion steps...
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>QAT in PyTorch is performed in full-precision (<code class="language-plaintext highlighter-rouge">float32</code>) mode to leverage existing GPU or CPU hardware during training. This technique simulates the effects of quantization during training to improve model robustness when deployed with quantized weights. QAT is particularly beneficial for convolutional neural networks (CNNs), especially lightweight models such as MobileNet, where static or dynamic post-training quantization may result in unacceptable accuracy degradation.</p>
  </li>
</ul>

<h5 id="integration-in-torchvision">Integration in <code class="language-plaintext Highlighter-rouge">torchvision</code></h5>

<ul>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">torchvision</code> library includes integrated support for quantization in several widely used neural network architectures. These include GoogLeNet, InceptionV3, ResNet (various depths), ResNeXt, MobileNet (V2 and V3), and ShuffleNet. The support is provided in three distinct forms to enable a range of workflows:</p>

    <ol>
      <li><strong>Pre-trained quantized model weights</strong>: These models are fully quantized and can be used directly for inference without additional fine-tuning.</li>
      <li><strong>Quantization-ready model definitions</strong>: These are versions of the models with quantization stubs pre-inserted, making them suitable for post-training quantization or QAT.</li>
      <li><strong>QAT scripts</strong>: Scripts are available to perform QAT on supported models. While these scripts are applicable to all the models listed above, empirical evaluations show that QAT tends to yield significant accuracy benefits primarily for lightweight models like MobileNet.</li>
    </ol>
  </li>
  <li>
    <p><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb">Here’s</a> a dedicated tutorial demonstrating how to perform transfer learning with quantization using pre-trained models from <code class="language-plaintext highlighter-rouge">torchvision</code>. This enables developers to take advantage of quantized inference while still adapting models to custom datasets and deployment scenarios.</p>
  </li>
</ul>

<h5 id="resources">Resources</h5>

<ul>
  <li>To get started on quantizing your models in PyTorch, start with the tutorials on the <a href="https://pytorch.org/tutorials/#model-optimization">PyTorch website</a>.</li>
  <li>If you are working with sequence data, start with…
    <ul>
      <li><a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html">Dynamic quantization for LSTM</a>, or</li>
      <li><a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">Dynamic quantization for BERT</a></li>
    </ul>
  </li>
  <li>If you are working with image data then we recommend starting with the <a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb">transfer learning with quantization tutorial</a>. Then you can explore <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">static post training quantization</a>.
    <ul>
      <li>If you find that the accuracy drop with post training quantization is too high, then try <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">quantization aware training</a>.</li>
    </ul>
  </li>
</ul>

<h4 id="tensorflow">TensorFlow</h4>

<ul>
  <li>
    <p>Quantization support in TensorFlow is primarily centered around deployment through TensorFlow Lite (TFLite). Only a subset of TensorFlow operations are supported in quantized form when converting models to run efficiently on edge devices. For a comprehensive list of quantization-compatible operators, refer to the official TFLite operator compatibility documentation <a href="https://ai.google.dev/edge/litert/models/ops_compatibility">here</a>.</p>
  </li>
  <li>
    <p>Quantized inference in TensorFlow is enabled through TensorFlow Lite (TFLite) delegates, which provide optimized execution across various hardware backends. These include:</p>

    <ul>
      <li><strong>CPU Delegate</strong>: Supports <code class="language-plaintext highlighter-rouge">int8</code> and <code class="language-plaintext highlighter-rouge">float16</code> quantized models using XNNPack kernels, which are enabled by default in modern TFLite runtimes.</li>
      <li><strong>GPU Delegate</strong>: Accelerates inference on mobile and embedded GPUs. It supports <code class="language-plaintext highlighter-rouge">float16</code> quantization and, in limited cases, <code class="language-plaintext highlighter-rouge">int8</code> precision. The delegate is available on both Android and iOS platforms.</li>
      <li><strong>NNAPI Delegate</strong> (Android only): Interfaces with on-device hardware acceleration drivers. Quantized <code class="language-plaintext highlighter-rouge">int8</code> models are typically supported and can see performance improvements depending on the device and vendor-specific drivers.</li>
      <li>
        <p><strong>Edge TPU Delegate</strong>: Targets Google’s Coral hardware and supports only fully integer quantized models with <code class="language-plaintext highlighter-rouge">int8</code> weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.</p>
      </li>
      <li>The level of operator support and performance characteristics differ by delegate. For example, the Edge TPU requires that all operations be quantized and supported by its limited op set. Any unsupported operations will result in compilation failure or will require fallback to CPU, which can significantly affect performance. As such, developers must validate operator compatibility prior to deployment by reviewing the <a href="https://ai.google.dev/edge/litert/models/ops_compatibility">TFLite ops compatibility guide</a> and testing with their target delegate.</li>
    </ul>
  </li>
  <li>
    <p>QAT in TensorFlow is implemented using the <code class="language-plaintext highlighter-rouge">tfmot.quantization.keras.quantize_model</code> API available through the TensorFlow Model Optimization Toolkit (TFMOT). Similar to PyTorch, QAT in TensorFlow is performed in floating point, allowing the model to simulate quantized behavior during training while still leveraging GPU acceleration. This helps preserve accuracy for models that do not respond well to post-training quantization, such as compact architectures like MobileNet or custom CNNs. The general trade-offs between PTQ and QAT in TensorFlow align closely with those in PyTorch, although some feature and operator support mismatches still exist between the two frameworks.</p>
  </li>
  <li>
    <p>When using post-training quantization or QAT, it’s important to validate that all critical model operations are supported in TFLite with quantized equivalents. Unsupported operations may be automatically left in float, potentially degrading the intended performance benefits of quantization.</p>
  </li>
</ul>

<h5 id="integration-in-tfkerasapplications">Integration in <code class="language-plaintext Highlighter-rouge">tf.keras.applications</code></h5>

<ul>
  <li>
    <p>While TensorFlow does not provide pre-quantized models in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications"><code class="language-plaintext highlighter-rouge">tf.keras.applications</code></a>, the Model Optimization Toolkit provides utilities to quantize these models post-training or prepare them for QAT. Developers can load a model from <code class="language-plaintext highlighter-rouge">tf.keras.applications</code>, apply quantization via TFMOT, and then convert it to TFLite. The process typically involves:</p>

    <ol>
      <li>Cloning the model with quantization-aware layers using <code class="language-plaintext highlighter-rouge">quantize_model</code>.</li>
      <li>Fine-tuning the quantized model if needed.</li>
      <li>Converting the trained model to TFLite using the TFLiteConverter.</li>
    </ol>
  </li>
  <li>
    <p>TFLite provides tools and guidelines for performing transfer learning with quantized models, though, as with PyTorch, QAT tends to be necessary mainly for accuracy-sensitive lightweight models.</p>
  </li>
</ul>

<h5 id="resources-1">Resources</h5>

<ul>
  <li>
    <p>TensorFlow’s PTQ techniques are detailed in the <a href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training">post-training quantization guide</a>.</p>
  </li>
  <li>
    <p>QAT is covered in the <a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">QAT guide</a>.</p>
  </li>
</ul>

<h4 id="coreml">CoreML</h4>

<ul>
  <li>
    <p>Quantization support in CoreML is integrated directly into the CoreML Tools conversion pipeline. Quantization can be applied during model conversion from popular frameworks (such as PyTorch or TensorFlow) to the <code class="language-plaintext highlighter-rouge">.mlmodel</code> format using the <a href="https://coremltools.readme.io/">coremltools</a> Python API. The supported quantization schemes are primarily weight-only quantization, with formats including:</p>

    <ul>
      <li>
        <p><strong><code class="language-plaintext highlighter-rouge">float16</code></strong>: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in <code class="language-plaintext highlighter-rouge">float16</code>, so <code class="language-plaintext highlighter-rouge">float16</code> quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.</p>
      </li>
      <li>
        <p><strong>Linear <code class="language-plaintext highlighter-rouge">int8</code> Weight Quantization</strong>: Supported through offline quantization in <code class="language-plaintext highlighter-rouge">coremltools</code>, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">float32</code> internally on GPU/CPU/NPU. Operator and backend support for <code class="language-plaintext highlighter-rouge">int8</code> quantization is more limited compared to <code class="language-plaintext highlighter-rouge">float16</code>.</p>
      </li>
      <li>
        <p><strong>Custom bit-width quantization</strong>: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Post-training quantization (PTQ) in CoreML is performed by passing additional parameters to <code class="language-plaintext highlighter-rouge">coremltools.convert</code> or applying the <code class="language-plaintext highlighter-rouge">coremltools.models.neural_network.quantization_utils</code> module to an existing <code class="language-plaintext highlighter-rouge">.mlmodel</code>. For example, <code class="language-plaintext highlighter-rouge">float16</code> weight quantization is typically invoked as:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code12"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code12"><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="n">ct</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">traced_model</span><span class="p">,</span> 
    <span class="n">convert_to</span><span class="o">=</span><span class="s">"mlprogram"</span><span class="p">,</span>
    <span class="n">compute_units</span><span class="o">=</span><span class="n">ct</span><span class="p">.</span><span class="n">ComputeUnit</span><span class="p">.</span><span class="n">ALL</span>
<span class="p">)</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">neural_network</span><span class="p">.</span><span class="n">quantization_utils</span><span class="p">.</span><span class="n">quantize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nbits</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">quantized_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"model_float16.mlmodel"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>CoreML does not currently provide a native, framework-integrated Quantization-Aware Training (QAT) pipeline equivalent to TensorFlow Model Optimization Toolkit or PyTorch’s QAT modules. Instead, QAT must be performed in the source framework prior to export, and the resulting quantization parameters must be preserved during conversion—if the target CoreML format and operators support them. In practice, this is mostly applicable to simulated <code class="language-plaintext highlighter-rouge">float16</code> or weight-clipped models, as CoreML conversion generally re-encodes models in its own quantization formats.</p>
  </li>
  <li>
    <p>Hardware execution backends in CoreML include:</p>

    <ul>
      <li><strong>CPU</strong>: Executes in <code class="language-plaintext highlighter-rouge">float32</code> or <code class="language-plaintext highlighter-rouge">float16</code>, with weight-only <code class="language-plaintext highlighter-rouge">int8</code> quantization supported in some cases.</li>
      <li><strong>GPU</strong>: Primarily executes in <code class="language-plaintext highlighter-rouge">float16</code> precision. <code class="language-plaintext highlighter-rouge">float16</code> weight quantization typically does not change GPU arithmetic precision but reduces memory usage.</li>
      <li><strong>Apple Neural Engine (ANE)</strong>: Supports some <code class="language-plaintext highlighter-rouge">int8</code> operations and mixed-precision execution. Operator coverage for quantized <code class="language-plaintext highlighter-rouge">int8</code> is limited and depends on both CoreML runtime version and the specific ANE generation.</li>
    </ul>
  </li>
  <li>
    <p>Developers should verify operator compatibility after quantization, as unsupported quantized layers will be automatically dequantized and executed in higher precision, potentially negating performance or memory savings. The <a href="https://coremltools.readme.io/docs/quantization">CoreML Tools documentation on quantization</a> provides detailed guidance on supported modes and API usage.</p>
  </li>
</ul>

<h5 id="integration-with-pytorch-and-tensorflow-models">Integration with PyTorch and TensorFlow Models</h5>

<ul>
  <li>
    <p>When converting PyTorch models to CoreML using <code class="language-plaintext highlighter-rouge">torch.jit.trace</code> or <code class="language-plaintext highlighter-rouge">torch.jit.script</code>, quantization should generally be applied during or after conversion via <code class="language-plaintext highlighter-rouge">coremltools</code> rather than relying on PyTorch’s native quantization formats, as these may not be mapped directly to CoreML equivalents.</p>
  </li>
  <li>
    <p>TensorFlow models exported via SavedModel or TFLite can be converted to CoreML, but quantized TFLite <code class="language-plaintext highlighter-rouge">int8</code> models are usually re-encoded in <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">float32</code> in the final <code class="language-plaintext highlighter-rouge">.mlmodel</code> unless explicitly mapped to CoreML’s weight-only <code class="language-plaintext highlighter-rouge">int8</code> quantization.</p>
  </li>
  <li>
    <p>In both cases, the recommended process for <code class="language-plaintext highlighter-rouge">float16</code> quantization is:</p>

    <ol>
      <li>Train or fine-tune the model in the source framework.</li>
      <li>Export to an intermediate format (TorchScript, ONNX, SavedModel).</li>
      <li>Convert to CoreML using <code class="language-plaintext highlighter-rouge">coremltools.convert</code>.</li>
      <li>Apply <code class="language-plaintext highlighter-rouge">float16</code> weight quantization via <code class="language-plaintext highlighter-rouge">quantize_weights</code>.</li>
      <li>Validate model accuracy and operator execution backend in Xcode or using the CoreML runtime.</li>
    </ol>
  </li>
</ul>

<h5 id="resources-2">Resources</h5>

<ul>
  <li><a href="https://coremltools.readme.io/docs/quantization">CoreML Tools Quantization Documentation</a></li>
  <li><a href="https://github.com/apple/coremltools">CoreML Tools GitHub Repository</a></li>
</ul>

<h4 id="jax">JAX</h4>

<ul>
  <li>
    <p>Quantization support in JAX is not built directly into the core library, as JAX is designed to be a high-performance array computation framework with a functional API and just-in-time (JIT) compilation through the XLA compiler (Accelerated Linear Algebra). Instead, quantization workflows in JAX are implemented through external libraries and ecosystem tools that target specific hardware backends. Prominent examples include:</p>

    <ul>
      <li><strong>Flax</strong>: A neural network library for JAX that provides high-level model definitions but does not offer native quantization APIs. Quantization is typically performed by integrating Flax models with downstream compilers or deployment toolchains.</li>
      <li><strong><code class="language-plaintext highlighter-rouge">jax.lax</code></strong> and <strong>custom lowering to XLA</strong>: Developers can manually simulate quantization during training by inserting quantization and dequantization operations using <code class="language-plaintext highlighter-rouge">jax.lax</code> primitives. These operations are compiled into the XLA graph and can be mapped to quantized kernels if the target backend supports them.</li>
      <li>
        <p><strong>External compilers</strong>:</p>

        <ul>
          <li><strong>TensorFlow Lite via <code class="language-plaintext highlighter-rouge">jax2tf</code></strong>: Models written in JAX can be converted to TensorFlow using <code class="language-plaintext highlighter-rouge">jax.experimental.jax2tf</code>, and then quantized using TFLite’s post-training or QAT pipelines.</li>
          <li><strong>XLA backends for TPU/CPU</strong>: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.</li>
          <li><strong>OpenXLA / IREE</strong>: Experimental support exists for lowering JAX computations to IREE, which can target <code class="language-plaintext highlighter-rouge">int8</code> quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Because JAX itself does not provide a quantized operator registry, operator support for quantized execution depends entirely on the downstream backend. For example:</p>

    <ul>
      <li>When exporting to TFLite, the available quantized ops match those documented in the <a href="https://ai.google.dev/edge/litert/models/ops_compatibility">TFLite operator compatibility guide</a>.</li>
      <li>When compiling to <code class="language-plaintext highlighter-rouge">XLA:TPU</code> or <code class="language-plaintext highlighter-rouge">XLA:CPU</code>, support for integer quantization is highly backend-specific and often limited to linear and convolution operations.</li>
      <li>On GPUs, JAX generally runs computations in <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code> for mixed-precision training/inference, rather than integer quantization.</li>
    </ul>
  </li>
  <li>
    <p><strong>Quantization-Aware Training (QAT)</strong> in JAX is typically implemented manually, as there is no built-in helper API equivalent to PyTorch’s <code class="language-plaintext highlighter-rouge">torch.quantization</code> or TensorFlow’s TFMOT. The common workflow is:</p>

    <ol>
      <li>Insert fake quantization nodes (scale + round + clip) into the model during training to simulate integer precision effects.</li>
      <li>Train the model using JAX transformations (<code class="language-plaintext highlighter-rouge">jit</code>, <code class="language-plaintext highlighter-rouge">grad</code>, <code class="language-plaintext highlighter-rouge">pmap</code>) as usual, with quantization simulation integrated into the forward pass.</li>
      <li>Export the trained model to a backend that supports true integer quantized kernels (e.g., TFLite or IREE).</li>
    </ol>

    <ul>
      <li>This approach requires careful control over numerical ranges and scale factors, which are not automatically managed by the framework.</li>
    </ul>
  </li>
  <li>
    <p><strong>Hardware backend considerations</strong> for quantized JAX models:</p>

    <ul>
      <li><strong>CPU (<code class="language-plaintext highlighter-rouge">XLA:CPU</code>)</strong>: Can execute integer operations if the compiled XLA graph contains integer kernels, but full <code class="language-plaintext highlighter-rouge">int8</code> operator coverage is limited compared to float execution.</li>
      <li><strong>TPU (<code class="language-plaintext highlighter-rouge">XLA:TPU</code>)</strong>: Supports <code class="language-plaintext highlighter-rouge">int8</code> matmul and convolution on newer TPU architectures, but model preparation for TPU quantization requires manual lowering.</li>
      <li><strong>GPU (<code class="language-plaintext highlighter-rouge">XLA:GPU</code>)</strong>: Typically favors <code class="language-plaintext highlighter-rouge">float16</code>/<code class="language-plaintext highlighter-rouge">bfloat16</code> mixed-precision execution. Integer quantization is not a standard deployment path.</li>
      <li><strong>Edge/Embedded</strong>: Usually requires exporting to TFLite or another inference framework; quantization support and operator coverage then depend entirely on that target runtime.</li>
    </ul>
  </li>
</ul>

<h5 id="integration-examples">Integration Examples</h5>

<ul>
  <li>
    <p><strong>Exporting JAX models for quantized deployment via TFLite</strong>:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code13"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code13"><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">jax2tf</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Define a simple JAX function
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,)))</span>
<span class="n">tf_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">jax2tf</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">model</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">with_gradient</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="n">input_signature</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">([</span><span class="bp">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Convert to TFLite with quantization
</span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">TFLiteConverter</span><span class="p">.</span><span class="n">from_concrete_functions</span><span class="p">([</span><span class="n">tf_model</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">()])</span>
<span class="n">converter</span><span class="p">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">Optimize</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">tflite_quant_model</span> <span class="o">=</span> <span class="n">converter</span><span class="p">.</span><span class="n">convert</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Manual QAT simulation in JAX</strong>:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code14"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code14"><span class="k">def</span> <span class="nf">fake_quant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">qmin</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">qmax</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">x_scaled</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">scale</span>
    <span class="n">x_rounded</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">x_scaled</span><span class="p">),</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_rounded</span> <span class="o">*</span> <span class="n">scale</span>
</code></pre></div>    </div>

    <ul>
      <li>This kind of manual fake-quant insertion is common when prototyping QAT in JAX.</li>
    </ul>
  </li>
</ul>

<h5 id="resources-3">Resources</h5>

<ul>
  <li><a href="https://jax.readthedocs.io/en/latest/jax2tf.html">jax2tf Documentation</a> – Guide to converting JAX functions to TensorFlow for deployment.</li>
  <li><a href="https://flax.readthedocs.io/en/latest/">Flax Documentation</a> – JAX-based neural network library (does not provide quantization directly).</li>
  <li><a href="https://www.tensorflow.org/lite/performance/post_training_quantization">TFLite Quantization Guide</a> – Relevant for JAX models exported via TensorFlow.</li>
  <li><a href="https://openxla.org/">OpenXLA Project</a> – Future direction for cross-framework compilation, including quantization support.</li>
</ul>

<h3 id="choosing-the-right-quantization-approach">Choosing the Right Quantization Approach</h3>

<ul>
  <li>
    <p>The choice of which scheme to use depends on multiple factors:</p>

    <ul>
      <li><strong>Model/Target requirements:</strong> Some models might be sensitive to quantization, requiring QAT.</li>
      <li><strong>Operator/Backend support:</strong> Some backends require fully quantized operators.</li>
    </ul>
  </li>
  <li>
    <p>Currently, operator coverage in PyTorch is limited and may restrict the choices listed in the table below. The table below from <a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">PyTorch: Introduction to Quantization on PyTorch</a> provides a guideline.</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/archquant.jpg" alt=""></p>

<h3 id="performance-results">Performance Results</h3>

<ul>
  <li>The table below from <a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">PyTorch: Introduction to Quantization on PyTorch</a> offers some sample results:</li>
</ul>

<p><img src="/primers/ai/assets/model-compression/perfres.jpg" alt=""></p>

<ul>
  <li>As seen in the table above, quantization yields a substantial reduction in both model loading time and memory footprint, driven by the 4× smaller model size compared to floating-point implementations. Furthermore, it offers a speedup of 2× to 3× compared to floating-point implementations, depending on the hardware platform and the model being benchmarked.</li>
</ul>

<h3 id="accuracy-results">Accuracy Results</h3>

<ul>
  <li>The tables below from <a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">PyTorch: Introduction to Quantization on PyTorch</a> compares the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we compared the F1 score of BERT on the GLUE benchmark for MRPC.</li>
</ul>

<h4 id="computer-vision-model-accuracy">Computer Vision Model Accuracy</h4>

<p><img src="/primers/ai/assets/model-compression/cvmodelacc.jpg" alt=""></p>

<h4 id="speech-and-nlp-model-accuracy">Speech and NLP Model Accuracy</h4>

<p><img src="/primers/ai/assets/model-compression/speechnlpmodelacc.jpg" alt=""></p>

<h3 id="popular-quantization-libraries">Popular Quantization Libraries</h3>

<ul>
  <li>This section surveys widely used quantization libraries you’ll encounter in practice, what precision/algorithms they support, how they integrate with common stacks, and caveats that matter on-device or in production.</li>
</ul>

<h4 id="bitsandbytes">BitsAndBytes</h4>

<ul>
  <li>
    <p>BitsAndBytes is a CUDA-accelerated library for low-bit matrix multiply and optimizers. It popularized <code class="language-plaintext highlighter-rouge">LLM.int8()</code>, which uses vector-wise 8-bit quantization but routes “outlier” channels through higher precision matmuls, preserving accuracy while halving memory. It also provides 4-bit weight types used by QLoRA (NF4/<code class="language-plaintext highlighter-rouge">float4</code>), plus 8-bit optimizers. In Transformers you typically pass a BitsAndBytesConfig (<code class="language-plaintext highlighter-rouge">load_in_4bit</code> or <code class="language-plaintext highlighter-rouge">load_in_8bit</code>, <code class="language-plaintext highlighter-rouge">bnb_4bit_quant_type="nf4"</code> or <code class="language-plaintext highlighter-rouge">"fp4"</code>, <code class="language-plaintext highlighter-rouge">bnb_4bit_use_double_quant</code>, and <code class="language-plaintext highlighter-rouge">bnb_4bit_compute_dtype</code> for BF16/FP16 accumulation).</p>
  </li>
  <li>
    <p><strong>Typical usage (Transformers):</strong></p>

    <ul>
      <li>Construct BitsAndBytesConfig with <code class="language-plaintext highlighter-rouge">load_in_4bit=True</code> and your preferred compute dtype (often BF16 for stability)</li>
      <li>Load the model with <code class="language-plaintext highlighter-rouge">device_map="auto"</code>` to shard to available GPUs/CPU if needed</li>
      <li>When merging adapters back into base weights, dequantize before merging to avoid rounding artifacts, then requantize.</li>
    </ul>
  </li>
  <li>
    <p><strong>Notes and caveats:</strong></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">LLM.int8</code> is weight-only at matmul inputs; activations remain FP16/BF16, so runtime memory isn’t halved unless your serving stack fuses kernels well.</li>
      <li>Performance varies by kernel/backend (CUDA vs. ROCm); vendor guides show the expected bottlenecks and setup.</li>
    </ul>
  </li>
</ul>

<h4 id="hugging-face-optimum">Hugging Face Optimum</h4>

<ul>
  <li>
    <p>Optimum is a meta-tooling layer that wraps several backends and exposes unified quantization APIs.</p>

    <ul>
      <li><strong>Optimum ONNX Runtime:</strong> PTQ/QAT flows via ORTConfig/ORTQuantizer; supports dynamic (weights int8, activations at runtime), static/QLinear/QDQ with calibration, and QAT. Target for CPU, some NPUs, and cross-platform packaging.</li>
      <li><strong>Optimum Intel (OpenVINO):</strong> hooks into NNCF for <code class="language-plaintext highlighter-rouge">int8</code> PTQ and QAT; good CPU/iGPU latency and small memory footprint; provides export to OpenVINO IR and pipeline examples for LLMs.</li>
      <li><strong>Optimum Quanto:</strong> a PyTorch quantization backend that supports linear quantization of weights to float8, int8, int4, and even int2; works in eager mode, supports QAT, is device-agnostic, and integrates with torch.compile. Great for quick per-module weight-only experiments.</li>
      <li><strong>Optimum Habana (Gaudi):</strong> integrates Intel Neural Compressor flows to enable <code class="language-plaintext highlighter-rouge">float8</code>/<code class="language-plaintext highlighter-rouge">uint4</code> inference/training paths on HPU.</li>
      <li><strong>Transformers integration summary:</strong> Transformers supports AWQ, GPTQ, and bitsandbytes out-of-the-box; Optimum layers sit on top to add ORT/OpenVINO/Quanto/TensorRT flows.</li>
    </ul>
  </li>
</ul>

<h4 id="onnx-runtime-quantization">ONNX Runtime Quantization</h4>

<ul>
  <li>
    <p>ONNX Runtime implements 8-bit linear quantization with scale and zero-point per tensor or per channel, using either static calibration or dynamic quantization. The core mapping follows <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-106-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mn&gt;32&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1304" style="width: 8.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1007.09em, 2.711em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1305"><span class="msubsup" id="MathJax-Span-1306"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1307" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1308"><span class="mrow" id="MathJax-Span-1309"><span class="mi" id="MathJax-Span-1310" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1311" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">p</span><span class="mn" id="MathJax-Span-1312" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">32</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1313" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1314" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">s</span><span class="mo" id="MathJax-Span-1315" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mo" id="MathJax-Span-1316" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">(</span><span class="mi" id="MathJax-Span-1317" style="font-family: STIXGeneral-Italic;">q</span><span class="mo" id="MathJax-Span-1318" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-1319" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">z</span><span class="mo" id="MathJax-Span-1320" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">f</mi><mi mathvariant="normal">p</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>⋅</mo><mo stretchy="false">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-106">x_\mathrm{fp32} = s \cdot (q - z)</script>; APIs include <code class="language-plaintext highlighter-rouge">quantize_dynamic</code>, <code class="language-plaintext highlighter-rouge">quantize_static</code> (with calibration), and <code class="language-plaintext highlighter-rouge">quantize_qat</code> (for QAT-authored models).</p>
  </li>
  <li>
    <p><strong>Why choose ORT:</strong> stable operator coverage, portable deployment as a single ONNX artifact, and strong CPU performance. Use when you want one model to run across x86/ARM servers and many NPUs with the same runtime.</p>
  </li>
</ul>

<h4 id="nvidia-tensorrt--tensorrt-llm">NVIDIA TensorRT / TensorRT-LLM</h4>

<ul>
  <li>
    <p>TensorRT provides PTQ (with explicit quantization replacing older calibration APIs) and supports QAT. TensorRT-LLM adds LLM-specific kernels and recipes: <code class="language-plaintext highlighter-rouge">float8</code>/<code class="language-plaintext highlighter-rouge">float4</code>, <code class="language-plaintext highlighter-rouge">int4</code> AWQ, and <code class="language-plaintext highlighter-rouge">int8</code> SmoothQuant, plus inflight batching and paged KV cache for high throughput. Use TensorRT-LLM when serving NVIDIA-GPU LLMs with low latency at scale.</p>
  </li>
  <li>
    <p><strong>Practical tip:</strong> for <code class="language-plaintext highlighter-rouge">int8</code> LLMs on NVIDIA GPUs, SmoothQuant (W8A8) with per-channel weight and per-tensor activation scales is a common “safe” baseline; for weight-only speedups, AWQ (W4A16) is widely supported.</p>
  </li>
</ul>

<h4 id="intel-neural-compressor-inc">Intel Neural Compressor (INC)</h4>

<ul>
  <li>INC is a framework-agnostic toolkit (PyTorch, TensorFlow, ONNX Runtime, MXNet) offering PTQ, QAT, tuning strategies, and hardware-aware search for accuracy/latency/size trade-offs. It’s also the quantization backend used by several Optimum integrations (e.g., Gaudi). Use INC to automate calibration and accuracy recovery across Intel hardware.</li>
</ul>

<h4 id="openvino-tooling">OpenVINO Tooling</h4>

<ul>
  <li>OpenVINO provides NNCF-based post-training <code class="language-plaintext highlighter-rouge">int8</code> quantization and QAT, strong CPU/iGPU kernels, and file-size/runtime wins for NLP and CV. Workflows include simple PTQ with a small calibration set and hybrid schemes (e.g., MatMul/Embedding weight-only + activation <code class="language-plaintext highlighter-rouge">int8</code> elsewhere). Good match for desktop/server CPUs and integrated GPUs.</li>
</ul>

<h4 id="tensorflow-lite">TensorFlow Lite</h4>

<ul>
  <li>TFLite supports dynamic range, full-integer (<code class="language-plaintext highlighter-rouge">int8</code>) PTQ, <code class="language-plaintext highlighter-rouge">float16</code> weight compression, and QAT, with a well-specified int8 scheme (two’s complement; per-tensor/axis). Choose when deploying to mobile/embedded via NNAPI/Metal/Vulkan delegates.</li>
</ul>

<h4 id="apple-core-ml-tools">Apple Core ML Tools</h4>

<ul>
  <li>Core ML Tools supports weight-only linear quantization and palettization (weight clustering into a LUT) with palettization-aware training APIs that insert fake-quant/palette layers during fine-tuning, then fold them into compact weights at export. Use for native iOS/macOS deployments.</li>
</ul>

<h4 id="llm-specific-quantizers-gptq-awq-and-their-python-toolkits">LLM-Specific Quantizers (GPTQ, AWQ) and Their Python Toolkits</h4>

<ul>
  <li>GPTQ (weight-only, usually 4-bit): popular for LLMs; Optimum exposes a GPTQQuantizer, and Transformers documents a GPTQModel path. Note AutoGPTQ was archived in 2025; newer stacks may use GPTQModel via Transformers/Optimum.</li>
  <li>AutoAWQ (AWQ implementation, W4A16): easy 4-bit quantization with fast kernels and wide model coverage; integrates with vLLM and various executors.</li>
</ul>

<h4 id="rules-of-thumb-for-choosing-a-library">Rules of Thumb for Choosing a Library</h4>

<ul>
  <li><strong>If you’re serving NVIDIA-GPU LLMs at scale:</strong> start with TensorRT-LLM (SmoothQuant W8A8, or AWQ W4A16), then tune.</li>
  <li><strong>If you need portable CPU inference or a single artifact for many devices:</strong> export to ONNX and quantize with ONNX Runtime (static/QDQ).</li>
  <li><strong>If you want the fastest “drop-in” memory cut for HF models on a single GPU:</strong> bitsandbytes (<code class="language-plaintext highlighter-rouge">LLM.int8</code> or <a href="https://youtu.be/TPcXVJ1VSRI?t=563"><code class="language-plaintext highlighter-rouge">NF4</code></a>) via Transformers.</li>
  <li><strong>If you target Intel CPUs/iGPUs or want accuracy-constrained auto-tuning:</strong> Optimum Intel with NNCF or bare INC.</li>
  <li><strong>If you need pure-PyTorch experimentation across devices with minimal graph rewriting:</strong> Optimum Quanto for weight-only quantization (including float8/int4/int2).</li>
  <li><strong>If you deploy to mobile:</strong> TFLite (Android) or Core ML Tools (iOS/macOS).</li>
</ul>

<h4 id="implementation-notes">Implementation Notes</h4>

<ul>
  <li><strong>Calibration for static/PTQ:</strong> for ORT/OpenVINO/TensorRT <code class="language-plaintext highlighter-rouge">int8</code>, pass a representative dataset; a few hundred samples often suffice for stable scales.</li>
  <li><strong>Algebra and mapping:</strong> the linear quantizer in ORT and many toolkits uses <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-107-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1321" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1005.73em, 2.607em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1322"><span class="mi" id="MathJax-Span-1323" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1324" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1325" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">s</span><span class="mo" id="MathJax-Span-1326" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mo" id="MathJax-Span-1327" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">(</span><span class="mi" id="MathJax-Span-1328" style="font-family: STIXGeneral-Italic;">q</span><span class="mo" id="MathJax-Span-1329" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-1330" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">z</span><span class="mo" id="MathJax-Span-1331" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>=</mo><mi>s</mi><mo>⋅</mo><mo stretchy="false">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-107">x = s \cdot (q - z)</script>. Keep <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-108-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1332" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1333"><span class="mi" id="MathJax-Span-1334" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-1335" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">&gt;</span><span class="mn" id="MathJax-Span-1336" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-108">s>0</script>, and prefer per-channel scales for weights in matmuls/convs to reduce error.</li>
  <li><strong>Mixed precision realities:</strong> many “weight-only” schemes still do FP16/BF16 accumulations; end-to-end memory reduction depends on your kernel fusion and runtime (vLLM/TensorRT-LLM/etc.).</li>
  <li><strong>Library status:</strong> GPTQ tooling has shifted; AutoGPTQ is archived—prefer the GPTQModel path in Transformers/Optimum for forward-compatibility.</li>
</ul>

<h3 id="how-far-can-quantization-be-pushed">How Far Can Quantization be Pushed?</h3>

<ul>
  <li>
    <p>Quantization can, in theory, be reduced to a single bit per parameter, enabling what are known as <strong>binary neural networks (BNNs)</strong>. In such models, both weights and, in some cases, activations are constrained to binary values (e.g., {−1, +1}), achieving the most extreme point on the accuracy–performance trade-off spectrum. Several notable research efforts have explored this concept, including:</p>

    <ul>
      <li><strong>BinaryConnect</strong> (<a href="https://arxiv.org/abs/1511.00363">Courbariaux et al., 2015</a>) – Introduced the idea of training networks with binary weights while retaining full-precision activations.</li>
      <li><strong>XNOR-Net</strong> (<a href="https://arxiv.org/abs/1603.05279">Rastegari et al., 2016</a>) – Extended the approach by binarizing both weights and activations, and introducing scaling factors to reduce the accuracy drop while enabling efficient bitwise operations.</li>
      <li><strong>XNOR-Net++</strong> (<a href="https://arxiv.org/abs/1909.13863">Bulat &amp; Tzimiropoulos, 2019</a>) – Improved upon XNOR-Net through better gradient approximation techniques and optimized binarization strategies, achieving state-of-the-art results among BNNs at the time. An official PyTorch implementation is available for experimentation.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation details</strong>: BNNs replace standard floating-point arithmetic with bitwise operations such as <code class="language-plaintext highlighter-rouge">XNOR</code> and <code class="language-plaintext highlighter-rouge">popcount</code>, which are significantly faster and require far less memory. For example, multiplying two binary vectors can be performed by an <code class="language-plaintext highlighter-rouge">XNOR</code> followed by a population count, reducing both computation cost and storage by up to 32× compared to <code class="language-plaintext highlighter-rouge">float32</code>. Training typically involves:</p>

    <ol>
      <li>Maintaining a full-precision copy of weights for gradient updates.</li>
      <li>Applying a binarization function during forward passes (e.g., <code class="language-plaintext highlighter-rouge">sign</code> function).</li>
      <li>Using a <strong>straight-through estimator (STE)</strong> to approximate gradients through the non-differentiable binarization step.</li>
      <li>Incorporating scaling factors to better approximate the dynamic range lost in binarization.</li>
    </ol>
  </li>
  <li>
    <p>Despite the efficiency gains, BNNs often incur substantial accuracy degradation, particularly on complex tasks such as ImageNet classification. Consequently, they are mostly confined to research contexts or highly resource-constrained applications where extreme performance gains justify the accuracy trade-off.</p>
  </li>
</ul>

<h3 id="further-reading-1">Further Reading</h3>

<ul>
  <li>
    <p><a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">PyTorch official documentation: Introduction to Quantization on PyTorch</a></p>
  </li>
  <li>
    <p><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">PyTorch official documentation: Advanced Quantization in PyTorch</a></p>
  </li>
  <li>
    <p><a href="https://pytorch.org/docs/stable/quantization.html">PyTorch official documentation: Quantization</a></p>
  </li>
  <li>
    <p><a href="https://coremltools.readme.io/docs/quantization">CoreML Tools documentation: Quantization</a></p>
  </li>
  <li>
    <p><a href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes GitHub (official repo)</a></p>
  </li>
  <li>
    <p><a href="https://huggingface.co/docs/transformers/en/quantization/bitsandbytes">Transformers: bitsandbytes quantization guide</a></p>
  </li>
  <li>
    <p><a href="https://huggingface.co/docs/transformers/en/main_classes/quantization">Transformers: Quantization overview (AWQ, GPTQ, bnb)</a></p>
  </li>
  <li>
    <p><a href="https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/quantization">Hugging Face Optimum: ONNX Runtime quantization</a></p>
  </li>
  <li>
    <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">ONNX Runtime: Quantize ONNX models</a></p>
  </li>
  <li>
    <p><a href="https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html">OpenVINO: Post-training quantization (NNCF)</a></p>
  </li>
  <li>
    <p><a href="https://github.com/openvinotoolkit/nncf">NNCF (Neural Network Compression Framework) repository</a></p>
  </li>
  <li>
    <p><a href="https://huggingface.co/docs/transformers/en/quantization/quanto">Hugging Face Optimum Quanto (Transformers guide)</a></p>
  </li>
  <li>
    <p><a href="https://huggingface.co/docs/optimum/en/habana/usage_guides/quantization">Hugging Face Optimum Habana: Quantization</a></p>
  </li>
  <li>
    <p><a href="https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html">NVIDIA TensorRT: Working with quantized types (PTQ/QAT)</a></p>
  </li>
  <li>
    <p><a href="https://nvidia.github.io/TensorRT-LLM/">TensorRT-LLM documentation</a></p>
  </li>
  <li>
    <p><a href="https://intel.github.io/neural-compressor/latest/docs/source/quantization.html">Intel Neural Compressor: Quantization docs</a></p>
  </li>
  <li>
    <p><a href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training">TensorFlow Lite: Post-training quantization guide</a></p>
  </li>
  <li>
    <p><a href="https://github.com/casper-hansen/AutoAWQ">AutoAWQ GitHub (AWQ quantization toolkit)</a></p>
  </li>
  <li>
    <p><a href="https://docs.vllm.ai/en/v0.9.1/features/quantization/gptqmodel.html">vLLM: GPTQModel integration docs</a></p>
  </li>
  <li>
    <p><a href="https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/model-quantization.html">AMD ROCm: Model quantization (GPTQ and bitsandbytes)</a></p>
  </li>
</ul>

<h2 id="knowledge-distillation">Knowledge Distillation</h2>

<ul>
  <li>Knowledge distillation is a model compression technique in which a smaller, lightweight <strong>student model</strong> is trained to replicate the behavior of a larger, typically pre-trained <strong>teacher model</strong>. Introduced in <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a> by Hinton et al. (2015), this approach allows for the deployment of computationally efficient models that maintain much of the predictive power of their larger counterparts.</li>
</ul>

<blockquote>
  <p>The central premise is to transfer the “dark knowledge” captured in the output distributions (logits) of the teacher model to the student model. These softened outputs provide richer learning signals than traditional one-hot labels, capturing inter-class similarities and uncertainty.</p>
</blockquote>

<ul>
  <li>
    <p>By learning from soft labels or intermediate representations, student models can inherit the generalization ability of much larger teachers—often improving latency, storage, and robustness in real-world deployment. But success hinges on careful calibration of student capacity, distillation type, and supervision strategies.</p>
  </li>
  <li>
    <p>The image below <a href="http://theaiedge.io/">(source</a>) illustrates the concept of Knowledge Distillation:</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/modeldist.jpeg" alt=""></p>

<h3 id="mechanism">Mechanism</h3>

<ul>
  <li>
    <p>In standard supervised learning, models are trained to predict hard, one-hot labels. In contrast, knowledge distillation augments this with <strong>soft targets</strong>—probability distributions produced by the teacher. These targets encode relative likelihoods across all classes, providing richer supervisory signals.</p>
  </li>
  <li>
    <p>The student is trained using a <strong>composite loss function</strong> that combines:</p>

    <ul>
      <li><strong>Soft target loss</strong>: Kullback-Leibler (KL) divergence between the teacher’s and student’s softened output distributions.</li>
      <li><strong>Hard target loss</strong>: Standard cross-entropy loss against ground-truth labels.</li>
    </ul>
  </li>
  <li>
    <p>The loss function is:</p>
  </li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-109-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;hard&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;distill&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1337" style="width: 14.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1012.45em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1338"><span class="mi" id="MathJax-Span-1339" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1340" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1341" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">α</span><span class="mo" id="MathJax-Span-1342" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-1343" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1344" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1345"><span class="mrow" id="MathJax-Span-1346"><span class="mtext" id="MathJax-Span-1347" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">hard</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1348" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mo" id="MathJax-Span-1349" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">(</span><span class="mn" id="MathJax-Span-1350" style="font-family: STIXGeneral-Regular;">1</span><span class="mo" id="MathJax-Span-1351" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-1352" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">α</span><span class="mo" id="MathJax-Span-1353" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1354" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="msubsup" id="MathJax-Span-1355" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1356" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1357"><span class="mrow" id="MathJax-Span-1358"><span class="mtext" id="MathJax-Span-1359" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">distill</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mtext>hard</mtext></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mtext>distill</mtext></mrow></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-109">L = \alpha \cdot L_{\text{hard}} + (1 - \alpha) \cdot L_{\text{distill}}</script>

<ul>
  <li>
    <p>where:</p>

    <ul>
      <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-110-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;hard&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1360" style="width: 2.294em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.88em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1361"><span class="msubsup" id="MathJax-Span-1362"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1363" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1364"><span class="mrow" id="MathJax-Span-1365"><span class="mtext" id="MathJax-Span-1366" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">hard</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mtext>hard</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-110">L_{\text{hard}}</script> is the cross-entropy loss with hard/true labels.</li>
      <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-111-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;soft&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1367" style="width: 2.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.67em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1368"><span class="msubsup" id="MathJax-Span-1369"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1370" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1371"><span class="mrow" id="MathJax-Span-1372"><span class="mtext" id="MathJax-Span-1373" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">soft</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mtext>soft</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-111">L_{\text{soft}}</script> is the KL divergence between the temperature-scaled softmax outputs:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-112-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;soft&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;KL&lt;/mtext&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mrow&gt;&lt;mtext&gt;Softmax&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;teacher&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mtext&gt;Softmax&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;student&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1374" style="width: 24.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 20.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1020.73em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1375"><span class="msubsup" id="MathJax-Span-1376"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1377" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1378"><span class="mrow" id="MathJax-Span-1379"><span class="mtext" id="MathJax-Span-1380" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">soft</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1381" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mtext" id="MathJax-Span-1382" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">KL</span><span class="mrow" id="MathJax-Span-1383" style="padding-left: 0.211em;"><span class="mo" id="MathJax-Span-1384" style="vertical-align: 0em;"><span style="font-family: STIXGeneral-Regular;">(</span></span><span class="mrow" id="MathJax-Span-1385"><span class="mtext" id="MathJax-Span-1386" style="font-family: STIXGeneral-Regular;">Softmax</span><span class="mo" id="MathJax-Span-1387" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1388"><span style="display: inline-block; position: relative; width: 2.503em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1389" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="texatom" id="MathJax-Span-1390"><span class="mrow" id="MathJax-Span-1391"><span class="mtext" id="MathJax-Span-1392" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">teacher</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="texatom" id="MathJax-Span-1393"><span class="mrow" id="MathJax-Span-1394"><span class="mo" id="MathJax-Span-1395" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mi" id="MathJax-Span-1396" style="font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1397" style="font-family: STIXGeneral-Regular;">)</span><span class="mspace" id="MathJax-Span-1398" style="height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1399" style="font-family: STIXGeneral-Regular;">‖</span><span class="mspace" id="MathJax-Span-1400" style="height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-1401" style="font-family: STIXGeneral-Regular;">Softmax</span><span class="mo" id="MathJax-Span-1402" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1403"><span style="display: inline-block; position: relative; width: 2.503em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1404" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="texatom" id="MathJax-Span-1405"><span class="mrow" id="MathJax-Span-1406"><span class="mtext" id="MathJax-Span-1407" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">student</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="texatom" id="MathJax-Span-1408"><span class="mrow" id="MathJax-Span-1409"><span class="mo" id="MathJax-Span-1410" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mi" id="MathJax-Span-1411" style="font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1412" style="font-family: STIXGeneral-Regular;">)</span></span><span class="mo" id="MathJax-Span-1413" style="vertical-align: 0em;"><span style="font-family: STIXGeneral-Regular;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mtext>soft</mtext></mrow></msub><mo>=</mo><mtext>KL</mtext><mrow><mo>(</mo><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mrow class="MJX-TeXAtom-ORD"><mtext>teacher</mtext></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo><mspace width="thinmathspace"></mspace><mo fence="false" stretchy="false">‖</mo><mspace width="thinmathspace"></mspace><mtext>Softmax</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mrow class="MJX-TeXAtom-ORD"><mtext>student</mtext></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-112">L_{\text{soft}} = \text{KL} \left( \text{Softmax}(z_{\text{teacher}} / T) \,\|\, \text{Softmax}(z_{\text{student}} / T) \right)</script>

    <ul>
      <li>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-113-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1414" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1415"><span class="mi" id="MathJax-Span-1416" style="font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-113">T</script> is the temperature parameter that controls the softness of the distribution. A higher <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-114-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1417" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1418"><span class="mi" id="MathJax-Span-1419" style="font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-114">T</script> yields softer probabilities, revealing class similarities.</li>
    </ul>
  </li>
  <li>
    <p>This dual-loss formulation helps the student generalize better by aligning both label fidelity and model semantics.</p>
  </li>
</ul>

<h3 id="types-of-knowledge-distillation">Types of Knowledge Distillation</h3>

<h4 id="response-based-distillation">Response-Based Distillation</h4>

<ul>
  <li>The most common and classical form.</li>
  <li>The student is trained to match the final output probabilities of the teacher model.</li>
  <li>Computationally simple and widely adopted.</li>
  <li>Used in frameworks like <strong>DistilBERT</strong>, <strong>TinyBERT</strong>, etc.</li>
</ul>

<h4 id="feature-based-distillation">Feature-Based Distillation</h4>

<ul>
  <li>The student mimics internal hidden states or feature representations of the teacher.</li>
  <li>Often involves aligning intermediate activations across corresponding layers.</li>
  <li>Useful in vision tasks where spatial features are important.</li>
  <li>Examples include <a href="https://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a> by Romero et al. (2015).</li>
</ul>

<h4 id="relation-based-distillation">Relation-Based Distillation</h4>

<ul>
  <li>Focuses on matching relationships (e.g., distance, similarity, or angle) between samples in feature space across models.</li>
  <li>Encourages the student to learn the structural knowledge encoded in the teacher’s representation space.</li>
  <li>Often used in metric learning and ranking tasks.</li>
  <li>Example: <a href="https://arxiv.org/abs/1904.05068">Relational Knowledge Distillation</a> by Park et al. (2019).</li>
</ul>

<h3 id="distillation-modes">Distillation Modes</h3>

<h4 id="offline-distillation">Offline Distillation</h4>

<ul>
  <li>The teacher is pre-trained and fixed.</li>
  <li>The student is trained using the frozen teacher’s outputs.</li>
  <li>This is the most common paradigm in industry.</li>
</ul>

<h4 id="online-distillation">Online Distillation</h4>

<ul>
  <li>Teacher and student are trained simultaneously.</li>
  <li>The teacher may itself be evolving (e.g., ensemble of students).</li>
  <li>Allows for dynamic refinement of knowledge but adds training complexity.</li>
</ul>

<h4 id="self-distillation">Self-Distillation</h4>

<ul>
  <li>The teacher and student share the same architecture.</li>
  <li>The teacher is typically an earlier version of the student (e.g., exponential moving average of weights).</li>
  <li>Demonstrated to improve performance even without model compression.</li>
</ul>

<h3 id="why-use-knowledge-distillation-instead-of-training-small-models-from-scratch">Why Use Knowledge Distillation Instead of Training Small Models from Scratch?</h3>

<ol>
  <li><strong>Richer Supervision leading to Enhanced Generalization</strong>: The soft labels/targets from the teacher offer added supervisory signal by encoding subtle inter-class similarities (e.g., cat vs. tiger) and uncertainties that hard labels miss. This can guide the student to generalize better.</li>
  <li><strong>Data Augmentation Effect</strong>: The additional information in soft labels effectively augments the supervision signal without needing more data.</li>
  <li><strong>Performance Boost</strong>: Student models trained via distillation often outperform the same architecture trained directly on hard labels.</li>
  <li><strong>Compression with Retention</strong>: Distillation enables substantial reduction in model size and latency, with minimal loss in accuracy.</li>
  <li><strong>Regularization Effect</strong>: Soft labels (and the resulting dense supervision) lead to smoother gradients, which can act as a form of regularization, improving robustness.</li>
  <li><strong>Data Efficiency</strong>: The student often requires fewer training epochs and can converge with less labeled data.</li>
  <li><strong>Architecture Agnosticism</strong>: Students need not replicate the teacher’s structure, offering flexibility in design.</li>
  <li><strong>Latency Reduction</strong>: Distilled students exhibit significant inference speedups, sometimes halving latency.</li>
</ol>

<h3 id="why-knowledge-distillation-works">Why Knowledge Distillation Works</h3>

<ul>
  <li>
    <p><strong>Soft targets</strong>: Soft targets offer weak but informative signals, guiding the student toward nuanced generalizations. Suppose a large teacher model is trained on CIFAR-100 and has convolutional filters that respond to features like pointy ears. When shown a Batman mask labeled “mask,” the model might still activate its cat filters slightly. This leads to a 0.1 probability for “cat.” Training the student on this soft distribution imparts a weak but useful signal that improves its understanding of cat features.</p>
  </li>
  <li>
    <p><strong>Ensemble effects</strong>: If an ensemble of models each captures different features—say, one detects pointy ears, another whiskers—distillation into a single student helps consolidate these distinct patterns, enhancing generalization.</p>
  </li>
  <li>
    <p><strong>Multiple views and theoretical foundations</strong>: Distillation behaves like weak supervision or multi-view learning. As explored in <a href="https://arxiv.org/abs/2012.09816">Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</a> by Allen-Zhu et al. (2023), distilled students can approximate ensemble behavior using soft targets.</p>
  </li>
</ul>

<h3 id="distillation-in-practice">Distillation in Practice</h3>

<ul>
  <li>
    <p>Knowledge distillation intersects with adversarial robustness, privacy preservation, and transfer learning.</p>
  </li>
  <li>
    <p>Most widely used form is response-based distillation, but feature-based and relation-based variants are active research areas.</p>
  </li>
  <li>
    <p>Implementation nuances—teacher-student architecture differences, scheduling, or layer alignment—often require trial and error.</p>
  </li>
  <li>
    <p>In <a href="https://arxiv.org/abs/1910.01348">On the Efficacy of Knowledge Distillation</a>, Cho and Hariharan (2019) found that large teacher models can hurt student performance if there’s a capacity mismatch. Bigger is not always better.</p>
  </li>
  <li>
    <p>In <a href="https://arxiv.org/abs/1902.03393">Improved Knowledge Distillation via Teacher Assistant</a>, Mirzadeh et al. (2019) emphasized that the gap in capacity between teacher and student should be moderate for best results.</p>
  </li>
  <li>
    <p>Thus, a practical takeaway is to perform offline, response-based distillation using a slightly smaller student model for performance gains with minimal tuning.</p>
  </li>
  <li>
    <p>Recent work such as <a href="https://arxiv.org/abs/1908.08962">Well-Read Students Learn Better</a> by Turc et al. (2019) shows that <strong>Pre-trained Distillation (PD)</strong>—pretraining compact models before distillation—yields better results in NLP tasks. The recommended 3-step process is:</p>

    <ol>
      <li><strong>Pre-train</strong> the compact model (student) on the same masked language modeling (MLM) objective used in BERT.</li>
      <li><strong>Distill</strong> from a large, task-specific teacher model using response-based offline distillation. For example, if the downstream task is Natural Language Inference (NLI), use the teacher to produce logits for each class (entailment, contradiction, neutral), and minimize KL divergence with the student’s logits.</li>
      <li><strong>Fine-tune</strong> the distilled student on the task-specific dataset, such as training on the CoNLL 2003 dataset for Named Entity Recognition (NER).</li>
    </ol>

    <ul>
      <li>This procedure has the advantage of being <strong>architecture-agnostic</strong>, making it practical for real-world deployment where the student architecture may differ substantially from the teacher.</li>
    </ul>
  </li>
  <li>
    <p>Pretrained distilled models are now widely accessible. In NLP, libraries such as Hugging Face provide compact and distilled versions like <strong>DistilBERT</strong> and <strong>TinyBERT</strong>, often with task-specific checkpoints. In computer vision, <a href="https://github.com/facebookresearch/d2go">Facebook’s d2go</a> and <a href="https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/">DeiT</a> offer mobile-ready image classification models that were distilled from larger vision transformers.</p>
  </li>
  <li>
    <p>Practitioners should consider leveraging these pretrained distilled models when seeking lower latency or deployment efficiency, especially when retraining from scratch is resource-prohibitive.</p>
  </li>
</ul>

<h3 id="reverse-distillation">Reverse Distillation</h3>

<ul>
  <li>
    <p>In reverse distillation, a <strong>small model acts as the teacher</strong> and a <strong>larger model is the student</strong>.</p>
  </li>
  <li>
    <p>Particularly useful in noisy datasets (e.g., CTR prediction) where large models overfit easily.</p>
  </li>
  <li>
    <p>In <a href="https://arxiv.org/abs/2208.08003">Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise</a>, Xue et al. (2024) demonstrates this technique in high-label-noise regimes. Particularly, they show that using soft labels from a smaller model can regularize and stabilize large model training under label noise.</p>
  </li>
  <li>
    <p>Process:</p>

    <ol>
      <li>Train a small, clean model on a curated subset.</li>
      <li>Use it to produce soft labels for the noisy data.</li>
      <li>Train the large model using these regularized targets.</li>
    </ol>
  </li>
</ul>

<h3 id="weak-supervision-via-distillation">Weak Supervision Via Distillation</h3>

<ul>
  <li>
    <p>Distillation enables semi-supervised learning. Here’s the procedure:</p>

    <ol>
      <li>Train a high-capacity teacher on a small labeled set.</li>
      <li>Use it to label a larger unlabeled dataset.</li>
      <li>Train a compact student model on the combined data.</li>
    </ol>
  </li>
  <li>
    <p>This approach has been successfully used in real-world settings such as <a href="https://arxiv.org/pdf/1904.01624.pdf">Lessons from building acoustic models with a million hours of speech</a> by Parthasarathi and Strom (2019), which trained acoustic models with over a million hours of speech data.</p>
  </li>
</ul>

<h3 id="compute-vs-memory-bottlenecks-1">Compute vs. Memory Bottlenecks</h3>

<ul>
  <li>
    <p>The latency gains from deploying a distilled student model depend heavily on whether inference in your target environment is <strong>compute-bound</strong> or <strong>memory-bound</strong>.</p>

    <ul>
      <li>
        <p><strong>Compute-bound workloads</strong>:</p>

        <ul>
          <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.</li>
          <li>These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.</li>
          <li>However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.</li>
        </ul>
      </li>
      <li>
        <p><strong>Memory-bound workloads</strong>:</p>

        <ul>
          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.</li>
          <li>This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.</li>
          <li>Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>In practice, many real-world deployments see <strong>mixed bottlenecks</strong>, and the benefits of knowledge distillation are twofold:</p>

    <ol>
      <li>Lower FLOPs <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-115-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1420" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1421"><span class="mo" id="MathJax-Span-1422" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-115">\rightarrow</script> improved compute-bound performance.</li>
      <li>Smaller parameter and activation footprint <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-116-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1423" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1424"><span class="mo" id="MathJax-Span-1425" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-116">\rightarrow</script> improved memory-bound performance.</li>
    </ol>
  </li>
  <li>
    <p>Importantly, unlike other model compression techniques such as quantization and pruning—which may keep much of the original model’s execution pattern—distillation produces a <em>new, smaller dense architecture</em>, making it easier for standard inference engines and hardware to exploit the reduction in both compute and memory requirements.</p>
  </li>
</ul>

<h3 id="limitations-and-challenges">Limitations and Challenges</h3>

<ul>
  <li>
    <p><strong>Capacity Gap</strong>: <a href="https://arxiv.org/abs/1910.01348">On the Efficacy of Knowledge Distillation</a> by Cho and Hariharan (2019) demonstrates that extremely large teacher models can be poor mentors if the student’s capacity is too low—it cannot mimic the teacher effectively. Put simply, if the student model is too weak, distillation may degrade performance. Early stopping of the teacher may yield less “over‑fitted” soft labels that are more deliverable to the student.</p>
  </li>
  <li>
    <p><strong>Architecture Mismatch</strong>: Effectiveness can be reduced when the teacher and student architectures differ substantially.</p>
  </li>
  <li>
    <p><strong>Poor transfer on complex tasks</strong>: Results on datasets like ImageNet often trail behind simpler benchmarks unless carefully tuned.</p>
  </li>
  <li>
    <p><strong>Higher tuning cost</strong>: In contrast to quantization and pruning, distillation often requires more experimentation and task-specific adaptation.</p>
  </li>
</ul>

<!-- ## Knowledge distillation

- Knowledge distillation is a method for creating smaller and more efficient models from large models, in which the large model (teacher) trains the smaller model (student). The reference work in this area is [Hinton et al., 2015](https://arxiv.org/abs/1503.02531).

- "Knowledge distillation is about transferring knowledge from one model to another. Typically from a large model to a smaller one. When the student model learns to produce similar output responses, that is response-based distillation. When the student model learns to reproduce similar intermediate layers, it is called feature-based distillation. When the student model learns to reproduce the interaction between layers, it is called relation-based distillation." [source: AiEdge.io](http://theaiedge.io/)

- The image below [(source: AiEdge.io)](http://theaiedge.io/), does a great job at illustrating this concept.

![](/primers/ai/assets/model-compression/modeldist.jpeg)

- Knowledge distillation is a mechanism that lets us deploy a relatively large deep learning model in production. Specifically, knowledge distillation trains a smaller model, the student, to mimic the generalization power of a larger model, the teacher.

> The key idea is that, instead of training the student with the same labeled data as the teacher, we use the output probability distribution of the teacher as a soft target to train the small model.

- Put simply, knowledge distillation is a technique where a smaller model (the student) is trained to imitate the behavior of a larger, typically pre-trained model (the teacher). The goal is to have the student model, which is more compact and computationally efficient, retain as much of the teacher model's performance as possible.
- The main idea behind knowledge distillation is to transfer the "knowledge" encoded in the teacher's soft output probabilities (logits) to the student, rather than just the hard label assignments. Soft probabilities carry more information about the data distribution and the teacher's internal representations.
- In a standard training process, the teacher learns to discriminate between many classes and maximizes the probability of the correct label. However, a side-effect is that the model assigns smaller probabilities to other classes, which can give us a lot of knowledge about how the model generalizes. For example, an image of a cat can have a low probability of being mistaken for a tiger, but that mistake is still many times more probable than mistaking it for a chair. We can take advantage of this knowledge to improve the student.
- The student is a network with fewer parameters than the teacher. However, it is recommended to use the same structure, for example, if we want BERT as a teacher, we can use DistillBERT, which has 40% fewer parameters.
- The student training loss is a combination of the original training loss of the teacher and a distillation loss. The distillation loss simply softens the probability distribution of the correct classes by scaling it with a temperature parameter.
- The effect is this temperature is effectively reducing the higher probabilities and increasing the smaller ones, to make a softer distribution that has more knowledge.
- Knowledge distillation can cut the latency of a machine learning model by half at the cost of reducing the model's accuracy minimally.
- In practice, suppose we have a classification task. Suppose our smaller student model is $$f_{\theta}$$, where $\theta$ is the set of parameters. We take either a large model or an ensemble of models (possibly even the same model trained with different initializations), and call it $F$ (we won't worry about its parameters). Then we train the student network with the following loss:

    $$
    \mathcal{L}=\sum_{i=1}^{n} \mathrm{KL}\left(F\left(x_{i}\right), f_{\theta}\left(x_{i}\right)\right)
    $$
    
    - where $$F\left(x_{i}\right)$$ is the probability distribution over the labels created by passing example $$x_{i}$$ through the network. 
    
- If you want, you can add in the **regular cross entropy loss using the proper labels (by passing in the one-hot ground truth distribution to the student as well)**:

$$
\mathcal{L}=\sum_{i=1}^{n}\left(\mathrm{KL}\left(F\left(x_{i}\right), f_{\theta}\left(x_{i}\right)\right)-\beta \cdot \sum_{k=1}^{K} y_{i}[k] \log f_{\theta}\left(x_{i}\right)[k]\right)
$$

- Note that this second term is just the KL divergence (can also be cross entropy loss) from the “true” distribution (i.e., the one-hot distribution from the labels) to the student model, since is one-hot.
- In summary, the loss function in knowledge distillation is a combination of two terms:
    1. **Distillation Loss (Soft Loss)**: This is the main component of the distillation process. It computes the difference between the teacher's soft output probabilities and the student's soft output probabilities. The soft probabilities are typically obtained using a softmax function with a temperature parameter $$T$$:
        $$\text{Softmax}(z_i) = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}$$
            - where $$z_i$$ are the logits (pre-softmax outputs) of the model. The temperature $$T$$ is used to control the softness of the probabilities. A higher value of $$T$$ produces softer probabilities, which are more informative for the distillation process. 
        - The distillation loss can be computed using the Kullback-Leibler divergence between the soft probabilities of the teacher and the student:
        $$L_{\text{distill}} = \text{KL} \left( \text{Softmax}(z_{\text{teacher}}/T) || \text{Softmax}(z_{\text{student}}/T) \right)$$
    2. **Ground Truth Loss (Hard Loss)**: This is a traditional supervised learning loss computed between the student's outputs and the true labels of the data. Common choices for this loss include cross-entropy for classification tasks.
        - The overall loss function for knowledge distillation is a weighted combination of the distillation loss and the ground truth loss:
        $$L = \alpha L_{\text{hard}} + (1 - \alpha) L_{\text{distill}}$$
            - where $$\alpha$$ is a hyperparameter to balance the two losses.
        - This combined loss function helps the student model to generalize better by leveraging both the ground truth labels and the rich knowledge embedded in the teacher's soft outputs.
- Why does this work? There’s no consensus in the field. The most compelling explanation is that distillation is a form of rough data augmentation. This paper is recommended to understand why: [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816), which is focused on the idea of multiple views. Here’s some thought experiments that might help explain what's happening under-the-hood.

### Distillation thought experiment

- Let’s say that we have a large teacher model that is trained to classify images (e.g., CIFAR-100). This model implicitly has a bunch of “feature-detectors” built-in, e.g., a set of convolutional filters that fire when pointy ears are seen, which increase the probability of a label like “cat”. Let’s say that there’s a training image of a Batman mask, labeled “mask”. The teacher model’s pointy ears filters might still fire, telling us that the model thinks that this looks 10% like a cat.

- When the student model is trained to match the probability distribution of the teacher, because the distribution is 0.1 cat, it will still get a small signal that this image is catlike, which might help the student model recognize cats better than it could otherwise. If the student model was trained on just the true labels, it would have no idea that this Batman mask looks a bit like a cat. This logic also supports why the student can out-perform the teacher in some cases.

### Ensembling thought experiment

- A similar, but slightly different idea explains why ensembles of models (even the same architecture) might work well. Let’s say there’s 3 pictures of a cat in a dataset we’re using for image classification. Let’s say that image 1 has a cat with feature A (e.g., pointed ears), image 2 has feature B (e.g., whiskers), and image 3 has both A and B.

- Then, let’s say the neural network learns feature A (e.g., by seeing image 1). When it sees image 3, that set of convolution filters will fire, and so the image will be correctly classified. So, there’ll be no gradient that tunes the net to recognize feature B, even though a good net would learn that.

- Once a neural network has become good enough, its signal from some data points decreases.

### Distillation in practice

- Knowledge distillation is a very deep and wide research area, touching adversarial attacks, knowledge transfer, and privacy.

- In practice, the method described above is called response-based distillation. There are also other forms of distillation, including feature-based and relation-based knowledge distillation, which are entire subfields based on what parts (or computations from) the student and teacher model we should tie together.

- Furthermore, there’s a division between offline distillation (i.e., train the student after the teacher), online distillation (train the student and teacher together), and self-distillation (where the teacher model has the same architecture as the student). Together this makes it difficult to track distillation in practice; a set of **adhoc model-specific techniques might be the best general recommendation**.

- In fact, [Cho & Hariharan, 2019](https://arxiv.org/abs/1910.01348) found that when the student model’s capacity is too low, using knowledge distillation will actually adversely affect training. They found that knowledge distillation papers rarely use ImageNet and so often don’t work well on difficult problems. Perplexingly, that paper and [Mirzadeh et al., 2019](https://arxiv.org/abs/1902.03393) found that better teacher models don’t always mean better distillation, and the farther the student and teacher model’s capacities are, the less effective distillation was. You can find a recent investigation in [Tang et al., 2021](https://arxiv.org/pdf/2002.03532.pdf).

- All in all, distillation is relatively difficult compared to quantization and pruning. You might be able to get some free performance points by training a student with a slightly smaller capacity and then using vanilla response-based offline distillation.

#### Weak supervision: Distillation as semi-supervised learning

- You can train a teacher model, which is a much more powerful model than the student, with a small set of labeled data. Next, use the teacher to automatically label unannotated data, which can be used to train a leaner, more efficient “student” network.
- For e.g., [Lessons from building acoustic models with a million hours of speech](https://arxiv.org/pdf/1904.01624.pdf) by Parthasarathi and Strom (2019), used a small set of annotated data (green) to train a powerful but impractically slow “teacher” network to convert frequency-level descriptions of audio data into sequences of phones. The teacher, in turn, labeled a much larger set of unannotated data (red). They then used both datasets to train a leaner, more efficient "student" model.

![](/primers/ai/assets/model-compression/kd.gif)

### Reverse distillation

- Based on [Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise](https://arxiv.org/abs/2208.08003v4), we see that large models are more susceptible to overfitting on noise in the dataset due to over-parameterization and this can hurt the performance as we scale. This is especially true for datasets with high label noise (e.g., CTR data collected through user-interactions in a weak supervision manner). 
- In order to overcome this and help the larger models learn better on inherently noisy datasets, a potential method could be training large models by leveraging knowledge from smaller models using a process called reverse distillation. Put simply, (we transfer knowledge from smaller models to larger models as part of reverse distillation.
- In this method, we first train a smaller model and then use this smaller model as a teacher to guide the training of much larger models.
- The hypothesis is that the soft labels help the large model generalize much better on noisy datasets and also improve the stability of training larger models. -->

<h2 id="model-pruning">Model Pruning</h2>

<ul>
  <li>Model pruning is a technique for compressing and accelerating deep neural networks by eliminating redundant parameters—either individual weights or structured components such as entire neurons, filters, or layers—without significantly degrading model performance. This compression facilitates faster inference, lower memory usage, and often improved generalization when carefully applied.</li>
</ul>

<h3 id="formal-definition">Formal Definition</h3>

<ul>
  <li>
    <p>Let <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-117-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1426" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.66em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1427"><span class="mi" id="MathJax-Span-1428" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1429" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1430" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1431" style="font-family: STIXGeneral-Regular;">;</span><span class="mi" id="MathJax-Span-1432" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1433" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-117">f(x; \theta)</script> be a trained neural network model parameterized by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-118-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1434" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1002.97em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1435"><span class="mi" id="MathJax-Span-1436" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1437" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1438" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1439"><span class="mrow" id="MathJax-Span-1440"><span class="mi" id="MathJax-Span-1441" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="mi" id="MathJax-Span-1442" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>θ</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-118">\theta \in \mathbb{R}^n</script>. Pruning aims to construct a sparsified parameter vector <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-119-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1443" style="width: 3.961em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1003.28em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1444"><span class="msup" id="MathJax-Span-1445"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1446" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.576em;"><span class="mo" id="MathJax-Span-1447" style="font-size: 70.7%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1448" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1449" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1450"><span class="mrow" id="MathJax-Span-1451"><span class="mi" id="MathJax-Span-1452" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="mi" id="MathJax-Span-1453" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>θ</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-119">\theta' \in \mathbb{R}^n</script> such that:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-120-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mtable columnalign=&quot;left left&quot; rowspacing=&quot;.2em&quot; columnspacing=&quot;1em&quot; displaystyle=&quot;false&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;if&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;otherwise&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;&gt;&lt;/mo&gt;&lt;/mrow&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mtext&gt;where&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;&amp;#x2282;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x2026;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;}&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1454" style="width: 23.023em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(2.294em, 1019.07em, 4.846em, -999.997em); top: -3.799em; left: 0em;"><span class="mrow" id="MathJax-Span-1455"><span class="msubsup" id="MathJax-Span-1456"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1457" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -4.32em; left: 0.576em;"><span class="mo" id="MathJax-Span-1458" style="font-size: 70.7%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.471em;"><span class="mi" id="MathJax-Span-1459" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1460" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mrow" id="MathJax-Span-1461" style="padding-left: 0.315em;"><span class="mo" id="MathJax-Span-1462" style="vertical-align: -0.466em;"><span><span style="font-size: 111%; font-family: STIXSizeTwoSym;">{</span></span></span><span class="mtable" id="MathJax-Span-1463" style="padding-right: 0.159em; padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 5.992em; height: 0px;"><span style="position: absolute; clip: rect(2.607em, 1000.94em, 5.003em, -999.997em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.581em; left: 0em;"><span class="mtd" id="MathJax-Span-1464"><span class="mrow" id="MathJax-Span-1465"><span class="mn" id="MathJax-Span-1466" style="font-family: STIXGeneral-Regular;">0</span><span class="mo" id="MathJax-Span-1467" style="font-family: STIXGeneral-Regular;">,</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.18em, 1000.94em, 4.326em, -999.997em); top: -3.331em; left: 0em;"><span class="mtd" id="MathJax-Span-1474"><span class="mrow" id="MathJax-Span-1475"><span class="msubsup" id="MathJax-Span-1476"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1477" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-1478" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1479" style="font-family: STIXGeneral-Regular;">,</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.607em, 1003.86em, 4.846em, -999.997em); top: -4.008em; left: 2.138em;"><span style="display: inline-block; position: relative; width: 3.909em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1003.08em, 4.221em, -999.997em); top: -4.581em; left: 0em;"><span class="mtd" id="MathJax-Span-1468"><span class="mrow" id="MathJax-Span-1469"><span class="mtext" id="MathJax-Span-1470" style="font-family: STIXGeneral-Regular;">if&nbsp;</span><span class="mi" id="MathJax-Span-1471" style="font-family: STIXGeneral-Italic;">i</span><span class="mo" id="MathJax-Span-1472" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="mi" id="MathJax-Span-1473" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">P</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.18em, 1003.86em, 4.169em, -999.997em); top: -3.331em; left: 0em;"><span class="mtd" id="MathJax-Span-1480"><span class="mrow" id="MathJax-Span-1481"><span class="mtext" id="MathJax-Span-1482" style="font-family: STIXGeneral-Regular;">otherwise</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1483"></span></span><span class="mspace" id="MathJax-Span-1484" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-1485" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">where&nbsp;</span><span class="mi" id="MathJax-Span-1486" style="font-family: STIXGeneral-Italic;">P</span><span class="mo" id="MathJax-Span-1487" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">⊂</span><span class="mo" id="MathJax-Span-1488" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">{</span><span class="mn" id="MathJax-Span-1489" style="font-family: STIXGeneral-Regular;">1</span><span class="mo" id="MathJax-Span-1490" style="font-family: STIXGeneral-Regular;">,</span><span class="mo" id="MathJax-Span-1491" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">…</span><span class="mo" id="MathJax-Span-1492" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">,</span><span class="mi" id="MathJax-Span-1493" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">n</span><span class="mo" id="MathJax-Span-1494" style="font-family: STIXGeneral-Regular;">}</span></span><span style="display: inline-block; width: 0px; height: 3.805em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>θ</mi><mi>i</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mtr><mtd><mn>0</mn><mo>,</mo></mtd><mtd><mtext>if&nbsp;</mtext><mi>i</mi><mo>∈</mo><mi>P</mi></mtd></mtr><mtr><mtd><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable><mo fence="true" stretchy="true" symmetric="true"></mo></mrow><mspace width="1em"></mspace><mtext>where&nbsp;</mtext><mi>P</mi><mo>⊂</mo><mo fence="false" stretchy="false">{</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>n</mi><mo fence="false" stretchy="false">}</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-120">\theta'_i = 
\begin{cases}
0, & \text{if } i \in P \\
\theta_i, & \text{otherwise}
\end{cases}
\quad \text{where } P \subset \{1, \ldots, n\}</script>

    <ul>
      <li>and the pruned model <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-121-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1495" style="width: 3.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.97em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1496"><span class="mi" id="MathJax-Span-1497" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1498" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1499" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1500" style="font-family: STIXGeneral-Regular;">;</span><span class="msup" id="MathJax-Span-1501" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1502" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.576em;"><span class="mo" id="MathJax-Span-1503" style="font-size: 70.7%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1504" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-121">f(x; \theta')</script> satisfies:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-122-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1505" style="width: 11.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.482em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1009.43em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1506"><span class="mi" id="MathJax-Span-1507" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1508" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1509" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1510" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1511" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1512" style="font-family: STIXGeneral-Regular;">;</span><span class="msup" id="MathJax-Span-1513" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1514" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.576em;"><span class="mo" id="MathJax-Span-1515" style="font-size: 70.7%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1516" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1517" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1518" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-1519" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1520" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1521" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span class="mo" id="MathJax-Span-1522" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1523" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1524" style="font-family: STIXGeneral-Regular;">;</span><span class="mi" id="MathJax-Span-1525" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1526" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1527" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>≈</mo><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-122">L(f(x; \theta')) \approx L(f(x; \theta))</script>

    <ul>
      <li>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-123-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1528" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1529"><span class="mi" id="MathJax-Span-1530" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></span></span><script type="math/tex" id="MathJax-Element-123">L</script> denotes the loss function over the task of interest.</li>
    </ul>
  </li>
</ul>

<h3 id="rationale-and-theoretical-motivation">Rationale and Theoretical Motivation</h3>

<ul>
  <li>The core insight underpinning pruning is that modern deep networks are typically overparameterized. Empirical studies, such as <a href="https://arxiv.org/abs/1506.02626">Learning both Weights and Connections for Efficient Neural Networks</a> by Han et al. (2015), demonstrate that up to 90% of weights in large-scale models can be pruned with negligible loss in accuracy. The underlying explanation aligns with the <strong>Lottery Ticket Hypothesis</strong> proposed in <a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
</a> by Frankle &amp; Carbin (2019), which posits that:</li>
</ul>

<blockquote>
  <p>“A dense, randomly-initialized neural network contains a subnetwork that, when trained in isolation, can match the performance of the original network.”</p>
</blockquote>

<ul>
  <li>This hypothesis supports the idea that training a large model enables discovery of a performant sparse subnetwork, which can be extracted via pruning.</li>
</ul>

<h3 id="types-of-pruning">Types of Pruning</h3>

<h4 id="unstructured-pruning">Unstructured Pruning</h4>

<ul>
  <li>
    <p>Unstructured pruning eliminates individual weights regardless of their position in the weight tensor. It is formally defined by selecting a mask <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-124-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;{&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;}&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1531" style="width: 6.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.003em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1005em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1532"><span class="mi" id="MathJax-Span-1533" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1534" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="mo" id="MathJax-Span-1535" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">{</span><span class="mn" id="MathJax-Span-1536" style="font-family: STIXGeneral-Regular;">0</span><span class="mo" id="MathJax-Span-1537" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-1538" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">1</span><span class="msubsup" id="MathJax-Span-1539"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-1540" style="font-family: STIXGeneral-Regular;">}</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.471em;"><span class="mi" id="MathJax-Span-1541" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mo>∈</mo><mo fence="false" stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo fence="false" stretchy="false">}</mo><mi>n</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-124">M \in \{0, 1\}^n</script> applied element-wise to the parameter vector <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-125-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1542" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1543"><span class="mi" id="MathJax-Span-1544" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>θ</mi></math></span></span><script type="math/tex" id="MathJax-Element-125">\theta</script>, resulting in a sparse model:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-126-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;&amp;#x2299;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1545" style="width: 5.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.85em, 2.503em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1546"><span class="msup" id="MathJax-Span-1547"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1548" style="font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.576em;"><span class="mo" id="MathJax-Span-1549" style="font-size: 70.7%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1550" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1551" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1552" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⊙</span><span class="mi" id="MathJax-Span-1553" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>θ</mi><mo>′</mo></msup><mo>=</mo><mi>M</mi><mo>⊙</mo><mi>θ</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-126">\theta' = M \odot \theta</script>

    <ul>
      <li>
        <p>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-127-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x2299;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1554" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1555"><span class="mo" id="MathJax-Span-1556" style="font-family: STIXGeneral-Regular;">⊙</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⊙</mo></math></span></span><script type="math/tex" id="MathJax-Element-127">\odot</script> denotes the Hadamard product. Criteria for zeroing elements commonly include:</p>

        <ul>
          <li>Magnitude-based pruning (e.g., remove smallest | weights |),</li>
          <li>Gradient-based importance metrics,</li>
          <li>Second-order methods (e.g., Hessian-based sensitivity).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Although unstructured pruning achieves high sparsity, it often provides limited inference acceleration on conventional hardware due to irregular memory access patterns.</p>
  </li>
</ul>

<p><strong>Implementation</strong>:</p>

<ul>
  <li><strong>PyTorch</strong>: <code class="language-plaintext highlighter-rouge">torch.nn.utils.prune</code> supports various unstructured pruning strategies.</li>
  <li>
    <p><strong>TensorFlow</strong>: <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization.sparsity.keras</code> allows weight pruning during training.</p>
  </li>
  <li>Example:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code15"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code15"><span class="kn">import</span> <span class="nn">torch.nn.utils.prune</span> <span class="k">as</span> <span class="n">prune</span>
<span class="n">prune</span><span class="p">.</span><span class="n">l1_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'weight'</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="structured-pruning">Structured Pruning</h4>

<ul>
  <li>
    <p>Structured pruning removes entire structural components of the model, such as:</p>

    <ul>
      <li><strong>Filters</strong> in convolutional layers,</li>
      <li><strong>Neurons</strong> in fully-connected layers,</li>
      <li><strong>Heads</strong> in transformers,</li>
      <li><strong>Layers</strong> or blocks in residual networks.</li>
    </ul>
  </li>
  <li>
    <p>This results in a reduced model size and faster inference due to preservation of dense matrix formats.</p>
  </li>
  <li>
    <p>Common importance metrics include:</p>

    <ul>
      <li>L1/L2 norm of filters, proposed in <a href="https://arxiv.org/abs/1506.02626">Han et al. (2015)</a>,</li>
      <li>Average activation magnitude,</li>
      <li>Taylor approximation of loss change,</li>
      <li>Shapley values.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implementation</strong>:</p>

    <ul>
      <li><strong>TorchPruner</strong> (<a href="https://github.com/marcoancona/TorchPruner">repo</a>) automates structured pruning for linear and convolutional layers.</li>
      <li><strong>Torch-Pruning</strong> (<a href="https://github.com/VainF/Torch-Pruning">repo</a>) provides advanced pruning methods and dependency tracking across layers.</li>
    </ul>
  </li>
</ul>

<h3 id="pruning-workflow">Pruning Workflow</h3>

<ul>
  <li>A typical pruning pipeline consists of the following stages:</li>
</ul>

<h4 id="step-1-train-the-full-model">Step 1: Train the Full Model</h4>

<ul>
  <li>Train the original model to convergence using standard procedures.</li>
</ul>

<h4 id="step-2-apply-pruning-mask">Step 2: Apply Pruning Mask</h4>

<ul>
  <li>
    <p>Determine and apply pruning masks using one of the supported strategies. This can occur:</p>

    <ul>
      <li><strong>Post-training</strong>: Prune after the model is fully trained.</li>
      <li><strong>During training</strong>: Gradually prune weights over several epochs.</li>
    </ul>
  </li>
</ul>

<h4 id="step-3-fine-tune-the-pruned-model">Step 3: Fine-Tune the Pruned Model</h4>

<ul>
  <li>
    <p>Retrain the pruned model to recover lost accuracy. The most effective method is <strong>learning rate rewinding</strong>, where:</p>

    <ul>
      <li>Training is resumed from an earlier weight checkpoint.</li>
      <li>The learning rate is reset to a higher value <a href="https://arxiv.org/abs/2003.02389">Comparing Rewinding and Fine-tuning in Neural Network Pruning</a> by Renda et al. (2020).</li>
    </ul>
  </li>
  <li>
    <p>Alternatively, <strong>weight rewinding</strong> may be used to reset weights of surviving parameters to their earlier values (e.g., 1/3 of training completed).</p>
  </li>
</ul>

<h3 id="compute-vs-memory-bottlenecks-2">Compute vs. Memory Bottlenecks</h3>

<ul>
  <li>
    <p>Whether pruning improves inference latency depends strongly on the <strong>primary performance bottleneck</strong> of the workload:</p>

    <p><strong>Compute-bound workloads</strong>:</p>

    <ul>
      <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., reducing model size only helps if pruning directly lowers the number of executed operations <em>and</em> the hardware can exploit the new shape efficiently.</li>
      <li>For example, moderate structured pruning may still yield negligible gains on GPUs optimized for dense matrix multiplication unless the reduction crosses hardware-friendly dimensions that change kernel execution patterns (e.g., multiples of warp sizes or tensor core tile sizes).</li>
      <li>Unstructured pruning typically does not lower FLOP count in standard dense kernels, so compute-bound latency may remain unchanged.</li>
    </ul>

    <p><strong>Memory-bound workloads</strong>:</p>

    <ul>
      <li>
        <ul>
          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.</li>
        </ul>
      </li>
      <li>This benefit is more pronounced in unstructured pruning or extreme structured pruning, where parameter reduction meaningfully shrinks memory traffic though this often requires specialized hardware/software support.</li>
    </ul>
  </li>
  <li>
    <p>In summary, if the system is compute-bound, the time saved by transferring fewer parameters is negligible because the main delay is in processing operations, not in moving data. Moreover, unstructured pruning may not reduce computational cost on dense-optimized hardware without sparse acceleration kernels, meaning FLOP counts—and thus latency—stay similar. Structured pruning fares better but still depends heavily on whether the hardware kernels adapt efficiently to the new tensor shapes.</p>
  </li>
</ul>

<h3 id="practical-considerations">Practical Considerations</h3>

<h4 id="target-sparsity">Target Sparsity</h4>

<ul>
  <li>Target sparsity (e.g., 80%) must be tuned experimentally. Aggressive sparsity often requires multiple pruning–fine-tuning cycles.</li>
</ul>

<h4 id="compatibility">Compatibility</h4>

<ul>
  <li>
    <p>Pruning can be difficult for architectures with:</p>

    <ul>
      <li><strong>Skip connections</strong> (e.g., ResNets),</li>
      <li><strong>Attention modules</strong> with tight dimensional constraints.</li>
    </ul>
  </li>
  <li>
    <p>Custom pruning logic may be required.</p>
  </li>
</ul>

<h4 id="deployment-readiness">Deployment Readiness</h4>

<ul>
  <li>Sparse inference is not universally supported. Quantization-aware or hardware-specific pruning (e.g., <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-128-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1557" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.61em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1558"><span class="mi" id="MathJax-Span-1559" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1560" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">:</span><span class="mi" id="MathJax-Span-1561" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mo>:</mo><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-128">N:M</script> sparsity) may be necessary for real-world acceleration. As noted in <a href="https://arxiv.org/abs/2003.03033">What is the State of Neural Network Pruning?</a> by Blalock et al. (2020), real benefits often depend on framework and hardware support.</li>
</ul>

<h3 id="comparative-analysis-2">Comparative Analysis</h3>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Aspect</strong></th>
<th class="tg-hcenter-valign-first"><strong>Unstructured Pruning</strong></th>
<th class="tg-hcenter-valign-second"><strong>Structured Pruning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Targets</td>
<td class="tg-tleft-valign-first">Individual weights</td>
<td class="tg-tleft-valign-second">Filters, neurons, heads, layers</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Benefits</td>
<td class="tg-tleft-valign-first">Compression</td>
<td class="tg-tleft-valign-second">Compression + inference acceleration</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Implementation Ease</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-second">Moderate to low</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Framework Support</td>
<td class="tg-tleft-valign-first">TensorFlow, PyTorch</td>
<td class="tg-tleft-valign-second">TorchPruner, Torch-Pruning</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Inference Speedup</td>
<td class="tg-tleft-valign-first">Limited</td>
<td class="tg-tleft-valign-second">Significant</td>
</tr>
</tbody>
</table>
</div>

<h3 id="implementing-pruning-in-pytorch-and-tensorflow">Implementing Pruning in PyTorch and TensorFlow</h3>

<ul>
  <li>Modern deep learning frameworks provide built-in utilities to simplify pruning workflows. Below, we describe practical approaches in both PyTorch and TensorFlow.</li>
</ul>

<h4 id="pytorch-pruning">PyTorch Pruning</h4>

<ul>
  <li>
    <p>PyTorch offers flexible pruning utilities via the <code class="language-plaintext highlighter-rouge">torch.nn.utils.prune</code> module. This supports both unstructured and structured pruning.</p>
  </li>
  <li>
    <p><strong>Unstructured Pruning Example (L1-based weight pruning):</strong></p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code16"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code16"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.utils.prune</span> <span class="k">as</span> <span class="n">prune</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Prune 50% of weights in the first Linear layer
</span><span class="n">prune</span><span class="p">.</span><span class="n">l1_unstructured</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'weight'</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># To remove the pruning reparameterization and finalize pruning
</span><span class="n">prune</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">'weight'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Structured Pruning Example (entire neuron/channel pruning):</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code17"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code17"><span class="c1"># Prune 30% of channels (columns) in the second Linear layer
</span><span class="n">prune</span><span class="p">.</span><span class="n">ln_structured</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'weight'</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">prune</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s">'weight'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>
    <p><strong>PyTorch Pruning Resources:</strong></p>

    <ul>
      <li><a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">PyTorch pruning tutorial</a></li>
      <li><a href="https://github.com/VainF/Torch-Pruning">Torch-Pruning library</a> – advanced dependency-aware structured pruning</li>
      <li><a href="https://github.com/marcoancona/TorchPruner">TorchPruner</a> – structured pruning with visual feedback</li>
    </ul>
  </li>
</ul>

<h4 id="tensorflow-pruning">TensorFlow Pruning</h4>

<ul>
  <li>
    <p>TensorFlow provides pruning support via the <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization</code> toolkit. It supports sparsity-aware training by gradually zeroing out weights.</p>
  </li>
  <li>
    <p><strong>Basic Workflow:</strong></p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code18"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code18"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_model_optimization</span> <span class="k">as</span> <span class="n">tfmot</span>

<span class="n">prune_low_magnitude</span> <span class="o">=</span> <span class="n">tfmot</span><span class="p">.</span><span class="n">sparsity</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">prune_low_magnitude</span>

<span class="c1"># Define prunable model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span> 
                        <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,)),</span>
    <span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Compile and train
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Strip pruning wrappers before saving/export
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tfmot</span><span class="p">.</span><span class="n">sparsity</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Set Pruning Schedule:</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code19"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code19"><span class="n">pruning_schedule</span> <span class="o">=</span> <span class="n">tfmot</span><span class="p">.</span><span class="n">sparsity</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">PolynomialDecay</span><span class="p">(</span>
    <span class="n">initial_sparsity</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">final_sparsity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">begin_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">end_step</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>
    <p><strong>TensorFlow Pruning Resources:</strong></p>

    <ul>
      <li><a href="https://www.tensorflow.org/model_optimization/guide/pruning">TensorFlow Model Optimization Guide</a></li>
      <li>Supports only unstructured pruning during training</li>
      <li>Can export TFLite-compatible sparse models for edge inference</li>
    </ul>
  </li>
</ul>

<h2 id="mixed-precision-training">Mixed Precision Training</h2>

<h3 id="overview-1">Overview</h3>

<ul>
  <li>
    <p>Mixed precision is a technique used to speed up neural network training by utilizing both 16-bit and 32-bit floating-point types—primarily <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">bfloat16</code>, and <code class="language-plaintext highlighter-rouge">float32</code>. Traditionally, models rely on the <code class="language-plaintext highlighter-rouge">float32</code> dtype, which uses 32 bits of memory per value. However, many modern hardware accelerators are optimized to perform faster computations and memory access with 16-bit types. This means that using <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code>, which only take 16 bits each, can lead to significant performance gains.</p>
  </li>
  <li>
    <p>The key insight behind mixed precision is that not all parts of a model require the full precision of <code class="language-plaintext highlighter-rouge">float32</code>. For example, many operations can be safely executed using <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code> without compromising the final evaluation metrics such as accuracy. This is where <a href="https://developer.nvidia.com/automatic-mixed-precision">Automatic Mixed Precision (AMP)</a> comes in—it automatically determines the most appropriate precision for each operation, balancing performance with numerical stability.</p>

    <ul>
      <li>The term “numeric stability” refers to how much using lower precision (like <code class="language-plaintext highlighter-rouge">float16</code>) affects a model’s quality. If certain operations are sensitive to precision loss, AMP keeps them in <code class="language-plaintext highlighter-rouge">float32</code> to preserve model accuracy. By assigning the appropriate dtype to each operation, mixed precision reduces the model’s runtime and memory footprint while maintaining comparable performance.</li>
    </ul>
  </li>
  <li>
    <p>In summary, mixed precision combines the efficiency of 16-bit operations with the robustness of 32-bit computations to optimize deep learning training workflows.</p>
  </li>
</ul>

<h3 id="how-mixed-precision-training-works">How Mixed Precision Training Works</h3>

<ul>
  <li>
    <p>Mixed precision training is a performance optimization technique that accelerates neural network training by leveraging lower-precision arithmetic—primarily half-precision floating-point (<code class="language-plaintext highlighter-rouge">float16</code>)—without compromising model accuracy or convergence stability.</p>
  </li>
  <li>
    <p>At its core, the concept is straightforward: replacing standard single-precision (<code class="language-plaintext highlighter-rouge">float32</code>) operations with half-precision (<code class="language-plaintext highlighter-rouge">float16</code>) can roughly halve memory usage and significantly reduce training time. However, implementing this substitution safely and effectively is non-trivial due to the numerical limitations of lower-precision formats.</p>
  </li>
  <li>
    <p>By combining dual weight representations, selective precision, and dynamic loss scaling, mixed precision training enables significant reductions in training time and memory consumption—often with negligible impact on model accuracy. As demonstrated in <a href="https://arxiv.org/pdf/1710.03740.pdf">Mixed Precision Training</a> by Narang et al. (2018), these methods allow a wide range of models to train to convergence reliably and efficiently using <code class="language-plaintext highlighter-rouge">float16</code> computations.</p>
  </li>
  <li>
    <p><strong>Challenges with Half Precision</strong>:</p>

    <ul>
      <li>
        <p>Lower-precision formats like <code class="language-plaintext highlighter-rouge">float16</code> have a reduced dynamic range and lower numerical precision compared to <code class="language-plaintext highlighter-rouge">float32</code>. One critical issue is <em>underflow</em>, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number), the model may fail to learn altogether.</p>
      </li>
      <li>
        <p>The following figure from the <a href="https://arxiv.org/pdf/1710.03740.pdf">Mixed Precision Training</a> paper illustrates a key finding that naïvely switching to <code class="language-plaintext highlighter-rouge">float16</code> causes any gradient smaller than <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-129-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;24&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1562" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1563"><span class="msubsup" id="MathJax-Span-1564"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1565" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1566"><span class="mrow" id="MathJax-Span-1567"><span class="mo" id="MathJax-Span-1568" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-1569" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">24</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-129">2^{-24}</script> to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:</p>
      </li>
    </ul>

    <p><img src="/primers/ai/assets/model-compression/mpt.avif" alt=""></p>
  </li>
  <li>
    <p><strong>Techniques for Safe Mixed Precision Training</strong>:</p>

    <ul>
      <li>
        <p>To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with <code class="language-plaintext highlighter-rouge">float16</code> precision:</p>

        <ol>
          <li><strong>Maintaining Dual Weight Copies (Master Weights Strategy)</strong>
            <ul>
              <li>Each model weight is stored in two formats: a full-precision (<code class="language-plaintext highlighter-rouge">float32</code>) “master copy” and a lower-precision (<code class="language-plaintext highlighter-rouge">float16</code>) copy. During forward and backward passes, computations are performed using the <code class="language-plaintext highlighter-rouge">float16</code> version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the <code class="language-plaintext highlighter-rouge">float32</code> master weights using gradients computed in <code class="language-plaintext highlighter-rouge">float16</code> but cast to <code class="language-plaintext highlighter-rouge">float32</code>. This preserves update accuracy and avoids the accumulation of precision errors during training.</li>
            </ul>
          </li>
          <li><strong>Selective Precision Application (Mixed-Dtype Execution)</strong>
            <ul>
              <li>Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in <code class="language-plaintext highlighter-rouge">float16</code>, while others—such as softmax, batch normalization, and gradient accumulation—require <code class="language-plaintext highlighter-rouge">float32</code> to maintain stability. Mixed precision training selectively applies <code class="language-plaintext highlighter-rouge">float16</code> where safe and retains <code class="language-plaintext highlighter-rouge">float32</code> where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.</li>
            </ul>
          </li>
          <li><strong>Loss Scaling</strong>
            <ul>
              <li>To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as <em>loss scaling</em>, proportionally increases all gradient values, elevating small gradients above the <code class="language-plaintext highlighter-rouge">float16</code> precision threshold of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-130-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;24&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1570" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1571"><span class="msubsup" id="MathJax-Span-1572"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1573" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1574"><span class="mrow" id="MathJax-Span-1575"><span class="mo" id="MathJax-Span-1576" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">−</span><span class="mn" id="MathJax-Span-1577" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">24</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-130">2^{-24}</script>. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.</li>
              <li>Care must be taken to avoid <em>overflow</em>, which occurs when values exceed the representable range of <code class="language-plaintext highlighter-rouge">float16</code>, leading to <code class="language-plaintext highlighter-rouge">Inf</code> or <code class="language-plaintext highlighter-rouge">NaN</code> values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h4 id="how-pytorch-automatic-mixed-precision-works">How PyTorch Automatic Mixed Precision Works</h4>

<ul>
  <li>
    <p>With a solid understanding of mixed precision training established, we can now explore how PyTorch streamlines this powerful optimization technique through its <strong>Automatic Mixed Precision (AMP)</strong> API. While mixed precision training has long been theoretically feasible—typically requiring manual tensor casting to <code class="language-plaintext highlighter-rouge">float16</code> and careful loss scaling—PyTorch removes much of this complexity. Its AMP API makes the process highly accessible, offering a streamlined, production-ready solution that demands only minimal code modifications.</p>
  </li>
  <li>
    <p>PyTorch’s AMP achieves this by abstracting the underlying mechanics through two key components: <code class="language-plaintext highlighter-rouge">autocast</code> and <code class="language-plaintext highlighter-rouge">GradScaler</code>. <code class="language-plaintext highlighter-rouge">autocast</code> enables selective precision execution, automatically determining which operations benefit from half-precision without sacrificing accuracy. Simultaneously, <code class="language-plaintext highlighter-rouge">GradScaler</code> manages dynamic loss scaling, helping to prevent issues like gradient underflow and ensuring stable convergence. This integration offers developers substantial speedups—often reducing training times by <strong>50–60%</strong>—and improves memory efficiency, all without compromising model stability or performance.</p>
  </li>
  <li>
    <p>This practical implementation is a direct evolution of the concepts outlined in the <a href="https://arxiv.org/pdf/1710.03740.pdf"><em>Mixed Precision Training</em></a> research paper. AMP embodies how advanced techniques from cutting-edge research can be distilled into user-friendly tools that enhance real-world machine learning workflows.</p>
  </li>
  <li>
    <p>Prior to AMP, implementing mixed precision was a labor-intensive process. Developers had to manually cast tensors, implement and tune custom loss scalers, and safeguard against the risks of instability. The introduction of PyTorch’s <code class="language-plaintext highlighter-rouge">torch.cuda.amp</code> module represents a major leap forward, encapsulating best practices and democratizing access to high-performance training.</p>
  </li>
  <li>
    <p>AMP is especially effective on modern NVIDIA GPUs—such as those based on Volta, Turing, Ampere, or newer architectures—which include specialized <strong>Tensor Cores</strong> designed for half-precision operations. However, even on older or unsupported hardware, users may still see performance benefits due to more efficient memory usage and reduced data movement.</p>
  </li>
  <li>
    <p>In summary, PyTorch’s AMP bridges the gap between theoretical efficiency and practical deployment, making state-of-the-art training techniques both accessible and impactful across a wide range of hardware and use cases.</p>
  </li>
</ul>

<h5 id="overview-of-amp-components">Overview of AMP Components</h5>

<ul>
  <li>
    <p>PyTorch’s AMP functionality is implemented via the <code class="language-plaintext highlighter-rouge">torch.cuda.amp</code> module and relies on two key primitives:</p>

    <ol>
      <li>
        <p><code class="language-plaintext highlighter-rouge">torch.cuda.amp.autocast</code>: A context manager that automatically casts operations to the appropriate precision (<code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">float32</code>) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">torch.cuda.amp.GradScaler</code>: A utility that handles <em>dynamic loss scaling</em>. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., <code class="language-plaintext highlighter-rouge">NaN</code> or <code class="language-plaintext highlighter-rouge">Inf</code>), adjusting the scale factor dynamically to maintain numerical stability.</p>
      </li>
    </ol>
  </li>
</ul>

<h5 id="practical-implementation-in-a-training-loop">Practical Implementation in a Training Loop</h5>

<ul>
  <li>The following example demonstrates how mixed precision training is incorporated into a standard PyTorch training loop. Lines marked with <code class="language-plaintext highlighter-rouge"># NEW</code> indicate additions or modifications required to enable AMP.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code20"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code20"><span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">max_lr</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">OneCycleLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_lr</span><span class="p">,</span>
    <span class="n">cycle_momentum</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">n_epochs</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)),</span>
<span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># NEW: Initialize GradScaler for dynamic loss scaling
</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_batch</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_batch</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># NEW: Forward pass with autocast for mixed precision
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">():</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>

        <span class="c1"># NEW: Scale loss and perform backward pass
</span>        <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">lv</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">n_epochs</span><span class="si">}</span><span class="s">; Batch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">; Loss </span><span class="si">{</span><span class="n">lv</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># NEW: Unscale gradients, perform optimizer step, update scaler
</span>        <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>

        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li>
    <p><strong>Implementation Notes and Best Practices</strong>:</p>

    <ul>
      <li>
        <p><strong>Device compatibility</strong>: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.</p>
      </li>
      <li>
        <p><strong>Model compatibility</strong>: Most standard PyTorch layers (e.g., <code class="language-plaintext highlighter-rouge">nn.Linear</code>, <code class="language-plaintext highlighter-rouge">nn.Conv2d</code>, <code class="language-plaintext highlighter-rouge">nn.ReLU</code>) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.</p>
      </li>
      <li>
        <p><strong>Gradient stability</strong>: The <code class="language-plaintext highlighter-rouge">GradScaler</code> performs <em>automatic gradient anomaly detection</em>, skipping optimizer steps when gradients contain <code class="language-plaintext highlighter-rouge">Inf</code> or <code class="language-plaintext highlighter-rouge">NaN</code> values. This safeguards training from diverging due to numerical instability.</p>
      </li>
      <li>
        <p><strong>Loss scaling strategy</strong>: The <code class="language-plaintext highlighter-rouge">GradScaler</code> uses <em>dynamic loss scaling</em> by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="loss-and-gradient-scaling-with-gradscaler">Loss and Gradient Scaling with <code class="language-plaintext Highlighter-rouge">GradScaler</code></h5>

<ul>
  <li>
    <p>A fundamental challenge of half-precision (<code class="language-plaintext highlighter-rouge">float16</code>) training is the limited dynamic range, which can cause small-magnitude gradients to underflow—i.e., round down to zero—during backpropagation. This occurs because when an operation receives <code class="language-plaintext highlighter-rouge">float16</code> inputs in the forward pass, the resulting gradients computed in the backward pass are also in <code class="language-plaintext highlighter-rouge">float16</code>, unless explicitly handled. In deep learning, many gradients—particularly in early layers or at later training stages—can be extremely small, and when these are flushed to zero, their corresponding weight updates are effectively lost, impeding learning.</p>
  </li>
  <li>
    <p>To mitigate this, PyTorch introduces <strong>loss scaling</strong>, a technique that amplifies loss values and their corresponding gradients during the backward pass to avoid underflow. The process works as follows:</p>

    <ol>
      <li>The loss is multiplied by a scale factor before backpropagation.</li>
      <li>Gradients are computed on this scaled loss, resulting in proportionally larger values.</li>
      <li>These gradients are then unscaled before the optimizer applies the update, preserving the intended learning dynamics.</li>
    </ol>
  </li>
  <li>
    <p>This technique is implemented via the <code class="language-plaintext highlighter-rouge">torch.cuda.amp.GradScaler</code> object, which automates both the scaling and unscaling process, as well as overflow detection and recovery. The goal is to find a balance: a scale factor high enough to preserve small gradients, yet not so high that large gradients overflow and become <code class="language-plaintext highlighter-rouge">inf</code>—maintaining a balance between underflow and overflow.</p>
  </li>
</ul>

<h6 id="dynamic-scaling-with-exponential-backoff">Dynamic Scaling with Exponential Backoff</h6>

<ul>
  <li>
    <p>There is no single static loss multiplier that suits all models or all stages of training. Gradient magnitudes are typically much larger at the beginning of training and diminish as convergence nears. Rather than asking users to manually tune this value, PyTorch uses an adaptive approach based on <strong>exponential backoff</strong>.</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">GradScaler</code> begins with an initial scale (default: 65,536 or <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-131-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1578" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1579"><span class="msubsup" id="MathJax-Span-1580"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1581" style="font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.523em;"><span class="texatom" id="MathJax-Span-1582"><span class="mrow" id="MathJax-Span-1583"><span class="mn" id="MathJax-Span-1584" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">16</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mn>16</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-131">2^{16}</script>) and periodically doubles it to maximize numerical range. If an overflow is detected—i.e., any gradient becomes <code class="language-plaintext highlighter-rouge">inf</code> or <code class="language-plaintext highlighter-rouge">NaN</code>—the current update step is skipped, the scale is halved, and a cooldown counter is reset. This approach allows PyTorch to adaptively find a safe and efficient scaling factor over time, much like TCP congestion control adapts network throughput.</p>
  </li>
  <li>
    <p>This behavior can be configured via the <code class="language-plaintext highlighter-rouge">GradScaler</code> constructor:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code21"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code21"><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">(</span>
    <span class="n">init_scale</span><span class="o">=</span><span class="mf">65536.0</span><span class="p">,</span>
    <span class="n">growth_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">backoff_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">growth_interval</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
    <span class="n">enabled</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div>    </div>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">init_scale</code>: The initial scaling factor.</li>
      <li><code class="language-plaintext highlighter-rouge">growth_factor</code>: Multiplicative increase rate when no overflows are detected.</li>
      <li><code class="language-plaintext highlighter-rouge">backoff_factor</code>: Reduction factor when an overflow is detected.</li>
      <li><code class="language-plaintext highlighter-rouge">growth_interval</code>: Number of successful steps before scale growth is attempted.</li>
      <li><code class="language-plaintext highlighter-rouge">enabled</code>: Whether AMP and scaling are active.</li>
    </ul>
  </li>
  <li>
    <p><strong>Operational Considerations</strong>:</p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">GradScaler</code> modifies key parts of the training loop:</p>

        <ul>
          <li><code class="language-plaintext highlighter-rouge">loss.backward()</code> becomes <code class="language-plaintext highlighter-rouge">scaler.scale(loss).backward()</code></li>
          <li><code class="language-plaintext highlighter-rouge">optimizer.step()</code> becomes <code class="language-plaintext highlighter-rouge">scaler.step(optimizer)</code></li>
          <li>The call to <code class="language-plaintext highlighter-rouge">scaler.update()</code> checks for overflows and adjusts the scale as needed.</li>
        </ul>
      </li>
      <li>
        <p>It is important to note that overflows (<code class="language-plaintext highlighter-rouge">inf</code>) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default <code class="language-plaintext highlighter-rouge">init_scale</code> mitigates this risk.</p>
      </li>
      <li>
        <p>Internally, before the optimizer updates the model weights, the gradients (<code class="language-plaintext highlighter-rouge">.grad</code>) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in <code class="language-plaintext highlighter-rouge">float32</code> training.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Checkpointing with GradScaler</strong>:</p>

    <ul>
      <li>Because <code class="language-plaintext highlighter-rouge">GradScaler</code> is a stateful object that adapts over time, it must be saved and restored along with the model and optimizer during checkpointing. PyTorch provides simple APIs for this:</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code22"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code22"><span class="c1"># Saving
</span><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">({</span>
    <span class="s">'model_state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">'optimizer_state_dict'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">'scaler_state_dict'</span><span class="p">:</span> <span class="n">scaler</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="p">},</span> <span class="s">'checkpoint.pt'</span><span class="p">)</span>

<span class="c1"># Loading
</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'checkpoint.pt'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'optimizer_state_dict'</span><span class="p">])</span>
<span class="n">scaler</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'scaler_state_dict'</span><span class="p">])</span>
</code></pre></div>    </div>

    <ul>
      <li>By integrating <code class="language-plaintext highlighter-rouge">GradScaler</code> into the training process, PyTorch ensures that the numerical precision limitations of <code class="language-plaintext highlighter-rouge">float16</code> do not compromise convergence, while still allowing significant performance and memory efficiency gains.</li>
    </ul>
  </li>
</ul>

<h5 id="automatic-precision-casting-with-the-autocast-context-manager">Automatic Precision Casting with the <code class="language-plaintext Highlighter-rouge">autocast</code> Context Manager</h5>

<ul>
  <li>
    <p>The second key component of PyTorch’s AMP system is the <code class="language-plaintext highlighter-rouge">torch.cuda.amp.autocast</code> context manager. While <code class="language-plaintext highlighter-rouge">GradScaler</code> addresses numerical stability during backpropagation via loss scaling, <code class="language-plaintext highlighter-rouge">autocast</code> is responsible for precision control during the forward pass.</p>
  </li>
  <li>
    <p>Mixed precision training derives its speed and memory benefits primarily by executing selected operations in <code class="language-plaintext highlighter-rouge">float16</code> rather than <code class="language-plaintext highlighter-rouge">float32</code>. However, not all operations are equally safe or efficient in half precision. Some are numerically stable and performant when cast to <code class="language-plaintext highlighter-rouge">float16</code>, while others require higher precision to avoid instability or incorrect outputs.</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">autocast</code> context manager dynamically casts operations to the most appropriate precision at runtime. This casting is done based on an internal whitelist/blacklist system defined by PyTorch, taking into account both the operation type and the tensor data types involved. This enables users to delegate dtype management to PyTorch, avoiding manual casting and type-checking logic.</p>
  </li>
  <li>
    <p><strong>How <code class="language-plaintext highlighter-rouge">autocast</code> Works Internally</strong>:</p>

    <ul>
      <li>Operations such as matrix multiplications (<code class="language-plaintext highlighter-rouge">matmul</code>), convolutions (<code class="language-plaintext highlighter-rouge">conv2d</code>), and other linear algebraic operations are generally safe to perform in <code class="language-plaintext highlighter-rouge">float16</code>, and thus are automatically downcast when inside an <code class="language-plaintext highlighter-rouge">autocast</code> context.</li>
      <li>
        <p>Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in <code class="language-plaintext highlighter-rouge">float32</code> to ensure computational accuracy.</p>
      </li>
      <li>The following visuals (<a href="https://pytorch.org/docs/master/amp.html#autocast-op-reference">source</a>), summarize these distinctions. The image below outlines common operations that benefit from <code class="language-plaintext highlighter-rouge">float16</code> execution. These include core building blocks of deep learning models like matrix multiplications, dot products, and convolutions. Their stability in half precision makes them ideal candidates for mixed precision acceleration.</li>
    </ul>

    <p><img src="/primers/ai/assets/model-compression/ops_widest.avif" alt=""></p>

    <ul>
      <li>In contrast, as shown in the image below (<a href="https://pytorch.org/docs/master/amp.html#autocast-op-reference">source</a>), operations involving logarithms, exponentials, or statistical reductions tend to suffer from rounding errors in <code class="language-plaintext highlighter-rouge">float16</code> and are therefore retained in <code class="language-plaintext highlighter-rouge">float32</code>.</li>
    </ul>

    <p><img src="/primers/ai/assets/model-compression/autocast_f32.avif" alt=""></p>
  </li>
  <li>
    <p><strong>Implications for Model Layers</strong>:</p>

    <ul>
      <li>
        <p>These rules imply that:</p>

        <ul>
          <li>Most <strong>layers</strong> (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.</li>
          <li>Most <strong>activation functions</strong> and <strong>normalization layers</strong> are less safe in <code class="language-plaintext highlighter-rouge">float16</code>, and autocast will retain full precision where necessary.</li>
          <li>The <strong>greatest performance gains</strong> are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Using <code class="language-plaintext highlighter-rouge">autocast</code> in Practice</strong>:</p>

    <ul>
      <li>Enabling autocasting is simple and requires wrapping the forward pass in a context manager:</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code23"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code23"><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">autocast</span><span class="p">():</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
</code></pre></div>    </div>

    <ul>
      <li>All operations within the <code class="language-plaintext highlighter-rouge">autocast()</code> context will be executed with optimal mixed precision, determined internally by PyTorch. Importantly, this casting behavior extends to the backward pass automatically—there is no need to wrap <code class="language-plaintext highlighter-rouge">loss.backward()</code>.</li>
    </ul>
  </li>
  <li>
    <p><strong>Best Practices and Notes</strong>:</p>

    <ul>
      <li>Autocast respects and supports a wide range of PyTorch operators out-of-the-box. Unless using custom operations or extensions, most models will run correctly without additional intervention.</li>
      <li>In-place operations (e.g., <code class="language-plaintext highlighter-rouge">.add_()</code> or <code class="language-plaintext highlighter-rouge">.relu_()</code>) can interfere with autocast’s internal precision control. Avoid in-place modifications inside <code class="language-plaintext highlighter-rouge">autocast()</code> blocks unless explicitly supported.</li>
      <li>Autocast is deterministic and composable. It can be used inside model layers, training loops, or custom modules with consistent behavior.</li>
      <li>For inference scenarios, autocasting is also beneficial and can be enabled in evaluation mode to reduce memory usage without requiring <code class="language-plaintext highlighter-rouge">GradScaler</code>.</li>
    </ul>
  </li>
</ul>

<h5 id="using-amp-with-multiple-gpus">Using AMP with Multiple GPUs</h5>

<ul>
  <li>
    <p>PyTorch’s Automatic Mixed Precision (AMP) functionality is fully compatible with multi-GPU training, enabling developers to scale up performance without sacrificing the benefits of mixed precision. Both of PyTorch’s multi-GPU parallelization strategies—<code class="language-plaintext highlighter-rouge">DistributedDataParallel</code> (DDP) and <code class="language-plaintext highlighter-rouge">DataParallel</code>—support autocasting and gradient scaling, with minimal adjustments.</p>
  </li>
  <li>
    <p>AMP’s multi-GPU support is robust and integrates seamlessly into distributed training workflows. With only minor adjustments, developers can leverage both horizontal scaling and mixed precision optimization, achieving faster training with efficient GPU utilization across multiple devices.</p>

    <ul>
      <li>
        <p><strong>DistributedDataParallel (DDP)</strong>: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and <code class="language-plaintext highlighter-rouge">GradScaler</code> instances per GPU, maintaining stability and efficiency.</p>
      </li>
      <li>
        <p><strong>DataParallel</strong>: AMP also works with <code class="language-plaintext highlighter-rouge">DataParallel</code>, but with a caveat. Since <code class="language-plaintext highlighter-rouge">DataParallel</code> uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official <a href="https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process">AMP Examples</a> guide. Specifically, ensure that loss scaling is only performed on the output of the model’s <code class="language-plaintext highlighter-rouge">.forward()</code> call on the main device, before broadcasting gradients.</p>
      </li>
      <li>
        <p><strong>Implementation Tips</strong>:</p>

        <ul>
          <li>Refer to the <a href="https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus">Working with Multiple GPUs</a> section in the PyTorch AMP documentation for detailed examples and best practices.</li>
          <li>Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends <a href="https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy">preferring binary cross entropy with logits over binary cross entropy</a>, as the logits version is more numerically stable and better suited for mixed precision.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="memory-considerations">Memory Considerations</h5>

<ul>
  <li>
    <p>One of the advertised benefits of mixed precision training, in addition to performance speedups, is reduced GPU memory consumption. As discussed in the earlier section on <a href="#how-mixed-precision-works">How Mixed Precision Works</a>, <code class="language-plaintext highlighter-rouge">float16</code> tensors require half the storage space of their <code class="language-plaintext highlighter-rouge">float32</code> counterparts. This reduction in memory footprint can be particularly advantageous in training large-scale models, where memory constraints often limit batch size or model complexity.</p>
  </li>
  <li>
    <p>Although GPU compute is generally the primary bottleneck in training workloads, optimizing memory usage remains important. Efficient memory utilization enables:</p>

    <ul>
      <li>Larger batch sizes, which can improve training stability and convergence.</li>
      <li>The ability to fit deeper or wider models within available hardware constraints.</li>
      <li>Reduced reliance on gradient checkpointing or memory-efficient architectures.</li>
    </ul>
  </li>
  <li>
    <p>PyTorch manages GPU memory allocation proactively. At the start of training, it reserves a block of GPU memory that it maintains throughout the training lifecycle. This behavior helps avoid runtime memory fragmentation and preempts crashes caused by other processes occupying memory mid-training. However, it also means that the effect of mixed precision on memory usage may not always be visible in a straightforward manner.</p>
  </li>
  <li>
    <p>The figure below illustrates PyTorch’s memory reservation behavior with and without AMP enabled:</p>
  </li>
</ul>

<p><img src="/primers/ai/assets/model-compression/amp_mem.avif" alt=""></p>

<ul>
  <li>
    <p>Interestingly, while both UNet and BERT models exhibit a reduction in memory usage when AMP is enabled, the gains are model-dependent. UNet, in particular, benefits significantly more than BERT. This discrepancy may result from differences in internal layer composition, memory allocation patterns, or the proportion of operations compatible with <code class="language-plaintext highlighter-rouge">float16</code>. PyTorch’s memory allocator is largely opaque, making it difficult to pinpoint exact causes without in-depth profiling.</p>
  </li>
  <li>
    <p>Nonetheless, practitioners can generally expect mixed precision to reduce overall memory usage, especially in convolution-heavy models like UNet. This makes AMP not only a tool for acceleration but also a practical memory optimization strategy, particularly beneficial for users working within the limits of consumer-grade GPUs or training on multiple models in parallel.</p>
  </li>
</ul>

<h5 id="further-reading-2">Further Reading</h5>

<ul>
  <li>
    <p>For further reading and in-depth examples, consult the official documentation:</p>

    <ul>
      <li><a href="https://pytorch.org/docs/master/amp.html">Automatic Mixed Precision package</a></li>
      <li><a href="https://pytorch.org/docs/master/notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
    </ul>
  </li>
</ul>

<h4 id="how-tensorflow-automatic-mixed-precision-works">How TensorFlow Automatic Mixed Precision Works</h4>

<ul>
  <li>
    <p>Mixed precision training in TensorFlow is designed to accelerate deep learning workloads by leveraging the efficiency of lower-precision (<code class="language-plaintext highlighter-rouge">float16</code>) arithmetic on supported hardware. With automatic mixed precision (AMP), TensorFlow streamlines the use of mixed-precision computation while preserving numerical stability and minimizing manual intervention.</p>
  </li>
  <li>
    <p>TensorFlow’s AMP support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes, using a hybrid approach similar to PyTorch. Operations are executed in <code class="language-plaintext highlighter-rouge">float16</code> where safe for performance and memory efficiency, while numerically sensitive computations remain in <code class="language-plaintext highlighter-rouge">float32</code> to preserve stability and ensure convergence.</p>
  </li>
  <li>
    <p>Built around the <code class="language-plaintext highlighter-rouge">mixed_precision</code> module, TensorFlow provides a high-level, intuitive interface for enabling efficient mixed-precision training. By leveraging global policies and loss scaling under the hood, TensorFlow abstracts away much of the complexity involved in training with <code class="language-plaintext highlighter-rouge">float16</code>. No manual casting or scaling logic is needed for most models, making integration straightforward for both experimentation and production. These features bring the practical benefits of faster computation and reduced memory usage—especially when training on modern GPUs—while maintaining the reliability of full-precision training. For a complete guide and reference examples, refer to the <a href="https://www.tensorflow.org/guide/mixed_precision">official TensorFlow mixed precision guide</a>.</p>
  </li>
</ul>

<h5 id="conceptual-overview">Conceptual Overview</h5>

<ul>
  <li>
    <p>Mixed precision training in TensorFlow works by executing computations in half-precision (<code class="language-plaintext highlighter-rouge">float16</code>) where safe, and in single-precision (<code class="language-plaintext highlighter-rouge">float32</code>) where required for numerical stability. This selective usage of data types reduces memory bandwidth and speeds up computation, particularly on GPUs equipped with NVIDIA Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>
  </li>
  <li>
    <p>The two core features of TensorFlow’s AMP system are:</p>

    <ul>
      <li>
        <p><strong>Global Policy Management</strong>: Mixed precision is enabled by setting a global or per-layer dtype policy to <code class="language-plaintext highlighter-rouge">'mixed_float16'</code>. This instructs TensorFlow to automatically cast eligible operations to <code class="language-plaintext highlighter-rouge">float16</code> while retaining critical variables (e.g., weights, certain accumulators) in <code class="language-plaintext highlighter-rouge">float32</code>.</p>
      </li>
      <li>
        <p><strong>Loss Scaling with <code class="language-plaintext highlighter-rouge">LossScaleOptimizer</code></strong>: To mitigate the risk of underflow—when gradient values fall below the representable range of <code class="language-plaintext highlighter-rouge">float16</code>—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with <code class="language-plaintext highlighter-rouge">tf.keras.mixed_precision.LossScaleOptimizer</code>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>These two features make mixed precision safe for most real-world training scenarios, enabling users to benefit from performance gains without manual tensor casting or custom scaling logic.</p>
  </li>
</ul>

<h5 id="practical-implementation-in-a-training-pipeline">Practical Implementation in a Training Pipeline</h5>

<ul>
  <li>TensorFlow’s mixed precision training is designed to be seamless, requiring only a few lines of code to enable. The following example demonstrates a typical setup using the <code class="language-plaintext highlighter-rouge">tf.keras</code> API.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code24"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code24"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">mixed_precision</span>

<span class="c1"># Enable mixed precision globally
</span><span class="n">mixed_precision</span><span class="p">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="s">'mixed_float16'</span><span class="p">)</span>

<span class="c1"># Define a model (example: simple MLP)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Wrap the optimizer with LossScaleOptimizer for stability
</span><span class="n">base_optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">mixed_precision</span><span class="p">.</span><span class="n">LossScaleOptimizer</span><span class="p">(</span><span class="n">base_optimizer</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>

<span class="c1"># Prepare training data
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Train the model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>
    <p><strong>Explanation of Key Components:</strong></p>

    <ul>
      <li>
        <p><strong><code class="language-plaintext highlighter-rouge">set_global_policy('mixed_float16')</code></strong>: This sets the default computation policy across all layers to use <code class="language-plaintext highlighter-rouge">float16</code> where safe, while storing variables such as model weights in <code class="language-plaintext highlighter-rouge">float32</code> to ensure stability.</p>
      </li>
      <li>
        <p><strong><code class="language-plaintext highlighter-rouge">LossScaleOptimizer</code></strong>: The base optimizer (e.g., <code class="language-plaintext highlighter-rouge">Adam</code>) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.</p>
      </li>
      <li>
        <p><strong>Hardware Requirements</strong>: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Layer Compatibility and Custom Layers</strong></p>

    <ul>
      <li>
        <p>Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:</p>

        <ul>
          <li>Operations numerically sensitive to precision are forced to <code class="language-plaintext highlighter-rouge">float32</code> if needed (using <code class="language-plaintext highlighter-rouge">tf.cast</code>).</li>
          <li>Custom gradients are correctly handled, especially in layers using <code class="language-plaintext highlighter-rouge">tf.custom_gradient</code>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Model Behavior and Performance Considerations</strong>:</p>

    <ul>
      <li>
        <p><strong>Storage Format</strong>: Weights are stored in <code class="language-plaintext highlighter-rouge">float32</code> internally, but computations are cast to <code class="language-plaintext highlighter-rouge">float16</code> where safe. This ensures a balance between performance and accuracy.</p>
      </li>
      <li>
        <p><strong>Layer Compatibility</strong>: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using <code class="language-plaintext highlighter-rouge">tf.cast()</code> or explicit dtype management.</p>
      </li>
      <li>
        <p><strong>Inference</strong>: After training, models trained with AMP can be saved and exported as usual. During inference, the <code class="language-plaintext highlighter-rouge">mixed_float16</code> policy can remain active to reduce latency and memory usage, particularly for large batch sizes.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Best Practices for TensorFlow AMP</strong>:</p>

    <ul>
      <li>Enable AMP by default (i.e., use <code class="language-plaintext highlighter-rouge">mixed_float16</code>) when training on Tensor Core GPUs, especially for models with substantial compute demands. Use dynamic loss scaling (enabled by default with <code class="language-plaintext highlighter-rouge">LossScaleOptimizer</code>) to maintain stability during training without the need for manual tuning.</li>
      <li>Monitor training for gradient anomalies (e.g., sudden spikes in loss). Although AMP is robust, occasional divergence may indicate the need for a lower initial loss scale or a refined model architecture.</li>
      <li>Use benchmark tools (e.g., TensorBoard, NVIDIA Nsight Systems) to validate performance gains and ensure your training benefits from mixed precision on supported hardware.</li>
    </ul>
  </li>
</ul>

<h5 id="performance-benchmarks">Performance Benchmarks</h5>

<ul>
  <li>To evaluate the real-world impact of mixed precision training, consider benchmarks run across three distinct neural network architectures using TensorFlow’s AMP implementation.</li>
  <li>These benchmarks demonstrate that <strong>automatic mixed precision should be one of the first performance optimizations you apply to your TensorFlow training scripts</strong>. In large-scale models, AMP can lead to dramatic reductions in training time—up to 60%—with minimal code changes (typically under 5 lines). Especially when training on modern GPU architectures, the performance uplift can be essential to reducing costs and iteration times in production-scale machine learning.</li>
  <li>
    <p>The experiments were conducted on AWS EC2 instances using both last-generation and current-generation NVIDIA GPUs.</p>
  </li>
  <li><strong>Hardware setup</strong>:
    <ul>
      <li><strong>V100</strong> (Volta architecture) via <code class="language-plaintext highlighter-rouge">p3.2xlarge</code></li>
      <li><strong>T4</strong> (Turing architecture) via <code class="language-plaintext highlighter-rouge">g4dn.xlarge</code></li>
    </ul>
  </li>
  <li>
    <p><strong>Framework</strong>: Recent TensorFlow builds with CUDA 10.0, orchestrated using the <a href="https://spell.ml/docs/run_overview/">Spell API</a></p>
  </li>
  <li>
    <p><strong>Models Tested</strong>:</p>

    <ol>
      <li>
        <p><strong>Feedforward Network</strong>
A fully connected feedforward network trained on tabular data from the <a href="https://www.kaggle.com/c/rossmann-store-sales">Rossmann Store Sales</a> Kaggle competition.
<em>Codebase</em>: <a href="https://github.com/ResidentMario/spell-feedforward-rossman">GitHub repository</a></p>
      </li>
      <li>
        <p><strong>UNet</strong>
A medium-sized convolutional model used for image segmentation on the <a href="https://www.kaggle.com/residentmario/segmented-bob-ross-images">Segmented Bob Ross Images</a> dataset.
<em>Codebase</em>: <a href="https://github.com/ResidentMario/spell-unet-bob-ross">GitHub repository</a></p>
      </li>
      <li>
        <p><strong>BERT</strong>
A large-scale transformer model (<code class="language-plaintext highlighter-rouge">bert-base-uncased</code>) trained on the <a href="https://www.kaggle.com/c/tweet-sentiment-extraction">Tweet Sentiment Extraction</a> dataset using Hugging Face’s Transformers.
<em>Codebase</em>: <a href="https://github.com/ResidentMario/spell-tweet-sentiment-extraction">GitHub repository</a></p>
      </li>
    </ol>
  </li>
  <li><strong>Benchmark Results</strong>:</li>
</ul>

<p><img src="/primers/ai/assets/model-compression/amp_time.avif" alt=""></p>

<ul>
  <li>
    <p><strong>Observations</strong>:</p>

    <ul>
      <li>
        <p><strong>Feedforward Network</strong>:
Being a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.</p>
      </li>
      <li>
        <p><strong>UNet (Medium-Scale Model)</strong>:
With approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:</p>

        <ul>
          <li>V100: ~5% training time reduction</li>
          <li>T4: ~30% reduction
This disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.</li>
        </ul>
      </li>
      <li>
        <p><strong>BERT (Large-Scale Model)</strong>:
Mixed precision provided <em>transformational</em> benefits for BERT:</p>

        <ul>
          <li>Training time reduced by <strong>50–60%</strong> on both GPU types</li>
          <li>No degradation in training loss or final model performance
This demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="key-takeaways">Key Takeaways</h5>

<ul>
  <li>
    <p>TensorFlow’s automatic mixed precision (AMP) support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes. By executing safe operations in <code class="language-plaintext highlighter-rouge">float16</code> while preserving critical numerical precision with <code class="language-plaintext highlighter-rouge">float32</code> where needed, TensorFlow achieves an optimal balance of performance and stability.</p>
  </li>
  <li>
    <p><strong>Ease of Integration</strong>: Mixed precision can be enabled in just a few lines using <code class="language-plaintext highlighter-rouge">mixed_precision.set_global_policy('mixed_float16')</code> and wrapping the optimizer with <code class="language-plaintext highlighter-rouge">LossScaleOptimizer</code>. No manual casting or scaling logic is needed for most models.</p>
  </li>
  <li>
    <p><strong>Hardware Acceleration</strong>: Significant speedups are realized on NVIDIA GPUs with Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>
  </li>
  <li>
    <p><strong>Scalability</strong>: The performance benefits of AMP scale with model size. While small models may see limited gains, medium-to-large models—particularly convolutional networks and transformers—can experience training time reductions of 30–60% or more.</p>
  </li>
  <li>
    <p><strong>Numerical Stability</strong>: Automatic loss scaling ensures that mixed precision does not compromise training convergence. Gradient underflows are mitigated adaptively, making AMP safe for most real-world training scenarios.</p>
  </li>
</ul>

<h5 id="recommendations">Recommendations</h5>

<ul>
  <li><strong>Enable AMP by default</strong> when training on Tensor Core GPUs, especially for models with substantial compute demands.</li>
  <li><strong>Benchmark performance</strong> for your specific model and dataset, as the impact of mixed precision can vary depending on architecture, data pipeline, and hardware.</li>
  <li><strong>Use dynamic loss scaling</strong> (enabled by default with <code class="language-plaintext highlighter-rouge">LossScaleOptimizer</code>) to maintain stability without the need for manual tuning.</li>
</ul>

<!-- 
## Aside: Inference optimizations

- A list of five techniques to optimize deep neural network model performance during inference.
    - Parallelization
    - Vectorization
    - Loop tiling
    - Operator fusion
    - Quantization
- Note that these techniques don't change the model architecture.
- Credits to [Sebastian Raschka](https://www.linkedin.com/in/sebastianraschka/) for the infographic below.

![](/primers/ai/assets/model-compression/inf-opt.jpeg) -->

<h2 id="low-rank-decomposition--adaptation">Low-Rank Decomposition &amp; Adaptation</h2>

<h3 id="overview-2">Overview</h3>

<ul>
  <li>
    <p>Large language models (LLMs) and deep neural networks often rely on massive weight matrices that are expensive to store and compute. However, in many cases, these matrices exhibit redundancy—meaning they can be approximated by lower-rank structures without significantly compromising the model’s predictive performance. Low-rank decomposition exploits this insight by factorizing large matrices into the product of two smaller matrices.</p>
  </li>
  <li>
    <p>The motivation is twofold:</p>

    <ol>
      <li><strong>Efficiency</strong>: Reducing the number of parameters and operations accelerates training and inference.</li>
      <li><strong>Compression</strong>: Enables model deployment on memory-constrained devices by lowering storage requirements.</li>
    </ol>
  </li>
  <li>
    <p>This technique is especially attractive when used with techniques like quantization, pruning, or transfer learning, as it complements them without requiring extensive retraining.</p>
  </li>
</ul>

<h3 id="formal-definition-1">Formal Definition</h3>

<ul>
  <li>
    <p>The core idea: replace a large dense weight matrix</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-132-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1585" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1586"><span class="mi" id="MathJax-Span-1587" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1588" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1589" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1590"><span class="mrow" id="MathJax-Span-1591"><span class="mi" id="MathJax-Span-1592" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1593"><span class="mrow" id="MathJax-Span-1594"><span class="mi" id="MathJax-Span-1595" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1596" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1597" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-132">W \in \mathbb{R}^{d \times d}</script>

    <ul>
      <li>with two smaller matrices:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-133-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1598" style="width: 11.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.273em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1009.27em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1599"><span class="mi" id="MathJax-Span-1600" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1601" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1602" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1603"><span class="mrow" id="MathJax-Span-1604"><span class="mi" id="MathJax-Span-1605" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1606"><span class="mrow" id="MathJax-Span-1607"><span class="mi" id="MathJax-Span-1608" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1609" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1610" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1611" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-1612" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-1613" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">B</span><span class="mo" id="MathJax-Span-1614" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1615" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1616"><span class="mrow" id="MathJax-Span-1617"><span class="mi" id="MathJax-Span-1618" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1619"><span class="mrow" id="MathJax-Span-1620"><span class="mi" id="MathJax-Span-1621" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1622" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1623" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width="1em"></mspace><mi>B</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-133">A \in \mathbb{R}^{d \times r}, \quad B \in \mathbb{R}^{r \times d}</script>

    <ul>
      <li>where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-134-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x226A;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1624" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1625"><span class="mi" id="MathJax-Span-1626" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1627" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≪</span><span class="mi" id="MathJax-Span-1628" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-134">r \ll d</script>. The original weight matrix is then approximated as:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-135-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1629" style="width: 4.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.6em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1630"><span class="mi" id="MathJax-Span-1631" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1632" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-1633" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">A</span><span class="mi" id="MathJax-Span-1634" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>≈</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-135">W \approx A B</script>
  </li>
  <li>
    <p>This reduces the parameter count from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-136-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1635" style="width: 2.919em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1636"><span class="mi" id="MathJax-Span-1637" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-1638" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1639"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1640" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.576em;"><span class="mn" id="MathJax-Span-1641" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1642" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-136">O(d^2)</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-137-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1643" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1644"><span class="mi" id="MathJax-Span-1645" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-1646" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1647" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-1648" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1649" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-137">O(rd)</script>, which is a significant gain when <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-138-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x226A;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1650" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1651"><span class="mi" id="MathJax-Span-1652" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1653" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≪</span><span class="mi" id="MathJax-Span-1654" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-138">r \ll d</script>. During inference or training, instead of computing:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-139-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1655" style="width: 3.857em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.18em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1656"><span class="mi" id="MathJax-Span-1657" style="font-family: STIXGeneral-Italic;">y</span><span class="mo" id="MathJax-Span-1658" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1659" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1660" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-139">y = Wx</script>

    <ul>
      <li>we compute:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-140-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1661" style="width: 5.159em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.273em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1662"><span class="mi" id="MathJax-Span-1663" style="font-family: STIXGeneral-Italic;">y</span><span class="mo" id="MathJax-Span-1664" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1665" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">A</span><span class="mo" id="MathJax-Span-1666" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1667" style="font-family: STIXGeneral-Italic;">B</span><span class="mi" id="MathJax-Span-1668" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1669" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>y</mi><mo>=</mo><mi>A</mi><mo stretchy="false">(</mo><mi>B</mi><mi>x</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-140">y = A (B x)</script>
  </li>
  <li>
    <p>This allows models to retain much of their representational power while becoming faster and more compact.</p>
  </li>
</ul>

<h3 id="concept">Concept</h3>

<ul>
  <li>
    <p>Freeze original pretrained weights; add trainable adapters <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-141-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1670" style="width: 2.294em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1671"><span class="mi" id="MathJax-Span-1672" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1673" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-1674" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>,</mo><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-141">A, B</script> for each dense layer such that:</p>
  </li>
  <li>Efficiency: Only train few parameters (e.g. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-142-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1675" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1676"><span class="mn" id="MathJax-Span-1677" style="font-family: STIXGeneral-Regular;">2</span><span class="mi" id="MathJax-Span-1678" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-1679" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mi>d</mi><mi>r</mi></math></span></span><script type="math/tex" id="MathJax-Element-142">2dr</script> per layer). For large LLMs, can reduce fine-tuning parameters by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-143-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2013;&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1680" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.01em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1681"><span class="msubsup" id="MathJax-Span-1682"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1683" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="mn" id="MathJax-Span-1684" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1685" style="font-family: STIXGeneral-Regular;">–</span><span class="msubsup" id="MathJax-Span-1686" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-1687" style="font-family: STIXGeneral-Regular;">10</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.992em;"><span class="texatom" id="MathJax-Span-1688"><span class="mrow" id="MathJax-Span-1689"><span class="mn" id="MathJax-Span-1690" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">4</span><span class="mo" id="MathJax-Span-1691" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>3</mn></msup><mo>–</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mn>4</mn><mo>×</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-143">10^3 – 10^{4×}</script> while achieving competitive results.</li>
  <li>Popular in federated settings (see above).</li>
</ul>

<h3 id="low-rank-correction-for-quantization">Low-Rank Correction for Quantization</h3>

<ul>
  <li>
    <p>Counteract quantization-induced error in activation domains. We approximate the full-precision weight matrix:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-144-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1692" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1693"><span class="mi" id="MathJax-Span-1694" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1695" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1696" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1697"><span class="mrow" id="MathJax-Span-1698"><span class="mi" id="MathJax-Span-1699" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1700"><span class="mrow" id="MathJax-Span-1701"><span class="mi" id="MathJax-Span-1702" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1703" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1704" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-144">W \in \mathbb{R}^{d \times d}</script>

    <ul>
      <li>as the product of two low-rank matrices:</li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-145-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mtext&gt;where&lt;/mtext&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;1em&quot; /&gt;&lt;mtext&gt;and&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x226A;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1705" style="width: 28.336em; display: inline-block;"><span style="display: inline-block; position: relative; width: 23.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1023.6em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1706"><span class="mi" id="MathJax-Span-1707" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1708" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-1709" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">A</span><span class="mi" id="MathJax-Span-1710" style="font-family: STIXGeneral-Italic;">B</span><span class="mo" id="MathJax-Span-1711" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-1712" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-1713" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">where</span><span class="mspace" id="MathJax-Span-1714" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-1715" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1716" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1717" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1718"><span class="mrow" id="MathJax-Span-1719"><span class="mi" id="MathJax-Span-1720" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1721"><span class="mrow" id="MathJax-Span-1722"><span class="mi" id="MathJax-Span-1723" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1724" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1725" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1726" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-1727" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-1728" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">B</span><span class="mo" id="MathJax-Span-1729" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1730" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1731"><span class="mrow" id="MathJax-Span-1732"><span class="mi" id="MathJax-Span-1733" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1734"><span class="mrow" id="MathJax-Span-1735"><span class="mi" id="MathJax-Span-1736" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1737" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1738" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1739" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-1740" style="height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-1741" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">and&nbsp;</span><span class="mi" id="MathJax-Span-1742" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1743" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≪</span><span class="mi" id="MathJax-Span-1744" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>≈</mo><mi>A</mi><mi>B</mi><mo>,</mo><mspace width="1em"></mspace><mtext>where</mtext><mspace width="1em"></mspace><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width="1em"></mspace><mi>B</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup><mo>,</mo><mspace width="1em"></mspace><mtext>and&nbsp;</mtext><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-145">W \approx A B, \quad \text{where} \quad A \in \mathbb{R}^{d \times r}, \quad B \in \mathbb{R}^{r \times d}, \quad \text{and } r \ll d</script>

    <ul>
      <li>
        <p>This reduces the parameter count from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-146-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1745" style="width: 2.919em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1746"><span class="mi" id="MathJax-Span-1747" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-1748" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1749"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1750" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.576em;"><span class="mn" id="MathJax-Span-1751" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1752" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-146">O(d^2)</script> to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-147-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1753" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1754"><span class="mi" id="MathJax-Span-1755" style="font-family: STIXGeneral-Italic;">O</span><span class="mo" id="MathJax-Span-1756" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1757" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-1758" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1759" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-147">O(rd)</script>, significantly lowering storage and compute cost.</p>
      </li>
      <li>
        <p>In quantization-aware settings, this is often extended to:</p>
      </li>
    </ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-148-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1760" style="width: 6.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.37em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1761"><span class="mi" id="MathJax-Span-1762" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1763" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-1764" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">Q</span><span class="mo" id="MathJax-Span-1765" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-1766" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">A</span><span class="mi" id="MathJax-Span-1767" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>≈</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-148">W \approx Q + A B</script>

    <ul>
      <li>
        <p>where:</p>

        <ul>
          <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-149-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1768" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1769"><span class="mi" id="MathJax-Span-1770" style="font-family: STIXGeneral-Italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-149">Q</script> is the quantized version of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-150-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1771" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1772"><span class="mi" id="MathJax-Span-1773" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-150">W</script>, e.g., <code class="language-plaintext highlighter-rouge">int4</code> or <code class="language-plaintext highlighter-rouge">int8</code></li>
          <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-151-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1774" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1775"><span class="mi" id="MathJax-Span-1776" style="font-family: STIXGeneral-Italic;">A</span><span class="mi" id="MathJax-Span-1777" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-151">A B</script> is a low-rank full-precision correction term on unquantized activations</li>
          <li>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-152-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1778" style="width: 9.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1008.13em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1779"><span class="mi" id="MathJax-Span-1780" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1781" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1782" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1783"><span class="mrow" id="MathJax-Span-1784"><span class="mi" id="MathJax-Span-1785" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1786"><span class="mrow" id="MathJax-Span-1787"><span class="mi" id="MathJax-Span-1788" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1789" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1790" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1791" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-1792" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">B</span><span class="mo" id="MathJax-Span-1793" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1794" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1795"><span class="mrow" id="MathJax-Span-1796"><span class="mi" id="MathJax-Span-1797" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.424em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1798"><span class="mrow" id="MathJax-Span-1799"><span class="mi" id="MathJax-Span-1800" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1801" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1802" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-152">A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times d}</script>
          </li>
        </ul>
      </li>
      <li>
        <p>This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.</p>
      </li>
    </ul>
  </li>
  <li>Solve via joint optimization: alternating minimization to fit both quantized and low-rank components to minimize output reconstruction error. With ranks at 10% of weight size, activation error gaps can be halved; with 30% rank, closed completely.</li>
  <li>Fits well with post-training quantization pipelines. Works across calibration sets without full retraining.</li>
</ul>

<h3 id="quantized-low-rank-adaptation-techniques">Quantized Low-Rank Adaptation Techniques</h3>

<ul>
  <li>Low-Rank Adaptation (LoRA) techniques allow efficient fine-tuning of large-scale pre-trained models by updating only a small number of additional low-rank matrices while freezing the original weights. Several recent advancements extend this idea by incorporating quantization, yielding memory-efficient and compute-friendly training pipelines. Below, we explore three major variants: <strong>LQ-LoRA</strong>, <strong>QLoRA</strong>, and <strong>QA-LoRA</strong>.</li>
</ul>

<h4 id="lqlora-quantized--low-rank-adaptation">LQ‑LoRA: Quantized + Low-Rank Adaptation</h4>

<ul>
  <li><strong>LQ-LoRA</strong> was introduced in <a href="https://arxiv.org/abs/2309.07063">Guo et al., 2023</a> as a memory-efficient fine-tuning approach that combines low-bit quantization with learnable low-rank adapters.</li>
</ul>

<h5 id="overview-3">Overview</h5>

<ul>
  <li>Each full-precision weight matrix <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-153-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1803" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1804"><span class="mi" id="MathJax-Span-1805" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1806" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1807" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1808"><span class="mrow" id="MathJax-Span-1809"><span class="mi" id="MathJax-Span-1810" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1811"><span class="mrow" id="MathJax-Span-1812"><span class="mi" id="MathJax-Span-1813" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1814" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1815" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-153">W \in \mathbb{R}^{d \times d}</script> is decomposed into:</li>
</ul>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-154-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1816" style="width: 6.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.37em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1817"><span class="mi" id="MathJax-Span-1818" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1819" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-1820" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">Q</span><span class="mo" id="MathJax-Span-1821" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-1822" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">A</span><span class="mi" id="MathJax-Span-1823" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>≈</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-154">W \approx Q + AB</script>

<ul>
  <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-155-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1824" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1003.96em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1825"><span class="mi" id="MathJax-Span-1826" style="font-family: STIXGeneral-Italic;">Q</span><span class="mo" id="MathJax-Span-1827" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1828" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1829"><span class="mrow" id="MathJax-Span-1830"><span class="mi" id="MathJax-Span-1831" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1832"><span class="mrow" id="MathJax-Span-1833"><span class="mi" id="MathJax-Span-1834" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1835" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1836" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-155">Q \in \mathbb{R}^{d \times d}</script>: a low-bit quantized matrix (e.g., 2.75 bits), kept <strong>frozen</strong></li>
  <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-156-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1837" style="width: 4.638em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1838"><span class="mi" id="MathJax-Span-1839" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1840" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1841" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1842"><span class="mrow" id="MathJax-Span-1843"><span class="mi" id="MathJax-Span-1844" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1845"><span class="mrow" id="MathJax-Span-1846"><span class="mi" id="MathJax-Span-1847" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1848" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1849" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-156">A \in \mathbb{R}^{d \times r}</script>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-157-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1850" style="width: 4.586em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1003.8em, 2.451em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1851"><span class="mi" id="MathJax-Span-1852" style="font-family: STIXGeneral-Italic;">B</span><span class="mo" id="MathJax-Span-1853" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1854" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1855"><span class="mrow" id="MathJax-Span-1856"><span class="mi" id="MathJax-Span-1857" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1858"><span class="mrow" id="MathJax-Span-1859"><span class="mi" id="MathJax-Span-1860" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1861" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1862" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-157">B \in \mathbb{R}^{r \times d}</script>: low-rank full-precision matrices, <strong>learnable</strong></li>
</ul>

<h5 id="key-properties">Key Properties</h5>

<ul>
  <li>The quantized base matrix <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-158-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1863" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1864"><span class="mi" id="MathJax-Span-1865" style="font-family: STIXGeneral-Italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-158">Q</script> captures the main representational power of the original model, without requiring updates during fine-tuning.</li>
  <li>The low-rank matrices <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-159-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1866" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1867"><span class="mi" id="MathJax-Span-1868" style="font-family: STIXGeneral-Italic;">A</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span><script type="math/tex" id="MathJax-Element-159">A</script> and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-160-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1869" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1870"><span class="mi" id="MathJax-Span-1871" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-160">B</script> adapt the model to the downstream task.</li>
  <li>Enables <strong>sub-3-bit quantization</strong> without major degradation in task performance.</li>
  <li>Requires only <strong>~27 GB</strong> of GPU memory to fine-tune a Llama 2 70B model, enabling large model training on commodity hardware.</li>
</ul>

<h5 id="pros-9">Pros</h5>

<ul>
  <li>High compression without sacrificing much accuracy.</li>
  <li>Applicable to ultra-large models (e.g., Llama 2 70B).</li>
  <li>Training only the LoRA adapters ensures stability even at low precision.</li>
</ul>

<h5 id="cons-9">Cons</h5>

<ul>
  <li>Fixed quantized base may limit adaptability in highly domain-shifted settings.</li>
  <li>Quantization granularity and calibration are critical for performance.</li>
</ul>

<h4 id="qlora-quantized-lora-with-4-bit-base-model">QLoRA: Quantized LoRA with 4-bit Base Model</h4>

<ul>
  <li><strong>QLoRA</strong>, proposed in <a href="https://arxiv.org/abs/2305.14314">Dettmers et al., 2023</a>, is an efficient fine-tuning method using a 4-bit quantized model backbone and LoRA adapters.</li>
</ul>

<h5 id="overview-4">Overview</h5>

<ul>
  <li>Applies <strong>4-bit NormalFloat (NF4)</strong> quantization to the pretrained weights.</li>
  <li>Performs <strong>double quantization</strong> to reduce memory further.</li>
  <li>Freezes the quantized weights and trains <strong>LoRA adapters</strong> over them.</li>
  <li>Uses paged optimizers and activation checkpointing for memory efficiency.</li>
</ul>

<h5 id="architecture">Architecture</h5>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-161-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;finetuned&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mtext&gt;Quantize&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;NF4&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;pretrained&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;LoRA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1872" style="width: 22.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1018.65em, 2.659em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1873"><span class="msubsup" id="MathJax-Span-1874"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1875" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-1876"><span class="mrow" id="MathJax-Span-1877"><span class="mtext" id="MathJax-Span-1878" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">finetuned</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1879" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-1880" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 4.951em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1003.6em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mtext" id="MathJax-Span-1881" style="font-family: STIXGeneral-Regular;">Quantize</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.799em; left: 3.596em;"><span class="texatom" id="MathJax-Span-1882"><span class="mrow" id="MathJax-Span-1883"><span class="mtext" id="MathJax-Span-1884" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">NF4</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1885" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-1886"><span style="display: inline-block; position: relative; width: 3.753em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1887" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-1888"><span class="mrow" id="MathJax-Span-1889"><span class="mtext" id="MathJax-Span-1890" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">pretrained</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1891" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1892" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-1893" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">Δ</span><span class="msubsup" id="MathJax-Span-1894"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1895" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-1896"><span class="mrow" id="MathJax-Span-1897"><span class="mtext" id="MathJax-Span-1898" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">LoRA</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mtext>finetuned</mtext></mrow></msub><mo>=</mo><msub><mtext>Quantize</mtext><mrow class="MJX-TeXAtom-ORD"><mtext>NF4</mtext></mrow></msub><mo stretchy="false">(</mo><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mtext>pretrained</mtext></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mtext>LoRA</mtext></mrow></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-161">W_{\text{finetuned}} = \text{Quantize}_{\text{NF4}}(W_{\text{pretrained}}) + \Delta W_{\text{LoRA}}</script>

<ul>
  <li>Only <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-162-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;LoRA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1899" style="width: 7.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.99em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1900"><span class="mi" id="MathJax-Span-1901" style="font-family: STIXGeneral-Regular;">Δ</span><span class="msubsup" id="MathJax-Span-1902"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1903" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-1904"><span class="mrow" id="MathJax-Span-1905"><span class="mtext" id="MathJax-Span-1906" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">LoRA</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1907" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-1908" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">A</span><span class="mi" id="MathJax-Span-1909" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mtext>LoRA</mtext></mrow></msub><mo>=</mo><mi>A</mi><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-162">\Delta W_{\text{LoRA}} = AB</script> is trained.</li>
  <li>Entire fine-tuning can be performed in <strong>&lt; 24 GB</strong> of GPU memory for models like Llama‑65B.</li>
</ul>

<h5 id="pros-10">Pros</h5>

<ul>
  <li>Fully open-source and hardware-efficient.</li>
  <li>Well-established tools in the ecosystem (e.g., Hugging Face <code class="language-plaintext highlighter-rouge">peft</code> and <code class="language-plaintext highlighter-rouge">bitsandbytes</code>).</li>
  <li>High accuracy retention even with 4-bit quantization.</li>
</ul>

<h5 id="cons-10">Cons</h5>

<ul>
  <li>Limited to NF4 quantization scheme.</li>
  <li>No learnability in the quantized weights themselves.</li>
</ul>

<h4 id="qa-lora-quantization-aware-lora">QA-LoRA: Quantization-Aware LoRA</h4>

<ul>
  <li><strong>QA-LoRA</strong>, proposed in <a href="https://arxiv.org/abs/2309.13680">Zhang et al., 2023</a>, adds quantization awareness to LoRA training by simulating quantization noise during fine-tuning.</li>
</ul>

<h5 id="overview-5">Overview</h5>

<ul>
  <li>Quantization-aware noise is <strong>injected</strong> into both the pretrained weights and the LoRA adapters during training.</li>
  <li>This simulates inference-time quantization effects during training, allowing the adapters to compensate more effectively.</li>
</ul>

<h5 id="architecture-1">Architecture</h5>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-163-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;finetuned&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;QuantNoise&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1910" style="width: 16.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1013.7em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1911"><span class="msubsup" id="MathJax-Span-1912"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1913" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="texatom" id="MathJax-Span-1914"><span class="mrow" id="MathJax-Span-1915"><span class="mtext" id="MathJax-Span-1916" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">finetuned</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1917" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mtext" id="MathJax-Span-1918" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">QuantNoise</span><span class="mo" id="MathJax-Span-1919" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1920" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1921" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-1922" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-1923" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">A</span><span class="mi" id="MathJax-Span-1924" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mtext>finetuned</mtext></mrow></msub><mo>=</mo><mtext>QuantNoise</mtext><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-163">W_{\text{finetuned}} = \text{QuantNoise}(W) + AB</script>

<ul>
  <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-164-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext&gt;QuantNoise&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1925" style="width: 7.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1006.41em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1926"><span class="mtext" id="MathJax-Span-1927" style="font-family: STIXGeneral-Regular;">QuantNoise</span><span class="mo" id="MathJax-Span-1928" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-1929" style="font-family: STIXGeneral-Italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-1930" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>QuantNoise</mtext><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-164">\text{QuantNoise}(W)</script>: Simulates quantization-induced errors on the frozen weights.</li>
  <li><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-165-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1931" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1932"><span class="mi" id="MathJax-Span-1933" style="font-family: STIXGeneral-Italic;">A</span><span class="mi" id="MathJax-Span-1934" style="font-family: STIXGeneral-Italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-165">AB</script>: LoRA component trained with awareness of quantization.</li>
</ul>

<h5 id="key-features">Key Features</h5>

<ul>
  <li>Supports ultra-low-bit quantization (e.g., 3- or 4-bit).</li>
  <li>Enables <strong>quantization-aware training (QAT)</strong> without modifying the original weight update path.</li>
</ul>

<h5 id="pros-11">Pros</h5>

<ul>
  <li>Improves robustness of LoRA adapters to quantization errors.</li>
  <li>Achieves lower perplexity and better accuracy than standard LoRA or QLoRA in low-bit settings.</li>
</ul>

<h5 id="cons-11">Cons</h5>

<ul>
  <li>Adds complexity to training pipeline.</li>
  <li>May require careful tuning of noise injection parameters.</li>
</ul>

<h4 id="comparative-analysis-3">Comparative Analysis</h4>

<ul>
  <li>This taxonomy of LoRA variants shows a clear evolution: from memory-focused quantized adapters (<strong>QLoRA</strong>) to ultra-low-bit efficient models (<strong>LQ-LoRA</strong>), to quantization-aware robust fine-tuning (<strong>QA-LoRA</strong>). Choice of method depends on the target compression level, hardware constraints, and sensitivity to quantization-induced artifacts.</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Feature</strong></th>
<th class="tg-hcenter-valign-first"><strong>LQ‑LoRA</strong></th>
<th class="tg-hcenter-valign-first"><strong>QLoRA</strong></th>
<th class="tg-hcenter-valign-second"><strong>QA‑LoRA</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Quantization Level</td>
<td class="tg-tleft-valign-first">≤ 3-bit (e.g., 2.75)</td>
<td class="tg-tleft-valign-first">4-bit NF4</td>
<td class="tg-tleft-valign-second">3–4-bit</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Trainable Params</td>
<td class="tg-tleft-valign-first">Low-rank adapters only</td>
<td class="tg-tleft-valign-first">LoRA adapters only</td>
<td class="tg-tleft-valign-second">LoRA adapters with QAT</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Quantized Weights</td>
<td class="tg-tleft-valign-first">Frozen, used as base</td>
<td class="tg-tleft-valign-first">Frozen, used as base</td>
<td class="tg-tleft-valign-second">Frozen + perturbed during training</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Noise Handling</td>
<td class="tg-tleft-valign-first">None</td>
<td class="tg-tleft-valign-first">None</td>
<td class="tg-tleft-valign-second">Simulated quantization noise</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Memory Efficiency</td>
<td class="tg-tleft-valign-first">~27 GB for Llama 2 70B</td>
<td class="tg-tleft-valign-first">~24 GB for Llama 65B</td>
<td class="tg-tleft-valign-second">Similar to QLoRA</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Complexity</td>
<td class="tg-tleft-valign-first">Medium</td>
<td class="tg-tleft-valign-first">Low</td>
<td class="tg-tleft-valign-second">High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Best Use Case</td>
<td class="tg-tleft-valign-first">Ultra-compressed deployment</td>
<td class="tg-tleft-valign-first">General-purpose fine-tuning</td>
<td class="tg-tleft-valign-second">Robustness under low-bit QAT</td>
</tr>
</tbody>
</table>
</div>

<h3 id="pros--cons">Pros &amp; Cons</h3>

<ul>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li><strong>Parameter-efficient fine-tuning</strong>: Minimal new parameters needed (e.g., LoRA, QLoRA, QA-LoRA).</li>
      <li><strong>Quantization synergy</strong>: Works well with 4-bit quantization (QLoRA) or ultra-low-bit regimes (LQ-LoRA).</li>
      <li><strong>Quantization-aware robustness</strong>: QA-LoRA improves low-bit model accuracy via simulated noise.</li>
      <li><strong>Adaptable to distributed settings</strong>: LoRA-based updates are lightweight and communication-efficient.</li>
      <li><strong>Accuracy retention</strong>: Strong accuracy, even under aggressive quantization.</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Effectiveness may degrade if base weights are not sufficiently low-rank (task-dependent).</li>
      <li>Combining quantization and adaptation (as in QA-LoRA or LQ-LoRA) introduces training complexity.</li>
      <li>Requires careful tuning of rank, quantization scheme, and noise injection (QA-LoRA).</li>
      <li>QLoRA assumes compatibility with NF4 quantization and specific tooling (bitsandbytes, Hugging Face PEFT).</li>
    </ul>
  </li>
</ul>

<h3 id="comparison--use-cases">Comparison &amp; Use Cases</h3>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Use Case</strong></th>
<th class="tg-hcenter-valign-first"><strong>Suggested Strategy</strong></th>
<th class="tg-hcenter-valign-second"><strong>Benefit</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Parameter-efficient tuning</td>
<td class="tg-tleft-valign-first">LoRA / QLoRA / ALoRA</td>
<td class="tg-tleft-valign-second">Reduces compute/memory footprint</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Extreme quantization</td>
<td class="tg-tleft-valign-first">LQ‑LoRA or Low-Rank Correction</td>
<td class="tg-tleft-valign-second">Sub‑3‑bit performance retention</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Federated fine‑tuning</td>
<td class="tg-tleft-valign-first">Federated LoRA / QLoRA adapters</td>
<td class="tg-tleft-valign-second">Minimal communication cost</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Quantization-aware training</td>
<td class="tg-tleft-valign-first">QA‑LoRA + QAT</td>
<td class="tg-tleft-valign-second">High fidelity under 3–4 bit settings</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">4-bit memory-efficient finetuning</td>
<td class="tg-tleft-valign-first">QLoRA</td>
<td class="tg-tleft-valign-second">Near full accuracy with 4-bit NF4</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Robust training under quantization noise</td>
<td class="tg-tleft-valign-first">QA‑LoRA</td>
<td class="tg-tleft-valign-second">Noise-aware adapters improve generalization</td>
</tr>
</tbody>
</table>
</div>

<h3 id="key-takeaways-1">Key Takeaways</h3>

<ul>
  <li>
    <p>Low-rank techniques improve the efficiency of training and inference by decomposing full-rank weight matrices into two smaller matrices (e.g., <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-166-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext&gt;&amp;#xA0;and&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mtext&gt;&amp;#xA0;where&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;&amp;#x226A;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1935" style="width: 19.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1016.62em, 2.555em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1936"><span class="mi" id="MathJax-Span-1937" style="font-family: STIXGeneral-Italic;">A</span><span class="mo" id="MathJax-Span-1938" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1939" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1940"><span class="mrow" id="MathJax-Span-1941"><span class="mi" id="MathJax-Span-1942" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1943"><span class="mrow" id="MathJax-Span-1944"><span class="mi" id="MathJax-Span-1945" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span><span class="mo" id="MathJax-Span-1946" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1947" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mtext" id="MathJax-Span-1948" style="font-family: STIXGeneral-Regular;">&nbsp;and&nbsp;</span><span class="mi" id="MathJax-Span-1949" style="font-family: STIXGeneral-Italic;">B</span><span class="mo" id="MathJax-Span-1950" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">∈</span><span class="msubsup" id="MathJax-Span-1951" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-1952"><span class="mrow" id="MathJax-Span-1953"><span class="mi" id="MathJax-Span-1954" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1955"><span class="mrow" id="MathJax-Span-1956"><span class="mi" id="MathJax-Span-1957" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1958" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1959" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">m</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1960" style="font-family: STIXGeneral-Regular;">,</span><span class="mtext" id="MathJax-Span-1961" style="font-family: STIXGeneral-Regular; padding-left: 0.211em;">&nbsp;where&nbsp;</span><span class="mi" id="MathJax-Span-1962" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1963" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≪</span><span class="mi" id="MathJax-Span-1964" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">n</span><span class="mo" id="MathJax-Span-1965" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-1966" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">m</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup><mtext>&nbsp;and&nbsp;</mtext><mi>B</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>r</mi><mo>×</mo><mi>m</mi></mrow></msup><mo>,</mo><mtext>&nbsp;where&nbsp;</mtext><mi>r</mi><mo>≪</mo><mi>n</mi><mo>,</mo><mi>m</mi></math></span></span><script type="math/tex" id="MathJax-Element-166">A \in \mathbb{R}^{n \times r} \text{ and } B \in \mathbb{R}^{r \times m}, \text{ where } r \ll n, m</script>). These factorizations can replace or augment full-weight updates, as seen in LoRA and its variants.</p>

    <ul>
      <li><strong>QLoRA</strong> combines low-rank adapters with 4-bit NF4 quantized base models, drastically reducing memory usage during training without sacrificing accuracy. It enables finetuning large LLMs (e.g., 65B+) on consumer hardware.</li>
      <li><strong>QA-LoRA</strong> extends this further by injecting simulated quantization noise into the training process, making LoRA adapters inherently robust to downstream quantization.</li>
      <li><strong>LQ-LoRA</strong> targets extremely low-bit regimes (e.g., sub-3-bit) by jointly optimizing low-rank corrections and quantized base weights.</li>
    </ul>
  </li>
  <li>
    <p>Overall, low-rank decomposition plays a central role in enabling quantization-aware fine-tuning pipelines, federated adaptation, and cross-device deployment, all while maintaining high performance and parameter efficiency.</p>
  </li>
</ul>

<h2 id="lightweight-model-design">Lightweight Model Design</h2>

<ul>
  <li>While compression methods like quantization, pruning, and distillation focus on reducing the size of existing models, lightweight model design starts from a different premise: create architectures that are efficient by construction. This approach is particularly valuable for scenarios where on-device inference is the primary goal, and where the constraints on memory, compute, and energy consumption are known upfront.</li>
  <li>Lightweight model design, especially when coupled with the aforementioned compression methods, provides a principled pathway to achieve sub-second inference on constrained devices while maintaining competitive accuracy. In many real-world edge AI scenarios, a well-designed small model can outperform a compressed large model in terms of both speed and stability.</li>
</ul>

<h3 id="principles-of-lightweight-design">Principles of Lightweight Design</h3>

<ol>
  <li>
    <p><strong>Parameter Efficiency</strong>: Instead of large dense layers, lightweight designs emphasize reducing parameter counts through smaller embedding dimensions, narrower feed-forward layers, and compact convolutional kernels. For instance, a conventional convolution with kernel size <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-167-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1967" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1968"><span class="mo" id="MathJax-Span-1969" style="font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-1970" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-167">\times k</script> and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-168-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1971" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1972"><span class="msubsup" id="MathJax-Span-1973"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1974" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-1975"><span class="mrow" id="MathJax-Span-1976"><span class="mtext" id="MathJax-Span-1977" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">in</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>in</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-168">C_{\text{in}}</script> input channels can be replaced with a depthwise convolution (cost <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-169-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1978" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.24em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1979"><span class="msubsup" id="MathJax-Span-1980"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1981" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.523em;"><span class="mn" id="MathJax-Span-1982" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-1983"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1984" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-1985"><span class="mrow" id="MathJax-Span-1986"><span class="mtext" id="MathJax-Span-1987" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">in</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>k</mi><mn>2</mn></msup><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>in</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-169">k^2 C_{\text{in}}</script>) followed by a pointwise <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-170-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1988" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1989"><span class="mn" id="MathJax-Span-1990" style="font-family: STIXGeneral-Regular;">1</span><span class="mo" id="MathJax-Span-1991" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">×</span><span class="mn" id="MathJax-Span-1992" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>×</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-170">1 \times 1</script> convolution (cost <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-171-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1993" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.97em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1994"><span class="msubsup" id="MathJax-Span-1995"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-1996" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-1997"><span class="mrow" id="MathJax-Span-1998"><span class="mtext" id="MathJax-Span-1999" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">in</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-2000"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-2001" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-2002"><span class="mrow" id="MathJax-Span-2003"><span class="mtext" id="MathJax-Span-2004" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">out</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>in</mtext></mrow></msub><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>out</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-171">C_{\text{in}} C_{\text{out}}</script>), drastically lowering Multiply-Accumulate operations (MACs).</p>
  </li>
  <li>
    <p><strong>Computational Sparsity</strong>: Many architectures employ sparsity patterns directly in their design—such as grouped convolutions, block-sparse attention, or factorized projections—to reduce the number of required operations without relying on post-hoc pruning.</p>
  </li>
  <li>
    <p><strong>Layer Reduction and Structural Reuse</strong>: Models like DistilBERT and TinyBERT achieve compactness by halving the number of transformer layers while using distillation losses to retain semantic fidelity. CNN variants often reuse small building blocks in repeated stages to maintain expressiveness without excessive depth.</p>
  </li>
  <li>
    <p><strong>Activation and Feature Map Optimization</strong>: Activations are often the main source of memory usage during inference. Designs that minimize activation size—via lower resolution feature maps, early downsampling, or reduced channel widths—reduce both memory footprint and bandwidth demands.</p>
  </li>
  <li>
    <p><strong>Weight Sharing</strong>: Weight sharing reduces storage requirements by reusing the same parameter values across multiple parts of the network. Instead of learning a unique weight for every connection, the model maintains a smaller set of shared weights and uses index mapping to assign them where needed.</p>
    <ul>
      <li><strong>Vector/Matrix Sharing</strong>: In RNNs and Transformers, the same weight matrix may be used for multiple layers or projections. A notable example is <em>weight tying</em> in language models, where the input embedding matrix and the output softmax weights are shared to both reduce parameters and improve perplexity, as proposed in <a href="https://arxiv.org/abs/1608.05859">Using the Output Embedding to Improve Language Models</a> by Press and Wolf (2017).</li>
      <li><strong>Hash-Based Sharing</strong>: Parameters are grouped by a hash function into a small number of “buckets,” each storing a single shared weight value.</li>
      <li><strong>Cyclic or Rotational Sharing</strong>: Convolutional kernels are repeated or rotated across channels or layers, reducing the total number of learned unique values.</li>
      <li><strong>Benefits</strong>: Dramatically reduces model storage and can improve generalization by limiting overfitting.</li>
      <li><strong>Trade-offs</strong>: May slightly reduce representational capacity if sharing is too aggressive, requiring careful balancing.</li>
    </ul>
  </li>
</ol>

<h3 id="design-methodologies">Design Methodologies</h3>

<ul>
  <li><strong>Manual Architecture Engineering</strong>: Historically, lightweight models such as SqueezeNet and MobileNetV1 emerged from manual exploration of kernel sizes, strides, and filter counts to balance accuracy and cost.</li>
  <li><strong>Neural Architecture Search (NAS)</strong>: Modern approaches leverage latency-aware NAS to discover architectures tailored to specific devices. Search objectives often incorporate hardware-measured inference latency, power draw, or memory footprint in addition to accuracy.</li>
  <li><strong>Hybrid Approaches</strong>: Many practical deployments combine lightweight design with compression techniques. For example, MobileBERT applies a bottlenecked Transformer architecture and then further compresses it via quantization and distillation.</li>
</ul>

<h3 id="representative-architectures">Representative Architectures</h3>

<ul>
  <li><strong>MobileNetV2/V3</strong>: Introduced inverted residual blocks with linear bottlenecks and squeeze-and-excitation modules for better accuracy-efficiency trade-offs.</li>
  <li><strong>EfficientNet-Lite</strong>: Scales network depth, width, and resolution using compound scaling optimized for mobile hardware.</li>
  <li><strong>DistilBERT</strong>: Retains 97% of BERT-base’s language understanding capability with 40% fewer parameters and 60% faster inference.</li>
  <li><strong>ConvNeXt-T</strong>: Adapts design elements from vision transformers into a lightweight CNN backbone for efficient vision tasks.</li>
</ul>

<h3 id="when-to-use-lightweight-models">When to Use Lightweight Models</h3>

<ul>
  <li>The target hardware has strict latency or power limits that even heavily compressed large models cannot meet.</li>
  <li>The deployment pipeline does not support large intermediate activations due to memory constraints.</li>
  <li>Model training and deployment budgets are limited, making retraining from scratch feasible but large-model compression less practical.</li>
</ul>

<h2 id="what-to-use-when-1">What to Use When?</h2>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Technique</strong></th>
<th class="tg-hcenter-valign-first"><strong>When to Use</strong></th>
<th class="tg-hcenter-valign-first"><strong>Key Benefits</strong></th>
<th class="tg-hcenter-valign-second"><strong>Potential Trade-offs</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Quantization</td>
<td class="tg-tleft-valign-first">When inference speed and reduced memory footprint are critical, especially for deployment on low-power devices (e.g., microcontrollers, mobile CPUs). Best for models tolerant to reduced precision.</td>
<td class="tg-tleft-valign-first">Significant reduction in model size and faster inference with minimal retraining.</td>
<td class="tg-tleft-valign-second">Possible accuracy loss, especially for sensitive models; requires hardware support for low-precision operations.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Pruning</td>
<td class="tg-tleft-valign-first">When the model has redundant weights or neurons and needs optimization without a full redesign. Works best for overparameterized models.</td>
<td class="tg-tleft-valign-first">Reduces computation and memory usage, potentially improving inference speed.</td>
<td class="tg-tleft-valign-second">Can require fine-tuning to recover accuracy; speedup depends on hardware’s ability to exploit sparsity.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Knowledge Distillation</td>
<td class="tg-tleft-valign-first">When you have a large, high-performing model (teacher) and want a smaller, faster model (student) without much accuracy loss. Useful for compressing complex architectures.</td>
<td class="tg-tleft-valign-first">Maintains competitive accuracy in a smaller model; flexible with architecture changes.</td>
<td class="tg-tleft-valign-second">Needs access to a trained large model; training time for student model can be substantial.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Low-Rank Factorization</td>
<td class="tg-tleft-valign-first">When large weight matrices dominate model size and computation (common in fully connected or large convolution layers). Best for models where large dense weight matrices dominate, such as transformer fully connected layers or high-dimensional convolution kernels, enabling significant memory and compute savings through decomposition.</td>
<td class="tg-tleft-valign-first">Reduces parameters and computations while keeping most of the representational power.</td>
<td class="tg-tleft-valign-second">May not yield large gains for already compact architectures; can require re-training to mitigate accuracy loss.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Lightweight Model Design</td>
<td class="tg-tleft-valign-first">When building a model from scratch for deployment on constrained devices, prioritizing efficiency from the start.</td>
<td class="tg-tleft-valign-first">Natively optimized for speed and memory; avoids heavy compression steps later.</td>
<td class="tg-tleft-valign-second">Might sacrifice some accuracy ceiling compared to larger models; offers limited benefits for large models not originally designed for efficiency, as architectural constraints may limit the achievable reductions in size or computation without significant redesign.
</td>
</tr>
</tbody>
</table>
</div>

<h2 id="combining-model-compression-techniques">Combining Model Compression Techniques</h2>

<ul>
  <li>
    <p>While individual compression methods—such as quantization, knowledge distillation, pruning, low‑rank decomposition, and lightweight design—each provide unique efficiency gains, their true potential is realized when strategically combined. These hybrid strategies leverage the strengths of each method to maximize performance improvements while mitigating individual drawbacks.</p>
  </li>
  <li><strong>Distilling to a Quantized Student Model</strong>:
    <ul>
      <li>A highly effective pipeline is to first perform knowledge distillation from a large, high‑accuracy teacher (in full precision), then quantize the student model. Distillation ensures the student inherits rich representational power and decision boundaries, while quantization compresses the student for deployment.</li>
      <li>A seminal work—<a href="https://arxiv.org/abs/1802.05668">Model compression via distillation and quantization</a> by Polino et al. (2018)—pioneered <strong>quantized distillation</strong>, where distillation loss is integrated into training a quantized student network, achieving similar accuracy to full‑precision teachers with significant compression and speedup.</li>
    </ul>
  </li>
  <li><strong>Quantization with Pruning</strong>:
    <ul>
      <li>Pruning removes low‑contributing weights or structures, while quantization reduces the precision of the remaining parameters. Pruning before quantization yields simpler weight distributions, making quantization more effective.</li>
      <li>A unified method called <a href="https://arxiv.org/abs/2106.14681">PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation</a> by Kim et al. (2021), demonstrates how combining these can lead to highly efficient models—especially for edge devices—by iteratively pruning with quantization-aware training, then distilling using a teacher model derived from the pruned weights.</li>
    </ul>
  </li>
  <li><strong>Low-Rank Decomposition plus Quantization-Aware Training</strong>:
    <ul>
      <li>Low‑rank decomposition approximates large weight matrices with factorized forms, while QAT helps the model adapt to quantization noise. Combining both allows the model to handle compression noise and structural constraints simultaneously—particularly valuable in transformer architectures.</li>
      <li>Numerous compression surveys highlight this hybrid approach as a strong candidate for further research. For instance:
        <ul>
          <li><a href="https://www.mdpi.com/2073-431X/12/3/60">Model Compression for Deep Neural Networks: A Survey</a> by Li et al. (2023) provides an in-depth overview of techniques—including pruning, parameter quantization, low‑rank decomposition, knowledge distillation, and lightweight model design—that each form key components in hybrid compression pipelines.</li>
          <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full">A survey of model compression techniques: past, present, and future</a> by Liu et al. (2025) categorizes compression methods into pruning, low‑rank decomposition, quantization, and distillation—explicitly framing them within a taxonomy that supports multi‑technique combinations.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Lightweight Design with Mixed Precision</strong>:
    <ul>
      <li>Lightweight architectures (e.g., MobileBERT, DistilBERT, ConvNeXt‑Tiny) can be crafted for mixed-precision usage from the outset—assigning <code class="language-plaintext highlighter-rouge">float16</code> or <code class="language-plaintext highlighter-rouge">bfloat16</code> to numerically sensitive layers, and <code class="language-plaintext highlighter-rouge">int8</code> or lower precision to others. This combination maximizes compression while preserving stability and accuracy.</li>
      <li>Comprehensive surveys on model compression, which cover lightweight design alongside pruning, quantization, and distillation, include:
        <ul>
          <li><a href="https://www.mdpi.com/2073-431X/12/3/60">Model Compression for Deep Neural Networks: A Survey</a> by Li et al. (2023).</li>
          <li><a href="https://link.springer.com/article/10.1007/s10489-024-05747-w">A comprehensive review of model compression techniques in machine learning</a> by Dantas et al. (2024).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Multi-Stage Pipelines</strong>:
    <ul>
      <li>
        <p>Advanced workflows often chain multiple methods in sequence:</p>

        <ul>
          <li>Train a large teacher model.</li>
          <li>Distill to a smaller student using architectural tweaks (lightweight design).</li>
          <li>Apply structured pruning to eliminate redundancy.</li>
          <li>Fine-tune with QAT to prepare for low‑bit execution.</li>
          <li>Deploy with mixed precision and operator fusion optimized for specific hardware.</li>
        </ul>
      </li>
      <li>
        <p>This kind of multi‑stage approach is emphasized by surveys that explore combinations of compression strategies. Notable works include:</p>
        <ul>
          <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full">A survey of model compression techniques: past, present, and future</a> by Liu et al. (2025).</li>
          <li><a href="https://arxiv.org/abs/2402.09748">Model Compression and Efficient Inference for Large Language Models: A Survey</a> by Wang et al. (2024).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Key Benefits of Combining Techniques</strong>:</p>

    <ul>
      <li><strong>Complementary Strengths:</strong> Distillation can recover accuracy lost due to pruning or quantization.</li>
      <li><strong>Hardware Adaptation:</strong> Enables tailoring for CPUs, GPUs, TPUs, or edge accelerators.</li>
      <li><strong>Cumulative Efficiency:</strong> Size, speed, and power savings accumulate when methods are stacked, often surpassing single-method improvements.</li>
      <li><strong>Accuracy Preservation:</strong> Well-ordered workflows—particularly those involving distillation and QAT—can keep accuracy within ~1–2 % of the original full-precision model.</li>
    </ul>
  </li>
  <li>In practice, the optimal combination depends on model architecture, target hardware, and deployment constraints. Systematic experimentation—monitoring accuracy, latency, memory footprint, and energy—is essential for arriving at the best compression strategy.</li>
</ul>

<h2 id="further-reading-3">Further Reading</h2>

<ul>
  <li><a href="https://pytorch.org/docs/stable/quantization.html">PyTorch: Quantization</a></li>
  <li><a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">PyTorch: Introduction to Quantization on PyTorch</a></li>
  <li><a href="https://www.tensorflow.org/model_optimization/guide/pruning/">TensorFlow: Pruning Tutorial</a></li>
  <li><a href="https://leimao.github.io/blog/PyTorch-Dynamic-Quantization/">Lei Mao’s Dynamic Quantization blog</a></li>
  <li><a href="https://medium.com/%40florian_algo/model-quantization-2-uniform-and-non-uniform-quantization-47ca5b5d3ec0">Model Quantization 2: Uniform and non-Uniform Quantization</a></li>
  <li><a href="https://openreview.net/pdf/b3b080d65bcd62decbabee21c95624d3051e802b.pdf">PTNQ: Post-Training Non-Linear Quantization</a></li>
  <li><a href="https://arxiv.org/abs/1902.08153">Learned Step Size Quantization</a></li>
  <li><a href="https://www.youtube.com/watch?v=9u7GnYTrFgM">MLconf Online 2021 - MLOps Event: Efficient Deep Learning by Gaurav Menghani</a></li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://stackoverflow.com/questions/70503585/pytorch-model-optimization-automatic-mixed-precision-vs-quantization">PyTorch Model Optimization: Automatic Mixed Precision vs Quantization</a></li>
  <li><a href="https://rachitsingh.com/deep-learning-model-compression/">Deep Learning Model Compression by Rachit Singh</a></li>
  <li><a href="https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam">A developer-friendly guide to mixed precision training with PyTorch</a></li>
  <li><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">Nvidia’s Matrix Multiplication Background User’s Guide</a></li>
  <li><a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34/">4-bit Quantization with GPTQ</a></li>
  <li><a href="https://ai.stackexchange.com/questions/43054/when-to-use-pruning-quantization-distillation-and-others-when-optimizing-spee">When to use Pruning, Quantization, Distillation and others when optimizing speed</a></li>
</ul>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code25"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code25">@article{Chadha2020DistilledModelCompression,
  title   = {Model Compression},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895470&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fmodel-compression%2F&amp;pra=5&amp;wgl=1&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766922948121&amp;bpp=1&amp;bdt=85&amp;idt=8&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=4163967498992&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31095903%2C31096041%2C95376241%2C95376583%2C95378749%2C95344787%2C95340252%2C95340254%2C95379823&amp;oid=2&amp;pvsid=3590158141983007&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=33792&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=11" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe src="https://www.google.com/recaptcha/api2/aframe" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>