[
  {
    "id": "ai-vision-language-models-generation-tasks-1",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Vision-language Tasks",
    "title": "Generation Tasks",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Visual Question Answering (VQA) refers to the process of providing an answer to a question given a visual input (image or video).</p>\n  </li>\n  <li>\n    <p>Visual Captioning (VC) generates descriptions for a given visual input.</p>\n  </li>\n  <li>\n    <p>Visual Commonsense Reasoning (VCR) infers common-sense information and cognitive understanding given a visual input.</p>\n  </li>\n  <li>\n    <p>Visual Generation (VG) generates visual output from a textual input, as shown in the image.</p>\n  </li>\n  <li>\n    <p>The following image from OpenAI’s blog shows AI-generated images based on a user-fed input prompt:</p>\n  </li>\n</ul>\n<p>Visual Question Answering (VQA) refers to the process of providing an answer to a question given a visual input (image or video).</p>\n<p>Visual Captioning (VC) generates descriptions for a given visual input.</p>\n<p>Visual Commonsense Reasoning (VCR) infers common-sense information and cognitive understanding given a visual input.</p>\n<p>Visual Generation (VG) generates visual output from a textual input, as shown in the image.</p>\n<p>The following image from OpenAI’s blog shows AI-generated images based on a user-fed input prompt:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/image-generation.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Visual Question Answering (VQA) refers to the process of providing an answer to a question given a visual input (image or video).\n    \n*   Visual Captioning (VC) generates descriptions for a given visual input.\n    \n*   Visual Commonsense Reasoning (VCR) infers common-sense information and cognitive understanding given a visual input.\n    \n*   Visual Generation (VG) generates visual output from a textual input, as shown in the image.\n    \n*   The following image from OpenAI’s blog shows AI-generated images based on a user-fed input prompt:\n    \n\nVisual Question Answering (VQA) refers to the process of providing an answer to a question given a visual input (image or video).\n\nVisual Captioning (VC) generates descriptions for a given visual input.\n\nVisual Commonsense Reasoning (VCR) infers common-sense information and cognitive understanding given a visual input.\n\nVisual Generation (VG) generates visual output from a textual input, as shown in the image.\n\nThe following image from OpenAI’s blog shows AI-generated images based on a user-fed input prompt:\n\n![](/primers/ai/assets/vision-language-models/image-generation.png)",
    "contentLength": 1284,
    "wordCount": 158,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#generation-tasks"
  },
  {
    "id": "ai-vision-language-models-classification-tasks-2",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Vision-language Tasks",
    "title": "Classification Tasks",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Multimodal Affective Computing (MAC) interprets visual affective activity from visual and textual input. In a way, it can be seen as multimodal sentiment analysis.</p>\n  </li>\n  <li>\n    <p>Natural Language for Visual Reasoning (NLVR) determines if a statement regarding a visual input is correct or not.</p>\n  </li>\n</ul>\n<p>Multimodal Affective Computing (MAC) interprets visual affective activity from visual and textual input. In a way, it can be seen as multimodal sentiment analysis.</p>\n<p>Natural Language for Visual Reasoning (NLVR) determines if a statement regarding a visual input is correct or not.</p>",
    "contentMarkdown": "*   Multimodal Affective Computing (MAC) interprets visual affective activity from visual and textual input. In a way, it can be seen as multimodal sentiment analysis.\n    \n*   Natural Language for Visual Reasoning (NLVR) determines if a statement regarding a visual input is correct or not.\n    \n\nMultimodal Affective Computing (MAC) interprets visual affective activity from visual and textual input. In a way, it can be seen as multimodal sentiment analysis.\n\nNatural Language for Visual Reasoning (NLVR) determines if a statement regarding a visual input is correct or not.",
    "contentLength": 634,
    "wordCount": 86,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/vision-language-models/#classification-tasks"
  },
  {
    "id": "ai-vision-language-models-retrieval-tasks-3",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Vision-language Tasks",
    "title": "Retrieval Tasks",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Visual Retrieval (VR) retrieves images based only on a textual description.</p>\n  </li>\n  <li>\n    <p>Vision-Language Navigation (VLN) is the task of an agent navigating through a space based on textual instructions.</p>\n  </li>\n  <li>\n    <p>Multimodal Machine Translation (MMT) involves translating a description from one language to another with additional visual information.</p>\n  </li>\n  <li>\n    <p>The following image shows the taxonomy of popular visual language tasks:</p>\n  </li>\n</ul>\n<p>Visual Retrieval (VR) retrieves images based only on a textual description.</p>\n<p>Vision-Language Navigation (VLN) is the task of an agent navigating through a space based on textual instructions.</p>\n<p>Multimodal Machine Translation (MMT) involves translating a description from one language to another with additional visual information.</p>\n<p>The following image shows the taxonomy of popular visual language tasks:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/vl-tasks.png\" alt=\"\"></p>\n<ul>\n  <li>Depending on the task at hand, different architectures have been proposed over the years. In this article, we will explore some of the most popular ones.</li>\n</ul>",
    "contentMarkdown": "*   Visual Retrieval (VR) retrieves images based only on a textual description.\n    \n*   Vision-Language Navigation (VLN) is the task of an agent navigating through a space based on textual instructions.\n    \n*   Multimodal Machine Translation (MMT) involves translating a description from one language to another with additional visual information.\n    \n*   The following image shows the taxonomy of popular visual language tasks:\n    \n\nVisual Retrieval (VR) retrieves images based only on a textual description.\n\nVision-Language Navigation (VLN) is the task of an agent navigating through a space based on textual instructions.\n\nMultimodal Machine Translation (MMT) involves translating a description from one language to another with additional visual information.\n\nThe following image shows the taxonomy of popular visual language tasks:\n\n![](/primers/ai/assets/vision-language-models/vl-tasks.png)\n\n*   Depending on the task at hand, different architectures have been proposed over the years. In this article, we will explore some of the most popular ones.",
    "contentLength": 1201,
    "wordCount": 144,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#retrieval-tasks"
  },
  {
    "id": "ai-vision-language-models-two-stream-models-vilbert-4",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "BERT-like Architectures",
    "title": "Two-stream Models: ViLBERT",
    "order": 4,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Two-stream model is a literature term that refers to VL models which process text and images using two separate modules. <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> and <a href=\"https://arxiv.org/abs/1908.07490\">LXMERT</a> fall into this category.</p>\n  </li>\n  <li>\n    <p><a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> 4 is trained on image-text pairs. The text is encoded with the standard transformer process using tokenization and positional embeddings. It is then processed by the self-attention modules of the transformer. Images are decomposed into non-overlapping patches projected in a vector, as in vision transformer’s patch embeddings.</p>\n  </li>\n  <li>\n    <p>To learn a joint representation of images and text, a “co-attention” module is used. The “co-attention” module calculates importance scores based on both images and text embeddings.</p>\n  </li>\n  <li>\n    <p>The following diagram compares standard self-attention vs. <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a>’s proposed co-attention:</p>\n  </li>\n</ul>\n<p>Two-stream model is a literature term that refers to VL models which process text and images using two separate modules. <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> and <a href=\"https://arxiv.org/abs/1908.07490\">LXMERT</a> fall into this category.</p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> 4 is trained on image-text pairs. The text is encoded with the standard transformer process using tokenization and positional embeddings. It is then processed by the self-attention modules of the transformer. Images are decomposed into non-overlapping patches projected in a vector, as in vision transformer’s patch embeddings.</p>\n<p>To learn a joint representation of images and text, a “co-attention” module is used. The “co-attention” module calculates importance scores based on both images and text embeddings.</p>\n<p>The following diagram compares standard self-attention vs. <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a>’s proposed co-attention:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/co-attention.png\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>In a way, the model is learning the alignment between words and image regions. Another transformer module is added on top for refinement. This “co-attention”/transformer block can, of course, be repeated many times.</p>\n  </li>\n  <li>\n    <p>The following diagram shows that <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> processes images and text in two parallel streams that interact through co-attention:</p>\n  </li>\n</ul>\n<p>In a way, the model is learning the alignment between words and image regions. Another transformer module is added on top for refinement. This “co-attention”/transformer block can, of course, be repeated many times.</p>\n<p>The following diagram shows that <a href=\"https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\">ViLBERT</a> processes images and text in two parallel streams that interact through co-attention:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/vil-bert.png\" alt=\"\"></p>\n<ul>\n  <li>The two sides of the model are initialized separately. Regarding the text stream (purple), the weights are set by pretraining the model on a standard text corpus, while for the image stream (green), <a href=\"https://paperswithcode.com/method/faster-r-cnn\">Faster R-CNN</a> is used. The entire model is trained on a dataset of image-text pairs with the end objective being to understand the relationship between text and images. The pretrained model can then be fine-tuned to a variety of downstream VL tasks.</li>\n</ul>",
    "contentMarkdown": "*   Two-stream model is a literature term that refers to VL models which process text and images using two separate modules. [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) and [LXMERT](https://arxiv.org/abs/1908.07490) fall into this category.\n    \n*   [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) 4 is trained on image-text pairs. The text is encoded with the standard transformer process using tokenization and positional embeddings. It is then processed by the self-attention modules of the transformer. Images are decomposed into non-overlapping patches projected in a vector, as in vision transformer’s patch embeddings.\n    \n*   To learn a joint representation of images and text, a “co-attention” module is used. The “co-attention” module calculates importance scores based on both images and text embeddings.\n    \n*   The following diagram compares standard self-attention vs. [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)’s proposed co-attention:\n    \n\nTwo-stream model is a literature term that refers to VL models which process text and images using two separate modules. [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) and [LXMERT](https://arxiv.org/abs/1908.07490) fall into this category.\n\n[ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) 4 is trained on image-text pairs. The text is encoded with the standard transformer process using tokenization and positional embeddings. It is then processed by the self-attention modules of the transformer. Images are decomposed into non-overlapping patches projected in a vector, as in vision transformer’s patch embeddings.\n\nTo learn a joint representation of images and text, a “co-attention” module is used. The “co-attention” module calculates importance scores based on both images and text embeddings.\n\nThe following diagram compares standard self-attention vs. [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)’s proposed co-attention:\n\n![](/primers/ai/assets/vision-language-models/co-attention.png)\n\n*   In a way, the model is learning the alignment between words and image regions. Another transformer module is added on top for refinement. This “co-attention”/transformer block can, of course, be repeated many times.\n    \n*   The following diagram shows that [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) processes images and text in two parallel streams that interact through co-attention:\n    \n\nIn a way, the model is learning the alignment between words and image regions. Another transformer module is added on top for refinement. This “co-attention”/transformer block can, of course, be repeated many times.\n\nThe following diagram shows that [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) processes images and text in two parallel streams that interact through co-attention:\n\n![](/primers/ai/assets/vision-language-models/vil-bert.png)\n\n*   The two sides of the model are initialized separately. Regarding the text stream (purple), the weights are set by pretraining the model on a standard text corpus, while for the image stream (green), [Faster R-CNN](https://paperswithcode.com/method/faster-r-cnn) is used. The entire model is trained on a dataset of image-text pairs with the end objective being to understand the relationship between text and images. The pretrained model can then be fine-tuned to a variety of downstream VL tasks.",
    "contentLength": 4078,
    "wordCount": 410,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#two-stream-models:-vilbert"
  },
  {
    "id": "ai-vision-language-models-single-stream-models-5",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "BERT-like Architectures",
    "title": "Single-stream Models",
    "order": 5,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>In contrast, models such as <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a>, <a href=\"https://arxiv.org/abs/1908.08530\">VL-BERT</a>, <a href=\"https://arxiv.org/abs/1909.11740\">UNITER</a> encode both modalities within the same module. For example, <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> combines image regions and language with a transformer in order for self-attention to discover alignments between them. In essence, they added a visual embedding to the standard BERT architecture. The visual embedding consists of:</p>\n\n    <ol>\n      <li>A visual feature representation of the region produced by a CNN.</li>\n      <li>A segment embedding that distinguishes image from text embeddings.</li>\n      <li>A positional embedding to align regions with words if provided in the input.</li>\n    </ol>\n  </li>\n  <li>\n    <p>The following diagram shows <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> which combines image regions and text with a transformer module:</p>\n  </li>\n</ul>\n<p>In contrast, models such as <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a>, <a href=\"https://arxiv.org/abs/1908.08530\">VL-BERT</a>, <a href=\"https://arxiv.org/abs/1909.11740\">UNITER</a> encode both modalities within the same module. For example, <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> combines image regions and language with a transformer in order for self-attention to discover alignments between them. In essence, they added a visual embedding to the standard BERT architecture. The visual embedding consists of:</p>\n<ol>\n      <li>A visual feature representation of the region produced by a CNN.</li>\n      <li>A segment embedding that distinguishes image from text embeddings.</li>\n      <li>A positional embedding to align regions with words if provided in the input.</li>\n    </ol>\n<p>The following diagram shows <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> which combines image regions and text with a transformer module:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/visual-bert.png\" alt=\"\"></p>",
    "contentMarkdown": "*   In contrast, models such as [VisualBERT](https://arxiv.org/abs/1908.03557), [VL-BERT](https://arxiv.org/abs/1908.08530), [UNITER](https://arxiv.org/abs/1909.11740) encode both modalities within the same module. For example, [VisualBERT](https://arxiv.org/abs/1908.03557) combines image regions and language with a transformer in order for self-attention to discover alignments between them. In essence, they added a visual embedding to the standard BERT architecture. The visual embedding consists of:\n    \n    1.  A visual feature representation of the region produced by a CNN.\n    2.  A segment embedding that distinguishes image from text embeddings.\n    3.  A positional embedding to align regions with words if provided in the input.\n*   The following diagram shows [VisualBERT](https://arxiv.org/abs/1908.03557) which combines image regions and text with a transformer module:\n    \n\nIn contrast, models such as [VisualBERT](https://arxiv.org/abs/1908.03557), [VL-BERT](https://arxiv.org/abs/1908.08530), [UNITER](https://arxiv.org/abs/1909.11740) encode both modalities within the same module. For example, [VisualBERT](https://arxiv.org/abs/1908.03557) combines image regions and language with a transformer in order for self-attention to discover alignments between them. In essence, they added a visual embedding to the standard BERT architecture. The visual embedding consists of:\n\n1.  A visual feature representation of the region produced by a CNN.\n2.  A segment embedding that distinguishes image from text embeddings.\n3.  A positional embedding to align regions with words if provided in the input.\n\nThe following diagram shows [VisualBERT](https://arxiv.org/abs/1908.03557) which combines image regions and text with a transformer module:\n\n![](/primers/ai/assets/vision-language-models/visual-bert.png)",
    "contentLength": 2097,
    "wordCount": 209,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#single-stream-models"
  },
  {
    "id": "ai-vision-language-models-pretraining-strategies-6",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Pretraining and Fine-tuning",
    "title": "Pretraining Strategies",
    "order": 6,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Let’s explore some common pretraining strategies:\n    <ul>\n      <li>Masked Language Modeling is often used when the transformer is trained only on text. Certain tokens of the input are being masked at random. The model is trained to simply predict the masked tokens (words). In the case of BERT, bidirectional training enables the model to use both previous and following tokens as context for prediction.</li>\n      <li>Next Sequence Prediction works again only with text as input and evaluates if a sentence is an appropriate continuation of the input sentence. By using both false and correct sentences as training data, the model is able to capture long-term dependencies.</li>\n      <li>Masked Region Modeling masks image regions in a similar way to masked language modeling. The model is then trained to predict the features of the masked region.</li>\n      <li>Image-Text Matching forces the model to predict if a sentence is appropriate for a specific image.</li>\n      <li>Word-Region Alignment finds correlations between image region and words.</li>\n      <li>Masked Region Classification predicts the object class for each masked region.</li>\n      <li>Masked Region Feature Regression learns to regress the masked image region to its visual features.</li>\n    </ul>\n  </li>\n  <li>\n    <p>For example, <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> is pretrained with the Masked Language Modeling and Image-text matching on an image-caption dataset.</p>\n  </li>\n  <li>\n    <p>The above methods create supervised learning objectives. Either the label is derived from the input, aka self-supervised or a labeled dataset (usually image-text pairs) is used. Are there any other attempts? Of course.</p>\n  </li>\n  <li>The following strategies are also used in VL modeling. They are often combined on various proposals:\n    <ul>\n      <li>Unsupervised VL Pretraining usually refers to pretraining without paired image-text data but rather with a single modality. During fine-tuning though, the model is fully-supervised.</li>\n      <li>Multi-task Learning is the concept of joint learning across multiple tasks in order to transfer the learnings from one task to another.</li>\n      <li>Contrastive Learning is used to learn visual-semantic embeddings in a self-supervised way. The main idea is to learn such an embedding space in which similar pairs stay close to each other while dissimilar ones are - far apart.</li>\n      <li>Zero-shot learning is the ability to generalize at inference time on samples from unseen classes.</li>\n    </ul>\n  </li>\n  <li>Let’s now proceed with some of the most popular architectures.</li>\n</ul>\n<ul>\n      <li>Masked Language Modeling is often used when the transformer is trained only on text. Certain tokens of the input are being masked at random. The model is trained to simply predict the masked tokens (words). In the case of BERT, bidirectional training enables the model to use both previous and following tokens as context for prediction.</li>\n      <li>Next Sequence Prediction works again only with text as input and evaluates if a sentence is an appropriate continuation of the input sentence. By using both false and correct sentences as training data, the model is able to capture long-term dependencies.</li>\n      <li>Masked Region Modeling masks image regions in a similar way to masked language modeling. The model is then trained to predict the features of the masked region.</li>\n      <li>Image-Text Matching forces the model to predict if a sentence is appropriate for a specific image.</li>\n      <li>Word-Region Alignment finds correlations between image region and words.</li>\n      <li>Masked Region Classification predicts the object class for each masked region.</li>\n      <li>Masked Region Feature Regression learns to regress the masked image region to its visual features.</li>\n    </ul>\n<p>For example, <a href=\"https://arxiv.org/abs/1908.03557\">VisualBERT</a> is pretrained with the Masked Language Modeling and Image-text matching on an image-caption dataset.</p>\n<p>The above methods create supervised learning objectives. Either the label is derived from the input, aka self-supervised or a labeled dataset (usually image-text pairs) is used. Are there any other attempts? Of course.</p>\n<ul>\n      <li>Unsupervised VL Pretraining usually refers to pretraining without paired image-text data but rather with a single modality. During fine-tuning though, the model is fully-supervised.</li>\n      <li>Multi-task Learning is the concept of joint learning across multiple tasks in order to transfer the learnings from one task to another.</li>\n      <li>Contrastive Learning is used to learn visual-semantic embeddings in a self-supervised way. The main idea is to learn such an embedding space in which similar pairs stay close to each other while dissimilar ones are - far apart.</li>\n      <li>Zero-shot learning is the ability to generalize at inference time on samples from unseen classes.</li>\n    </ul>",
    "contentMarkdown": "*   Let’s explore some common pretraining strategies:\n    *   Masked Language Modeling is often used when the transformer is trained only on text. Certain tokens of the input are being masked at random. The model is trained to simply predict the masked tokens (words). In the case of BERT, bidirectional training enables the model to use both previous and following tokens as context for prediction.\n    *   Next Sequence Prediction works again only with text as input and evaluates if a sentence is an appropriate continuation of the input sentence. By using both false and correct sentences as training data, the model is able to capture long-term dependencies.\n    *   Masked Region Modeling masks image regions in a similar way to masked language modeling. The model is then trained to predict the features of the masked region.\n    *   Image-Text Matching forces the model to predict if a sentence is appropriate for a specific image.\n    *   Word-Region Alignment finds correlations between image region and words.\n    *   Masked Region Classification predicts the object class for each masked region.\n    *   Masked Region Feature Regression learns to regress the masked image region to its visual features.\n*   For example, [VisualBERT](https://arxiv.org/abs/1908.03557) is pretrained with the Masked Language Modeling and Image-text matching on an image-caption dataset.\n    \n*   The above methods create supervised learning objectives. Either the label is derived from the input, aka self-supervised or a labeled dataset (usually image-text pairs) is used. Are there any other attempts? Of course.\n    \n*   The following strategies are also used in VL modeling. They are often combined on various proposals:\n    *   Unsupervised VL Pretraining usually refers to pretraining without paired image-text data but rather with a single modality. During fine-tuning though, the model is fully-supervised.\n    *   Multi-task Learning is the concept of joint learning across multiple tasks in order to transfer the learnings from one task to another.\n    *   Contrastive Learning is used to learn visual-semantic embeddings in a self-supervised way. The main idea is to learn such an embedding space in which similar pairs stay close to each other while dissimilar ones are - far apart.\n    *   Zero-shot learning is the ability to generalize at inference time on samples from unseen classes.\n*   Let’s now proceed with some of the most popular architectures.\n\n*   Masked Language Modeling is often used when the transformer is trained only on text. Certain tokens of the input are being masked at random. The model is trained to simply predict the masked tokens (words). In the case of BERT, bidirectional training enables the model to use both previous and following tokens as context for prediction.\n*   Next Sequence Prediction works again only with text as input and evaluates if a sentence is an appropriate continuation of the input sentence. By using both false and correct sentences as training data, the model is able to capture long-term dependencies.\n*   Masked Region Modeling masks image regions in a similar way to masked language modeling. The model is then trained to predict the features of the masked region.\n*   Image-Text Matching forces the model to predict if a sentence is appropriate for a specific image.\n*   Word-Region Alignment finds correlations between image region and words.\n*   Masked Region Classification predicts the object class for each masked region.\n*   Masked Region Feature Regression learns to regress the masked image region to its visual features.\n\nFor example, [VisualBERT](https://arxiv.org/abs/1908.03557) is pretrained with the Masked Language Modeling and Image-text matching on an image-caption dataset.\n\nThe above methods create supervised learning objectives. Either the label is derived from the input, aka self-supervised or a labeled dataset (usually image-text pairs) is used. Are there any other attempts? Of course.\n\n*   Unsupervised VL Pretraining usually refers to pretraining without paired image-text data but rather with a single modality. During fine-tuning though, the model is fully-supervised.\n*   Multi-task Learning is the concept of joint learning across multiple tasks in order to transfer the learnings from one task to another.\n*   Contrastive Learning is used to learn visual-semantic embeddings in a self-supervised way. The main idea is to learn such an embedding space in which similar pairs stay close to each other while dissimilar ones are - far apart.\n*   Zero-shot learning is the ability to generalize at inference time on samples from unseen classes.",
    "contentLength": 5007,
    "wordCount": 707,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/vision-language-models/#pretraining-strategies"
  },
  {
    "id": "ai-vision-language-models-dall-e-7",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "VL Generative Models",
    "title": "DALL-E",
    "order": 7,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/pdf/2102.12092.pdf\">DALL-E</a> tackles the visual generation (VG) problem by being able to generate accurate images from a text description. The architecture is again trained with a text-images pair dataset.</p>\n  </li>\n  <li>\n    <p>DALL-E uses a discrete variational autoencoder (<a href=\"https://arxiv.org/abs/1609.02200\">dVAE</a>) to map the images to image tokens. dVAE essentially uses a discrete latent space compared to a typical VAE. The text is tokenized with <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\">byte-pair encoding</a>. The image and text tokens are concatenated and processed as a single data stream.</p>\n  </li>\n  <li>\n    <p>The following diagram shows the training pipeline of DALL-E mini, which is slightly different from the original DALL-e:</p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/pdf/2102.12092.pdf\">DALL-E</a> tackles the visual generation (VG) problem by being able to generate accurate images from a text description. The architecture is again trained with a text-images pair dataset.</p>\n<p>DALL-E uses a discrete variational autoencoder (<a href=\"https://arxiv.org/abs/1609.02200\">dVAE</a>) to map the images to image tokens. dVAE essentially uses a discrete latent space compared to a typical VAE. The text is tokenized with <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\">byte-pair encoding</a>. The image and text tokens are concatenated and processed as a single data stream.</p>\n<p>The following diagram shows the training pipeline of DALL-E mini, which is slightly different from the original DALL-e:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/Training-pipeline-of-DALLE-mini.png\" alt=\"\"></p>\n<ul>\n  <li>DALL-E uses an autoregressive transformer to process the stream in order to model the joint distribution of text and images. In the transformer’s decoder, each image can attend to all text tokens. At inference time, we concatenate the tokenized target caption with a sample from the dVAE, and pass the data stream to the autoregressive decoder, which will output a novel token image.</li>\n</ul>\n<blockquote>\n  <p>DALL-E generates realistic images based on a textual description and provides some exceptional results (although admittedly a little cartoonized) as you can see in the image below (source: <a href=\"https://openai.com/blog/dall-e/\">DALL·E: Creating Images from Text</a>):</p>\n</blockquote>\n<p>DALL-E generates realistic images based on a textual description and provides some exceptional results (although admittedly a little cartoonized) as you can see in the image below (source: <a href=\"https://openai.com/blog/dall-e/\">DALL·E: Creating Images from Text</a>):</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/dall-e.png\" alt=\"\"></p>",
    "contentMarkdown": "*   [DALL-E](https://arxiv.org/pdf/2102.12092.pdf) tackles the visual generation (VG) problem by being able to generate accurate images from a text description. The architecture is again trained with a text-images pair dataset.\n    \n*   DALL-E uses a discrete variational autoencoder ([dVAE](https://arxiv.org/abs/1609.02200)) to map the images to image tokens. dVAE essentially uses a discrete latent space compared to a typical VAE. The text is tokenized with [byte-pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). The image and text tokens are concatenated and processed as a single data stream.\n    \n*   The following diagram shows the training pipeline of DALL-E mini, which is slightly different from the original DALL-e:\n    \n\n[DALL-E](https://arxiv.org/pdf/2102.12092.pdf) tackles the visual generation (VG) problem by being able to generate accurate images from a text description. The architecture is again trained with a text-images pair dataset.\n\nDALL-E uses a discrete variational autoencoder ([dVAE](https://arxiv.org/abs/1609.02200)) to map the images to image tokens. dVAE essentially uses a discrete latent space compared to a typical VAE. The text is tokenized with [byte-pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). The image and text tokens are concatenated and processed as a single data stream.\n\nThe following diagram shows the training pipeline of DALL-E mini, which is slightly different from the original DALL-e:\n\n![](/primers/ai/assets/vision-language-models/Training-pipeline-of-DALLE-mini.png)\n\n*   DALL-E uses an autoregressive transformer to process the stream in order to model the joint distribution of text and images. In the transformer’s decoder, each image can attend to all text tokens. At inference time, we concatenate the tokenized target caption with a sample from the dVAE, and pass the data stream to the autoregressive decoder, which will output a novel token image.\n\n> DALL-E generates realistic images based on a textual description and provides some exceptional results (although admittedly a little cartoonized) as you can see in the image below (source: [DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)):\n\nDALL-E generates realistic images based on a textual description and provides some exceptional results (although admittedly a little cartoonized) as you can see in the image below (source: [DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)):\n\n![](/primers/ai/assets/vision-language-models/dall-e.png)",
    "contentLength": 2795,
    "wordCount": 322,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#dall-e"
  },
  {
    "id": "ai-vision-language-models-glide-8",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "VL Generative Models",
    "title": "GLIDE",
    "order": 8,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Following the work of DALL-E, <a href=\"https://arxiv.org/abs/2112.10741\">GLIDE</a> is another generative model that seems to outperform previous efforts. GLIDE is essentially a <a href=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\">diffusion model</a>.</li>\n  <li>Diffusion models consists of multiple diffusion steps that slowly add random noise to the data. Then, they aim to learn to reverse the diffusion process to construct samples from the data distribution from noise. The following image from <a href=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\">Lilian Weng’s blog</a> illustrates this idea:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/vision-language-models/Overview-of-diffusion-models.png\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>Diffusion models, in a nutshell, work by slowly injecting random noise to the data in a sequential fashion (formulated as a Markov chain). They then learn to reverse the process in order to construct novel data from the noise. So instead of sampling from the original unknown data distribution, they can sample from a known data distribution produced after a series of diffusion steps. In fact, it can be proved that if we add gaussian noise, the end (limit) distribution will be a typical normal distribution.</p>\n  </li>\n  <li>\n    <p>The diffusion model receives input as images and can output novel ones. But it can also be conditioned on textual information so that the generated image will be appropriate for specific text inputs. And that’s exactly what GLIDE does. It experiments with a variety of methods to “guide” the diffusion models.</p>\n  </li>\n  <li>\n    <p>Mathematically, the diffusion process can be formulated as follows. If we take a sample <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mn\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">x_{0}</script> from a data distribution <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi><mrow><mo>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn></mrow></msub><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-8\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mrow\" id=\"MathJax-Span-11\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-13\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-15\"><span class=\"mrow\" id=\"MathJax-Span-16\"><span class=\"mn\" id=\"MathJax-Span-17\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi><mrow><mo>(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn></mrow></msub><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">q\\left(x_{0}\\right)</script>, we can produce a Markov chain of latent variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 4.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1003.54em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"msubsup\" id=\"MathJax-Span-21\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-23\"><span class=\"mrow\" id=\"MathJax-Span-24\"><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"msubsup\" id=\"MathJax-Span-28\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">x_{1}, \\ldots x_{T}</script> by progressively adding Gaussian noise of magnitude <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.4em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-37\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>−</mo><msub><mi>a</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">1-a_{t}</script>:</p>\n  </li>\n</ul>\n<p>Diffusion models, in a nutshell, work by slowly injecting random noise to the data in a sequential fashion (formulated as a Markov chain). They then learn to reverse the process in order to construct novel data from the noise. So instead of sampling from the original unknown data distribution, they can sample from a known data distribution produced after a series of diffusion steps. In fact, it can be proved that if we add gaussian noise, the end (limit) distribution will be a typical normal distribution.</p>\n<p>The diffusion model receives input as images and can output novel ones. But it can also be conditioned on textual information so that the generated image will be appropriate for specific text inputs. And that’s exactly what GLIDE does. It experiments with a variety of methods to “guide” the diffusion models.</p>\n<p>Mathematically, the diffusion process can be formulated as follows. If we take a sample <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mn\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">x_{0}</script> from a data distribution <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi><mrow><mo>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn></mrow></msub><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-8\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mrow\" id=\"MathJax-Span-11\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-13\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-15\"><span class=\"mrow\" id=\"MathJax-Span-16\"><span class=\"mn\" id=\"MathJax-Span-17\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi><mrow><mo>(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn></mrow></msub><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">q\\left(x_{0}\\right)</script>, we can produce a Markov chain of latent variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 4.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1003.54em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"msubsup\" id=\"MathJax-Span-21\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-23\"><span class=\"mrow\" id=\"MathJax-Span-24\"><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"msubsup\" id=\"MathJax-Span-28\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">x_{1}, \\ldots x_{T}</script> by progressively adding Gaussian noise of magnitude <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.4em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-37\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>−</mo><msub><mi>a</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">1-a_{t}</script>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mi>t</mi><mo>&amp;#x2223;</mo><mi>x</mi><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo><mo>:=</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mi>t</mi><mo>;</mo><msqrt><mi>a</mi><mi>t</mi></msqrt><mi>x</mi><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo>,</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>a</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mi>I</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 20.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1017.14em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">=</span></span><span class=\"mi\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"msqrt\" id=\"MathJax-Span-60\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0.732em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1000.84em, 3.388em, -999.997em); top: -3.904em; left: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.315em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1000.78em, 4.169em, -999.997em); top: -3.799em; left: 0em;\"><span style=\"font-family: STIXVariants;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">(</span><span class=\"mn\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">a</span><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>q</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi>t</mi><mo>∣</mo><mi>x</mi><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>:=</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi>t</mi><mo>;</mo><msqrt><mi>a</mi><mi>t</mi></msqrt><mi>x</mi><mi>t</mi><mo>−</mo><mn>1</mn><mo>,</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>a</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mi>I</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>That way, we can well-define the posterior <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi><mrow><mo>(</mo><mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-77\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1004.69em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mrow\" id=\"MathJax-Span-80\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-81\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-82\"><span class=\"msubsup\" id=\"MathJax-Span-83\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-85\"><span class=\"mrow\" id=\"MathJax-Span-86\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-89\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-91\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-93\"><span class=\"mrow\" id=\"MathJax-Span-94\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-96\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi><mrow><mo>(</mo><mrow><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi></mrow></msub></mrow><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">q\\left(x_{t-1} \\mid x_{t}\\right)</script> and approximate it using a model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mi>&amp;#x03B8;</mi><mrow><mo>(</mo><mrow><mi>x</mi><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-97\" style=\"width: 7.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.89em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-98\"><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mrow\" id=\"MathJax-Span-101\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-102\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"mi\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-109\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-111\"><span class=\"mrow\" id=\"MathJax-Span-112\"><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mi>θ</mi><mrow><mo>(</mo><mrow><mi>x</mi><mi>t</mi><mo>−</mo><mn>1</mn><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi></mrow></msub></mrow><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">p \\theta\\left(x t-1 \\mid x_{t}\\right)</script>.</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo>&amp;#x2223;</mo><mi>x</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>:=</mo><mi>N</mi><mrow><mo>(</mo><mrow><mi>&amp;#x03BC;</mi><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><msub><mi mathvariant=&quot;normal&quot;>&amp;#x03A3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 17.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1014.38em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">=</span></span><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mrow\" id=\"MathJax-Span-130\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-131\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-140\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">Σ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-142\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi>t</mi><mo>−</mo><mn>1</mn><mo>∣</mo><mi>x</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mo>:=</mo><mi>N</mi><mrow><mo>(</mo><mrow><mi>μ</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mo>,</mo><msub><mi mathvariant=\"normal\">Σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></math></span></span></div>\n<ul>\n  <li>\n    <p>To better understand diffusion models, I highly recommend this excellent <a href=\"https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html\">article by Lilian Weng</a>.</p>\n  </li>\n  <li>\n    <p>GLIDE results are even more impressive and more realistic than DALLE. However, as the authors themselves admit, there have been quite a few failure cases for specific unusual objects or scenarios. Note that you can try it yourself using <a href=\"https://huggingface.co/spaces/valhalla/glide-text2im\">hugging face spaces</a>. The following image shows examples of generated images by GLIDE:</p>\n  </li>\n</ul>\n<p>To better understand diffusion models, I highly recommend this excellent <a href=\"https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html\">article by Lilian Weng</a>.</p>\n<p>GLIDE results are even more impressive and more realistic than DALLE. However, as the authors themselves admit, there have been quite a few failure cases for specific unusual objects or scenarios. Note that you can try it yourself using <a href=\"https://huggingface.co/spaces/valhalla/glide-text2im\">hugging face spaces</a>. The following image shows examples of generated images by GLIDE:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/glide.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Following the work of DALL-E, [GLIDE](https://arxiv.org/abs/2112.10741) is another generative model that seems to outperform previous efforts. GLIDE is essentially a [diffusion model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).\n*   Diffusion models consists of multiple diffusion steps that slowly add random noise to the data. Then, they aim to learn to reverse the diffusion process to construct samples from the data distribution from noise. The following image from [Lilian Weng’s blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) illustrates this idea:\n\n![](/primers/ai/assets/vision-language-models/Overview-of-diffusion-models.png)\n\n*   Diffusion models, in a nutshell, work by slowly injecting random noise to the data in a sequential fashion (formulated as a Markov chain). They then learn to reverse the process in order to construct novel data from the noise. So instead of sampling from the original unknown data distribution, they can sample from a known data distribution produced after a series of diffusion steps. In fact, it can be proved that if we add gaussian noise, the end (limit) distribution will be a typical normal distribution.\n    \n*   The diffusion model receives input as images and can output novel ones. But it can also be conditioned on textual information so that the generated image will be appropriate for specific text inputs. And that’s exactly what GLIDE does. It experiments with a variety of methods to “guide” the diffusion models.\n    \n*   Mathematically, the diffusion process can be formulated as follows. If we take a sample x0x0x\\_{0} from a data distribution q(x0)q(x0)q\\\\left(x\\_{0}\\\\right), we can produce a Markov chain of latent variables x1,…xTx1,…xTx\\_{1}, \\\\ldots x\\_{T} by progressively adding Gaussian noise of magnitude 1−at1−at1-a\\_{t}:\n    \n\nDiffusion models, in a nutshell, work by slowly injecting random noise to the data in a sequential fashion (formulated as a Markov chain). They then learn to reverse the process in order to construct novel data from the noise. So instead of sampling from the original unknown data distribution, they can sample from a known data distribution produced after a series of diffusion steps. In fact, it can be proved that if we add gaussian noise, the end (limit) distribution will be a typical normal distribution.\n\nThe diffusion model receives input as images and can output novel ones. But it can also be conditioned on textual information so that the generated image will be appropriate for specific text inputs. And that’s exactly what GLIDE does. It experiments with a variety of methods to “guide” the diffusion models.\n\nMathematically, the diffusion process can be formulated as follows. If we take a sample x0x0x\\_{0} from a data distribution q(x0)q(x0)q\\\\left(x\\_{0}\\\\right), we can produce a Markov chain of latent variables x1,…xTx1,…xTx\\_{1}, \\\\ldots x\\_{T} by progressively adding Gaussian noise of magnitude 1−at1−at1-a\\_{t}:\n\nq(xt∣xt−1):\\=N(xt;at‾‾√xt−1,(1−at)I)q(xt∣xt−1):=N(xt;atxt−1,(1−at)I)\n\n*   That way, we can well-define the posterior q(xt−1∣xt)q(xt−1∣xt)q\\\\left(x\\_{t-1} \\\\mid x\\_{t}\\\\right) and approximate it using a model pθ(xt−1∣xt)pθ(xt−1∣xt)p \\\\theta\\\\left(x t-1 \\\\mid x\\_{t}\\\\right).\n\npθ(xt−1∣xt):\\=N(μθ(xt),Σθ(xt))pθ(xt−1∣xt):=N(μθ(xt),Σθ(xt))\n\n*   To better understand diffusion models, I highly recommend this excellent [article by Lilian Weng](https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html).\n    \n*   GLIDE results are even more impressive and more realistic than DALLE. However, as the authors themselves admit, there have been quite a few failure cases for specific unusual objects or scenarios. Note that you can try it yourself using [hugging face spaces](https://huggingface.co/spaces/valhalla/glide-text2im). The following image shows examples of generated images by GLIDE:\n    \n\nTo better understand diffusion models, I highly recommend this excellent [article by Lilian Weng](https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html).\n\nGLIDE results are even more impressive and more realistic than DALLE. However, as the authors themselves admit, there have been quite a few failure cases for specific unusual objects or scenarios. Note that you can try it yourself using [hugging face spaces](https://huggingface.co/spaces/valhalla/glide-text2im). The following image shows examples of generated images by GLIDE:\n\n![](/primers/ai/assets/vision-language-models/glide.png)",
    "contentLength": 47750,
    "wordCount": 585,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#glide"
  },
  {
    "id": "ai-vision-language-models-clip-9",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "VL Models Based on Contrastive Learning",
    "title": "CLIP",
    "order": 9,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\">CLIP</a> targets the Natural Language for Visual Reasoning (NLVR) problem as it tries to classify an image to a specific label based on its context. The label is usually a phrase or a sentence describing the image. More interestingly, it’s a zero-shot classifier in terms that it can be used to previously unseen labels.</p>\n  </li>\n  <li>\n    <p>Its admittedly impressive zero-shot performance is heavily affected by the fact that it is trained on a highly-diversified, huge (400 million) dataset. The training data consist of images and their corresponding textual descriptions. The images are encoded by either a ResNet or a transformer, while a transformer module is also used for text.</p>\n  </li>\n  <li>\n    <p>The training’s objective is to “connect” image representations with text representations. In a few words, the model tries to discover which text vector is more “appropriate” for a given image vector. This is why it’s referred to as contrastive learning.</p>\n  </li>\n  <li>\n    <p>For those familiar with purely vision-based contrastive learning, here instead of bringing together views of the same image, we are pulling together the positive image and text “views”, while pulling apart texts that do not correspond to the correct image (negatives). So even though it’s contrastive training it’s 100% supervised, meaning that labeled pairs are required.</p>\n  </li>\n  <li>\n    <p>By training the model to assign high similarity for fitting image-text pairs and low similarity for unfitting ones, the model can be used in a variety of downstream tasks such as image recognition.</p>\n  </li>\n</ul>\n<p><a href=\"https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\">CLIP</a> targets the Natural Language for Visual Reasoning (NLVR) problem as it tries to classify an image to a specific label based on its context. The label is usually a phrase or a sentence describing the image. More interestingly, it’s a zero-shot classifier in terms that it can be used to previously unseen labels.</p>\n<p>Its admittedly impressive zero-shot performance is heavily affected by the fact that it is trained on a highly-diversified, huge (400 million) dataset. The training data consist of images and their corresponding textual descriptions. The images are encoded by either a ResNet or a transformer, while a transformer module is also used for text.</p>\n<p>The training’s objective is to “connect” image representations with text representations. In a few words, the model tries to discover which text vector is more “appropriate” for a given image vector. This is why it’s referred to as contrastive learning.</p>\n<p>For those familiar with purely vision-based contrastive learning, here instead of bringing together views of the same image, we are pulling together the positive image and text “views”, while pulling apart texts that do not correspond to the correct image (negatives). So even though it’s contrastive training it’s 100% supervised, meaning that labeled pairs are required.</p>\n<p>By training the model to assign high similarity for fitting image-text pairs and low similarity for unfitting ones, the model can be used in a variety of downstream tasks such as image recognition.</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/clip.png\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>In CLIP, the image encoder and the text encoder are trained jointly in a contrastive fashion 14</p>\n  </li>\n  <li>\n    <p>Borrowed from the original paper, you can find a pseudocode implementation below:</p>\n  </li>\n</ul>\n<p>In CLIP, the image encoder and the text encoder are trained jointly in a contrastive fashion 14</p>\n<p>Borrowed from the original paper, you can find a pseudocode implementation below:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"c1\"># image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n</span>\n<span class=\"c1\"># I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - learned temperature parameter\n</span>\n<span class=\"c1\"># extract feature representations of each modality\n</span><span class=\"n\">I_f</span> <span class=\"o\">=</span> <span class=\"n\">image_encoder</span><span class=\"p\">(</span><span class=\"n\">I</span><span class=\"p\">)</span> <span class=\"c1\">#[n, d_i]\n</span><span class=\"n\">T_f</span> <span class=\"o\">=</span> <span class=\"n\">text_encoder</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"c1\">#[n, d_t]\n</span>\n<span class=\"c1\"># joint multimodal embedding [n, d_e]\n</span><span class=\"n\">I_e</span> <span class=\"o\">=</span> <span class=\"n\">l2_normalize</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">I_f</span><span class=\"p\">,</span> <span class=\"n\">W_i</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">T_e</span> <span class=\"o\">=</span> <span class=\"n\">l2_normalize</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">T_f</span><span class=\"p\">,</span> <span class=\"n\">W_t</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># scaled pairwise cosine similarities [n, n]\n</span><span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">I_e</span><span class=\"p\">,</span> <span class=\"n\">T_e</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># symmetric loss function\n</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"n\">loss_i</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">loss_t</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">loss_i</span> <span class=\"o\">+</span> <span class=\"n\">loss_t</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mi\">2</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"c1\"># image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n</span>\n<span class=\"c1\"># I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - learned temperature parameter\n</span>\n<span class=\"c1\"># extract feature representations of each modality\n</span><span class=\"n\">I_f</span> <span class=\"o\">=</span> <span class=\"n\">image_encoder</span><span class=\"p\">(</span><span class=\"n\">I</span><span class=\"p\">)</span> <span class=\"c1\">#[n, d_i]\n</span><span class=\"n\">T_f</span> <span class=\"o\">=</span> <span class=\"n\">text_encoder</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"c1\">#[n, d_t]\n</span>\n<span class=\"c1\"># joint multimodal embedding [n, d_e]\n</span><span class=\"n\">I_e</span> <span class=\"o\">=</span> <span class=\"n\">l2_normalize</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">I_f</span><span class=\"p\">,</span> <span class=\"n\">W_i</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">T_e</span> <span class=\"o\">=</span> <span class=\"n\">l2_normalize</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">T_f</span><span class=\"p\">,</span> <span class=\"n\">W_t</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># scaled pairwise cosine similarities [n, n]\n</span><span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">I_e</span><span class=\"p\">,</span> <span class=\"n\">T_e</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># symmetric loss function\n</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"n\">loss_i</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">loss_t</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">loss_i</span> <span class=\"o\">+</span> <span class=\"n\">loss_t</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mi\">2</span>\n</code></pre>\n<blockquote>\n  <p>The results are again quite impressive, but limitations still exist. For example, CLIP seems to struggle with abstract concepts and has poor generalization to images not covered in its pre-training dataset. The following image (from <a href=\"https://openai.com/blog/clip/\">CLIP: Connecting Text and Images</a>) shows an example of caption prediction for an image using CLIP:</p>\n</blockquote>\n<p>The results are again quite impressive, but limitations still exist. For example, CLIP seems to struggle with abstract concepts and has poor generalization to images not covered in its pre-training dataset. The following image (from <a href=\"https://openai.com/blog/clip/\">CLIP: Connecting Text and Images</a>) shows an example of caption prediction for an image using CLIP:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/clip-results.png\" alt=\"\"></p>",
    "contentMarkdown": "*   [CLIP](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) targets the Natural Language for Visual Reasoning (NLVR) problem as it tries to classify an image to a specific label based on its context. The label is usually a phrase or a sentence describing the image. More interestingly, it’s a zero-shot classifier in terms that it can be used to previously unseen labels.\n    \n*   Its admittedly impressive zero-shot performance is heavily affected by the fact that it is trained on a highly-diversified, huge (400 million) dataset. The training data consist of images and their corresponding textual descriptions. The images are encoded by either a ResNet or a transformer, while a transformer module is also used for text.\n    \n*   The training’s objective is to “connect” image representations with text representations. In a few words, the model tries to discover which text vector is more “appropriate” for a given image vector. This is why it’s referred to as contrastive learning.\n    \n*   For those familiar with purely vision-based contrastive learning, here instead of bringing together views of the same image, we are pulling together the positive image and text “views”, while pulling apart texts that do not correspond to the correct image (negatives). So even though it’s contrastive training it’s 100% supervised, meaning that labeled pairs are required.\n    \n*   By training the model to assign high similarity for fitting image-text pairs and low similarity for unfitting ones, the model can be used in a variety of downstream tasks such as image recognition.\n    \n\n[CLIP](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) targets the Natural Language for Visual Reasoning (NLVR) problem as it tries to classify an image to a specific label based on its context. The label is usually a phrase or a sentence describing the image. More interestingly, it’s a zero-shot classifier in terms that it can be used to previously unseen labels.\n\nIts admittedly impressive zero-shot performance is heavily affected by the fact that it is trained on a highly-diversified, huge (400 million) dataset. The training data consist of images and their corresponding textual descriptions. The images are encoded by either a ResNet or a transformer, while a transformer module is also used for text.\n\nThe training’s objective is to “connect” image representations with text representations. In a few words, the model tries to discover which text vector is more “appropriate” for a given image vector. This is why it’s referred to as contrastive learning.\n\nFor those familiar with purely vision-based contrastive learning, here instead of bringing together views of the same image, we are pulling together the positive image and text “views”, while pulling apart texts that do not correspond to the correct image (negatives). So even though it’s contrastive training it’s 100% supervised, meaning that labeled pairs are required.\n\nBy training the model to assign high similarity for fitting image-text pairs and low similarity for unfitting ones, the model can be used in a variety of downstream tasks such as image recognition.\n\n![](/primers/ai/assets/vision-language-models/clip.png)\n\n*   In CLIP, the image encoder and the text encoder are trained jointly in a contrastive fashion 14\n    \n*   Borrowed from the original paper, you can find a pseudocode implementation below:\n    \n\nIn CLIP, the image encoder and the text encoder are trained jointly in a contrastive fashion 14\n\nBorrowed from the original paper, you can find a pseudocode implementation below:\n\n![](https://aman.ai/images/copy.png)\n\n`# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1)  # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t)  # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2`\n\n![](https://aman.ai/images/copy.png)\n\n`# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1)  # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t)  # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2`\n\n> The results are again quite impressive, but limitations still exist. For example, CLIP seems to struggle with abstract concepts and has poor generalization to images not covered in its pre-training dataset. The following image (from [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)) shows an example of caption prediction for an image using CLIP:\n\nThe results are again quite impressive, but limitations still exist. For example, CLIP seems to struggle with abstract concepts and has poor generalization to images not covered in its pre-training dataset. The following image (from [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)) shows an example of caption prediction for an image using CLIP:\n\n![](/primers/ai/assets/vision-language-models/clip-results.png)",
    "contentLength": 12245,
    "wordCount": 896,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#clip"
  },
  {
    "id": "ai-vision-language-models-align-10",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "VL Models Based on Contrastive Learning",
    "title": "ALIGN",
    "order": 10,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>In a very similar way, <a href=\"https://arxiv.org/abs/2102.05918\">ALIGN</a> utilizes a dual-encoder that learns to align visual and language representations of image-text pairs. The encoder is trained with a contrastive loss, which is formalized as a normalized softmax. In more detail, they authors use two loss terms, one for image-to-text classification and one for text-to-image classification.</p>\n  </li>\n  <li>\n    <p>Given $x i$ and $y j$ the normalized embedding of the image in the $i^{th}$ pair and that of text in the $j^{th}$ pair respectively, $N$ the batch size, and $\\sigma$ the temperature to scale the logits, the loss functions can be defined as:</p>\n  </li>\n</ul>\n<p>In a very similar way, <a href=\"https://arxiv.org/abs/2102.05918\">ALIGN</a> utilizes a dual-encoder that learns to align visual and language representations of image-text pairs. The encoder is trained with a contrastive loss, which is formalized as a normalized softmax. In more detail, they authors use two loss terms, one for image-to-text classification and one for text-to-image classification.</p>\n<p>Given $x i$ and $y j$ the normalized embedding of the image in the $i^{th}$ pair and that of text in the $j^{th}$ pair respectively, $N$ the batch size, and $\\sigma$ the temperature to scale the logits, the loss functions can be defined as:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable columnalign=&quot;right left right left right left right left right left right left&quot; rowspacing=&quot;3pt&quot; columnspacing=&quot;0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em&quot; displaystyle=&quot;true&quot;><mtr><mtd><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mn>2</mn><mi>t</mi></mrow></msub></mtd><mtd><mi></mi><mo>=</mo><mo>&amp;#x2212;</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mfrac><mrow><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x0131;</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>&amp;#x03C3;</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo><mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x0131;</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>&amp;#x03C3;</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mtd></mtr><mtr><mtd><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mn>2</mn><mi>i</mi></mrow></msub></mtd><mtd><mi></mi><mo>=</mo><mo>&amp;#x2212;</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mfrac><mrow><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>&amp;#x03C3;</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo><mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>&amp;#x03C3;</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mtd></mtr></mtable></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 19.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-1.56em, 1015.89em, 5.315em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"mtable\" id=\"MathJax-Span-152\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 15.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.659em, 1001.41em, 7.242em, -999.997em); top: -5.258em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.41em, 4.326em, -999.997em); top: -5.779em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-153\"><span class=\"mrow\" id=\"MathJax-Span-154\"><span class=\"msubsup\" id=\"MathJax-Span-155\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-157\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mn\" id=\"MathJax-Span-160\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.41em, 4.326em, -999.997em); top: -2.341em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-245\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"msubsup\" id=\"MathJax-Span-247\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-249\"><span class=\"mrow\" id=\"MathJax-Span-250\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mn\" id=\"MathJax-Span-252\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 5.263em;\"></span></span><span style=\"position: absolute; clip: rect(3.909em, 1014.33em, 10.784em, -999.997em); top: -7.602em; left: 1.409em;\"><span style=\"display: inline-block; position: relative; width: 14.326em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.19em, 1014.33em, 5.628em, -999.997em); top: -5.883em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-162\"><span class=\"mrow\" id=\"MathJax-Span-163\"><span class=\"mi\" id=\"MathJax-Span-164\"></span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-167\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-170\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-172\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-175\"><span class=\"mrow\" id=\"MathJax-Span-176\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-179\"></span><span class=\"mfrac\" id=\"MathJax-Span-180\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.971em, 1005.26em, 4.534em, -999.997em); top: -4.841em; left: 50%; margin-left: -2.706em;\"><span class=\"mrow\" id=\"MathJax-Span-181\"><span class=\"mo\" id=\"MathJax-Span-182\"></span><span class=\"mrow\" id=\"MathJax-Span-183\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-185\"></span><span class=\"mrow\" id=\"MathJax-Span-186\"><span class=\"mo\" id=\"MathJax-Span-187\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-188\"><span class=\"msubsup\" id=\"MathJax-Span-189\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-191\"><span class=\"mrow\" id=\"MathJax-Span-192\"><span class=\"mi\" id=\"MathJax-Span-193\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.26em, 4.169em, -999.997em); top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-194\"><span class=\"mrow\" id=\"MathJax-Span-195\"><span class=\"mi\" id=\"MathJax-Span-196\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ı</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-197\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-199\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mo\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-206\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-207\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1007.5em, 4.638em, -999.997em); top: -3.07em; left: 50%; margin-left: -3.852em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mo\" id=\"MathJax-Span-209\"></span><span class=\"mrow\" id=\"MathJax-Span-210\"><span class=\"munderover\" id=\"MathJax-Span-211\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-213\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.326em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-216\"><span class=\"mrow\" id=\"MathJax-Span-217\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-220\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-222\"></span><span class=\"mrow\" id=\"MathJax-Span-223\"><span class=\"mo\" id=\"MathJax-Span-224\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-225\"><span class=\"msubsup\" id=\"MathJax-Span-226\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-228\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mi\" id=\"MathJax-Span-230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.26em, 4.169em, -999.997em); top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-231\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mi\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ı</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-234\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-236\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-239\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mo\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.82em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.815em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span><span style=\"position: absolute; clip: rect(2.19em, 1014.27em, 5.628em, -999.997em); top: -2.445em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-254\"><span class=\"mrow\" id=\"MathJax-Span-255\"><span class=\"mi\" id=\"MathJax-Span-256\"></span><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-262\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-264\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-267\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-271\"></span><span class=\"mfrac\" id=\"MathJax-Span-272\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.971em, 1005.21em, 4.534em, -999.997em); top: -4.841em; left: 50%; margin-left: -2.654em;\"><span class=\"mrow\" id=\"MathJax-Span-273\"><span class=\"mo\" id=\"MathJax-Span-274\"></span><span class=\"mrow\" id=\"MathJax-Span-275\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-277\"></span><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mo\" id=\"MathJax-Span-279\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-280\"><span class=\"msubsup\" id=\"MathJax-Span-281\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-283\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-286\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-289\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-291\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-294\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mo\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1007.45em, 4.638em, -999.997em); top: -3.07em; left: 50%; margin-left: -3.799em;\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mo\" id=\"MathJax-Span-301\"></span><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"munderover\" id=\"MathJax-Span-303\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-305\"><span class=\"mrow\" id=\"MathJax-Span-306\"><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.326em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-308\"><span class=\"mrow\" id=\"MathJax-Span-309\"><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-312\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-314\"></span><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mo\" id=\"MathJax-Span-316\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-317\"><span class=\"msubsup\" id=\"MathJax-Span-318\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-320\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-323\"><span class=\"mrow\" id=\"MathJax-Span-324\"><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-326\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-328\"><span class=\"mrow\" id=\"MathJax-Span-329\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-331\"><span class=\"mrow\" id=\"MathJax-Span-332\"><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-335\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.76em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.763em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 7.607em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -3.684em; border-left: 0px solid; width: 0px; height: 8.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtable columnalign=\"right left right left right left right left right left right left\" rowspacing=\"3pt\" columnspacing=\"0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em\" displaystyle=\"true\"><mtr><mtd><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mn>2</mn><mi>t</mi></mrow></msub></mtd><mtd><mi></mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mfrac><mrow><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ı</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msubsup><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>σ</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo><mrow><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ı</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msubsup><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>σ</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mtd></mtr><mtr><mtd><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mn>2</mn><mi>i</mi></mrow></msub></mtd><mtd><mi></mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mfrac><mrow><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msubsup><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>σ</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo><mrow><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>T</mi></mrow></msubsup><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>σ</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mtd></mtr></mtable></math></span></span></div>\n<ul>\n  <li>It’s other main contribution is that the training is performed with a noisy dataset of one billion image-text pairs. So instead of doing expensive preprocessing on the data as similar methods do, they show that the scale of the dataset can compensate for the extra noise. The following image delineates In ALIGN, Visual and language representation are learned jointly with contrastive learning 15</li>\n</ul>\n<p><img src=\"/primers/ai/assets/vision-language-models/align.png\" alt=\"\"></p>",
    "contentMarkdown": "*   In a very similar way, [ALIGN](https://arxiv.org/abs/2102.05918) utilizes a dual-encoder that learns to align visual and language representations of image-text pairs. The encoder is trained with a contrastive loss, which is formalized as a normalized softmax. In more detail, they authors use two loss terms, one for image-to-text classification and one for text-to-image classification.\n    \n*   Given $x i$ and $y j$ the normalized embedding of the image in the $i^{th}$ pair and that of text in the $j^{th}$ pair respectively, $N$ the batch size, and $\\\\sigma$ the temperature to scale the logits, the loss functions can be defined as:\n    \n\nIn a very similar way, [ALIGN](https://arxiv.org/abs/2102.05918) utilizes a dual-encoder that learns to align visual and language representations of image-text pairs. The encoder is trained with a contrastive loss, which is formalized as a normalized softmax. In more detail, they authors use two loss terms, one for image-to-text classification and one for text-to-image classification.\n\nGiven $x i$ and $y j$ the normalized embedding of the image in the $i^{th}$ pair and that of text in the $j^{th}$ pair respectively, $N$ the batch size, and $\\\\sigma$ the temperature to scale the logits, the loss functions can be defined as:\n\nLi2tLt2i\\=−1N∑iNlogexp(xTıyi/σ))∑Nj\\=1exp(xTıyi/σ))\\=−1N∑iNlogexp(yTixi/σ))∑Nj\\=1exp(yTixi/σ))Li2t\\=−1N∑iNlog⁡exp⁡(xıTyi/σ))∑j\\=1Nexp⁡(xıTyi/σ))Lt2i\\=−1N∑iNlog⁡exp⁡(yiTxi/σ))∑j\\=1Nexp⁡(yiTxi/σ))\n\n*   It’s other main contribution is that the training is performed with a noisy dataset of one billion image-text pairs. So instead of doing expensive preprocessing on the data as similar methods do, they show that the scale of the dataset can compensate for the extra noise. The following image delineates In ALIGN, Visual and language representation are learned jointly with contrastive learning 15\n\n![](/primers/ai/assets/vision-language-models/align.png)",
    "contentLength": 38169,
    "wordCount": 262,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#align"
  },
  {
    "id": "ai-vision-language-models-florence-11",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "VL Models Based on Contrastive Learning",
    "title": "FLORENCE",
    "order": 11,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/pdf/2111.11432.pdf\">Florence</a> combines many of the aforementioned techniques to propose a new paradigm of end-to-end learning for VL tasks. The authors view Florence as a foundation model (following the terminology proposed by the Stanford team at <a href=\"https://arxiv.org/abs/2108.07258\">Bommasani et al</a>). Florence is the most recent architecture in this article and seems to perform SOTA results in many different tasks. Its main contributions include:</p>\n\n    <ul>\n      <li>\n        <p>For pretraining, they use a hierarchical vision transformer (<a href=\"https://arxiv.org/abs/2103.14030\">Swin</a>) as the image encoder and a modified CLIP as the language decoder.</p>\n      </li>\n      <li>\n        <p>The training is performed on “image-label-description” triplets.</p>\n      </li>\n      <li>\n        <p>They use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.</p>\n      </li>\n      <li>\n        <p>They enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>That way, the model can be applied into many distinct tasks and appears to have very good zero-shot and few-shot performance. The following diagram from the <a href=\"https://arxiv.org/pdf/2111.11432.pdf\">Florence</a> paper shows an illustration of the Florence architecture:</p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/pdf/2111.11432.pdf\">Florence</a> combines many of the aforementioned techniques to propose a new paradigm of end-to-end learning for VL tasks. The authors view Florence as a foundation model (following the terminology proposed by the Stanford team at <a href=\"https://arxiv.org/abs/2108.07258\">Bommasani et al</a>). Florence is the most recent architecture in this article and seems to perform SOTA results in many different tasks. Its main contributions include:</p>\n<ul>\n      <li>\n        <p>For pretraining, they use a hierarchical vision transformer (<a href=\"https://arxiv.org/abs/2103.14030\">Swin</a>) as the image encoder and a modified CLIP as the language decoder.</p>\n      </li>\n      <li>\n        <p>The training is performed on “image-label-description” triplets.</p>\n      </li>\n      <li>\n        <p>They use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.</p>\n      </li>\n      <li>\n        <p>They enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.</p>\n      </li>\n    </ul>\n<p>For pretraining, they use a hierarchical vision transformer (<a href=\"https://arxiv.org/abs/2103.14030\">Swin</a>) as the image encoder and a modified CLIP as the language decoder.</p>\n<p>The training is performed on “image-label-description” triplets.</p>\n<p>They use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.</p>\n<p>They enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.</p>\n<p>That way, the model can be applied into many distinct tasks and appears to have very good zero-shot and few-shot performance. The following diagram from the <a href=\"https://arxiv.org/pdf/2111.11432.pdf\">Florence</a> paper shows an illustration of the Florence architecture:</p>\n<p><img src=\"/primers/ai/assets/vision-language-models/florence.png\" alt=\"\"></p>",
    "contentMarkdown": "*   [Florence](https://arxiv.org/pdf/2111.11432.pdf) combines many of the aforementioned techniques to propose a new paradigm of end-to-end learning for VL tasks. The authors view Florence as a foundation model (following the terminology proposed by the Stanford team at [Bommasani et al](https://arxiv.org/abs/2108.07258)). Florence is the most recent architecture in this article and seems to perform SOTA results in many different tasks. Its main contributions include:\n    \n    *   For pretraining, they use a hierarchical vision transformer ([Swin](https://arxiv.org/abs/2103.14030)) as the image encoder and a modified CLIP as the language decoder.\n        \n    *   The training is performed on “image-label-description” triplets.\n        \n    *   They use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.\n        \n    *   They enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.\n        \n*   That way, the model can be applied into many distinct tasks and appears to have very good zero-shot and few-shot performance. The following diagram from the [Florence](https://arxiv.org/pdf/2111.11432.pdf) paper shows an illustration of the Florence architecture:\n    \n\n[Florence](https://arxiv.org/pdf/2111.11432.pdf) combines many of the aforementioned techniques to propose a new paradigm of end-to-end learning for VL tasks. The authors view Florence as a foundation model (following the terminology proposed by the Stanford team at [Bommasani et al](https://arxiv.org/abs/2108.07258)). Florence is the most recent architecture in this article and seems to perform SOTA results in many different tasks. Its main contributions include:\n\n*   For pretraining, they use a hierarchical vision transformer ([Swin](https://arxiv.org/abs/2103.14030)) as the image encoder and a modified CLIP as the language decoder.\n    \n*   The training is performed on “image-label-description” triplets.\n    \n*   They use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.\n    \n*   They enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.\n    \n\nFor pretraining, they use a hierarchical vision transformer ([Swin](https://arxiv.org/abs/2103.14030)) as the image encoder and a modified CLIP as the language decoder.\n\nThe training is performed on “image-label-description” triplets.\n\nThey use a unified image-text learning scheme, which can be seen as bidirectional contrastive learning. Without diving too deep, the loss contains two contrastive terms; an image-to-language contrastive loss and a language-to-image contrastive loss. In a way, they try to combine two common learning tasks: the mapping of images to the labels and the assignment of a description to a unique label.\n\nThey enhance the pretrained representations into more fine-grained representations with the use of “adapter” models. The fine-grained representations depend on the task: object-level representations, visual-language representations, video representations.\n\nThat way, the model can be applied into many distinct tasks and appears to have very good zero-shot and few-shot performance. The following diagram from the [Florence](https://arxiv.org/pdf/2111.11432.pdf) paper shows an illustration of the Florence architecture:\n\n![](/primers/ai/assets/vision-language-models/florence.png)",
    "contentLength": 4753,
    "wordCount": 557,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#florence"
  },
  {
    "id": "ai-vision-language-models-vinvl-12",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Enhanced Visual Representations",
    "title": "VinVL",
    "order": 12,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Towards that goal, the authors of <a href=\"https://arxiv.org/abs/2101.00529\">VinVL</a> pretrained a novel model on object detection using four public datasets. They then added an “attribute” branch and fine-tuned it, making it capable of detecting both objects and attributes.</li>\n</ul>\n<blockquote>\n  <p>An attribute is a small textual description related to the image.</p>\n</blockquote>\n<p>An attribute is a small textual description related to the image.</p>\n<ul>\n  <li>The resulted object-attribute detection model is a modification of the <a href=\"https://blog.paperspace.com/faster-r-cnn-explained-object-detection/\">Faster-RCNN</a> model and can be used to derive accurate image representations</li>\n</ul>",
    "contentMarkdown": "*   Towards that goal, the authors of [VinVL](https://arxiv.org/abs/2101.00529) pretrained a novel model on object detection using four public datasets. They then added an “attribute” branch and fine-tuned it, making it capable of detecting both objects and attributes.\n\n> An attribute is a small textual description related to the image.\n\nAn attribute is a small textual description related to the image.\n\n*   The resulted object-attribute detection model is a modification of the [Faster-RCNN](https://blog.paperspace.com/faster-r-cnn-explained-object-detection/) model and can be used to derive accurate image representations",
    "contentLength": 724,
    "wordCount": 82,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/vision-language-models/#vinvl"
  },
  {
    "id": "ai-vision-language-models-simvlm-13",
    "articleSlug": "vision-language-models",
    "articleTitle": "VLM Architectures",
    "category": "Multimodal AI/VLMs",
    "chapter": "Enhanced Visual Representations",
    "title": "SimVLM",
    "order": 13,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2108.10904\">SimVLM</a>, on the other hand, utilizes a version of the <a href=\"https://theaisummer.com/vision-transformer/\">vision transformer (ViT)</a>. In fact, they replaced the well-known patch projection with three ResNet blocks to extract image patch vectors (Conv stage in the image below). The ResNet blocks are trained together with the entire model, contrary to other methods where a fully-pretrained image module is used. The following diagram from the <a href=\"https://arxiv.org/abs/2108.10904\">SimVLM</a> paper illustrates the proposed architecture. The model is pretrained with a unified objective, similar to language modeling, using large-scale weakly labeled data:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/vision-language-models/simvlm.png\" alt=\"\"></p>",
    "contentMarkdown": "*   [SimVLM](https://arxiv.org/abs/2108.10904), on the other hand, utilizes a version of the [vision transformer (ViT)](https://theaisummer.com/vision-transformer/). In fact, they replaced the well-known patch projection with three ResNet blocks to extract image patch vectors (Conv stage in the image below). The ResNet blocks are trained together with the entire model, contrary to other methods where a fully-pretrained image module is used. The following diagram from the [SimVLM](https://arxiv.org/abs/2108.10904) paper illustrates the proposed architecture. The model is pretrained with a unified objective, similar to language modeling, using large-scale weakly labeled data:\n\n![](/primers/ai/assets/vision-language-models/simvlm.png)",
    "contentLength": 812,
    "wordCount": 87,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/vision-language-models/#simvlm"
  }
]