[
  {
    "id": "ai-data-imbalance-oversampling-techniques-1",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Data-based Methods",
    "title": "Oversampling Techniques",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Oversampling involves increasing the representation of minority classes in the dataset by duplicating or generating synthetic samples.</p>\n  </li>\n  <li><strong>Random Oversampling</strong>:\n    <ul>\n      <li>Randomly duplicates existing samples from the minority class until the class sizes are balanced.</li>\n      <li>While simple to implement, random oversampling can lead to overfitting, as it replicates the same samples multiple times without introducing new information.</li>\n    </ul>\n  </li>\n  <li><strong>Advanced Variants</strong>:\n    <ul>\n      <li><strong>Augmentation-Based Oversampling</strong>:\n        <ul>\n          <li>Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.</li>\n          <li>Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>SMOTE Variants (Synthetic Minority Oversampling Technique)</strong>:\n    <ul>\n      <li>SMOTE generates synthetic data points by interpolating between existing minority class samples and their nearest neighbors. Advanced SMOTE variants include:\n        <ul>\n          <li><strong>K-Means SMOTE</strong>:\n            <ul>\n              <li>Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.</li>\n            </ul>\n          </li>\n          <li><strong>SMOTE-Tomek</strong>:\n            <ul>\n              <li>Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.</li>\n            </ul>\n          </li>\n          <li><strong>SVM-SMOTE</strong>:\n            <ul>\n              <li>Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>GAN-Based Data Augmentation/Oversampling</strong>:\n    <ul>\n      <li><strong>Conditional GANs (cGANs)</strong>:\n        <ul>\n          <li>Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.</li>\n          <li>Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.</li>\n        </ul>\n      </li>\n      <li><strong>CycleGANs</strong> for domain-specific augmentation (e.g., converting aerial images to street views).</li>\n      <li><strong>Variational Autoencoders (VAEs)</strong> to generate synthetic tabular data.</li>\n    </ul>\n  </li>\n</ul>\n<p>Oversampling involves increasing the representation of minority classes in the dataset by duplicating or generating synthetic samples.</p>\n<ul>\n      <li>Randomly duplicates existing samples from the minority class until the class sizes are balanced.</li>\n      <li>While simple to implement, random oversampling can lead to overfitting, as it replicates the same samples multiple times without introducing new information.</li>\n    </ul>\n<ul>\n      <li><strong>Augmentation-Based Oversampling</strong>:\n        <ul>\n          <li>Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.</li>\n          <li>Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.</li>\n          <li>Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.</li>\n        </ul>\n<ul>\n      <li>SMOTE generates synthetic data points by interpolating between existing minority class samples and their nearest neighbors. Advanced SMOTE variants include:\n        <ul>\n          <li><strong>K-Means SMOTE</strong>:\n            <ul>\n              <li>Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.</li>\n            </ul>\n          </li>\n          <li><strong>SMOTE-Tomek</strong>:\n            <ul>\n              <li>Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.</li>\n            </ul>\n          </li>\n          <li><strong>SVM-SMOTE</strong>:\n            <ul>\n              <li>Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>K-Means SMOTE</strong>:\n            <ul>\n              <li>Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.</li>\n            </ul>\n          </li>\n          <li><strong>SMOTE-Tomek</strong>:\n            <ul>\n              <li>Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.</li>\n            </ul>\n          </li>\n          <li><strong>SVM-SMOTE</strong>:\n            <ul>\n              <li>Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li>Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.</li>\n            </ul>\n<ul>\n              <li>Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.</li>\n            </ul>\n<ul>\n              <li>Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.</li>\n            </ul>\n<ul>\n      <li><strong>Conditional GANs (cGANs)</strong>:\n        <ul>\n          <li>Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.</li>\n          <li>Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.</li>\n        </ul>\n      </li>\n      <li><strong>CycleGANs</strong> for domain-specific augmentation (e.g., converting aerial images to street views).</li>\n      <li><strong>Variational Autoencoders (VAEs)</strong> to generate synthetic tabular data.</li>\n    </ul>\n<ul>\n          <li>Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.</li>\n          <li>Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.</li>\n        </ul>",
    "contentMarkdown": "*   Oversampling involves increasing the representation of minority classes in the dataset by duplicating or generating synthetic samples.\n    \n*   **Random Oversampling**:\n    *   Randomly duplicates existing samples from the minority class until the class sizes are balanced.\n    *   While simple to implement, random oversampling can lead to overfitting, as it replicates the same samples multiple times without introducing new information.\n*   **Advanced Variants**:\n    *   **Augmentation-Based Oversampling**:\n        *   Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.\n        *   Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.\n*   **SMOTE Variants (Synthetic Minority Oversampling Technique)**:\n    *   SMOTE generates synthetic data points by interpolating between existing minority class samples and their nearest neighbors. Advanced SMOTE variants include:\n        *   **K-Means SMOTE**:\n            *   Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.\n        *   **SMOTE-Tomek**:\n            *   Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.\n        *   **SVM-SMOTE**:\n            *   Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.\n*   **GAN-Based Data Augmentation/Oversampling**:\n    *   **Conditional GANs (cGANs)**:\n        *   Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.\n        *   Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.\n    *   **CycleGANs** for domain-specific augmentation (e.g., converting aerial images to street views).\n    *   **Variational Autoencoders (VAEs)** to generate synthetic tabular data.\n\nOversampling involves increasing the representation of minority classes in the dataset by duplicating or generating synthetic samples.\n\n*   Randomly duplicates existing samples from the minority class until the class sizes are balanced.\n*   While simple to implement, random oversampling can lead to overfitting, as it replicates the same samples multiple times without introducing new information.\n\n*   **Augmentation-Based Oversampling**:\n    *   Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.\n    *   Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.\n\n*   Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.\n*   Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.\n\n*   SMOTE generates synthetic data points by interpolating between existing minority class samples and their nearest neighbors. Advanced SMOTE variants include:\n    *   **K-Means SMOTE**:\n        *   Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.\n    *   **SMOTE-Tomek**:\n        *   Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.\n    *   **SVM-SMOTE**:\n        *   Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.\n\n*   **K-Means SMOTE**:\n    *   Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.\n*   **SMOTE-Tomek**:\n    *   Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.\n*   **SVM-SMOTE**:\n    *   Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.\n\n*   Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.\n\n*   Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.\n\n*   Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.\n\n*   **Conditional GANs (cGANs)**:\n    *   Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.\n    *   Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.\n*   **CycleGANs** for domain-specific augmentation (e.g., converting aerial images to street views).\n*   **Variational Autoencoders (VAEs)** to generate synthetic tabular data.\n\n*   Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.\n*   Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.",
    "contentLength": 7803,
    "wordCount": 768,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#oversampling-techniques"
  },
  {
    "id": "ai-data-imbalance-undersampling-techniques-2",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Data-based Methods",
    "title": "Undersampling Techniques",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Undersampling reduces the size of the majority class to achieve a balanced dataset. This approach is effective when the majority class has redundant or non-informative samples.</p>\n  </li>\n  <li><strong>Edited Nearest Neighbors (ENN)</strong>:\n    <ul>\n      <li>Removes samples from the majority class that are misclassified by their k-nearest neighbors, ensuring that noisy and overlapping samples are eliminated, leading to cleaner decision boundaries.</li>\n    </ul>\n  </li>\n  <li><strong>Tomek Links</strong>:\n    <ul>\n      <li>Identifies pairs of samples from different classes that are each other’s nearest neighbors and removes the majority class samples from these pairs.</li>\n      <li>Improves class separability by reducing overlap between classes.</li>\n    </ul>\n  </li>\n  <li><strong>Cluster-Based Undersampling</strong>:\n    <ul>\n      <li>Uses clustering algorithms (e.g., k-means) to identify representative samples from the majority class, reducing redundancy while retaining critical patterns.</li>\n      <li>Preserves the diversity of majority class data, preventing loss of important information.</li>\n    </ul>\n  </li>\n</ul>\n<p>Undersampling reduces the size of the majority class to achieve a balanced dataset. This approach is effective when the majority class has redundant or non-informative samples.</p>\n<ul>\n      <li>Removes samples from the majority class that are misclassified by their k-nearest neighbors, ensuring that noisy and overlapping samples are eliminated, leading to cleaner decision boundaries.</li>\n    </ul>\n<ul>\n      <li>Identifies pairs of samples from different classes that are each other’s nearest neighbors and removes the majority class samples from these pairs.</li>\n      <li>Improves class separability by reducing overlap between classes.</li>\n    </ul>\n<ul>\n      <li>Uses clustering algorithms (e.g., k-means) to identify representative samples from the majority class, reducing redundancy while retaining critical patterns.</li>\n      <li>Preserves the diversity of majority class data, preventing loss of important information.</li>\n    </ul>",
    "contentMarkdown": "*   Undersampling reduces the size of the majority class to achieve a balanced dataset. This approach is effective when the majority class has redundant or non-informative samples.\n    \n*   **Edited Nearest Neighbors (ENN)**:\n    *   Removes samples from the majority class that are misclassified by their k-nearest neighbors, ensuring that noisy and overlapping samples are eliminated, leading to cleaner decision boundaries.\n*   **Tomek Links**:\n    *   Identifies pairs of samples from different classes that are each other’s nearest neighbors and removes the majority class samples from these pairs.\n    *   Improves class separability by reducing overlap between classes.\n*   **Cluster-Based Undersampling**:\n    *   Uses clustering algorithms (e.g., k-means) to identify representative samples from the majority class, reducing redundancy while retaining critical patterns.\n    *   Preserves the diversity of majority class data, preventing loss of important information.\n\nUndersampling reduces the size of the majority class to achieve a balanced dataset. This approach is effective when the majority class has redundant or non-informative samples.\n\n*   Removes samples from the majority class that are misclassified by their k-nearest neighbors, ensuring that noisy and overlapping samples are eliminated, leading to cleaner decision boundaries.\n\n*   Identifies pairs of samples from different classes that are each other’s nearest neighbors and removes the majority class samples from these pairs.\n*   Improves class separability by reducing overlap between classes.\n\n*   Uses clustering algorithms (e.g., k-means) to identify representative samples from the majority class, reducing redundancy while retaining critical patterns.\n*   Preserves the diversity of majority class data, preventing loss of important information.",
    "contentLength": 2124,
    "wordCount": 248,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#undersampling-techniques"
  },
  {
    "id": "ai-data-imbalance-hybrid-approaches-3",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Data-based Methods",
    "title": "Hybrid Approaches",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Hybrid approaches combine oversampling and undersampling techniques to leverage the benefits of both.</p>\n  </li>\n  <li><strong>SMOTE + ENN</strong>:\n    <ul>\n      <li>Applies SMOTE to oversample the minority class and ENN to remove noisy or overlapping samples from the majority class.</li>\n      <li>Results in a balanced and clean dataset, especially for datasets with significant noise or class overlap.</li>\n    </ul>\n  </li>\n  <li><strong>ADASYN + Cluster Centroids</strong>:\n    <ul>\n      <li>ADASYN (Adaptive Synthetic Sampling) generates synthetic samples for harder-to-classify minority samples, while cluster centroids reduce the size of the majority class by representing clusters as single samples.</li>\n      <li>Ensures balanced yet simplified data for training.</li>\n    </ul>\n  </li>\n</ul>\n<p>Hybrid approaches combine oversampling and undersampling techniques to leverage the benefits of both.</p>\n<ul>\n      <li>Applies SMOTE to oversample the minority class and ENN to remove noisy or overlapping samples from the majority class.</li>\n      <li>Results in a balanced and clean dataset, especially for datasets with significant noise or class overlap.</li>\n    </ul>\n<ul>\n      <li>ADASYN (Adaptive Synthetic Sampling) generates synthetic samples for harder-to-classify minority samples, while cluster centroids reduce the size of the majority class by representing clusters as single samples.</li>\n      <li>Ensures balanced yet simplified data for training.</li>\n    </ul>",
    "contentMarkdown": "*   Hybrid approaches combine oversampling and undersampling techniques to leverage the benefits of both.\n    \n*   **SMOTE + ENN**:\n    *   Applies SMOTE to oversample the minority class and ENN to remove noisy or overlapping samples from the majority class.\n    *   Results in a balanced and clean dataset, especially for datasets with significant noise or class overlap.\n*   **ADASYN + Cluster Centroids**:\n    *   ADASYN (Adaptive Synthetic Sampling) generates synthetic samples for harder-to-classify minority samples, while cluster centroids reduce the size of the majority class by representing clusters as single samples.\n    *   Ensures balanced yet simplified data for training.\n\nHybrid approaches combine oversampling and undersampling techniques to leverage the benefits of both.\n\n*   Applies SMOTE to oversample the minority class and ENN to remove noisy or overlapping samples from the majority class.\n*   Results in a balanced and clean dataset, especially for datasets with significant noise or class overlap.\n\n*   ADASYN (Adaptive Synthetic Sampling) generates synthetic samples for harder-to-classify minority samples, while cluster centroids reduce the size of the majority class by representing clusters as single samples.\n*   Ensures balanced yet simplified data for training.",
    "contentLength": 1514,
    "wordCount": 182,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#hybrid-approaches"
  },
  {
    "id": "ai-data-imbalance-stratified-splitting-4",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Data-based Methods",
    "title": "Stratified Splitting",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Stratified splitting ensures that the class distribution in the training, validation, and test splits matches the original dataset. This prevents data leakage and ensures that the model’s performance metrics are evaluated fairly across all classes.</li>\n  <li>Implementation:\n    <ul>\n      <li>Tools like Python’s <code class=\"language-plaintext highlighter-rouge\">scikit-learn</code> provide a <code class=\"language-plaintext highlighter-rouge\">stratify</code> parameter in the <code class=\"language-plaintext highlighter-rouge\">train_test_split</code> function to maintain class proportions across splits.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Tools like Python’s <code class=\"language-plaintext highlighter-rouge\">scikit-learn</code> provide a <code class=\"language-plaintext highlighter-rouge\">stratify</code> parameter in the <code class=\"language-plaintext highlighter-rouge\">train_test_split</code> function to maintain class proportions across splits.</li>\n    </ul>",
    "contentMarkdown": "*   Stratified splitting ensures that the class distribution in the training, validation, and test splits matches the original dataset. This prevents data leakage and ensures that the model’s performance metrics are evaluated fairly across all classes.\n*   Implementation:\n    *   Tools like Python’s `scikit-learn` provide a `stratify` parameter in the `train_test_split` function to maintain class proportions across splits.\n\n*   Tools like Python’s `scikit-learn` provide a `stratify` parameter in the `train_test_split` function to maintain class proportions across splits.",
    "contentLength": 992,
    "wordCount": 76,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#stratified-splitting"
  },
  {
    "id": "ai-data-imbalance-focal-loss-5",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Loss Function-based Methods",
    "title": "Focal Loss",
    "order": 5,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Tailored for extreme class imbalance, Focal Loss emphasizes harder-to-classify samples by down-weighting easy samples.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo>=</mo><mo>&amp;#x2212;</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>p</mi><mi>t</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mi>&amp;#x03B3;</mi></msup><mi>log</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>p</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 11.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1009.59em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-10\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-13\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-17\"></span><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo>=</mo><mo>−</mo><mi>α</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>t</mi></msub><msup><mo stretchy=\"false\">)</mo><mi>γ</mi></msup><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">L = -\\alpha (1 - p_t)^\\gamma \\log(p_t).</script>\n    <ul>\n      <li>Parameters:\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-24\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\alpha</script>: Balances class contributions to the loss.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\gamma</script>: Focuses training on hard samples, making the model more sensitive to the minority class.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Parameters:\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-24\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\alpha</script>: Balances class contributions to the loss.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\gamma</script>: Focuses training on hard samples, making the model more sensitive to the minority class.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-24\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\alpha</script>: Balances class contributions to the loss.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\gamma</script>: Focuses training on hard samples, making the model more sensitive to the minority class.</li>\n        </ul>",
    "contentMarkdown": "*   Tailored for extreme class imbalance, Focal Loss emphasizes harder-to-classify samples by down-weighting easy samples. L\\=−α(1−pt)γlog(pt).L\\=−α(1−pt)γlog⁡(pt).L = -\\\\alpha (1 - p\\_t)^\\\\gamma \\\\log(p\\_t).\n    *   Parameters:\n        *   αα\\\\alpha: Balances class contributions to the loss.\n        *   γγ\\\\gamma: Focuses training on hard samples, making the model more sensitive to the minority class.\n\n*   Parameters:\n    *   αα\\\\alpha: Balances class contributions to the loss.\n    *   γγ\\\\gamma: Focuses training on hard samples, making the model more sensitive to the minority class.\n\n*   αα\\\\alpha: Balances class contributions to the loss.\n*   γγ\\\\gamma: Focuses training on hard samples, making the model more sensitive to the minority class.",
    "contentLength": 13535,
    "wordCount": 98,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#focal-loss"
  },
  {
    "id": "ai-data-imbalance-weighted-loss-functions-6",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Loss Function-based Methods",
    "title": "Weighted Loss Functions",
    "order": 6,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Assigns higher weights to minority classes, increasing their influence on the loss and model updates.</li>\n  <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>c</mi></msub><mo>=</mo><mfrac><mi>N</mi><msub><mi>n</mi><mi>c</mi></msub></mfrac><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-30\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.815em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"msubsup\" id=\"MathJax-Span-32\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-36\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.63em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.31em;\"><span class=\"msubsup\" id=\"MathJax-Span-38\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>c</mi></msub><mo>=</mo><mfrac><mi>N</mi><msub><mi>n</mi><mi>c</mi></msub></mfrac><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">w_c = \\frac{N}{n_c},</script>\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">N</script> is the total number of samples and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">n_c</script> is the number of samples in class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">c</script>.</li>\n    </ul>\n  </li>\n  <li>Widely supported in frameworks like TensorFlow, PyTorch, and scikit-learn.</li>\n</ul>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">N</script> is the total number of samples and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">n_c</script> is the number of samples in class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">c</script>.</li>\n    </ul>",
    "contentMarkdown": "*   Assigns higher weights to minority classes, increasing their influence on the loss and model updates.\n*   Example: wc\\=Nnc,wc\\=Nnc,w\\_c = \\\\frac{N}{n\\_c},\n    *   where NNN is the total number of samples and ncncn\\_c is the number of samples in class ccc.\n*   Widely supported in frameworks like TensorFlow, PyTorch, and scikit-learn.\n\n*   where NNN is the total number of samples and ncncn\\_c is the number of samples in class ccc.",
    "contentLength": 12983,
    "wordCount": 69,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#weighted-loss-functions"
  },
  {
    "id": "ai-data-imbalance-benefits-of-ensemble-methods-for-class-imbalance-7",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Model-based Methods",
    "title": "Benefits of Ensemble Methods for Class Imbalance",
    "order": 7,
    "orderInChapter": 1,
    "contentHtml": "<ol>\n  <li><strong>Improved Generalization</strong>:\n    <ul>\n      <li>By combining multiple models, ensemble methods reduce the risk of bias toward the majority class and ensure robust performance across classes.</li>\n    </ul>\n  </li>\n  <li><strong>Flexible Sampling</strong>:\n    <ul>\n      <li>Bagging and boosting can be combined with oversampling, undersampling, or hybrid sampling strategies, enhancing their adaptability to class imbalance.</li>\n    </ul>\n  </li>\n  <li><strong>Customizable Weighting</strong>:\n    <ul>\n      <li>Most ensemble frameworks, particularly boosting methods, allow for class-based weighting in their objectives, enabling precise control over class contributions to the final model.</li>\n    </ul>\n  </li>\n  <li><strong>Enhanced Decision Boundaries</strong>:\n    <ul>\n      <li>Ensembles, especially boosting methods, refine decision boundaries iteratively, ensuring minority class regions are not overlooked.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>By combining multiple models, ensemble methods reduce the risk of bias toward the majority class and ensure robust performance across classes.</li>\n    </ul>\n<ul>\n      <li>Bagging and boosting can be combined with oversampling, undersampling, or hybrid sampling strategies, enhancing their adaptability to class imbalance.</li>\n    </ul>\n<ul>\n      <li>Most ensemble frameworks, particularly boosting methods, allow for class-based weighting in their objectives, enabling precise control over class contributions to the final model.</li>\n    </ul>\n<ul>\n      <li>Ensembles, especially boosting methods, refine decision boundaries iteratively, ensuring minority class regions are not overlooked.</li>\n    </ul>\n<ul>\n  <li>In practice, the choice between bagging and boosting depends on the dataset and model goals:\n    <ul>\n      <li><strong>Bagging</strong> is better for reducing overfitting and leveraging parallelism.</li>\n      <li><strong>Boosting</strong> excels in capturing complex patterns, particularly for skewed distributions, with the added advantage of class weighting options in frameworks like XGBoost and LightGBM.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Bagging</strong> is better for reducing overfitting and leveraging parallelism.</li>\n      <li><strong>Boosting</strong> excels in capturing complex patterns, particularly for skewed distributions, with the added advantage of class weighting options in frameworks like XGBoost and LightGBM.</li>\n    </ul>",
    "contentMarkdown": "1.  **Improved Generalization**:\n    *   By combining multiple models, ensemble methods reduce the risk of bias toward the majority class and ensure robust performance across classes.\n2.  **Flexible Sampling**:\n    *   Bagging and boosting can be combined with oversampling, undersampling, or hybrid sampling strategies, enhancing their adaptability to class imbalance.\n3.  **Customizable Weighting**:\n    *   Most ensemble frameworks, particularly boosting methods, allow for class-based weighting in their objectives, enabling precise control over class contributions to the final model.\n4.  **Enhanced Decision Boundaries**:\n    *   Ensembles, especially boosting methods, refine decision boundaries iteratively, ensuring minority class regions are not overlooked.\n\n*   By combining multiple models, ensemble methods reduce the risk of bias toward the majority class and ensure robust performance across classes.\n\n*   Bagging and boosting can be combined with oversampling, undersampling, or hybrid sampling strategies, enhancing their adaptability to class imbalance.\n\n*   Most ensemble frameworks, particularly boosting methods, allow for class-based weighting in their objectives, enabling precise control over class contributions to the final model.\n\n*   Ensembles, especially boosting methods, refine decision boundaries iteratively, ensuring minority class regions are not overlooked.\n\n*   In practice, the choice between bagging and boosting depends on the dataset and model goals:\n    *   **Bagging** is better for reducing overfitting and leveraging parallelism.\n    *   **Boosting** excels in capturing complex patterns, particularly for skewed distributions, with the added advantage of class weighting options in frameworks like XGBoost and LightGBM.\n\n*   **Bagging** is better for reducing overfitting and leveraging parallelism.\n*   **Boosting** excels in capturing complex patterns, particularly for skewed distributions, with the added advantage of class weighting options in frameworks like XGBoost and LightGBM.",
    "contentLength": 2480,
    "wordCount": 263,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#benefits-of-ensemble-methods-for-class-imbalance"
  },
  {
    "id": "ai-data-imbalance-bagging-8",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Model-based Methods",
    "title": "Bagging",
    "order": 8,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Bagging (Bootstrap Aggregating) reduces variance and improves model robustness by training multiple models on different subsets of data sampled with replacement. In the context of class imbalance, bagging methods help by:</p>\n  </li>\n  <li><strong>Boosting Minority Representation</strong>:\n    <ul>\n      <li>Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be applied within each bootstrap sample.</li>\n      <li>Ensures that minority classes are adequately represented in the training data for each model.</li>\n    </ul>\n  </li>\n  <li><strong>Random Forest</strong>:\n    <ul>\n      <li>As a bagging method, Random Forest can handle class imbalance by:\n        <ul>\n          <li>Adjusting the class distribution in each bootstrap sample.</li>\n          <li>Assigning class weights inversely proportional to their frequencies during tree construction.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Class-Specific Aggregation</strong>:\n    <ul>\n      <li>Combines predictions across multiple models, often weighting minority class predictions higher to correct for imbalance.</li>\n    </ul>\n  </li>\n</ul>\n<p>Bagging (Bootstrap Aggregating) reduces variance and improves model robustness by training multiple models on different subsets of data sampled with replacement. In the context of class imbalance, bagging methods help by:</p>\n<ul>\n      <li>Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be applied within each bootstrap sample.</li>\n      <li>Ensures that minority classes are adequately represented in the training data for each model.</li>\n    </ul>\n<ul>\n      <li>As a bagging method, Random Forest can handle class imbalance by:\n        <ul>\n          <li>Adjusting the class distribution in each bootstrap sample.</li>\n          <li>Assigning class weights inversely proportional to their frequencies during tree construction.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Adjusting the class distribution in each bootstrap sample.</li>\n          <li>Assigning class weights inversely proportional to their frequencies during tree construction.</li>\n        </ul>\n<ul>\n      <li>Combines predictions across multiple models, often weighting minority class predictions higher to correct for imbalance.</li>\n    </ul>",
    "contentMarkdown": "*   Bagging (Bootstrap Aggregating) reduces variance and improves model robustness by training multiple models on different subsets of data sampled with replacement. In the context of class imbalance, bagging methods help by:\n    \n*   **Boosting Minority Representation**:\n    *   Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be applied within each bootstrap sample.\n    *   Ensures that minority classes are adequately represented in the training data for each model.\n*   **Random Forest**:\n    *   As a bagging method, Random Forest can handle class imbalance by:\n        *   Adjusting the class distribution in each bootstrap sample.\n        *   Assigning class weights inversely proportional to their frequencies during tree construction.\n*   **Class-Specific Aggregation**:\n    *   Combines predictions across multiple models, often weighting minority class predictions higher to correct for imbalance.\n\nBagging (Bootstrap Aggregating) reduces variance and improves model robustness by training multiple models on different subsets of data sampled with replacement. In the context of class imbalance, bagging methods help by:\n\n*   Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be applied within each bootstrap sample.\n*   Ensures that minority classes are adequately represented in the training data for each model.\n\n*   As a bagging method, Random Forest can handle class imbalance by:\n    *   Adjusting the class distribution in each bootstrap sample.\n    *   Assigning class weights inversely proportional to their frequencies during tree construction.\n\n*   Adjusting the class distribution in each bootstrap sample.\n*   Assigning class weights inversely proportional to their frequencies during tree construction.\n\n*   Combines predictions across multiple models, often weighting minority class predictions higher to correct for imbalance.",
    "contentLength": 2391,
    "wordCount": 264,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#bagging"
  },
  {
    "id": "ai-data-imbalance-boosting-9",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Model-based Methods",
    "title": "Boosting",
    "order": 9,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Boosting sequentially trains models, focusing on samples that previous models misclassified. It is inherently suited to handling class imbalance due to its iterative adjustment of sample weights.</p>\n  </li>\n  <li><strong>Focusing on Hard-to-Classify Samples</strong>:\n    <ul>\n      <li>Boosting algorithms (e.g., AdaBoost, Gradient Boosting) assign higher weights to misclassified samples, often aligning with minority class instances in imbalanced datasets.</li>\n    </ul>\n  </li>\n  <li><strong>Specialized Boosting Variants</strong>:\n    <ul>\n      <li><strong>SMOTEBoost</strong>:\n        <ul>\n          <li>Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.</li>\n        </ul>\n      </li>\n      <li><strong>RUSBoost</strong>:\n        <ul>\n          <li>Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Class Weight Support</strong>:\n    <ul>\n      <li>Many modern boosting frameworks, such as <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong>, allow specifying <strong>class weights</strong> directly in their loss functions.</li>\n      <li>Class weights allow the algorithm to penalize misclassifications of minority class samples more heavily, improving balance. Put simply, this feature prioritizes the minority class by increasing its contribution to the optimization objective, further mitigating imbalance effects.</li>\n    </ul>\n  </li>\n</ul>\n<p>Boosting sequentially trains models, focusing on samples that previous models misclassified. It is inherently suited to handling class imbalance due to its iterative adjustment of sample weights.</p>\n<ul>\n      <li>Boosting algorithms (e.g., AdaBoost, Gradient Boosting) assign higher weights to misclassified samples, often aligning with minority class instances in imbalanced datasets.</li>\n    </ul>\n<ul>\n      <li><strong>SMOTEBoost</strong>:\n        <ul>\n          <li>Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.</li>\n        </ul>\n      </li>\n      <li><strong>RUSBoost</strong>:\n        <ul>\n          <li>Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.</li>\n        </ul>\n<ul>\n          <li>Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.</li>\n        </ul>\n<ul>\n      <li>Many modern boosting frameworks, such as <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong>, allow specifying <strong>class weights</strong> directly in their loss functions.</li>\n      <li>Class weights allow the algorithm to penalize misclassifications of minority class samples more heavily, improving balance. Put simply, this feature prioritizes the minority class by increasing its contribution to the optimization objective, further mitigating imbalance effects.</li>\n    </ul>",
    "contentMarkdown": "*   Boosting sequentially trains models, focusing on samples that previous models misclassified. It is inherently suited to handling class imbalance due to its iterative adjustment of sample weights.\n    \n*   **Focusing on Hard-to-Classify Samples**:\n    *   Boosting algorithms (e.g., AdaBoost, Gradient Boosting) assign higher weights to misclassified samples, often aligning with minority class instances in imbalanced datasets.\n*   **Specialized Boosting Variants**:\n    *   **SMOTEBoost**:\n        *   Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.\n    *   **RUSBoost**:\n        *   Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.\n*   **Class Weight Support**:\n    *   Many modern boosting frameworks, such as **XGBoost**, **LightGBM**, and **CatBoost**, allow specifying **class weights** directly in their loss functions.\n    *   Class weights allow the algorithm to penalize misclassifications of minority class samples more heavily, improving balance. Put simply, this feature prioritizes the minority class by increasing its contribution to the optimization objective, further mitigating imbalance effects.\n\nBoosting sequentially trains models, focusing on samples that previous models misclassified. It is inherently suited to handling class imbalance due to its iterative adjustment of sample weights.\n\n*   Boosting algorithms (e.g., AdaBoost, Gradient Boosting) assign higher weights to misclassified samples, often aligning with minority class instances in imbalanced datasets.\n\n*   **SMOTEBoost**:\n    *   Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.\n*   **RUSBoost**:\n    *   Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.\n\n*   Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.\n\n*   Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.\n\n*   Many modern boosting frameworks, such as **XGBoost**, **LightGBM**, and **CatBoost**, allow specifying **class weights** directly in their loss functions.\n*   Class weights allow the algorithm to penalize misclassifications of minority class samples more heavily, improving balance. Put simply, this feature prioritizes the minority class by increasing its contribution to the optimization objective, further mitigating imbalance effects.",
    "contentLength": 3422,
    "wordCount": 345,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#boosting"
  },
  {
    "id": "ai-data-imbalance-evaluation-metrics-10",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Metrics-based Methods",
    "title": "Evaluation Metrics",
    "order": 10,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Precision, Recall, and F1-Score</strong>:\n    <ul>\n      <li>These metrics go beyond overall accuracy by focusing on specific aspects of model performance, particularly for the minority class:\n        <ul>\n          <li><strong>Precision</strong>:\n            <ul>\n              <li>Represents the proportion of correctly identified positive samples out of all samples predicted as positive.</li>\n              <li>Useful in scenarios where false positives are costly, such as fraud detection.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1007.71em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mtext\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-57\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-58\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\text{Precision} = \\frac{TP}{TP + FP}.</script></li>\n            </ul>\n          </li>\n          <li><strong>Recall</strong>:\n            <ul>\n              <li>Measures the proportion of actual positives correctly identified by the model.</li>\n              <li>Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 8.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.62em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mtext\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-72\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\text{Recall} = \\frac{TP}{TP + FN}.</script></li>\n            </ul>\n          </li>\n          <li><strong>F1-Score</strong>:\n            <ul>\n              <li>The harmonic mean of Precision and Recall, balancing their trade-offs.</li>\n              <li>Provides a single, interpretable metric to assess a model’s focus on minority classes.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>&amp;#x22C5;</mo><mfrac><mrow><mtext>Precision</mtext><mo>&amp;#x22C5;</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.96em, 2.815em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mfrac\" id=\"MathJax-Span-90\"><span style=\"display: inline-block; position: relative; width: 5.055em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.64em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -2.341em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mtext\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-94\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1004.9em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -2.445em;\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"mtext\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.05em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.055em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>⋅</mo><mfrac><mrow><mtext>Precision</mtext><mo>⋅</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.</script></li>\n            </ul>\n          </li>\n          <li><strong>AUC-PR (Precision-Recall Curve)</strong>:\n            <ul>\n              <li>Evaluates performance across different thresholds by plotting Precision against Recall.</li>\n              <li>More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Class-Specific Metrics</strong>:\n    <ul>\n      <li>Evaluate metrics like Precision, Recall, and F1 for each class separately, offering a detailed understanding of how the model performs for the minority class versus the majority class.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>These metrics go beyond overall accuracy by focusing on specific aspects of model performance, particularly for the minority class:\n        <ul>\n          <li><strong>Precision</strong>:\n            <ul>\n              <li>Represents the proportion of correctly identified positive samples out of all samples predicted as positive.</li>\n              <li>Useful in scenarios where false positives are costly, such as fraud detection.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1007.71em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mtext\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-57\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-58\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\text{Precision} = \\frac{TP}{TP + FP}.</script></li>\n            </ul>\n          </li>\n          <li><strong>Recall</strong>:\n            <ul>\n              <li>Measures the proportion of actual positives correctly identified by the model.</li>\n              <li>Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 8.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.62em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mtext\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-72\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\text{Recall} = \\frac{TP}{TP + FN}.</script></li>\n            </ul>\n          </li>\n          <li><strong>F1-Score</strong>:\n            <ul>\n              <li>The harmonic mean of Precision and Recall, balancing their trade-offs.</li>\n              <li>Provides a single, interpretable metric to assess a model’s focus on minority classes.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>&amp;#x22C5;</mo><mfrac><mrow><mtext>Precision</mtext><mo>&amp;#x22C5;</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.96em, 2.815em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mfrac\" id=\"MathJax-Span-90\"><span style=\"display: inline-block; position: relative; width: 5.055em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.64em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -2.341em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mtext\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-94\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1004.9em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -2.445em;\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"mtext\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.05em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.055em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>⋅</mo><mfrac><mrow><mtext>Precision</mtext><mo>⋅</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.</script></li>\n            </ul>\n          </li>\n          <li><strong>AUC-PR (Precision-Recall Curve)</strong>:\n            <ul>\n              <li>Evaluates performance across different thresholds by plotting Precision against Recall.</li>\n              <li>More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Precision</strong>:\n            <ul>\n              <li>Represents the proportion of correctly identified positive samples out of all samples predicted as positive.</li>\n              <li>Useful in scenarios where false positives are costly, such as fraud detection.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1007.71em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mtext\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-57\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-58\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\text{Precision} = \\frac{TP}{TP + FP}.</script></li>\n            </ul>\n          </li>\n          <li><strong>Recall</strong>:\n            <ul>\n              <li>Measures the proportion of actual positives correctly identified by the model.</li>\n              <li>Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 8.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.62em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mtext\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-72\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\text{Recall} = \\frac{TP}{TP + FN}.</script></li>\n            </ul>\n          </li>\n          <li><strong>F1-Score</strong>:\n            <ul>\n              <li>The harmonic mean of Precision and Recall, balancing their trade-offs.</li>\n              <li>Provides a single, interpretable metric to assess a model’s focus on minority classes.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>&amp;#x22C5;</mo><mfrac><mrow><mtext>Precision</mtext><mo>&amp;#x22C5;</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.96em, 2.815em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mfrac\" id=\"MathJax-Span-90\"><span style=\"display: inline-block; position: relative; width: 5.055em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.64em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -2.341em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mtext\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-94\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1004.9em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -2.445em;\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"mtext\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.05em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.055em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>⋅</mo><mfrac><mrow><mtext>Precision</mtext><mo>⋅</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.</script></li>\n            </ul>\n          </li>\n          <li><strong>AUC-PR (Precision-Recall Curve)</strong>:\n            <ul>\n              <li>Evaluates performance across different thresholds by plotting Precision against Recall.</li>\n              <li>More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li>Represents the proportion of correctly identified positive samples out of all samples predicted as positive.</li>\n              <li>Useful in scenarios where false positives are costly, such as fraud detection.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1007.71em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mtext\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-57\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-58\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\text{Precision} = \\frac{TP}{TP + FP}.</script></li>\n            </ul>\n<ul>\n              <li>Measures the proportion of actual positives correctly identified by the model.</li>\n              <li>Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 8.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.62em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mtext\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-72\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\text{Recall} = \\frac{TP}{TP + FN}.</script></li>\n            </ul>\n<ul>\n              <li>The harmonic mean of Precision and Recall, balancing their trade-offs.</li>\n              <li>Provides a single, interpretable metric to assess a model’s focus on minority classes.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>&amp;#x22C5;</mo><mfrac><mrow><mtext>Precision</mtext><mo>&amp;#x22C5;</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.96em, 2.815em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mfrac\" id=\"MathJax-Span-90\"><span style=\"display: inline-block; position: relative; width: 5.055em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.64em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -2.341em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mtext\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-94\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1004.9em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -2.445em;\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"mtext\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.05em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.055em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>⋅</mo><mfrac><mrow><mtext>Precision</mtext><mo>⋅</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.</script></li>\n            </ul>\n<ul>\n              <li>Evaluates performance across different thresholds by plotting Precision against Recall.</li>\n              <li>More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.</li>\n            </ul>\n<ul>\n      <li>Evaluate metrics like Precision, Recall, and F1 for each class separately, offering a detailed understanding of how the model performs for the minority class versus the majority class.</li>\n    </ul>",
    "contentMarkdown": "*   **Precision, Recall, and F1-Score**:\n    *   These metrics go beyond overall accuracy by focusing on specific aspects of model performance, particularly for the minority class:\n        *   **Precision**:\n            *   Represents the proportion of correctly identified positive samples out of all samples predicted as positive.\n            *   Useful in scenarios where false positives are costly, such as fraud detection. Precision\\=TPTP+FP.Precision\\=TPTP+FP.\\\\text{Precision} = \\\\frac{TP}{TP + FP}.\n        *   **Recall**:\n            *   Measures the proportion of actual positives correctly identified by the model.\n            *   Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences. Recall\\=TPTP+FN.Recall\\=TPTP+FN.\\\\text{Recall} = \\\\frac{TP}{TP + FN}.\n        *   **F1-Score**:\n            *   The harmonic mean of Precision and Recall, balancing their trade-offs.\n            *   Provides a single, interpretable metric to assess a model’s focus on minority classes. F1\\=2⋅Precision⋅RecallPrecision+Recall.F1\\=2⋅Precision⋅RecallPrecision+Recall.F1 = 2 \\\\cdot \\\\frac{\\\\text{Precision} \\\\cdot \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}.\n        *   **AUC-PR (Precision-Recall Curve)**:\n            *   Evaluates performance across different thresholds by plotting Precision against Recall.\n            *   More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.\n*   **Class-Specific Metrics**:\n    *   Evaluate metrics like Precision, Recall, and F1 for each class separately, offering a detailed understanding of how the model performs for the minority class versus the majority class.\n\n*   These metrics go beyond overall accuracy by focusing on specific aspects of model performance, particularly for the minority class:\n    *   **Precision**:\n        *   Represents the proportion of correctly identified positive samples out of all samples predicted as positive.\n        *   Useful in scenarios where false positives are costly, such as fraud detection. Precision\\=TPTP+FP.Precision\\=TPTP+FP.\\\\text{Precision} = \\\\frac{TP}{TP + FP}.\n    *   **Recall**:\n        *   Measures the proportion of actual positives correctly identified by the model.\n        *   Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences. Recall\\=TPTP+FN.Recall\\=TPTP+FN.\\\\text{Recall} = \\\\frac{TP}{TP + FN}.\n    *   **F1-Score**:\n        *   The harmonic mean of Precision and Recall, balancing their trade-offs.\n        *   Provides a single, interpretable metric to assess a model’s focus on minority classes. F1\\=2⋅Precision⋅RecallPrecision+Recall.F1\\=2⋅Precision⋅RecallPrecision+Recall.F1 = 2 \\\\cdot \\\\frac{\\\\text{Precision} \\\\cdot \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}.\n    *   **AUC-PR (Precision-Recall Curve)**:\n        *   Evaluates performance across different thresholds by plotting Precision against Recall.\n        *   More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.\n\n*   **Precision**:\n    *   Represents the proportion of correctly identified positive samples out of all samples predicted as positive.\n    *   Useful in scenarios where false positives are costly, such as fraud detection. Precision\\=TPTP+FP.Precision\\=TPTP+FP.\\\\text{Precision} = \\\\frac{TP}{TP + FP}.\n*   **Recall**:\n    *   Measures the proportion of actual positives correctly identified by the model.\n    *   Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences. Recall\\=TPTP+FN.Recall\\=TPTP+FN.\\\\text{Recall} = \\\\frac{TP}{TP + FN}.\n*   **F1-Score**:\n    *   The harmonic mean of Precision and Recall, balancing their trade-offs.\n    *   Provides a single, interpretable metric to assess a model’s focus on minority classes. F1\\=2⋅Precision⋅RecallPrecision+Recall.F1\\=2⋅Precision⋅RecallPrecision+Recall.F1 = 2 \\\\cdot \\\\frac{\\\\text{Precision} \\\\cdot \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}.\n*   **AUC-PR (Precision-Recall Curve)**:\n    *   Evaluates performance across different thresholds by plotting Precision against Recall.\n    *   More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.\n\n*   Represents the proportion of correctly identified positive samples out of all samples predicted as positive.\n*   Useful in scenarios where false positives are costly, such as fraud detection. Precision\\=TPTP+FP.Precision\\=TPTP+FP.\\\\text{Precision} = \\\\frac{TP}{TP + FP}.\n\n*   Measures the proportion of actual positives correctly identified by the model.\n*   Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences. Recall\\=TPTP+FN.Recall\\=TPTP+FN.\\\\text{Recall} = \\\\frac{TP}{TP + FN}.\n\n*   The harmonic mean of Precision and Recall, balancing their trade-offs.\n*   Provides a single, interpretable metric to assess a model’s focus on minority classes. F1\\=2⋅Precision⋅RecallPrecision+Recall.F1\\=2⋅Precision⋅RecallPrecision+Recall.F1 = 2 \\\\cdot \\\\frac{\\\\text{Precision} \\\\cdot \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}.\n\n*   Evaluates performance across different thresholds by plotting Precision against Recall.\n*   More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.\n\n*   Evaluate metrics like Precision, Recall, and F1 for each class separately, offering a detailed understanding of how the model performs for the minority class versus the majority class.",
    "contentLength": 52644,
    "wordCount": 660,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#evaluation-metrics"
  },
  {
    "id": "ai-data-imbalance-additional-diagnostic-tools-confusion-matrix-and-c-11",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Metrics-based Methods",
    "title": "Additional Diagnostic Tools (Confusion Matrix and Correlation Coefficients)",
    "order": 11,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><strong>Confusion Matrix Analysis</strong>:\n    <ul>\n      <li>Provides a granular view of model performance across all prediction outcomes, including true/false positives and negatives.</li>\n      <li>Enables targeted optimization for minority classes by identifying patterns in errors.</li>\n    </ul>\n  </li>\n  <li><strong>Matthews Correlation Coefficient (MCC)</strong>:\n    <ul>\n      <li>A comprehensive metric for binary classification that considers true and false positives and negatives.</li>\n      <li>Particularly robust for imbalanced datasets as it evaluates all four quadrants of the confusion matrix, providing a balanced measure.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>MCC</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>&amp;#x22C5;</mo><mi>T</mi><mi>N</mi><mo>&amp;#x2212;</mo><mi>F</mi><mi>P</mi><mo>&amp;#x22C5;</mo><mi>F</mi><mi>N</mi></mrow><msqrt><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo></msqrt></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-100\" style=\"width: 19.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1015.84em, 3.128em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mtext\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular;\">MCC</span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-104\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 11.982em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.59em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -2.289em;\"><span class=\"mrow\" id=\"MathJax-Span-105\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-111\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1011.88em, 4.43em, -999.997em); top: -3.487em; left: 50%; margin-left: -5.935em;\"><span class=\"msqrt\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 11.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1011.2em, 4.273em, -999.997em); top: -4.008em; left: 0.68em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-133\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.94em, 1011.2em, 1.253em, -999.997em); top: -1.664em; left: 0.68em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.049em; border-top: 1.2px solid; width: 11.201em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.378em, -999.997em); top: -3.904em; left: 0em;\"><span><span style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">√</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1011.98em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 11.982em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>MCC</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>⋅</mo><mi>T</mi><mi>N</mi><mo>−</mo><mi>F</mi><mi>P</mi><mo>⋅</mo><mi>F</mi><mi>N</mi></mrow><msqrt><mo stretchy=\"false\">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=\"false\">)</mo></msqrt></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.</script></li>\n    </ul>\n  </li>\n  <li><strong>Cohen’s Kappa</strong>:\n    <ul>\n      <li>Measures agreement between predicted and actual labels, adjusted for chance.</li>\n      <li>Effective for class imbalance, as it accounts for the disparity in class proportions.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Kappa</mtext><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mi>o</mi></msub><mo>&amp;#x2212;</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>P</mi><mi>e</mi></msub></mrow></mfrac><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-148\" style=\"width: 7.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1006.25em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"mtext\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular;\">Kappa</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-152\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.93em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.935em;\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"msubsup\" id=\"MathJax-Span-154\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-158\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.57em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.779em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mn\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-164\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.03em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.034em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Kappa</mtext><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mi>o</mi></msub><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow></mfrac><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">\\text{Kappa} = \\frac{P_o - P_e}{1 - P_e},</script></li>\n      <li>where (P_o) is the observed agreement and (P_e) is the agreement expected by chance.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Provides a granular view of model performance across all prediction outcomes, including true/false positives and negatives.</li>\n      <li>Enables targeted optimization for minority classes by identifying patterns in errors.</li>\n    </ul>\n<ul>\n      <li>A comprehensive metric for binary classification that considers true and false positives and negatives.</li>\n      <li>Particularly robust for imbalanced datasets as it evaluates all four quadrants of the confusion matrix, providing a balanced measure.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>MCC</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>&amp;#x22C5;</mo><mi>T</mi><mi>N</mi><mo>&amp;#x2212;</mo><mi>F</mi><mi>P</mi><mo>&amp;#x22C5;</mo><mi>F</mi><mi>N</mi></mrow><msqrt><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo></msqrt></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-100\" style=\"width: 19.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1015.84em, 3.128em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mtext\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular;\">MCC</span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-104\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 11.982em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1004.59em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -2.289em;\"><span class=\"mrow\" id=\"MathJax-Span-105\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-111\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1011.88em, 4.43em, -999.997em); top: -3.487em; left: 50%; margin-left: -5.935em;\"><span class=\"msqrt\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 11.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1011.2em, 4.273em, -999.997em); top: -4.008em; left: 0.68em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-133\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.94em, 1011.2em, 1.253em, -999.997em); top: -1.664em; left: 0.68em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.049em; border-top: 1.2px solid; width: 11.201em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.378em, -999.997em); top: -3.904em; left: 0em;\"><span><span style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">√</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1011.98em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 11.982em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>MCC</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>⋅</mo><mi>T</mi><mi>N</mi><mo>−</mo><mi>F</mi><mi>P</mi><mo>⋅</mo><mi>F</mi><mi>N</mi></mrow><msqrt><mo stretchy=\"false\">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy=\"false\">)</mo></msqrt></mfrac><mo>.</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.</script></li>\n    </ul>\n<ul>\n      <li>Measures agreement between predicted and actual labels, adjusted for chance.</li>\n      <li>Effective for class imbalance, as it accounts for the disparity in class proportions.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Kappa</mtext><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mi>o</mi></msub><mo>&amp;#x2212;</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>P</mi><mi>e</mi></msub></mrow></mfrac><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-148\" style=\"width: 7.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1006.25em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"mtext\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular;\">Kappa</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-152\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.93em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.935em;\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"msubsup\" id=\"MathJax-Span-154\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-158\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.57em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.779em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mn\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-164\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.03em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.034em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Kappa</mtext><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mi>o</mi></msub><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow></mfrac><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">\\text{Kappa} = \\frac{P_o - P_e}{1 - P_e},</script></li>\n      <li>where (P_o) is the observed agreement and (P_e) is the agreement expected by chance.</li>\n    </ul>",
    "contentMarkdown": "*   **Confusion Matrix Analysis**:\n    *   Provides a granular view of model performance across all prediction outcomes, including true/false positives and negatives.\n    *   Enables targeted optimization for minority classes by identifying patterns in errors.\n*   **Matthews Correlation Coefficient (MCC)**:\n    *   A comprehensive metric for binary classification that considers true and false positives and negatives.\n    *   Particularly robust for imbalanced datasets as it evaluates all four quadrants of the confusion matrix, providing a balanced measure. MCC\\=TP⋅TN−FP⋅FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)√.MCC\\=TP⋅TN−FP⋅FN(TP+FP)(TP+FN)(TN+FP)(TN+FN).\\\\text{MCC} = \\\\frac{TP \\\\cdot TN - FP \\\\cdot FN}{\\\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.\n*   **Cohen’s Kappa**:\n    *   Measures agreement between predicted and actual labels, adjusted for chance.\n    *   Effective for class imbalance, as it accounts for the disparity in class proportions. Kappa\\=Po−Pe1−Pe,Kappa\\=Po−Pe1−Pe,\\\\text{Kappa} = \\\\frac{P\\_o - P\\_e}{1 - P\\_e},\n    *   where (P\\_o) is the observed agreement and (P\\_e) is the agreement expected by chance.\n\n*   Provides a granular view of model performance across all prediction outcomes, including true/false positives and negatives.\n*   Enables targeted optimization for minority classes by identifying patterns in errors.\n\n*   A comprehensive metric for binary classification that considers true and false positives and negatives.\n*   Particularly robust for imbalanced datasets as it evaluates all four quadrants of the confusion matrix, providing a balanced measure. MCC\\=TP⋅TN−FP⋅FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)√.MCC\\=TP⋅TN−FP⋅FN(TP+FP)(TP+FN)(TN+FP)(TN+FN).\\\\text{MCC} = \\\\frac{TP \\\\cdot TN - FP \\\\cdot FN}{\\\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.\n\n*   Measures agreement between predicted and actual labels, adjusted for chance.\n*   Effective for class imbalance, as it accounts for the disparity in class proportions. Kappa\\=Po−Pe1−Pe,Kappa\\=Po−Pe1−Pe,\\\\text{Kappa} = \\\\frac{P\\_o - P\\_e}{1 - P\\_e},\n*   where (P\\_o) is the observed agreement and (P\\_e) is the agreement expected by chance.",
    "contentLength": 33723,
    "wordCount": 252,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#additional-diagnostic-tools-(confusion-matrix-and-correlation-coefficients)"
  },
  {
    "id": "ai-data-imbalance-calibration-metrics-12",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Metrics-based Methods",
    "title": "Calibration Metrics",
    "order": 12,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><strong>Brier Score</strong>:\n    <ul>\n      <li>Assesses the accuracy of probabilistic predictions by penalizing confidence in incorrect predictions and rewarding well-calibrated probabilities.</li>\n      <li>Especially relevant for imbalanced datasets, where models often exhibit calibration issues due to overconfidence in the majority class.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Brier Score</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 15.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1012.55em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"mtext\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">Brier Score</span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-172\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-173\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-175\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-177\"><span class=\"mrow\" id=\"MathJax-Span-178\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-180\"><span class=\"mrow\" id=\"MathJax-Span-181\"><span class=\"mi\" id=\"MathJax-Span-182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-183\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-184\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-186\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-190\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-193\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Brier Score</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (f_i - y_i)^2,</script>\nwhere (f_i) is the predicted probability for sample (i) and (y_i) is the actual label.</li>\n    </ul>\n  </li>\n  <li><strong>Expected Calibration Error (ECE)</strong>:\n    <ul>\n      <li>Measures the difference between predicted probabilities and actual outcomes, providing insight into the reliability of a model’s probabilistic outputs.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Assesses the accuracy of probabilistic predictions by penalizing confidence in incorrect predictions and rewarding well-calibrated probabilities.</li>\n      <li>Especially relevant for imbalanced datasets, where models often exhibit calibration issues due to overconfidence in the majority class.\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Brier Score</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 15.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1012.55em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"mtext\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">Brier Score</span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-172\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-173\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-175\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-177\"><span class=\"mrow\" id=\"MathJax-Span-178\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-180\"><span class=\"mrow\" id=\"MathJax-Span-181\"><span class=\"mi\" id=\"MathJax-Span-182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-183\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-184\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-186\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-190\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-193\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Brier Score</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (f_i - y_i)^2,</script>\nwhere (f_i) is the predicted probability for sample (i) and (y_i) is the actual label.</li>\n    </ul>\n<ul>\n      <li>Measures the difference between predicted probabilities and actual outcomes, providing insight into the reliability of a model’s probabilistic outputs.</li>\n    </ul>",
    "contentMarkdown": "*   **Brier Score**:\n    *   Assesses the accuracy of probabilistic predictions by penalizing confidence in incorrect predictions and rewarding well-calibrated probabilities.\n    *   Especially relevant for imbalanced datasets, where models often exhibit calibration issues due to overconfidence in the majority class. Brier Score\\=1N∑Ni\\=1(fi−yi)2,Brier Score\\=1N∑i\\=1N(fi−yi)2,\\\\text{Brier Score} = \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} (f\\_i - y\\_i)^2, where (f\\_i) is the predicted probability for sample (i) and (y\\_i) is the actual label.\n*   **Expected Calibration Error (ECE)**:\n    *   Measures the difference between predicted probabilities and actual outcomes, providing insight into the reliability of a model’s probabilistic outputs.\n\n*   Assesses the accuracy of probabilistic predictions by penalizing confidence in incorrect predictions and rewarding well-calibrated probabilities.\n*   Especially relevant for imbalanced datasets, where models often exhibit calibration issues due to overconfidence in the majority class. Brier Score\\=1N∑Ni\\=1(fi−yi)2,Brier Score\\=1N∑i\\=1N(fi−yi)2,\\\\text{Brier Score} = \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} (f\\_i - y\\_i)^2, where (f\\_i) is the predicted probability for sample (i) and (y\\_i) is the actual label.\n\n*   Measures the difference between predicted probabilities and actual outcomes, providing insight into the reliability of a model’s probabilistic outputs.",
    "contentLength": 16293,
    "wordCount": 170,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#calibration-metrics"
  },
  {
    "id": "ai-data-imbalance-practical-recommendations-13",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "Metrics-based Methods",
    "title": "Practical Recommendations",
    "order": 13,
    "orderInChapter": 4,
    "contentHtml": "<ol>\n  <li><strong>Data-Level Techniques</strong>:\n    <ul>\n      <li>Employ <strong>SMOTE + Tomek Links</strong> to oversample the minority class and remove overlapping samples that introduce noise.</li>\n    </ul>\n  </li>\n  <li><strong>Algorithm Adjustments</strong>:\n    <ul>\n      <li>Train a <strong>Weighted XGBoost</strong> model with <strong>Focal Loss</strong> to dynamically focus on difficult samples, especially in imbalanced datasets.</li>\n    </ul>\n  </li>\n  <li><strong>Evaluation</strong>:\n    <ul>\n      <li>Prioritize metrics that emphasize the minority class, such as <strong>Precision-Recall curves</strong>, <strong>F1-Scores</strong>, and <strong>MCC</strong>.</li>\n    </ul>\n  </li>\n  <li><strong>Calibration</strong>:\n    <ul>\n      <li>Validate model outputs with <strong>Brier Score</strong> and calibration plots to ensure reliable probabilistic predictions.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>Employ <strong>SMOTE + Tomek Links</strong> to oversample the minority class and remove overlapping samples that introduce noise.</li>\n    </ul>\n<ul>\n      <li>Train a <strong>Weighted XGBoost</strong> model with <strong>Focal Loss</strong> to dynamically focus on difficult samples, especially in imbalanced datasets.</li>\n    </ul>\n<ul>\n      <li>Prioritize metrics that emphasize the minority class, such as <strong>Precision-Recall curves</strong>, <strong>F1-Scores</strong>, and <strong>MCC</strong>.</li>\n    </ul>\n<ul>\n      <li>Validate model outputs with <strong>Brier Score</strong> and calibration plots to ensure reliable probabilistic predictions.</li>\n    </ul>\n<ul>\n  <li>Class imbalance can be addressed effectively by leveraging a combination of these methods, tuned to the problem’s specific needs.</li>\n</ul>",
    "contentMarkdown": "1.  **Data-Level Techniques**:\n    *   Employ **SMOTE + Tomek Links** to oversample the minority class and remove overlapping samples that introduce noise.\n2.  **Algorithm Adjustments**:\n    *   Train a **Weighted XGBoost** model with **Focal Loss** to dynamically focus on difficult samples, especially in imbalanced datasets.\n3.  **Evaluation**:\n    *   Prioritize metrics that emphasize the minority class, such as **Precision-Recall curves**, **F1-Scores**, and **MCC**.\n4.  **Calibration**:\n    *   Validate model outputs with **Brier Score** and calibration plots to ensure reliable probabilistic predictions.\n\n*   Employ **SMOTE + Tomek Links** to oversample the minority class and remove overlapping samples that introduce noise.\n\n*   Train a **Weighted XGBoost** model with **Focal Loss** to dynamically focus on difficult samples, especially in imbalanced datasets.\n\n*   Prioritize metrics that emphasize the minority class, such as **Precision-Recall curves**, **F1-Scores**, and **MCC**.\n\n*   Validate model outputs with **Brier Score** and calibration plots to ensure reliable probabilistic predictions.\n\n*   Class imbalance can be addressed effectively by leveraging a combination of these methods, tuned to the problem’s specific needs.",
    "contentLength": 1757,
    "wordCount": 164,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#practical-recommendations"
  },
  {
    "id": "ai-data-imbalance-how-do-ensemble-methods-help-with-class-imbalance-14",
    "articleSlug": "data-imbalance",
    "articleTitle": "Data Imbalance",
    "category": "Data/Training",
    "chapter": "FAQs",
    "title": "How Do Ensemble Methods Help with Class Imbalance?",
    "order": 14,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Ensemble methods are effective tools for addressing class imbalance, as they combine multiple models to improve overall performance, reduce overfitting, and mitigate the bias toward majority classes. By amplifying the signal from minority class data and leveraging the diversity of models, these methods enhance prediction accuracy and fairness across all classes. When paired with complementary techniques such as resampling, adjusting class weights, or generating synthetic data, ensemble methods can yield even more robust results in handling imbalanced datasets.</li>\n</ul>\n<h4 id=\"bagging-methods-eg-random-forest\">Bagging Methods (e.g., Random Forest)</h4>\n<ul>\n  <li><strong>How It Helps:</strong>\n    <ul>\n      <li>Bagging trains multiple models on different bootstrapped (randomly sampled with replacement) subsets of the data.</li>\n      <li>You can apply techniques like oversampling the minority class or undersampling the majority class within each bootstrapped sample to improve representation of minority classes.</li>\n      <li>Random Forests average predictions across trees, which helps mitigate the bias introduced by imbalanced data.</li>\n    </ul>\n  </li>\n  <li><strong>Advantages:</strong>\n    <ul>\n      <li>Reduces variance and prevents overfitting.</li>\n      <li>Can handle imbalance if combined with balanced sampling strategies.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Bagging trains multiple models on different bootstrapped (randomly sampled with replacement) subsets of the data.</li>\n      <li>You can apply techniques like oversampling the minority class or undersampling the majority class within each bootstrapped sample to improve representation of minority classes.</li>\n      <li>Random Forests average predictions across trees, which helps mitigate the bias introduced by imbalanced data.</li>\n    </ul>\n<ul>\n      <li>Reduces variance and prevents overfitting.</li>\n      <li>Can handle imbalance if combined with balanced sampling strategies.</li>\n    </ul>\n<h4 id=\"boosting-methods-eg-adaboost-gradient-boosting-xgboost\">Boosting Methods (e.g., AdaBoost, Gradient Boosting, XGBoost)</h4>\n<ul>\n  <li><strong>How It Helps:</strong>\n    <ul>\n      <li>Boosting focuses on correcting the mistakes of previous models by assigning higher weights to misclassified instances.</li>\n      <li>In the case of imbalanced datasets, boosting naturally places more emphasis on minority class samples, as they are more likely to be misclassified in early iterations.</li>\n      <li>Many boosting frameworks (e.g., XGBoost, LightGBM) allow specifying <strong>class weights</strong>, which further prioritize the minority class.</li>\n    </ul>\n  </li>\n  <li><strong>Advantages:</strong>\n    <ul>\n      <li>Effective at focusing on hard-to-classify samples (often minority class).</li>\n      <li>Customizable with parameters like learning rate and class weights.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Boosting focuses on correcting the mistakes of previous models by assigning higher weights to misclassified instances.</li>\n      <li>In the case of imbalanced datasets, boosting naturally places more emphasis on minority class samples, as they are more likely to be misclassified in early iterations.</li>\n      <li>Many boosting frameworks (e.g., XGBoost, LightGBM) allow specifying <strong>class weights</strong>, which further prioritize the minority class.</li>\n    </ul>\n<ul>\n      <li>Effective at focusing on hard-to-classify samples (often minority class).</li>\n      <li>Customizable with parameters like learning rate and class weights.</li>\n    </ul>\n<h4 id=\"ensemble-of-resampled-datasets\">Ensemble of Resampled Datasets</h4>\n<ul>\n  <li><strong>How It Helps:</strong>\n    <ul>\n      <li>Build multiple models, each trained on a dataset that has been resampled to balance the classes.</li>\n      <li>For example:\n        <ul>\n          <li><strong>Over-sampling:</strong> Duplicate samples of the minority class.</li>\n          <li><strong>Under-sampling:</strong> Reduce samples of the majority class.</li>\n        </ul>\n      </li>\n      <li>Combine predictions using voting or averaging to reduce individual model biases.</li>\n    </ul>\n  </li>\n  <li><strong>Advantages:</strong>\n    <ul>\n      <li>Balances class representation while maintaining diversity among models.</li>\n      <li>Reduces overfitting to the majority class.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Build multiple models, each trained on a dataset that has been resampled to balance the classes.</li>\n      <li>For example:\n        <ul>\n          <li><strong>Over-sampling:</strong> Duplicate samples of the minority class.</li>\n          <li><strong>Under-sampling:</strong> Reduce samples of the majority class.</li>\n        </ul>\n      </li>\n      <li>Combine predictions using voting or averaging to reduce individual model biases.</li>\n    </ul>\n<ul>\n          <li><strong>Over-sampling:</strong> Duplicate samples of the minority class.</li>\n          <li><strong>Under-sampling:</strong> Reduce samples of the majority class.</li>\n        </ul>\n<ul>\n      <li>Balances class representation while maintaining diversity among models.</li>\n      <li>Reduces overfitting to the majority class.</li>\n    </ul>\n<h4 id=\"cost-sensitive-learning-with-ensembles\">Cost-Sensitive Learning with Ensembles</h4>\n<ul>\n  <li><strong>How It Helps:</strong>\n    <ul>\n      <li>Modify the objective function of ensemble models to include misclassification costs.</li>\n      <li>Penalize misclassifications of the minority class more heavily, forcing the model to focus on getting those predictions right.</li>\n      <li>Many frameworks, such as XGBoost, support custom loss functions that incorporate class imbalance.</li>\n    </ul>\n  </li>\n  <li><strong>Advantages:</strong>\n    <ul>\n      <li>Directly addresses the imbalance by prioritizing the minority class.</li>\n      <li>Avoids the need for resampling.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Modify the objective function of ensemble models to include misclassification costs.</li>\n      <li>Penalize misclassifications of the minority class more heavily, forcing the model to focus on getting those predictions right.</li>\n      <li>Many frameworks, such as XGBoost, support custom loss functions that incorporate class imbalance.</li>\n    </ul>\n<ul>\n      <li>Directly addresses the imbalance by prioritizing the minority class.</li>\n      <li>Avoids the need for resampling.</li>\n    </ul>\n<h4 id=\"hybrid-approaches-1\">Hybrid Approaches</h4>\n<ul>\n  <li><strong>How It Helps:</strong>\n    <ul>\n      <li>Combine ensemble methods with other imbalance techniques, such as SMOTE (Synthetic Minority Oversampling Technique).</li>\n      <li>For example:\n        <ul>\n          <li>Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Advantages:</strong>\n    <ul>\n      <li>Leverages the strengths of both resampling and ensemble learning.</li>\n      <li>Can yield high performance even for severely imbalanced datasets.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Combine ensemble methods with other imbalance techniques, such as SMOTE (Synthetic Minority Oversampling Technique).</li>\n      <li>For example:\n        <ul>\n          <li>Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.</li>\n        </ul>\n<ul>\n      <li>Leverages the strengths of both resampling and ensemble learning.</li>\n      <li>Can yield high performance even for severely imbalanced datasets.</li>\n    </ul>\n<h4 id=\"key-advantages-of-using-ensembles-for-class-imbalance\">Key Advantages of Using Ensembles for Class Imbalance</h4>\n<ul>\n  <li><strong>Improved Robustness:</strong> Ensembles aggregate predictions, reducing the likelihood of bias from a single model.</li>\n  <li><strong>Focus on Hard Cases:</strong> Methods like boosting inherently focus on hard-to-classify samples, which are often from the minority class.</li>\n  <li><strong>Flexibility:</strong> Many ensemble methods can integrate class weights or cost-sensitive learning to handle imbalance directly.</li>\n  <li><strong>Versatility:</strong> Ensembles can be combined with other preprocessing or algorithmic approaches for greater effectiveness.</li>\n</ul>",
    "contentMarkdown": "*   Ensemble methods are effective tools for addressing class imbalance, as they combine multiple models to improve overall performance, reduce overfitting, and mitigate the bias toward majority classes. By amplifying the signal from minority class data and leveraging the diversity of models, these methods enhance prediction accuracy and fairness across all classes. When paired with complementary techniques such as resampling, adjusting class weights, or generating synthetic data, ensemble methods can yield even more robust results in handling imbalanced datasets.\n\n#### Bagging Methods (e.g., Random Forest)\n\n*   **How It Helps:**\n    *   Bagging trains multiple models on different bootstrapped (randomly sampled with replacement) subsets of the data.\n    *   You can apply techniques like oversampling the minority class or undersampling the majority class within each bootstrapped sample to improve representation of minority classes.\n    *   Random Forests average predictions across trees, which helps mitigate the bias introduced by imbalanced data.\n*   **Advantages:**\n    *   Reduces variance and prevents overfitting.\n    *   Can handle imbalance if combined with balanced sampling strategies.\n\n*   Bagging trains multiple models on different bootstrapped (randomly sampled with replacement) subsets of the data.\n*   You can apply techniques like oversampling the minority class or undersampling the majority class within each bootstrapped sample to improve representation of minority classes.\n*   Random Forests average predictions across trees, which helps mitigate the bias introduced by imbalanced data.\n\n*   Reduces variance and prevents overfitting.\n*   Can handle imbalance if combined with balanced sampling strategies.\n\n#### Boosting Methods (e.g., AdaBoost, Gradient Boosting, XGBoost)\n\n*   **How It Helps:**\n    *   Boosting focuses on correcting the mistakes of previous models by assigning higher weights to misclassified instances.\n    *   In the case of imbalanced datasets, boosting naturally places more emphasis on minority class samples, as they are more likely to be misclassified in early iterations.\n    *   Many boosting frameworks (e.g., XGBoost, LightGBM) allow specifying **class weights**, which further prioritize the minority class.\n*   **Advantages:**\n    *   Effective at focusing on hard-to-classify samples (often minority class).\n    *   Customizable with parameters like learning rate and class weights.\n\n*   Boosting focuses on correcting the mistakes of previous models by assigning higher weights to misclassified instances.\n*   In the case of imbalanced datasets, boosting naturally places more emphasis on minority class samples, as they are more likely to be misclassified in early iterations.\n*   Many boosting frameworks (e.g., XGBoost, LightGBM) allow specifying **class weights**, which further prioritize the minority class.\n\n*   Effective at focusing on hard-to-classify samples (often minority class).\n*   Customizable with parameters like learning rate and class weights.\n\n#### Ensemble of Resampled Datasets\n\n*   **How It Helps:**\n    *   Build multiple models, each trained on a dataset that has been resampled to balance the classes.\n    *   For example:\n        *   **Over-sampling:** Duplicate samples of the minority class.\n        *   **Under-sampling:** Reduce samples of the majority class.\n    *   Combine predictions using voting or averaging to reduce individual model biases.\n*   **Advantages:**\n    *   Balances class representation while maintaining diversity among models.\n    *   Reduces overfitting to the majority class.\n\n*   Build multiple models, each trained on a dataset that has been resampled to balance the classes.\n*   For example:\n    *   **Over-sampling:** Duplicate samples of the minority class.\n    *   **Under-sampling:** Reduce samples of the majority class.\n*   Combine predictions using voting or averaging to reduce individual model biases.\n\n*   **Over-sampling:** Duplicate samples of the minority class.\n*   **Under-sampling:** Reduce samples of the majority class.\n\n*   Balances class representation while maintaining diversity among models.\n*   Reduces overfitting to the majority class.\n\n#### Cost-Sensitive Learning with Ensembles\n\n*   **How It Helps:**\n    *   Modify the objective function of ensemble models to include misclassification costs.\n    *   Penalize misclassifications of the minority class more heavily, forcing the model to focus on getting those predictions right.\n    *   Many frameworks, such as XGBoost, support custom loss functions that incorporate class imbalance.\n*   **Advantages:**\n    *   Directly addresses the imbalance by prioritizing the minority class.\n    *   Avoids the need for resampling.\n\n*   Modify the objective function of ensemble models to include misclassification costs.\n*   Penalize misclassifications of the minority class more heavily, forcing the model to focus on getting those predictions right.\n*   Many frameworks, such as XGBoost, support custom loss functions that incorporate class imbalance.\n\n*   Directly addresses the imbalance by prioritizing the minority class.\n*   Avoids the need for resampling.\n\n#### Hybrid Approaches\n\n*   **How It Helps:**\n    *   Combine ensemble methods with other imbalance techniques, such as SMOTE (Synthetic Minority Oversampling Technique).\n    *   For example:\n        *   Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.\n*   **Advantages:**\n    *   Leverages the strengths of both resampling and ensemble learning.\n    *   Can yield high performance even for severely imbalanced datasets.\n\n*   Combine ensemble methods with other imbalance techniques, such as SMOTE (Synthetic Minority Oversampling Technique).\n*   For example:\n    *   Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.\n\n*   Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.\n\n*   Leverages the strengths of both resampling and ensemble learning.\n*   Can yield high performance even for severely imbalanced datasets.\n\n#### Key Advantages of Using Ensembles for Class Imbalance\n\n*   **Improved Robustness:** Ensembles aggregate predictions, reducing the likelihood of bias from a single model.\n*   **Focus on Hard Cases:** Methods like boosting inherently focus on hard-to-classify samples, which are often from the minority class.\n*   **Flexibility:** Many ensemble methods can integrate class weights or cost-sensitive learning to handle imbalance directly.\n*   **Versatility:** Ensembles can be combined with other preprocessing or algorithmic approaches for greater effectiveness.",
    "contentLength": 8530,
    "wordCount": 918,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/data-imbalance/#how-do-ensemble-methods-help-with-class-imbalance?"
  }
]