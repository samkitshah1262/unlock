<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • Distributed Training Parallelism</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/distributed-training-parallelism/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fdistributed-training-parallelism&amp;ers=2"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxX0I-1XfXRdyhssDhSbeB61vHYUgPXTFCMdZECzB7_Lzqmj_I_kfMhW2jyXvPaP7SxeeT_YRl5YEBtt9FzZMFZhWrAofsFEIfFJnNlbD3487PZrz43ZpDvD4LqAiiOLo_vZhLlAuQ==?fccs=W1siQUtzUm9sXzdtQTlLWVk4dHZMcjhnOXJCeTJUX1kzRUVsSHhBY095MjhfcENIUXhLVmtlcmhkTW44cnA4Z0pkd0FuWFIyalgzb1oyaS1oTm9LWTFBQTBEUkJQZmFIOGRhbTBsaV9BZEk3Q1NKRF9oSHFZem1HQmNTSWdGNmNick9YU296QjRTSy0yVkVFTm9YQlBqb0JBWEd6SDY0RHFmcC1BPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI2MDUsMTA5MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9kaXN0cmlidXRlZC10cmFpbmluZy1wYXJhbGxlbGlzbS8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbOTUzNDAyNTMsOTUzNDAyNTVdXSJdLFsyOSwiZmFsc2UiXV1d"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxUdErKAv_m9HESCTBv6sEBqdpa04dHs4UzRjzjK9NfiByNzO0rZFsiwy7SNziX3Wt7ky0XbWI4n4dK6bMminhsv-ddRlgcAoPkCqPQp40zGQjTWbj_Wcq9VdV8SAVuZrVdvB94BOA==?fccs=W1siQUtzUm9sXzdtQTlLWVk4dHZMcjhnOXJCeTJUX1kzRUVsSHhBY095MjhfcENIUXhLVmtlcmhkTW44cnA4Z0pkd0FuWFIyalgzb1oyaS1oTm9LWTFBQTBEUkJQZmFIOGRhbTBsaV9BZEk3Q1NKRF9oSHFZem1HQmNTSWdGNmNick9YU296QjRTSy0yVkVFTm9YQlBqb0JBWEd6SDY0RHFmcC1BPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI2MDUsMjMxMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5XSxudWxsLDIsbnVsbCwiZW4iXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvZGlzdHJpYnV0ZWQtdHJhaW5pbmctcGFyYWxsZWxpc20vIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI1LCJbWzk1MzQwMjUzLDk1MzQwMjU1XV0iXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxV95Hh7KrH4JA-WI6i5BRfh99E5kjiQxj6YgY5P-n40ViCVNav--hBv3OAnHOEh5M1HXyfC5oC9A3onjmHchHZZnDp943e0X5XD1pwVFySSJDFasvnNgAzTzX0idZRD4PbFgLqmLA==?fccs=W1siQUtzUm9sXzdtQTlLWVk4dHZMcjhnOXJCeTJUX1kzRUVsSHhBY095MjhfcENIUXhLVmtlcmhkTW44cnA4Z0pkd0FuWFIyalgzb1oyaS1oTm9LWTFBQTBEUkJQZmFIOGRhbTBsaV9BZEk3Q1NKRF9oSHFZem1HQmNTSWdGNmNick9YU296QjRTSy0yVkVFTm9YQlBqb0JBWEd6SDY0RHFmcC1BPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI2MDYsMTg0MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5LDZdLG51bGwsMixudWxsLCJlbiIsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLDFdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9kaXN0cmlidXRlZC10cmFpbmluZy1wYXJhbGxlbGlzbS8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbOTUzNDAyNTMsOTUzNDAyNTVdXSJdLFsyOSwiZmFsc2UiXV1d"></script></head>


    <body><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script><ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895470&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fdistributed-training-parallelism%2F&amp;pra=5&amp;wgl=1&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766922604644&amp;bpp=2&amp;bdt=13&amp;idt=8&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=1284463939601&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31096041%2C95376242%2C95376582%2C95378749%2C95344788%2C95340253%2C95340255&amp;oid=2&amp;pvsid=51611712186136&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=32768&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=13" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • Distributed Training Parallelism</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#overview-types-of-parallelism" id="markdown-toc-overview-types-of-parallelism">Overview: Types of Parallelism</a>    <ul>
      <li><a href="#model-parallelism" id="markdown-toc-model-parallelism">Model Parallelism</a>        <ul>
          <li><a href="#concept" id="markdown-toc-concept">Concept</a></li>
          <li><a href="#mechanism" id="markdown-toc-mechanism">Mechanism</a></li>
          <li><a href="#pros-and-cons" id="markdown-toc-pros-and-cons">Pros and Cons</a></li>
          <li><a href="#use-cases" id="markdown-toc-use-cases">Use Cases</a></li>
          <li><a href="#implementation-in-pytorch" id="markdown-toc-implementation-in-pytorch">Implementation in PyTorch</a></li>
          <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
          <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
        </ul>
      </li>
      <li><a href="#data-parallelism" id="markdown-toc-data-parallelism">Data Parallelism</a>        <ul>
          <li><a href="#concept-1" id="markdown-toc-concept-1">Concept</a></li>
          <li><a href="#mechanism-1" id="markdown-toc-mechanism-1">Mechanism</a></li>
          <li><a href="#pros-and-cons-1" id="markdown-toc-pros-and-cons-1">Pros and Cons</a></li>
          <li><a href="#use-cases-1" id="markdown-toc-use-cases-1">Use Cases</a></li>
          <li><a href="#implementation-in-pytorch-1" id="markdown-toc-implementation-in-pytorch-1">Implementation in PyTorch</a></li>
          <li><a href="#conclusion-1" id="markdown-toc-conclusion-1">Conclusion</a></li>
          <li><a href="#summary-1" id="markdown-toc-summary-1">Summary</a></li>
        </ul>
      </li>
      <li><a href="#pipeline-parallelism" id="markdown-toc-pipeline-parallelism">Pipeline Parallelism</a></li>
      <li><a href="#tensor-parallelism" id="markdown-toc-tensor-parallelism">Tensor Parallelism</a></li>
      <li><a href="#choosing-the-right-strategy-data-vs-model-vs-pipeline-vs-tensor-parallelism" id="markdown-toc-choosing-the-right-strategy-data-vs-model-vs-pipeline-vs-tensor-parallelism">Choosing the Right Strategy: Data V/s Model V/s Pipeline V/s Tensor Parallelism</a></li>
    </ul>
  </li>
  <li><a href="#data-parallelism-1" id="markdown-toc-data-parallelism-1">Data Parallelism</a>    <ul>
      <li><a href="#dataparallel-dp" id="markdown-toc-dataparallel-dp">DataParallel (DP)</a>        <ul>
          <li><a href="#how-dp-works" id="markdown-toc-how-dp-works">How DP Works</a></li>
          <li><a href="#key-steps-in-using-dp" id="markdown-toc-key-steps-in-using-dp">Key Steps in Using DP</a></li>
          <li><a href="#code-sample" id="markdown-toc-code-sample">Code Sample</a></li>
          <li><a href="#explanation-of-the-code" id="markdown-toc-explanation-of-the-code">Explanation of the Code</a></li>
        </ul>
      </li>
      <li><a href="#distributed-data-parallel-ddp" id="markdown-toc-distributed-data-parallel-ddp">Distributed Data Parallel (DDP)</a>        <ul>
          <li><a href="#key-features-of-ddp" id="markdown-toc-key-features-of-ddp">Key Features of DDP</a></li>
          <li><a href="#technical-steps-to-use-ddp" id="markdown-toc-technical-steps-to-use-ddp">Technical Steps to Use DDP</a></li>
          <li><a href="#code-sample-1" id="markdown-toc-code-sample-1">Code Sample</a></li>
          <li><a href="#explanation-of-the-code-1" id="markdown-toc-explanation-of-the-code-1">Explanation of the Code</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#model-parallelism-1" id="markdown-toc-model-parallelism-1">Model Parallelism</a>    <ul>
      <li><a href="#layer-wise-parallelism" id="markdown-toc-layer-wise-parallelism">Layer-wise Parallelism</a></li>
      <li><a href="#tensor-wise-parallelism" id="markdown-toc-tensor-wise-parallelism">Tensor-wise Parallelism</a></li>
      <li><a href="#operator-wise-parallelism" id="markdown-toc-operator-wise-parallelism">Operator-wise Parallelism</a></li>
      <li><a href="#summary-2" id="markdown-toc-summary-2">Summary</a></li>
      <li><a href="#comparative-analysis-types-of-model-parallelism" id="markdown-toc-comparative-analysis-types-of-model-parallelism">Comparative Analysis: Types of Model Parallelism</a>        <ul>
          <li><a href="#criteria-for-comparison" id="markdown-toc-criteria-for-comparison">Criteria for Comparison</a></li>
          <li><a href="#layer-wise-parallelism-1" id="markdown-toc-layer-wise-parallelism-1">Layer-wise Parallelism</a></li>
          <li><a href="#tensor-wise-parallelism-1" id="markdown-toc-tensor-wise-parallelism-1">Tensor-wise Parallelism</a></li>
          <li><a href="#operator-wise-parallelism-1" id="markdown-toc-operator-wise-parallelism-1">Operator-wise Parallelism</a></li>
        </ul>
      </li>
      <li><a href="#summary-table" id="markdown-toc-summary-table">Summary Table</a></li>
      <li><a href="#choosing-the-right-type" id="markdown-toc-choosing-the-right-type">Choosing the Right Type</a></li>
    </ul>
  </li>
  <li><a href="#hybrid-model-and-data-parallelism" id="markdown-toc-hybrid-model-and-data-parallelism">Hybrid (Model and Data) Parallelism</a>    <ul>
      <li><a href="#fully-sharded-data-parallel-fsdp" id="markdown-toc-fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</a>        <ul>
          <li><a href="#key-features-of-fsdp" id="markdown-toc-key-features-of-fsdp">Key Features of FSDP</a></li>
          <li><a href="#technical-details" id="markdown-toc-technical-details">Technical Details</a></li>
          <li><a href="#code-sample-2" id="markdown-toc-code-sample-2">Code Sample</a></li>
          <li><a href="#explanation-of-the-code-2" id="markdown-toc-explanation-of-the-code-2">Explanation of the Code</a></li>
          <li><a href="#benefits-of-fsdp" id="markdown-toc-benefits-of-fsdp">Benefits of FSDP</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#tensor-parallelism-1" id="markdown-toc-tensor-parallelism-1">Tensor Parallelism</a>    <ul>
      <li><a href="#concept-2" id="markdown-toc-concept-2">Concept</a></li>
      <li><a href="#mechanism-2" id="markdown-toc-mechanism-2">Mechanism</a></li>
      <li><a href="#types-of-tensor-parallelism" id="markdown-toc-types-of-tensor-parallelism">Types of Tensor Parallelism</a></li>
      <li><a href="#pros-and-cons-2" id="markdown-toc-pros-and-cons-2">Pros and Cons</a></li>
      <li><a href="#use-cases-2" id="markdown-toc-use-cases-2">Use Cases</a></li>
      <li><a href="#implementation-in-pytorch-2" id="markdown-toc-implementation-in-pytorch-2">Implementation in PyTorch</a></li>
      <li><a href="#conclusion-2" id="markdown-toc-conclusion-2">Conclusion</a></li>
      <li><a href="#summary-3" id="markdown-toc-summary-3">Summary</a></li>
    </ul>
  </li>
  <li><a href="#pipeline-parallelism-1" id="markdown-toc-pipeline-parallelism-1">Pipeline Parallelism</a>    <ul>
      <li><a href="#concept-3" id="markdown-toc-concept-3">Concept</a></li>
      <li><a href="#mechanism-3" id="markdown-toc-mechanism-3">Mechanism</a></li>
      <li><a href="#types-of-pipeline-parallelism" id="markdown-toc-types-of-pipeline-parallelism">Types of Pipeline Parallelism</a></li>
      <li><a href="#pros-and-cons-3" id="markdown-toc-pros-and-cons-3">Pros and Cons</a></li>
      <li><a href="#use-cases-3" id="markdown-toc-use-cases-3">Use Cases</a></li>
      <li><a href="#implementation-in-pytorch-3" id="markdown-toc-implementation-in-pytorch-3">Implementation in PyTorch</a></li>
      <li><a href="#conclusion-3" id="markdown-toc-conclusion-3">Conclusion</a></li>
      <li><a href="#summary-4" id="markdown-toc-summary-4">Summary</a></li>
      <li><a href="#deepspeed" id="markdown-toc-deepspeed">DeepSpeed</a>        <ul>
          <li><a href="#key-features-of-deepspeed" id="markdown-toc-key-features-of-deepspeed">Key Features of DeepSpeed</a></li>
          <li><a href="#technical-details-1" id="markdown-toc-technical-details-1">Technical Details</a></li>
          <li><a href="#code-sample-3" id="markdown-toc-code-sample-3">Code Sample</a></li>
          <li><a href="#explanation-of-the-code-3" id="markdown-toc-explanation-of-the-code-3">Explanation of the Code</a></li>
          <li><a href="#benefits-of-deepspeed" id="markdown-toc-benefits-of-deepspeed">Benefits of DeepSpeed</a></li>
        </ul>
      </li>
      <li><a href="#deepspeed-zero" id="markdown-toc-deepspeed-zero">DeepSpeed ZeRO</a>        <ul>
          <li><a href="#key-features-of-deepspeed-zero" id="markdown-toc-key-features-of-deepspeed-zero">Key Features of DeepSpeed ZeRO</a></li>
          <li><a href="#technical-details-2" id="markdown-toc-technical-details-2">Technical Details</a></li>
          <li><a href="#benefits" id="markdown-toc-benefits">Benefits</a></li>
          <li><a href="#code-sample-4" id="markdown-toc-code-sample-4">Code Sample</a></li>
          <li><a href="#explanation-of-the-code-4" id="markdown-toc-explanation-of-the-code-4">Explanation of the Code</a></li>
          <li><a href="#comparison-of-zero-stages" id="markdown-toc-comparison-of-zero-stages">Comparison of ZeRO Stages</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#comparative-analysis-dp-ddp-fsdp-deepspeed-and-deepspeed-zero" id="markdown-toc-comparative-analysis-dp-ddp-fsdp-deepspeed-and-deepspeed-zero">Comparative Analysis: DP, DDP, FSDP, DeepSpeed, and DeepSpeed ZeRO</a>    <ul>
      <li><a href="#data-parallel-dp" id="markdown-toc-data-parallel-dp">Data Parallel (DP)</a>        <ul>
          <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
          <li><a href="#pros" id="markdown-toc-pros">Pros</a></li>
          <li><a href="#cons" id="markdown-toc-cons">Cons</a></li>
          <li><a href="#code-sample-5" id="markdown-toc-code-sample-5">Code Sample</a></li>
        </ul>
      </li>
      <li><a href="#distributed-data-parallel-ddp-1" id="markdown-toc-distributed-data-parallel-ddp-1">Distributed Data Parallel (DDP)</a>        <ul>
          <li><a href="#overview-1" id="markdown-toc-overview-1">Overview</a></li>
          <li><a href="#pros-1" id="markdown-toc-pros-1">Pros</a></li>
          <li><a href="#cons-1" id="markdown-toc-cons-1">Cons</a></li>
          <li><a href="#code-sample-6" id="markdown-toc-code-sample-6">Code Sample</a></li>
        </ul>
      </li>
      <li><a href="#fully-sharded-data-parallel-fsdp-1" id="markdown-toc-fully-sharded-data-parallel-fsdp-1">Fully Sharded Data Parallel (FSDP)</a>        <ul>
          <li><a href="#overview-2" id="markdown-toc-overview-2">Overview</a></li>
          <li><a href="#pros-2" id="markdown-toc-pros-2">Pros</a></li>
          <li><a href="#cons-2" id="markdown-toc-cons-2">Cons</a></li>
          <li><a href="#code-sample-7" id="markdown-toc-code-sample-7">Code Sample</a></li>
        </ul>
      </li>
      <li><a href="#deepspeed-1" id="markdown-toc-deepspeed-1">DeepSpeed</a>        <ul>
          <li><a href="#overview-3" id="markdown-toc-overview-3">Overview</a></li>
          <li><a href="#pros-3" id="markdown-toc-pros-3">Pros</a></li>
          <li><a href="#cons-3" id="markdown-toc-cons-3">Cons</a></li>
          <li><a href="#code-sample-8" id="markdown-toc-code-sample-8">Code Sample</a></li>
        </ul>
      </li>
      <li><a href="#deepspeed-zero-1" id="markdown-toc-deepspeed-zero-1">DeepSpeed ZeRO</a>        <ul>
          <li><a href="#overview-4" id="markdown-toc-overview-4">Overview</a></li>
          <li><a href="#pros-4" id="markdown-toc-pros-4">Pros</a></li>
          <li><a href="#cons-4" id="markdown-toc-cons-4">Cons</a></li>
          <li><a href="#code-sample-9" id="markdown-toc-code-sample-9">Code Sample</a></li>
        </ul>
      </li>
      <li><a href="#comparative-summary" id="markdown-toc-comparative-summary">Comparative Summary</a></li>
      <li><a href="#use-cases-4" id="markdown-toc-use-cases-4">Use Cases</a></li>
    </ul>
  </li>
  <li><a href="#further-reading" id="markdown-toc-further-reading">Further Reading</a>    <ul>
      <li><a href="#the-ultra-scale-playbook-training-llms-on-gpu-clusters" id="markdown-toc-the-ultra-scale-playbook-training-llms-on-gpu-clusters">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
      <li><a href="#how-to-scale-your-model" id="markdown-toc-how-to-scale-your-model">How to Scale Your Model</a></li>
      <li><a href="#making-deep-learning-go-brrrr-from-first-principles" id="markdown-toc-making-deep-learning-go-brrrr-from-first-principles">Making Deep Learning Go Brrrr from First Principles</a></li>
    </ul>
  </li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="background">Background</h2>

<ul>
  <li>Distributed training parallelism is crucial for efficiently training large-scale deep learning models that require extensive computational resources. This approach leverages multiple GPUs or machines to perform computations in parallel, significantly reducing training time and enabling the handling of larger datasets and models.</li>
  <li>There are four main strategies for parallelism in distributed training: model, data, pipeline, and tensor parallelism. Each has its own mechanisms, advantages, and challenges, and understanding them is essential for optimizing training performance in different scenarios.</li>
</ul>

<h2 id="overview-types-of-parallelism">Overview: Types of Parallelism</h2>

<h3 id="model-parallelism">Model Parallelism</h3>

<ul>
  <li>Model parallelism is a strategy for distributing the computation of a deep learning model across multiple GPUs or other computing devices. This approach is particularly beneficial when the model is too large to fit into the memory of a single GPU. Instead of duplicating the entire model on each GPU (as in data parallelism), different parts of the model are placed on different GPUs, allowing for the training of very large models.</li>
</ul>

<h4 id="concept">Concept</h4>

<ul>
  <li>In model parallelism, the model is partitioned such that each GPU is responsible for a subset of the model parameters. During the forward pass, data flows through these partitions sequentially, with intermediate results transferred between GPUs as needed. The backward pass similarly computes gradients for each partition, which are then used to update the parameters locally on each GPU.</li>
</ul>

<h4 id="mechanism">Mechanism</h4>

<ol>
  <li><strong>Model Partitioning</strong>:
    <ul>
      <li>The model is divided into distinct segments, each assigned to a different GPU. For example, in a neural network, different layers or groups of layers might be placed on different GPUs.</li>
    </ul>
  </li>
  <li><strong>Forward Pass</strong>:
    <ul>
      <li>Input data is fed into the first segment of the model on the first GPU. The output of this segment is transferred to the next GPU, which processes the next segment of the model, and so on until the final segment.</li>
    </ul>
  </li>
  <li><strong>Backward Pass</strong>:
    <ul>
      <li>During backpropagation, gradients are computed in the reverse order. Each GPU computes the gradients for its segment and passes the necessary information back to the previous GPU to continue the gradient computation.</li>
    </ul>
  </li>
  <li><strong>Parameter Update</strong>:
    <ul>
      <li>Each GPU updates the parameters of its segment locally. If needed, synchronization steps can ensure consistency across GPUs.</li>
    </ul>
  </li>
</ol>

<h4 id="pros-and-cons">Pros and Cons</h4>

<ul>
  <li><strong>Pros</strong>:
    <ul>
      <li><strong>Memory Efficiency</strong>: Allows for training models that are too large to fit into the memory of a single GPU by distributing the model’s parameters and intermediate activations across multiple GPUs.</li>
      <li><strong>Scalability</strong>: Enables the use of large-scale models in research and production, leveraging multiple GPUs to handle complex computations.</li>
      <li><strong>Resource Utilization</strong>: Can optimize the utilization of available computational resources by balancing the load across multiple GPUs.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li><strong>Complexity</strong>: Implementing model parallelism can be challenging due to the need to carefully manage data transfers and dependencies between different parts of the model.</li>
      <li><strong>Communication Overhead</strong>: Transferring intermediate results between GPUs introduces latency, which can become a bottleneck, especially if the network bandwidth is limited.</li>
      <li><strong>Synchronization Issues</strong>: Ensuring that all parts of the model remain synchronized during training can be difficult, particularly in distributed environments.</li>
    </ul>
  </li>
</ul>

<h4 id="use-cases">Use Cases</h4>

<ul>
  <li>Model parallelism is especially useful in scenarios where the model size exceeds the memory capacity of a single GPU. Some common use cases include:
    <ul>
      <li><strong>Large Language Models</strong>: Models like GPT-3, which have billions of parameters, often require model parallelism to be trained effectively.</li>
      <li><strong>Deep Convolutional Networks</strong>: Very deep networks with many layers can benefit from model parallelism by distributing layers across multiple GPUs.</li>
      <li><strong>3D Convolutional Networks</strong>: Used in applications like video processing and 3D object recognition, where the model size can be substantial.</li>
    </ul>
  </li>
</ul>

<h4 id="implementation-in-pytorch">Implementation in PyTorch</h4>

<ul>
  <li>In PyTorch, model parallelism can be implemented by manually assigning different parts of the model to different GPUs. Here’s a basic example:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code0"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code0"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Define a simple model with two linear layers
</span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>  <span class="c1"># Place on GPU 0
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>  <span class="c1"># Place on GPU 1
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>  <span class="c1"># Move intermediate output to GPU 1
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Initialize model, loss, and optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Dummy input and target
</span><span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>

<span class="c1"># Forward pass
</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># Backward pass
</span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Optimizer step
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li>In this example, the model has two linear layers, each placed on a different GPU. During the forward pass, the input is moved between GPUs as needed. The backward pass and optimizer step handle the parameter updates for each layer separately.</li>
</ul>

<h4 id="conclusion">Conclusion</h4>

<ul>
  <li>Model parallelism is a powerful technique for training large models that cannot fit into the memory of a single GPU. While it introduces complexity in terms of implementation and communication overhead, it enables the training of state-of-the-art models in various fields, from natural language processing to computer vision. Properly balancing the model segments and managing the data flow are key challenges, but with frameworks like PyTorch providing the necessary tools, model parallelism can be effectively leveraged to push the boundaries of deep learning research and applications.</li>
</ul>

<h4 id="summary">Summary</h4>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Model parallelism involves splitting the model itself across multiple GPUs. Different layers or parts of the model are assigned to different GPUs. This is particularly useful when a model is too large to fit into the memory of a single GPU.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>Data flows sequentially through the model parts across different GPUs.</li>
      <li>Each GPU computes its assigned part of the forward and backward passes.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Enables training of very large models that do not fit into a single GPU’s memory.</li>
      <li>Can optimize memory usage and computational load distribution.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>More complex to implement due to the need to manage dependencies and data transfers between GPUs.</li>
      <li>May introduce latency due to data transfer between GPUs.</li>
    </ul>
  </li>
</ul>

<h3 id="data-parallelism">Data Parallelism</h3>

<ul>
  <li>Data parallelism is a technique in parallel computing where data is distributed across multiple processors or machines, and each processor performs the same operation on different parts of the data simultaneously. This approach is particularly effective in scenarios where the same set of operations need to be applied to large datasets.</li>
</ul>

<h4 id="concept-1">Concept</h4>

<ul>
  <li>The fundamental concept of data parallelism is to break down a large dataset into smaller chunks and process these chunks in parallel. Each processor (or core) receives a portion of the data and applies the same computational operations. After processing, the results are aggregated to form the final output.</li>
</ul>

<h4 id="mechanism-1">Mechanism</h4>

<ol>
  <li>
    <p><strong>Data Splitting</strong>: The dataset is divided into smaller, equally-sized chunks. Each chunk is assigned to a different processor or computing unit.</p>
  </li>
  <li>
    <p><strong>Parallel Execution</strong>: Each processor performs the same operation on its assigned chunk of data. This step happens concurrently, leveraging the parallel nature of the computing environment.</p>
  </li>
  <li>
    <p><strong>Synchronization</strong>: After processing, the results from each processor are synchronized. This step may involve combining or aggregating the processed data to produce the final result.</p>
  </li>
  <li>
    <p><strong>Result Aggregation</strong>: The partial results from each processor are combined to form the complete output. This step may require communication between processors to gather and combine the data.</p>
  </li>
</ol>

<h4 id="pros-and-cons-1">Pros and Cons</h4>

<p><strong>Pros:</strong></p>
<ul>
  <li><strong>Increased Efficiency</strong>: By processing data in parallel, the overall computational time is reduced significantly.</li>
  <li><strong>Scalability</strong>: Data parallelism can scale with the number of processors, making it suitable for large-scale data processing tasks.</li>
  <li><strong>Resource Utilization</strong>: Efficiently utilizes available computing resources by distributing the workload.</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li><strong>Communication Overhead</strong>: Synchronizing and aggregating results can introduce overhead, especially in distributed systems.</li>
  <li><strong>Complexity</strong>: Implementing data parallelism can be complex, requiring careful management of data distribution and synchronization.</li>
  <li><strong>Limited by Data Size</strong>: The size of the dataset must be sufficiently large to justify the overhead of parallelization.</li>
</ul>

<h4 id="use-cases-1">Use Cases</h4>

<ul>
  <li><strong>Deep Learning</strong>: Training large neural networks where massive datasets are divided and processed in parallel across multiple GPUs.</li>
  <li><strong>Big Data Analytics</strong>: Processing large volumes of data in parallel to extract insights and perform analytics.</li>
  <li><strong>Scientific Simulations</strong>: Running simulations that require processing large datasets, such as weather forecasting or molecular dynamics.</li>
</ul>

<h4 id="implementation-in-pytorch-1">Implementation in PyTorch</h4>

<ul>
  <li>PyTorch provides several utilities to implement data parallelism easily:</li>
</ul>

<ol>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code></strong>: A simple way to wrap a model to enable parallel processing across multiple GPUs.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code1"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code1"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Define the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">MyModel</span><span class="p">())</span>

<span class="c1"># Move the model to GPU
</span><span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>

<span class="c1"># Define the optimizer and loss function
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Distributed Data Parallel (DDP)</strong>: For more advanced and scalable parallelism, PyTorch offers <code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code>. This approach is more efficient and scalable for large-scale applications.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code2"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code2"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="c1"># Initialize the process group
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"gloo"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># Create model and move it to GPU with rank
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>

    <span class="c1"># Define loss function and optimizer
</span>    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="c1"># Training loop
</span>    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Clean up
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
</ol>

<h4 id="conclusion-1">Conclusion</h4>

<ul>
  <li>Data parallelism is a powerful technique to enhance computational efficiency by distributing data across multiple processors or machines. It is highly beneficial in scenarios involving large datasets and complex computations, such as deep learning and big data analytics. While data parallelism offers significant advantages in terms of speed and scalability, it also introduces challenges like communication overhead and implementation complexity. PyTorch provides robust tools for implementing data parallelism, making it accessible for both simple and advanced use cases.</li>
</ul>

<h4 id="summary-1">Summary</h4>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Data parallelism involves splitting the dataset into smaller batches and distributing these batches across multiple GPUs. Each GPU holds a complete copy of the model and processes a subset of the data independently. After computation, the results (such as gradients) are synchronized and aggregated to update the model parameters.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>Each GPU computes the forward and backward passes for its portion of the data.</li>
      <li>Gradients are averaged across all GPUs.</li>
      <li>Model parameters are updated synchronously to ensure all GPUs have identical parameters.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Easy to implement and highly effective for many types of models.</li>
      <li>Scales well with the number of GPUs.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Limited by the memory of a single GPU, as each GPU must hold the entire model.</li>
      <li>Communication overhead during synchronization can become a bottleneck.</li>
    </ul>
  </li>
</ul>

<h3 id="pipeline-parallelism">Pipeline Parallelism</h3>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Pipeline parallelism is a specific form of model parallelism where different stages of the model are distributed across multiple GPUs. The model is divided into stages, and data is processed in a pipeline fashion, allowing different GPUs to work on different mini-batches of data simultaneously.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>The model is split into several stages, each assigned to a different GPU.</li>
      <li>Mini-batches are divided into micro-batches and fed sequentially through the stages.</li>
    </ul>
  </li>
  <li>
    <p>While one GPU is processing a micro-batch, another GPU can start processing the next micro-batch.</p>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Reduces idle time by overlapping computation and communication.</li>
      <li>Effective for deep models with many sequential layers.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Introduces pipeline bubbles (idle time) at the start and end of the pipeline.</li>
      <li>Requires careful tuning to balance the stages and manage the pipeline efficiently.</li>
    </ul>
  </li>
</ul>

<h3 id="tensor-parallelism">Tensor Parallelism</h3>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Tensor parallelism involves splitting individual tensors (such as weight matrices) across multiple GPUs. This approach allows large tensors to be distributed and processed in parallel, effectively balancing the computation load.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>Tensors are divided along one or more dimensions, and each GPU holds a slice of the tensor.</li>
      <li>During computation, operations are performed on these slices in parallel, and the results are combined as needed.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Can handle extremely large tensors that exceed the memory capacity of a single GPU.</li>
      <li>Distributes computational load more evenly across GPUs.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Requires sophisticated implementation to manage tensor slicing and the corresponding operations.</li>
      <li>Communication overhead for combining the results can impact performance.</li>
    </ul>
  </li>
</ul>

<h3 id="choosing-the-right-strategy-data-vs-model-vs-pipeline-vs-tensor-parallelism">Choosing the Right Strategy: Data V/s Model V/s Pipeline V/s Tensor Parallelism</h3>

<ul>
  <li>
    <p>Each parallelism paradigm offers unique advantages and is suitable for different scenarios:</p>

    <ul>
      <li><strong>Data Parallelism</strong> is simple and effective for many models but limited by GPU memory constraints.</li>
      <li><strong>Model Parallelism</strong> allows for training very large models but requires careful management of data flow between GPUs.</li>
      <li><strong>Pipeline Parallelism</strong> optimizes utilization by overlapping computations but introduces complexity in balancing the pipeline.</li>
      <li><strong>Tensor Parallelism</strong> handles extremely large tensors efficiently but involves intricate implementation and communication overhead.</li>
    </ul>
  </li>
  <li>Choosing the right parallelism strategy depends on the specific requirements of the model, the hardware architecture, and the desired performance characteristics.</li>
  <li>The infographic below (credits to <a href="https://www.linkedin.com/in/sebastianraschka/">Sebastian Raschka</a>) offers a quick overview of the four different paradigms for multi-GPU training, such as data parallelism, model parallelism, pipeline parallelism, and tensor parallelism.</li>
</ul>

<p><img src="/primers/ai/assets/multi-gpu-parallelism/multi-gpu-parallelism.jpeg" alt=""></p>

<h2 id="data-parallelism-1">Data Parallelism</h2>

<ul>
  <li>Data parallelism involves splitting the data across multiple devices (such as GPUs). Each device processes a different chunk of the data, but they all use the same model to perform computations. After processing, the results are combined to update the model. This is accomplished by synchronizing the gradients during the backward pass to ensure consistent updates.</li>
</ul>

<h3 id="dataparallel-dp">DataParallel (DP)</h3>

<ul>
  <li>Data parallelism in PyTorch refers to the process of distributing data across multiple GPUs to perform parallel computations. This approach helps to accelerate training by utilizing the computational power of multiple GPUs simultaneously. The primary mechanism for achieving data parallelism in PyTorch is through the <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code> module.</li>
  <li>Here’s a detailed explanation of how <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code> works and a small code sample to demonstrate its usage.</li>
</ul>

<h4 id="how-dp-works">How DP Works</h4>

<ol>
  <li><strong>Model Replication:</strong> The model is replicated across multiple GPUs, with each replica handling a portion of the input data.</li>
  <li><strong>Data Splitting:</strong> The input data is split into smaller batches, and each smaller batch is sent to a different GPU.</li>
  <li><strong>Parallel Computation:</strong> Each GPU processes its portion of the data in parallel, performing forward and backward passes independently.</li>
  <li><strong>Gradient Aggregation:</strong> Gradients from all GPUs are gathered and averaged.</li>
  <li><strong>Model Update:</strong> The averaged gradients are used to update the model parameters.</li>
</ol>

<h4 id="key-steps-in-using-dp">Key Steps in Using DP</h4>

<ol>
  <li><strong>Model Definition:</strong> Define the model as usual.</li>
  <li><strong>Wrap with DataParallel:</strong> Use <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code> to wrap the model.</li>
  <li><strong>Data Loading:</strong> Use <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code> to load the dataset.</li>
  <li><strong>Forward and Backward Pass:</strong> Perform the forward and backward pass in the same way as with a single GPU, but now the computations will be distributed across multiple GPUs.</li>
</ol>

<h4 id="code-sample">Code Sample</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code3"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code3"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="c1"># Define a simple CNN model
</span><span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Check if multiple GPUs are available
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Instantiate the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">()</span>

<span class="c1"># Wrap the model with DataParallel
</span><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s"> GPUs"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Move the model to the appropriate device
</span><span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define a loss function and optimizer
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Data loading and preprocessing
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
<span class="p">])</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>    <span class="c1"># Print every 100 mini-batches
</span>            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training finished.'</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="explanation-of-the-code">Explanation of the Code</h4>

<ol>
  <li><strong>Model Definition:</strong> A simple CNN model is defined.</li>
  <li><strong>Device Check:</strong> The code checks if CUDA is available and sets the appropriate device.</li>
  <li><strong>Model Wrapping:</strong> The model is wrapped with <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code> if multiple GPUs are available.</li>
  <li><strong>Data Loading:</strong> The MNIST dataset is loaded and transformed.</li>
  <li><strong>Training Loop:</strong> The model is trained in the usual way, but the computations are distributed across multiple GPUs if available.</li>
</ol>

<h3 id="distributed-data-parallel-ddp">Distributed Data Parallel (DDP)</h3>

<ul>
  <li>By using <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code>, you can easily scale your model training to utilize multiple GPUs, which can significantly speed up the training process.</li>
  <li>Distributed Data Parallel (DDP) in PyTorch is a more advanced and efficient method for parallelizing training across multiple GPUs, potentially across multiple nodes. Unlike <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code>, which uses a single process to manage all devices, DDP creates a separate process for each GPU, allowing for better scalability and reduced inter-GPU communication overhead. This approach is particularly beneficial for large-scale deep learning tasks.</li>
</ul>

<h4 id="key-features-of-ddp">Key Features of DDP</h4>

<ol>
  <li><strong>Separate Processes:</strong> Each GPU is managed by a separate process, which helps in minimizing the bottleneck caused by the Global Interpreter Lock (GIL) in Python.</li>
  <li><strong>Gradient Synchronization:</strong> Gradients are synchronized across processes using an all-reduce operation, ensuring that all processes have consistent model parameters.</li>
  <li><strong>Scalability:</strong> Better performance and scalability compared to <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code>, especially for large-scale training.</li>
</ol>

<h4 id="technical-steps-to-use-ddp">Technical Steps to Use DDP</h4>

<ol>
  <li><strong>Initialize the Process Group:</strong> Set up the communication backend and initialize the process group.</li>
  <li><strong>Create DDP Model:</strong> Wrap the model with <code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code>.</li>
  <li><strong>Set Up Distributed Sampler:</strong> Use a distributed sampler to ensure each process gets a unique subset of the dataset.</li>
  <li><strong>Configure Data Loaders:</strong> Set up data loaders with the distributed sampler.</li>
  <li><strong>Launch Training Processes:</strong> Use a launcher utility to spawn multiple processes for training.</li>
</ol>

<h4 id="code-sample-1">Code Sample</h4>

<ul>
  <li>Here’s a basic example demonstrating how to set up and use DDP in PyTorch.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code4"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code4"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'localhost'</span>
    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'12355'</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
            
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">, Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="n">cleanup</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="explanation-of-the-code-1">Explanation of the Code</h4>

<ol>
  <li><strong>Setup and Cleanup:</strong> Functions to initialize and destroy the process group.</li>
  <li><strong>Model Definition:</strong> A simple CNN model is defined.</li>
  <li><strong>Train Function:</strong>
    <ul>
      <li>Initializes the process group.</li>
      <li>Creates a distributed sampler and data loader.</li>
      <li>Wraps the model with <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>.</li>
      <li>Runs the training loop.</li>
    </ul>
  </li>
  <li><strong>Main Function:</strong> Uses <code class="language-plaintext highlighter-rouge">mp.spawn</code> to launch multiple processes for training.</li>
</ol>

<ul>
  <li>This setup ensures that each GPU gets its own process and subset of the data, with gradients synchronized across processes, allowing for efficient parallel training.</li>
</ul>

<h2 id="model-parallelism-1">Model Parallelism</h2>

<ul>
  <li>Model parallelism involves splitting the neural network model across multiple devices (such as GPUs) so that each device is responsible for a part of the model. This approach is particularly useful when the model is too large to fit into the memory of a single device. By distributing different parts of the model across multiple GPUs, model parallelism enables the training of very large models.</li>
  <li><strong>Types of Model Parallelism</strong>:
    <ol>
      <li><strong>Layer-wise Parallelism</strong></li>
      <li><strong>Tensor-wise Parallelism</strong></li>
      <li><strong>Operator-wise Parallelism</strong></li>
    </ol>
  </li>
</ul>

<h3 id="layer-wise-parallelism">Layer-wise Parallelism</h3>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Different layers or groups of layers of the model are assigned to different GPUs. For example, in a deep neural network, you might place the first few layers on one GPU, the next few layers on another GPU, and so on.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>During the forward pass, the output of one GPU is transferred to the next GPU for further processing.</li>
      <li>During the backward pass, gradients are computed in the reverse order, with each GPU handling the gradients for its assigned layers.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Simple to implement for sequential models.</li>
      <li>Good for models with distinct layers or blocks.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Can lead to inefficient GPU utilization if layers have varying computational loads.</li>
      <li>Communication overhead can be significant if layers produce large outputs.</li>
    </ul>
  </li>
  <li><strong>Example</strong>:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code5"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code5"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LayerwiseModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerwiseModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="tensor-wise-parallelism">Tensor-wise Parallelism</h3>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Individual tensors, such as weight matrices or activations, are split across multiple GPUs. This approach involves slicing tensors along one or more dimensions and distributing these slices across different GPUs.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>During computation, each GPU processes its slice of the tensor in parallel.</li>
      <li>Intermediate results are combined as needed to complete the computation.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Efficiently utilizes GPU memory by distributing large tensors.</li>
      <li>Balances computational load more evenly across GPUs.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Requires sophisticated implementation to handle tensor slicing and combining.</li>
      <li>High communication overhead for combining intermediate results.</li>
    </ul>
  </li>
  <li><strong>Example</strong>:
    <ul>
      <li>Tensor-wise parallelism is often implemented at a lower level within deep learning frameworks and may not be directly visible in user-defined model code. It’s commonly used in distributed training libraries like Megatron-LM for large-scale models.</li>
    </ul>
  </li>
</ul>

<h3 id="operator-wise-parallelism">Operator-wise Parallelism</h3>

<ul>
  <li><strong>Concept</strong>:
    <ul>
      <li>Different operations (or parts of operations) within a layer are assigned to different GPUs. This fine-grained parallelism involves splitting the computation of individual operations across multiple devices.</li>
    </ul>
  </li>
  <li><strong>Mechanism</strong>:
    <ul>
      <li>Operations within a layer are divided, with each GPU performing a portion of the computation.</li>
      <li>Results from each GPU are combined to produce the final output of the operation.</li>
    </ul>
  </li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Can achieve high parallel efficiency by leveraging all available GPUs.</li>
      <li>Useful for very large operations that can be broken down into smaller, parallelizable tasks.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>Complex to implement and manage.</li>
      <li>Communication overhead can be significant, particularly for operations with large intermediate results.</li>
    </ul>
  </li>
  <li><strong>Example</strong>:
    <ul>
      <li>Similar to tensor-wise parallelism, operator-wise parallelism is usually implemented within deep learning libraries and frameworks, optimizing specific operations like matrix multiplications.</li>
    </ul>
  </li>
</ul>

<h3 id="summary-2">Summary</h3>

<ul>
  <li>
    <p>Model parallelism can be categorized into three main types, each suited to different scenarios and offering unique advantages and challenges:</p>

    <ul>
      <li><strong>Layer-wise Parallelism</strong>: Simple to implement and effective for models with clear layer separations but may lead to imbalanced GPU utilization and communication overhead.</li>
      <li><strong>Tensor-wise Parallelism</strong>: Efficiently uses GPU memory and balances computational load by splitting large tensors but requires sophisticated slicing and communication management.</li>
      <li><strong>Operator-wise Parallelism</strong>: Maximizes parallel efficiency by splitting operations within layers but is complex to implement and can incur significant communication overhead.</li>
    </ul>
  </li>
  <li>
    <p>Choosing the appropriate type of model parallelism depends on the model architecture, the available hardware, and the specific requirements of the training task. Advanced deep learning frameworks and libraries provide tools and functionalities to facilitate the implementation of these parallelism strategies, enabling the training of large and complex models.</p>
  </li>
</ul>

<h3 id="comparative-analysis-types-of-model-parallelism">Comparative Analysis: Types of Model Parallelism</h3>

<h4 id="criteria-for-comparison">Criteria for Comparison</h4>
<ol>
  <li><strong>Implementation Complexity</strong></li>
  <li><strong>GPU Utilization</strong></li>
  <li><strong>Communication Overhead</strong></li>
  <li><strong>Scalability</strong></li>
  <li><strong>Suitable Use Cases</strong></li>
</ol>

<h4 id="layer-wise-parallelism-1">Layer-wise Parallelism</h4>

<ul>
  <li><strong>Implementation Complexity</strong>:
    <ul>
      <li><strong>Low to Medium</strong>: It’s relatively straightforward to assign different layers or groups of layers to different GPUs. This method is particularly simple for models with a clear sequential structure.</li>
    </ul>
  </li>
  <li><strong>GPU Utilization</strong>:
    <ul>
      <li><strong>Variable</strong>: The utilization can be uneven if different layers have varying computational loads. Some GPUs may be underutilized while waiting for others to complete their tasks.</li>
    </ul>
  </li>
  <li><strong>Communication Overhead</strong>:
    <ul>
      <li><strong>High</strong>: Significant overhead can arise from transferring large intermediate results between GPUs. This can be a bottleneck, especially with deep models producing large outputs.</li>
    </ul>
  </li>
  <li><strong>Scalability</strong>:
    <ul>
      <li><strong>Moderate</strong>: Scalability is limited by the number of layers that can be effectively divided among GPUs. Deep models with many layers can benefit, but shallow models may not.</li>
    </ul>
  </li>
  <li><strong>Suitable Use Cases</strong>:
    <ul>
      <li>Sequential models with distinct layers, such as feedforward neural networks or deep convolutional networks.</li>
    </ul>
  </li>
</ul>

<h4 id="tensor-wise-parallelism-1">Tensor-wise Parallelism</h4>

<ul>
  <li><strong>Implementation Complexity</strong>:
    <ul>
      <li><strong>High</strong>: Requires advanced techniques to split and manage tensors across multiple GPUs. It involves slicing tensors along specific dimensions and ensuring correct aggregation of results.</li>
    </ul>
  </li>
  <li><strong>GPU Utilization</strong>:
    <ul>
      <li><strong>High</strong>: Distributes large tensors effectively across multiple GPUs, leading to balanced utilization. Each GPU handles a portion of the tensor, optimizing memory and computational resources.</li>
    </ul>
  </li>
  <li><strong>Communication Overhead</strong>:
    <ul>
      <li><strong>Moderate to High</strong>: While each GPU processes its tensor slice independently, the results must be combined, leading to communication overhead. However, this is often more manageable compared to layer-wise parallelism.</li>
    </ul>
  </li>
  <li><strong>Scalability</strong>:
    <ul>
      <li><strong>High</strong>: Scales well with the number of GPUs, as large tensors can be divided into more slices to distribute the load evenly.</li>
    </ul>
  </li>
  <li><strong>Suitable Use Cases</strong>:
    <ul>
      <li>Large models with extremely large tensors, such as language models with massive weight matrices.</li>
    </ul>
  </li>
</ul>

<h4 id="operator-wise-parallelism-1">Operator-wise Parallelism</h4>

<ul>
  <li><strong>Implementation Complexity</strong>:
    <ul>
      <li><strong>Very High</strong>: This method is the most complex to implement. It requires decomposing individual operations within a layer and distributing these subtasks across multiple GPUs. Fine-grained synchronization is necessary.</li>
    </ul>
  </li>
  <li><strong>GPU Utilization</strong>:
    <ul>
      <li><strong>Very High</strong>: Offers optimal utilization by leveraging parallelism within operations. Each GPU can work on different parts of the same operation concurrently, maximizing efficiency.</li>
    </ul>
  </li>
  <li><strong>Communication Overhead</strong>:
    <ul>
      <li><strong>High</strong>: The fine-grained nature of this parallelism results in frequent communication between GPUs, which can be a significant overhead if not managed carefully.</li>
    </ul>
  </li>
  <li><strong>Scalability</strong>:
    <ul>
      <li><strong>Very High</strong>: Highly scalable as even small operations can be divided among GPUs, making it suitable for extremely large models with complex operations.</li>
    </ul>
  </li>
  <li><strong>Suitable Use Cases</strong>:
    <ul>
      <li>Extremely large and complex models, such as those used in cutting-edge research in AI and deep learning, where maximizing computational efficiency is crucial.</li>
    </ul>
  </li>
</ul>

<h3 id="summary-table">Summary Table</h3>

<div align="center">
<table class="tg">
 <thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Criterion</strong></th>
<th class="tg-hcenter-valign-first"><strong>Layer-wise Parallelism</strong></th>
<th class="tg-hcenter-valign-first"><strong>Tensor-wise Parallelism</strong></th>
<th class="tg-hcenter-valign-second"><strong>Operator-wise Parallelism</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Implementation Complexity</td>
<td class="tg-tleft-valign-first">Low to Medium</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-second">Very High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">GPU Utilization</td>
<td class="tg-tleft-valign-first">Variable</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-second">Very High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Communication Overhead</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-first">Moderate to High</td>
<td class="tg-tleft-valign-second">High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Scalability</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-second">Very High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Suitable Use Cases</td>
<td class="tg-tleft-valign-first">Sequential models with distinct layers</td>
<td class="tg-tleft-valign-first">Large models with very large tensors</td>
<td class="tg-tleft-valign-second">Extremely large and complex models</td>
</tr>
</tbody>
</table>
</div>

<h3 id="choosing-the-right-type">Choosing the Right Type</h3>

<ul>
  <li><strong>Layer-wise Parallelism</strong>: Best suited for simpler models with clear, sequential layers where ease of implementation is a priority, and communication overhead can be tolerated.</li>
  <li><strong>Tensor-wise Parallelism</strong>: Ideal for models with very large tensors where balanced GPU utilization and memory efficiency are critical.</li>
  <li>
    <p><strong>Operator-wise Parallelism</strong>: Appropriate for the most demanding and complex models where maximizing computational efficiency and scalability is essential, despite the high implementation complexity.</p>
  </li>
  <li>Understanding these differences can help in selecting the most appropriate model parallelism strategy based on the specific requirements and constraints of the neural network model and the available hardware.</li>
</ul>

<h2 id="hybrid-model-and-data-parallelism">Hybrid (Model and Data) Parallelism</h2>

<h3 id="fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</h3>

<ul>
  <li>Fully Sharded Data Parallel (FSDP) is a feature in PyTorch that addresses the limitations of data parallelism by allowing more efficient utilization of memory and compute resources. FSDP shards (i.e., splits) both the model parameters and optimizer states across all available GPUs, significantly reducing memory usage and enabling the training of very large models that otherwise wouldn’t fit into GPU memory.</li>
  <li>
    <p>Fully Sharded Data Parallel (FSDP) is primarily a form of model parallelism. However, it incorporates elements of data parallelism as well. Here’s how it works:</p>

    <ol>
      <li>
        <p><strong>Model Parallelism</strong>: FSDP divides the model parameters across multiple GPUs. This means that each GPU only holds a shard (a portion) of the entire model. This is a key characteristic of model parallelism, where the model is split and distributed across different devices to manage memory more efficiently and to enable training of larger models that wouldn’t fit into a single device’s memory.</p>
      </li>
      <li>
        <p><strong>Data Parallelism</strong>: Within each shard, FSDP also performs data parallel training. This means that the data is split across the GPUs, and each GPU processes a different subset of the data. Gradients are computed locally on each GPU and then synchronized across GPUs to update the model parameters consistently.</p>
      </li>
    </ol>
  </li>
  <li>By combining these two approaches, FSDP aims to maximize the efficiency of both memory usage and computational resources. This hybrid approach allows for the training of very large models that would be infeasible with either pure data parallelism or pure model parallelism alone.</li>
</ul>

<h4 id="key-features-of-fsdp">Key Features of FSDP</h4>

<ol>
  <li><strong>Parameter Sharding:</strong> Each GPU holds only a shard of the full model parameters, reducing memory overhead.</li>
  <li><strong>Optimizer State Sharding:</strong> Similar to parameter sharding, the optimizer states are also sharded across GPUs.</li>
  <li><strong>Gradient Sharding:</strong> During backpropagation, gradients are sharded across GPUs, minimizing memory usage.</li>
  <li><strong>Efficient Communication:</strong> Uses collective communication to gather and reduce gradients across GPUs.</li>
</ol>

<h4 id="technical-details">Technical Details</h4>

<ul>
  <li><strong>Initialization:</strong> The process group is initialized for communication between GPUs.</li>
  <li><strong>Wrapping the Model:</strong> The model is wrapped with <code class="language-plaintext highlighter-rouge">torch.distributed.fsdp.FullyShardedDataParallel</code>.</li>
  <li><strong>Data Loading:</strong> Data loaders and samplers are configured to distribute data across GPUs.</li>
  <li><strong>Training Loop:</strong> The training loop is similar to standard PyTorch, but with the added benefit of memory efficiency and scalability.</li>
</ul>

<h4 id="code-sample-2">Code Sample</h4>

<ul>
  <li>Here’s a simple example demonstrating how to set up and use FSDP in PyTorch.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code6"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code6"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="n">wrap</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'localhost'</span>
    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'12355'</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">wrap</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># Wrap the model with FSDP
</span>    
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
            
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">, Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="n">cleanup</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="explanation-of-the-code-2">Explanation of the Code</h4>

<ol>
  <li><strong>Setup and Cleanup:</strong> Functions to initialize and destroy the process group for distributed training.</li>
  <li><strong>Model Definition:</strong> A simple CNN model.</li>
  <li><strong>Train Function:</strong>
    <ul>
      <li>Initializes the process group.</li>
      <li>Sets up a distributed sampler and data loader.</li>
      <li>Wraps the model with FSDP for efficient sharding.</li>
      <li>Runs the training loop.</li>
    </ul>
  </li>
  <li><strong>Main Function:</strong> Uses <code class="language-plaintext highlighter-rouge">mp.spawn</code> to launch multiple processes for distributed training.</li>
</ol>

<h4 id="benefits-of-fsdp">Benefits of FSDP</h4>

<ul>
  <li><strong>Memory Efficiency:</strong> By sharding parameters and optimizer states, FSDP significantly reduces memory overhead, allowing for the training of larger models.</li>
  <li><strong>Scalability:</strong> Efficient communication and sharding mechanisms enable scaling to multiple GPUs and nodes with minimal overhead.</li>
  <li>
    <p><strong>Ease of Use:</strong> Integrates seamlessly with PyTorch’s existing APIs and workflows, requiring minimal changes to existing code.</p>
  </li>
  <li>Overall, FSDP is a powerful tool for scaling deep learning models efficiently across multiple GPUs, providing both memory and computational benefits.</li>
</ul>

<h2 id="tensor-parallelism-1">Tensor Parallelism</h2>

<h3 id="concept-2">Concept</h3>

<ul>
  <li>Tensor parallelism is a technique used in training deep learning models to distribute the computational load of processing large tensors (multi-dimensional arrays of numbers) across multiple devices, such as GPUs. This allows for the efficient training of very large models that would otherwise be too large to fit into the memory of a single device. By splitting the tensors across devices, tensor parallelism enables faster computation and better utilization of hardware resources.</li>
</ul>

<h3 id="mechanism-2">Mechanism</h3>

<ul>
  <li>The mechanism of tensor parallelism involves dividing large tensors along certain dimensions and distributing these chunks across multiple GPUs. Each GPU performs computations on its subset of the data, and the results are then combined to form the final output. This can be done at various stages of the model, such as during the forward pass, backward pass, or during gradient updates.</li>
</ul>

<h3 id="types-of-tensor-parallelism">Types of Tensor Parallelism</h3>

<ol>
  <li>
    <p><strong>Model Parallelism</strong>: This involves splitting the model itself across different devices. Each device handles a different part of the model. This is often combined with tensor parallelism for efficient training.</p>
  </li>
  <li>
    <p><strong>Data Parallelism</strong>: Here, the entire model is replicated across different devices, and each device processes different mini-batches of the data. The results are then averaged or combined.</p>
  </li>
  <li>
    <p><strong>Pipeline Parallelism</strong>: The model is divided into sequential stages, and each stage is assigned to a different device. Data is passed through these stages in a pipeline fashion.</p>
  </li>
  <li>
    <p><strong>Tensor-Slicing Parallelism</strong>: Specifically focuses on slicing tensors along specific dimensions and distributing the slices across multiple devices. This can be done within layers of a neural network.</p>
  </li>
</ol>

<h3 id="pros-and-cons-2">Pros and Cons</h3>

<p><strong>Pros:</strong></p>

<ul>
  <li><strong>Scalability</strong>: Allows for the training of larger models by utilizing the combined memory of multiple devices.</li>
  <li><strong>Efficiency</strong>: Improves computational efficiency by parallelizing operations.</li>
  <li><strong>Flexibility</strong>: Can be combined with other parallelism strategies like data and pipeline parallelism for optimal performance.</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li><strong>Complexity</strong>: Increases the complexity of model implementation and debugging.</li>
  <li><strong>Communication Overhead</strong>: Requires efficient communication between devices, which can become a bottleneck.</li>
  <li><strong>Resource Management</strong>: Needs careful management of resources and synchronization.</li>
</ul>

<h3 id="use-cases-2">Use Cases</h3>

<ul>
  <li><strong>Large Language Models</strong>: Training models like GPT-3 and BERT that require vast amounts of computational power and memory.</li>
  <li><strong>Computer Vision</strong>: Handling high-resolution images and videos in models like CNNs.</li>
  <li><strong>Scientific Computing</strong>: Simulations and modeling in physics, chemistry, and other sciences that involve large-scale tensor computations.</li>
  <li><strong>Recommender Systems</strong>: Managing and processing large embeddings and user-item interaction matrices.</li>
</ul>

<h3 id="implementation-in-pytorch-2">Implementation in PyTorch</h3>

<ul>
  <li>
    <p>Implementing tensor parallelism in PyTorch involves the following steps:</p>

    <ol>
      <li><strong>Model Partitioning</strong>: Split the model layers or tensors across different devices.</li>
      <li><strong>Data Distribution</strong>: Distribute the input data across these devices.</li>
      <li><strong>Computation</strong>: Perform the necessary computations on each device.</li>
      <li><strong>Synchronization</strong>: Combine the results from different devices.</li>
    </ol>
  </li>
  <li>
    <p>PyTorch provides various utilities and functions to facilitate tensor parallelism, such as:</p>

    <ul>
      <li><strong>torch.distributed</strong>: A package that includes functionalities for distributed computing.</li>
      <li><strong>torch.nn.parallel.DistributedDataParallel</strong>: Wraps a model for multi-GPU parallelism.</li>
      <li><strong>RPC Framework</strong>: Allows for remote procedure calls and managing distributed model components.</li>
    </ul>
  </li>
  <li>
    <p>Example:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code7"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code7"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="c1"># Initialize the process group
</span><span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">'nccl'</span><span class="p">)</span>

<span class="c1"># Create model and move to GPU
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Define optimizer and loss function
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="conclusion-2">Conclusion</h3>

<ul>
  <li>Tensor parallelism is a powerful technique for training large-scale deep learning models by distributing tensor computations across multiple devices. While it introduces some complexity and requires efficient communication mechanisms, it enables the training of models that would be infeasible on a single device. Combining tensor parallelism with other forms of parallelism can further optimize performance and resource utilization.</li>
</ul>

<h3 id="summary-3">Summary</h3>

<ul>
  <li><strong>Concept</strong>: Tensor parallelism distributes large tensor computations across multiple devices to enable efficient training of large models.</li>
  <li><strong>Mechanism</strong>: Involves dividing tensors and distributing them across devices, performing computations, and combining results.</li>
  <li><strong>Types</strong>: Includes model parallelism, data parallelism, pipeline parallelism, and tensor-slicing parallelism.</li>
  <li><strong>Pros and Cons</strong>: Offers scalability and efficiency but introduces complexity and potential communication overhead.</li>
  <li><strong>Use Cases</strong>: Suitable for large language models, computer vision, scientific computing, and recommender systems.</li>
  <li><strong>Implementation in PyTorch</strong>: Uses PyTorch’s distributed computing utilities to partition models, distribute data, and synchronize results.</li>
  <li><strong>Conclusion</strong>: Tensor parallelism is essential for training very large models, enabling the use of multiple devices to overcome memory and computational limitations.</li>
</ul>

<h2 id="pipeline-parallelism-1">Pipeline Parallelism</h2>

<h3 id="concept-3">Concept</h3>

<ul>
  <li>Pipeline parallelism in deep learning refers to the technique of distributing the workload of training a deep learning model across multiple devices or processing units. Unlike data parallelism, where different batches of data are processed simultaneously by multiple replicas of a model, pipeline parallelism splits the model itself into different stages and assigns each stage to a different device. This allows for the concurrent execution of different parts of the model on different devices, thus improving training efficiency and reducing time-to-train.</li>
</ul>

<h3 id="mechanism-3">Mechanism</h3>

<ul>
  <li>
    <p>The mechanism of pipeline parallelism involves dividing the model into sequential stages. Each stage consists of a subset of the model’s layers, which are assigned to different devices. During training, a minibatch of data is passed through the first stage on the first device, which then forwards the intermediate results (activations) to the next stage on the second device, and so on. While the first minibatch progresses through the pipeline, subsequent minibatches can start being processed by earlier stages, creating an overlapping execution of different minibatches.</p>
  </li>
  <li>
    <p>Key steps in the mechanism:</p>
    <ol>
      <li><strong>Model Partitioning:</strong> Split the model into sequential stages.</li>
      <li><strong>Device Allocation:</strong> Assign each stage to a different device.</li>
      <li><strong>Forward Pass:</strong> Process minibatches through the pipeline stages in sequence.</li>
      <li><strong>Backward Pass:</strong> Perform gradient computations and backpropagation in reverse order through the pipeline stages.</li>
    </ol>
  </li>
</ul>

<h3 id="types-of-pipeline-parallelism">Types of Pipeline Parallelism</h3>

<ol>
  <li><strong>1F1B (One Forward One Backward):</strong> Each device alternates between forward and backward passes, with only one minibatch being processed at a time per device.</li>
  <li><strong>GPipe:</strong> Introduces micro-batching, where a minibatch is further divided into smaller micro-batches to keep all devices busy by processing different micro-batches simultaneously.</li>
  <li><strong>Interleaved Pipeline Parallelism:</strong> Combines pipeline and data parallelism by interleaving multiple pipeline replicas, allowing for more efficient utilization of resources.</li>
</ol>

<h3 id="pros-and-cons-3">Pros and Cons</h3>

<ul>
  <li><strong>Pros:</strong>
    <ul>
      <li><strong>Efficient Resource Utilization:</strong> By splitting the model, all devices can be kept busy, reducing idle time.</li>
      <li><strong>Scalability:</strong> Allows training of larger models that do not fit into the memory of a single device.</li>
      <li><strong>Reduced Training Time:</strong> Concurrent execution of different parts of the model can lead to faster training.</li>
    </ul>
  </li>
  <li><strong>Cons:</strong>
    <ul>
      <li><strong>Complexity:</strong> Implementing pipeline parallelism requires careful model partitioning and synchronization between devices.</li>
      <li><strong>Communication Overhead:</strong> Transferring intermediate activations between devices can introduce significant communication overhead.</li>
      <li><strong>Latency:</strong> The sequential nature of the pipeline can introduce latency, especially if there are imbalances in the computational load across stages.</li>
    </ul>
  </li>
</ul>

<h3 id="use-cases-3">Use Cases</h3>

<ul>
  <li><strong>Large Model Training:</strong> Ideal for training very large models that exceed the memory capacity of a single device, such as transformer-based models in NLP.</li>
  <li><strong>Resource-Constrained Environments:</strong> Useful in environments with limited device memory but multiple devices available.</li>
  <li><strong>Performance Optimization:</strong> Can be used to optimize training performance by balancing the load across multiple devices.</li>
</ul>

<h3 id="implementation-in-pytorch-3">Implementation in PyTorch</h3>

<ul>
  <li>PyTorch provides support for pipeline parallelism through the <code class="language-plaintext highlighter-rouge">torch.distributed.pipeline.sync</code> package. Here’s a high-level overview of how to implement it:</li>
</ul>

<ol>
  <li><strong>Partition the Model:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code8"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code8"><span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="c1"># Assume 'model' is the original large model
# Split the model into two stages
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(...)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">model</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">model</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span>
<span class="p">)</span>

<span class="c1"># Wrap the model with Pipe
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Setup Devices:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code9"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code9"><span class="c1"># Assume we have 2 GPUs
</span><span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Training Loop:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code10"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code10"><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="conclusion-3">Conclusion</h3>

<ul>
  <li>Pipeline parallelism is a powerful technique for improving the efficiency and scalability of training deep learning models. By partitioning the model and distributing it across multiple devices, it enables concurrent processing and better resource utilization. However, it also introduces complexity in implementation and communication overhead. Tools like PyTorch make it easier to implement pipeline parallelism, allowing researchers and engineers to train larger models more efficiently.</li>
</ul>

<h3 id="summary-4">Summary</h3>

<ul>
  <li>Pipeline parallelism distributes the training of deep learning models across multiple devices by partitioning the model into stages. It allows for concurrent execution of different model parts, improving training efficiency and scalability. There are various types, such as 1F1B and GPipe, each with its own advantages and trade-offs. While it offers significant benefits in terms of resource utilization and training speed, it also presents challenges related to complexity and communication overhead. PyTorch provides tools to facilitate the implementation of pipeline parallelism, making it accessible for large-scale model training.</li>
</ul>

<h3 id="deepspeed">DeepSpeed</h3>

<ul>
  <li><a href="https://github.com/deepspeedai/DeepSpeed">DeepSpeed</a> is an open-source deep learning optimization library developed by Microsoft, as an extension to PyTorch. It is designed to enable the training of large-scale models efficiently by providing state-of-the-art optimizations for memory, computation, and distributed training. DeepSpeed combines various techniques and tools to achieve these goals, including memory optimization, parallelism strategies, and efficient kernel implementations.</li>
  <li>DeepSpeed offers both data parallelism and model parallelism, with a particular focus on optimizing and scaling large model training. Here’s how each is implemented:
    <ul>
      <li><strong>Data Parallelism</strong>:
        <ul>
          <li><strong>DeepSpeed</strong> provides standard data parallelism, where the dataset is split across multiple GPUs or nodes. Each GPU processes a different subset of the data, and the gradients are synchronized across GPUs to ensure consistent model updates.</li>
        </ul>
      </li>
      <li><strong>Tensor and Pipeline Parallelism</strong>:
        <ul>
          <li><strong>DeepSpeed</strong> supports tensor slicing or pipeline parallelism, where different layers or parts of the model are distributed across different GPUs.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="key-features-of-deepspeed">Key Features of DeepSpeed</h4>

<ol>
  <li><strong>ZeRO (Zero Redundancy Optimizer):</strong> DeepSpeed introduces ZeRO to optimize memory usage by partitioning model states across data parallel processes. ZeRO has different stages to progressively reduce memory consumption:
    <ul>
      <li><strong>ZeRO Stage 1:</strong> Shards optimizer states.</li>
      <li><strong>ZeRO Stage 2:</strong> Shards gradients.</li>
      <li><strong>ZeRO Stage 3:</strong> Shards model parameters.</li>
    </ul>
  </li>
  <li><strong>Memory Optimization:</strong> Techniques such as activation checkpointing, gradient accumulation, and offloading to CPU/NVMe to manage and reduce memory footprint.</li>
  <li><strong>Mixed Precision Training:</strong> Support for FP16 and BF16 training to leverage hardware acceleration and reduce memory usage.</li>
  <li><strong>Efficient Kernel Implementations:</strong> Optimized kernels for various operations to accelerate training.</li>
  <li><strong>Ease of Integration:</strong> Simple APIs to integrate DeepSpeed into existing PyTorch codebases with minimal changes.</li>
</ol>

<h4 id="technical-details-1">Technical Details</h4>

<ul>
  <li>DeepSpeed’s architecture is built around the ZeRO optimization framework, which targets memory efficiency and scalability. It breaks down the training process into manageable parts and distributes them across multiple GPUs, reducing the per-GPU memory load and enabling the training of very large models.</li>
</ul>

<h4 id="code-sample-3">Code Sample</h4>

<ul>
  <li>Here’s a basic example demonstrating how to set up and use DeepSpeed in a PyTorch training script.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code11"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code11"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Define transformations and dataset
</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Initialize model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">()</span>

    <span class="c1"># Define optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="c1"># DeepSpeed configuration
</span>    <span class="n">deepspeed_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s">"gradient_accumulation_steps"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"type"</span><span class="p">:</span> <span class="s">"Adam"</span><span class="p">,</span>
            <span class="s">"params"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
                <span class="s">"betas"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
                <span class="s">"eps"</span><span class="p">:</span> <span class="mf">1e-8</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="s">"fp16"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"enabled"</span><span class="p">:</span> <span class="bp">True</span>
        <span class="p">},</span>
        <span class="s">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"stage"</span><span class="p">:</span> <span class="mi">1</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="c1"># Initialize DeepSpeed
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">deepspeed_config</span><span class="p">)</span>
    
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># Training loop
</span>    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">model</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">model</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">'Training finished.'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="explanation-of-the-code-3">Explanation of the Code</h4>

<ol>
  <li><strong>Model Definition:</strong> A simple CNN model is defined.</li>
  <li><strong>Data Loading:</strong> The MNIST dataset is loaded and transformed.</li>
  <li><strong>Optimizer Definition:</strong> An Adam optimizer is defined.</li>
  <li><strong>DeepSpeed Configuration:</strong> A configuration dictionary for DeepSpeed is defined, specifying batch size, optimizer settings, mixed precision (fp16), and ZeRO optimization stage.</li>
  <li><strong>Initialize DeepSpeed:</strong> The model and optimizer are initialized with DeepSpeed, which wraps the model to handle optimization.</li>
  <li><strong>Training Loop:</strong> The training loop runs similarly to standard PyTorch but uses <code class="language-plaintext highlighter-rouge">model.backward(loss)</code> and <code class="language-plaintext highlighter-rouge">model.step()</code> to leverage DeepSpeed’s optimization.</li>
</ol>

<h4 id="benefits-of-deepspeed">Benefits of DeepSpeed</h4>

<ul>
  <li><strong>Memory Efficiency:</strong> Significantly reduces memory usage, enabling training of larger models.</li>
  <li><strong>Scalability:</strong> Efficiently scales training across multiple GPUs and nodes.</li>
  <li><strong>Performance:</strong> Provides performance improvements through optimized kernels and mixed precision training.</li>
  <li><strong>Ease of Use:</strong> Integrates seamlessly with PyTorch, requiring minimal code changes.</li>
  <li>Overall, DeepSpeed is a powerful tool for scaling deep learning models efficiently, providing advanced memory optimizations and performance enhancements to facilitate large-scale model training.</li>
</ul>

<h3 id="deepspeed-zero">DeepSpeed ZeRO</h3>

<ul>
  <li>DeepSpeed ZeRO (Zero Redundancy Optimizer) is a highly optimized and memory-efficient approach for training large-scale deep learning models. ZeRO reduces the memory footprint of model states (i.e., optimizer states, gradients, and model parameters) by partitioning them across data-parallel processes. This approach enables the training of models that are significantly larger than what could fit in the memory of a single GPU.</li>
  <li>ZeRO offers both data parallelism and model parallelism, with a particular focus on optimizing and scaling large model training. Here’s how each is implemented:
    <ul>
      <li><strong>Data Parallelism in ZeRO</strong>:
        <ul>
          <li><strong>ZeRO</strong> enhances data parallelism by minimizing memory redundancies. It splits the optimizer states, gradients, and parameters across the GPUs, reducing memory usage and allowing for the training of larger models. This is known as ZeRO Stage 1 and 2 optimizations.</li>
        </ul>
      </li>
      <li><strong>Model Parallelism in ZeRO</strong>:
        <ul>
          <li><strong>ZeRO</strong> implements model parallelism more effectively in its ZeRO Stage 3 optimization. In this stage, it further shards all elements of model states (including optimizer states, gradients, and parameters) across all GPUs, thus combining model parallelism with data parallelism. This allows for an even more efficient distribution of memory and computational load.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="key-features-of-deepspeed-zero">Key Features of DeepSpeed ZeRO</h4>

<ol>
  <li><strong>Optimizer State Partitioning (ZeRO Stage 1):</strong> Partitions optimizer states across GPUs, reducing memory redundancy.</li>
  <li><strong>Gradient Partitioning (ZeRO Stage 2):</strong> Partitions gradients across GPUs, further reducing memory usage.</li>
  <li><strong>Parameter Partitioning (ZeRO Stage 3):</strong> Partitions model parameters across GPUs, enabling the largest possible models to be trained.</li>
</ol>

<h4 id="technical-details-2">Technical Details</h4>

<ul>
  <li><strong>Stage 1 (Optimizer State Sharding):</strong> Optimizer states (e.g., momentum and variance in Adam) are divided among all data-parallel processes.</li>
  <li><strong>Stage 2 (Gradient Sharding):</strong> In addition to sharding optimizer states, gradients are also partitioned, reducing memory requirements during backpropagation.</li>
  <li><strong>Stage 3 (Parameter Sharding):</strong> Parameters are sharded across processes, with each process holding only a part of the model parameters. During forward and backward passes, parameters are gathered and then distributed again.</li>
</ul>

<h4 id="benefits">Benefits</h4>

<ul>
  <li><strong>Memory Efficiency:</strong> Significantly reduces memory overhead, allowing for training of larger models.</li>
  <li><strong>Scalability:</strong> Scales efficiently across multiple GPUs and nodes.</li>
  <li><strong>Performance:</strong> Maintains high training performance through efficient communication and computation strategies.</li>
</ul>

<h4 id="code-sample-4">Code Sample</h4>

<p>Here is a code sample demonstrating the use of DeepSpeed with ZeRO optimization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code12"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code12"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Define transformations and dataset
</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Initialize model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">()</span>

    <span class="c1"># Define optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="c1"># DeepSpeed configuration
</span>    <span class="n">deepspeed_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s">"gradient_accumulation_steps"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"type"</span><span class="p">:</span> <span class="s">"Adam"</span><span class="p">,</span>
            <span class="s">"params"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
                <span class="s">"betas"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
                <span class="s">"eps"</span><span class="p">:</span> <span class="mf">1e-8</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="s">"fp16"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"enabled"</span><span class="p">:</span> <span class="bp">True</span>
        <span class="p">},</span>
        <span class="s">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Use ZeRO Stage 2 for gradient sharding
</span>            <span class="s">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
            <span class="s">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="c1"># Initialize DeepSpeed
</span>    <span class="n">model_engine</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">deepspeed_config</span><span class="p">)</span>
    
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># Training loop
</span>    <span class="n">model_engine</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_engine</span><span class="p">.</span><span class="n">local_rank</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_engine</span><span class="p">.</span><span class="n">local_rank</span><span class="p">)</span>
            
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_engine</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">model_engine</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">model_engine</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">'Training finished.'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="explanation-of-the-code-4">Explanation of the Code</h4>

<ol>
  <li><strong>Model Definition:</strong> A simple CNN model is defined.</li>
  <li><strong>Data Loading:</strong> The MNIST dataset is loaded and transformed.</li>
  <li><strong>Optimizer Definition:</strong> An Adam optimizer is defined.</li>
  <li><strong>DeepSpeed Configuration:</strong> A configuration dictionary for DeepSpeed is defined, specifying batch size, optimizer settings, mixed precision (fp16), and ZeRO optimization stage.</li>
  <li><strong>Initialize DeepSpeed:</strong> The model and optimizer are initialized with DeepSpeed, which wraps the model to handle optimization.</li>
  <li><strong>Training Loop:</strong> The training loop runs similarly to standard PyTorch but uses <code class="language-plaintext highlighter-rouge">model_engine.backward(loss)</code> and <code class="language-plaintext highlighter-rouge">model_engine.step()</code> to leverage DeepSpeed’s optimization.</li>
</ol>

<h4 id="comparison-of-zero-stages">Comparison of ZeRO Stages</h4>

<ul>
  <li><strong>ZeRO Stage 1:</strong> Optimizer state partitioning, suitable for moderate memory savings.</li>
  <li><strong>ZeRO Stage 2:</strong> Adds gradient partitioning to further reduce memory usage.</li>
  <li><strong>ZeRO Stage 3:</strong> Full parameter partitioning, enabling the training of the largest models.</li>
  <li>DeepSpeed ZeRO is a powerful tool for scaling deep learning models efficiently, providing advanced memory optimizations and performance enhancements to facilitate large-scale model training.</li>
</ul>

<h2 id="comparative-analysis-dp-ddp-fsdp-deepspeed-and-deepspeed-zero">Comparative Analysis: DP, DDP, FSDP, DeepSpeed, and DeepSpeed ZeRO</h2>

<ul>
  <li>Here’s a comparative analysis of Data Parallel (DP), Distributed Data Parallel (DDP), Fully Sharded Data Parallel (FSDP), DeepSpeed, and DeepSpeed ZeRO, highlighting their differences, use cases, and key features:</li>
</ul>

<h3 id="data-parallel-dp">Data Parallel (DP)</h3>

<h4 id="overview">Overview</h4>
<ul>
  <li><strong>Mechanism:</strong> Splits the input data across multiple GPUs and replicates the model on each GPU.</li>
  <li><strong>Synchronization:</strong> Gradients are averaged across GPUs after each backward pass.</li>
  <li><strong>Scalability:</strong> Limited scalability due to single-process bottleneck and high communication overhead.</li>
  <li><strong>Ease of Use:</strong> Simple to implement using <code class="language-plaintext highlighter-rouge">torch.nn.DataParallel</code>.</li>
</ul>

<h4 id="pros">Pros</h4>
<ul>
  <li>Easy to set up and use.</li>
  <li>Suitable for small to medium-sized models and datasets.</li>
</ul>

<h4 id="cons">Cons</h4>
<ul>
  <li>Inefficient memory usage as each GPU holds a full copy of the model.</li>
  <li>Not scalable for large models or large-scale distributed training.</li>
</ul>

<h4 id="code-sample-5">Code Sample</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code13"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code13"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># Wrap the model for Data Parallel
</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="distributed-data-parallel-ddp-1">Distributed Data Parallel (DDP)</h3>

<h4 id="overview-1">Overview</h4>
<ul>
  <li><strong>Mechanism:</strong> Each GPU runs a separate process with its own replica of the model.</li>
  <li><strong>Synchronization:</strong> Uses an all-reduce operation to synchronize gradients across GPUs.</li>
  <li><strong>Scalability:</strong> More scalable than DP as it avoids the GIL bottleneck by using multiple processes.</li>
  <li><strong>Ease of Use:</strong> More complex setup than DP but provides better performance.</li>
</ul>

<h4 id="pros-1">Pros</h4>
<ul>
  <li>Better scalability and performance compared to DP.</li>
  <li>More efficient GPU utilization.</li>
</ul>

<h4 id="cons-1">Cons</h4>
<ul>
  <li>Slightly more complex to implement due to process management.</li>
  <li>Still requires each GPU to hold a full copy of the model.</li>
</ul>

<h4 id="code-sample-6">Code Sample</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code14"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code14"><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="c1"># Initialize the process group
</span><span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="fully-sharded-data-parallel-fsdp-1">Fully Sharded Data Parallel (FSDP)</h3>

<h4 id="overview-2">Overview</h4>
<ul>
  <li><strong>Mechanism:</strong> Shards model parameters, gradients, and optimizer states across GPUs.</li>
  <li><strong>Synchronization:</strong> Parameters are gathered and reduced as needed during forward and backward passes.</li>
  <li><strong>Scalability:</strong> Highly scalable, suitable for very large models.</li>
  <li><strong>Ease of Use:</strong> Requires more complex setup and understanding of model sharding.</li>
</ul>

<h4 id="pros-2">Pros</h4>
<ul>
  <li>Significantly reduces memory overhead by sharding model states.</li>
  <li>Allows training of very large models that don’t fit in memory with traditional DP or DDP.</li>
</ul>

<h4 id="cons-2">Cons</h4>
<ul>
  <li>More complex to set up and debug.</li>
  <li>Can introduce additional communication overhead depending on model size and shard granularity.</li>
</ul>

<h4 id="code-sample-7">Code Sample</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code15"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code15"><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="deepspeed-1">DeepSpeed</h3>

<h4 id="overview-3">Overview</h4>
<ul>
  <li><strong>Mechanism:</strong> Combines multiple optimization techniques, including ZeRO (Zero Redundancy Optimizer) which shards model states across GPUs.</li>
  <li><strong>Synchronization:</strong> Uses advanced techniques to optimize communication and memory usage.</li>
  <li><strong>Scalability:</strong> Extremely scalable, designed for training trillion-parameter models.</li>
  <li><strong>Ease of Use:</strong> Requires integration with DeepSpeed library and configuration.</li>
</ul>

<h4 id="pros-3">Pros</h4>
<ul>
  <li>State-of-the-art memory and performance optimizations.</li>
  <li>Supports very large models with efficient memory usage and compute.</li>
</ul>

<h4 id="cons-3">Cons</h4>
<ul>
  <li>More complex integration compared to basic DP or DDP.</li>
  <li>Requires careful tuning and configuration for optimal performance.</li>
</ul>

<h4 id="code-sample-8">Code Sample</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code16"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code16"><span class="kn">import</span> <span class="nn">deepspeed</span>

<span class="n">deepspeed_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s">"gradient_accumulation_steps"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"type"</span><span class="p">:</span> <span class="s">"Adam"</span><span class="p">,</span>
        <span class="s">"params"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.001</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="s">"fp16"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"enabled"</span><span class="p">:</span> <span class="bp">True</span>
    <span class="p">},</span>
    <span class="s">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"stage"</span><span class="p">:</span> <span class="mi">2</span>  <span class="c1"># Use ZeRO Stage 2 for gradient sharding
</span>    <span class="p">}</span>
<span class="p">}</span>

<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">deepspeed_config</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="deepspeed-zero-1">DeepSpeed ZeRO</h3>

<h4 id="overview-4">Overview</h4>
<ul>
  <li><strong>Mechanism:</strong> Part of DeepSpeed, specifically focused on Zero Redundancy Optimizer to partition model states (optimizer states, gradients, parameters) across GPUs.</li>
  <li><strong>Synchronization:</strong> Efficient communication strategies to gather and reduce necessary states during forward and backward passes.</li>
  <li><strong>Scalability:</strong> Highly scalable, designed for training extremely large models with minimal memory footprint.</li>
  <li><strong>Ease of Use:</strong> More complex setup but offers significant memory and performance benefits.</li>
</ul>

<h4 id="pros-4">Pros</h4>
<ul>
  <li>Optimizes memory usage by partitioning model states across GPUs.</li>
  <li>Supports training very large models that wouldn’t fit in memory otherwise.</li>
  <li>Three stages of optimization for varying degrees of memory savings.</li>
</ul>

<h4 id="cons-4">Cons</h4>
<ul>
  <li>More complex to configure and tune.</li>
  <li>Requires understanding of the ZeRO stages for optimal use.</li>
</ul>

<h4 id="code-sample-9">Code Sample</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code17"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code17"><span class="kn">import</span> <span class="nn">deepspeed</span>

<span class="n">deepspeed_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s">"gradient_accumulation_steps"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"type"</span><span class="p">:</span> <span class="s">"Adam"</span><span class="p">,</span>
        <span class="s">"params"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
            <span class="s">"betas"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
            <span class="s">"eps"</span><span class="p">:</span> <span class="mf">1e-8</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="s">"fp16"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"enabled"</span><span class="p">:</span> <span class="bp">True</span>
    <span class="p">},</span>
    <span class="s">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Use ZeRO Stage 2 for gradient sharding
</span>        <span class="s">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
        <span class="s">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">model_engine</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">deepspeed_config</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="comparative-summary">Comparative Summary</h3>

<div align="center">
<table class="tg">
 <thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Feature</strong></th>
<th class="tg-hcenter-valign-first"><strong>Data Parallel (DP)</strong></th>
<th class="tg-hcenter-valign-first"><strong>Distributed Data Parallel (DDP)</strong></th>
<th class="tg-hcenter-valign-first"><strong>Fully Sharded Data Parallel (FSDP)</strong></th>
<th class="tg-hcenter-valign-first"><strong>DeepSpeed</strong></th>
<th class="tg-hcenter-valign-first"><strong>DeepSpeed ZeRO</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Model Replicas</td>
<td class="tg-tleft-valign-first">Full model on each GPU</td>
<td class="tg-tleft-valign-first">Full model on each GPU</td>
<td class="tg-tleft-valign-first">Sharded model across GPUs</td>
<td class="tg-tleft-valign-first">Sharded model across GPUs</td>
<td class="tg-tleft-valign-first">Sharded model states</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Memory Usage</td>
<td class="tg-tleft-valign-first">High (full model on each GPU)</td>
<td class="tg-tleft-valign-first">High (full model on each GPU)</td>
<td class="tg-tleft-valign-first">Low (sharded model)</td>
<td class="tg-tleft-valign-first">Low (sharded model and states)</td>
<td class="tg-tleft-valign-first">Very Low (sharded states)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Gradient Synchronization</td>
<td class="tg-tleft-valign-first">Averaging across GPUs</td>
<td class="tg-tleft-valign-first">All-reduce across processes</td>
<td class="tg-tleft-valign-first">Gather and reduce as needed</td>
<td class="tg-tleft-valign-first">Advanced optimizations</td>
<td class="tg-tleft-valign-first">Efficient sharding strategies</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Scalability</td>
<td class="tg-tleft-valign-first">Limited</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-first">Very High</td>
<td class="tg-tleft-valign-first">Very High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Ease of Use</td>
<td class="tg-tleft-valign-first">Simple</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-first">Complex</td>
<td class="tg-tleft-valign-first">Complex</td>
<td class="tg-tleft-valign-first">Complex</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Performance</td>
<td class="tg-tleft-valign-first">Moderate</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-first">High</td>
<td class="tg-tleft-valign-first">Very High</td>
<td class="tg-tleft-valign-first">Very High</td>
</tr>
</tbody>
</table>
</div>

<h3 id="use-cases-4">Use Cases</h3>

<ul>
  <li><strong>Data Parallel (DP):</strong> Suitable for small to medium-sized models and datasets when ease of use is a priority.</li>
  <li><strong>Distributed Data Parallel (DDP):</strong> Preferred for scenarios requiring better scalability and efficiency compared to DP.</li>
  <li><strong>Fully Sharded Data Parallel (FSDP):</strong> Ideal for training very large models that don’t fit in memory using traditional methods.</li>
  <li><strong>DeepSpeed:</strong> Best for state-of-the-art large-scale model training, offering the most advanced optimizations for memory and performance.</li>
  <li><strong>DeepSpeed ZeRO:</strong> Specifically focused on optimizing memory usage, suitable for extremely large models and resource-constrained environments.</li>
  <li>Overall, Choosing the right parallelization strategy depends on the specific requirements of the model, available hardware, and desired scalability.</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<h3 id="the-ultra-scale-playbook-training-llms-on-gpu-clusters"><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></h3>

<ul>
  <li>This book explores scaling LLM training from one to thousands of GPUs, distilling 4,000+ experiments on up to 512 GPUs into practical theory, code, and benchmarks covering memory profiling, activation recomputation, gradient accumulation, ZeRO/FSDP, data/tensor/pipeline/context/expert parallelism, communication–compute overlap, kernel fusion/FlashAttention, mixed/FP8 precision, and tactics to pick high-throughput, memory-fit configurations.</li>
</ul>

<p><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview"><img src="../../../images/read/playbook.jpg" alt=""></a></p>

<h3 id="how-to-scale-your-model"><a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model</a></h3>

<ul>
  <li>This book explores how to efficiently scale Transformer models across TPUs and GPUs by analyzing compute, memory, and communication limits, choosing parallelism strategies, and understanding hardware to optimize training and inference at massive scale.</li>
</ul>

<p><a href="https://jax-ml.github.io/scaling-book/"><img src="../../../images/read/scale-book.gif" alt=""></a></p>

<h3 id="making-deep-learning-go-brrrr-from-first-principles"><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr from First Principles</a></h3>

<ul>
  <li>This blogpost explores how reasoning from first principles about compute, memory bandwidth, and overhead can guide effective optimizations to make deep learning models run efficiently, instead of relying on ad-hoc performance tricks.</li>
</ul>

<p><a href="https://horace.io/brrr_intro.html"><img src="../../../images/read/DLBrrrr.jpg" alt=""></a></p>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code18"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code18">@article{Chadha2020DistilledDistributedTrainingParallelism,
  title   = {Distributed Training Parallelism},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>