[
  {
    "id": "ai-LLM-contextualized-vs-non-contextualized-embeddings-1",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Embeddings",
    "title": "Contextualized vs. Non-Contextualized Embeddings",
    "subtitle": "Embeddings",
    "contentHtml": "<ul>\n  <li>Encoder models, like the Transformer-based BERT (Bidirectional Encoder Representations from Transformers), are designed to generate contextualized embeddings. Unlike traditional word embeddings that assign a static vector to each word (such as Word2Vec or GloVe), these models consider the context of a word (i.e., the words that surround it). This allows the model to capture a richer and more nuanced understanding of words since the same word can have different meanings based on the context it is used in.</li>\n</ul>",
    "contentMarkdown": "*   Encoder models, like the Transformer-based BERT (Bidirectional Encoder Representations from Transformers), are designed to generate contextualized embeddings. Unlike traditional word embeddings that assign a static vector to each word (such as Word2Vec or GloVe), these models consider the context of a word (i.e., the words that surround it). This allows the model to capture a richer and more nuanced understanding of words since the same word can have different meanings based on the context it is used in.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "transformer",
      "embedding",
      "bert"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 79,
      "contentLength": 531
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#contextualized-vs.-non-contextualized-embeddings",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-use-cases-of-embeddings-2",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Embeddings",
    "title": "Use-cases of Embeddings",
    "subtitle": "Embeddings",
    "contentHtml": "<ul>\n  <li>\n    <p>With embeddings, you can perform various arithmetic operations to carry out specific tasks:</p>\n\n    <ol>\n      <li>\n        <p><strong>Word similarity</strong>: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.</p>\n      </li>\n      <li>\n        <p><strong>Word analogy</strong>: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.</p>\n      </li>\n      <li>\n        <p><strong>Sentence similarity</strong>: If you want to measure the similarity between two sentences, you could use the embedding of the special <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.</p>\n      </li>\n    </ol>\n  </li>\n</ul>\n<p>With embeddings, you can perform various arithmetic operations to carry out specific tasks:</p>\n<ol>\n      <li>\n        <p><strong>Word similarity</strong>: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.</p>\n      </li>\n      <li>\n        <p><strong>Word analogy</strong>: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.</p>\n      </li>\n      <li>\n        <p><strong>Sentence similarity</strong>: If you want to measure the similarity between two sentences, you could use the embedding of the special <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.</p>\n      </li>\n    </ol>\n<p><strong>Word similarity</strong>: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.</p>\n<p><strong>Word analogy</strong>: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.</p>\n<p><strong>Sentence similarity</strong>: If you want to measure the similarity between two sentences, you could use the embedding of the special <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.</p>",
    "contentMarkdown": "*   With embeddings, you can perform various arithmetic operations to carry out specific tasks:\n    \n    1.  **Word similarity**: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.\n        \n    2.  **Word analogy**: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.\n        \n    3.  **Sentence similarity**: If you want to measure the similarity between two sentences, you could use the embedding of the special `[CLS]` token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.\n        \n\nWith embeddings, you can perform various arithmetic operations to carry out specific tasks:\n\n1.  **Word similarity**: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.\n    \n2.  **Word analogy**: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.\n    \n3.  **Sentence similarity**: If you want to measure the similarity between two sentences, you could use the embedding of the special `[CLS]` token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.\n    \n\n**Word similarity**: You can compare the embeddings of two words to understand their similarity. This is often done using cosine similarity, a metric that measures the cosine of the angle between two vectors. A higher cosine similarity between two word vectors indicates that the words are more similar in terms of their usage or meaning.\n\n**Word analogy**: Vector arithmetic can be used to solve word analogy tasks. For example, given the analogy task “man is to woman as king is to what?”, we can find the answer (queen) by performing the following operation on the word embeddings: “king” - “man” + “woman”.\n\n**Sentence similarity**: If you want to measure the similarity between two sentences, you could use the embedding of the special `[CLS]` token produced by models like BERT, which is designed to capture the aggregate meaning of the sentence. Alternatively, you could average the embeddings of all tokens in each sentence and compare these average vectors. Note that when it comes to sentence-level tasks like sentence similarity, Sentence-BERT (SBERT), a modification of the BERT model, is often a better choice. SBERT is specifically trained to produce sentence embeddings that are directly comparable in semantic space, which generally leads to better performance on sentence-level tasks. In SBERT, both sentences are fed into the model simultaneously, allowing it to understand the context of each sentence in relation to the other, resulting in more accurate sentence embeddings.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "embedding",
      "bert"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 738,
      "contentLength": 5275
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#use-cases-of-embeddings",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-similarity-search-with-embeddings-3",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Embeddings",
    "title": "Similarity Search with Embeddings",
    "subtitle": "Embeddings",
    "contentHtml": "<ul>\n  <li>For encoder models, contextualized embeddings are obtained at the output. Arithmetic operations can be performed on the embeddings to for various tasks such as understanding the similarity between two words, identifying word analogies, etc.</li>\n  <li>For the task of word similarity, the respective contextualized embeddings of the words can be used; while for sentence similarity, the output of the <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> token can be used or the word embeddings of all tokens can be averaged. For best performance on sentence similarity tasks, <a href=\"https://sbert.net\">Sentence BERT</a> variants of encoder models are preferred.</li>\n  <li>Word/sentence similarity is the measure of the degree to which two words/sentences are semantically equivalent in meaning.</li>\n  <li>Below are the two most common measures of word/sentence similarity (note that neither of them is a “distance metric”).</li>\n</ul>\n<h4 id=\"dot-product-similarity\">Dot Product Similarity</h4>\n<ul>\n  <li>The dot product of two vectors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">v</script> is defined as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>u</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1007.76em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">v</span><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"texatom\" id=\"MathJax-Span-13\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"texatom\" id=\"MathJax-Span-17\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-20\"><span class=\"mrow\" id=\"MathJax-Span-21\"><span class=\"mo\" id=\"MathJax-Span-22\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"texatom\" id=\"MathJax-Span-24\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">cos</span><span class=\"mo\" id=\"MathJax-Span-28\"></span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>u</mi><mo>⋅</mo><mi>v</mi><mo>=</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>u</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>cos</mi><mo>⁡</mo><mi>θ</mi></math></span></span></div>\n<ul>\n  <li>It’s perhaps easiest to visualize its use as a similarity measure when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo>=</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-30\" style=\"width: 3.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mo\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo>=</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">\\|v\\|=1</script>, as in the diagram (<a href=\"https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity\">source</a>) below, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi><mo>=</mo><mfrac><mrow><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi></mrow><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi></mrow><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 10.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1008.39em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Regular;\">cos</span><span class=\"mo\" id=\"MathJax-Span-40\"></span><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-43\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.84em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.03em, 4.326em, -999.997em); top: -3.591em; left: 50%; margin-left: -1.091em;\"><span class=\"mrow\" id=\"MathJax-Span-48\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.29em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.294em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-56\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.84em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-57\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.326em, -999.997em); top: -3.591em; left: 50%; margin-left: -0.518em;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.2em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.201em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\cos \\theta=\\frac{u \\cdot v}{\\|u\\|\\|v\\|} = \\frac{u \\cdot v}{\\|u\\|}</script>.</li>\n</ul>\n<p><img src=\"../assets/LLM/dotp.png\" align=\"center\" style=\"background-color: #fff; margin: 10px auto; width: 200px;\"></p>\n<ul>\n  <li>Here you can see that when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi><mo>=</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-65\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-66\"><span class=\"mi\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi><mo>=</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\theta=0</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi><mo>=</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-70\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.7em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">cos</span><span class=\"mo\" id=\"MathJax-Span-73\"></span><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>=</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\cos \\theta=1</script>, i.e., the vectors are colinear, the dot product is the element-wise product of the vectors. When <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-77\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\theta</script> is a right angle, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi><mo>=</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-80\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.8em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-81\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">cos</span><span class=\"mo\" id=\"MathJax-Span-83\"></span><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>=</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\cos \\theta=0</script>, i.e. the vectors are orthogonal, the dot product is 0. In general, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular;\">cos</span><span class=\"mo\" id=\"MathJax-Span-90\"></span><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>cos</mi><mo>⁡</mo><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\cos \\theta</script> tells you the similarity in terms of the direction of the vectors (it is -1 when they point in opposite directions). This holds as the number of dimensions is increased, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>cos</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Regular;\">cos</span><span class=\"mo\" id=\"MathJax-Span-95\"></span><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>cos</mi><mo>⁡</mo><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\cos \\theta</script> thus has important uses as a similarity measure in multidimensional space, which is why it is arguably the most commonly used similarity metric.</li>\n</ul>\n<h5 id=\"geometric-intuition\">Geometric Intuition</h5>\n<ul>\n  <li>The dot product between <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi><mo>,</mo><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-97\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1001.46em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-98\"><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi><mo>,</mo><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">u, v</script> can be interpreted as projecting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-102\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"mi\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">u</script> onto <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-105\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-106\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">v</script> (or vice-versa), and then taking product of projected length of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">u</script> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-111\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.67em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-112\"><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">(\\|u\\|)</script> with length of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">v</script> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.67em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"mo\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">(\\|v\\|)</script>.</li>\n  <li>When <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">u</script> is orthogonal to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">v</script>, projection of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-134\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">u</script> onto <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">v</script> is a zero length vector, yielding a zero product. If you visualize all possible rotations of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">u</script> while keeping <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-143\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">v</script> fixed, the dot product gives:\n    <ul>\n      <li>Zero value when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-146\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">u</script> is orthogonal to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">v</script> as the projection of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-152\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">u</script> onto <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-155\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-156\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">v</script> yields a vector of zero length. This corresponds to the intuition of zero similarity.</li>\n      <li>Largest value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-158\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-159\"><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">\\|u\\|\\|v\\|</script> when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-169\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">v</script> point in the same direction.</li>\n      <li>Lowest value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2212;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mo\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>−</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">-\\|u\\|\\|v\\|</script> when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-184\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">v</script> point in opposite direction.</li>\n    </ul>\n  </li>\n  <li>Dividing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-187\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.98em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-188\"><span class=\"mi\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi><mo>⋅</mo><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">u \\cdot v</script> by the magnitude of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-192\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-193\"><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-195\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-196\"><span class=\"mi\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">v</script>, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-198\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-199\"><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-202\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-203\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\|u\\|\\|v\\|</script>, limits the range to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mo>&amp;#x2212;</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-206\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mo\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">[-1,1]</script> making it scale invariant, which is what brings us to cosine similarity.</li>\n</ul>\n<ul>\n      <li>Zero value when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-146\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">u</script> is orthogonal to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">v</script> as the projection of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-152\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">u</script> onto <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-155\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-156\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">v</script> yields a vector of zero length. This corresponds to the intuition of zero similarity.</li>\n      <li>Largest value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-158\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-159\"><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">\\|u\\|\\|v\\|</script> when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-169\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">v</script> point in the same direction.</li>\n      <li>Lowest value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2212;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mo\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>−</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">-\\|u\\|\\|v\\|</script> when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-184\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">v</script> point in opposite direction.</li>\n    </ul>\n<h4 id=\"cosine-similarity\">Cosine Similarity</h4>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>cosine_similarity</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi></mrow><mrow><mrow><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo><mi>u</mi><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo></mrow><mrow><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo><mi>v</mi><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><mrow><msqrt><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup></msqrt><msqrt><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup></msqrt></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-214\" style=\"width: 28.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.471em, 1023.75em, 4.169em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-215\"><span class=\"mtext\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular;\">cosine_similarity</span><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">v</span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-223\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1001.72em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.883em;\"><span class=\"mrow\" id=\"MathJax-Span-224\"><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.13em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span class=\"mrow\" id=\"MathJax-Span-233\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.39em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.388em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-238\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.544em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.076em, 1003.7em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -1.872em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"munderover\" id=\"MathJax-Span-240\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.42em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-242\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-245\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-248\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-250\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-253\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-255\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.659em, 1008.44em, 4.846em, -999.997em); top: -2.862em; left: 50%; margin-left: -4.216em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"msqrt\" id=\"MathJax-Span-257\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1003.18em, 4.482em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"munderover\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.42em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-261\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mi\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-264\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-269\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1003.18em, 3.388em, -999.997em); top: -4.372em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 2.711em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.888em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.357em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.773em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.242em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.643em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msqrt\" id=\"MathJax-Span-273\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1003.13em, 4.482em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"munderover\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.42em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-280\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-285\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-287\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1003.13em, 3.388em, -999.997em); top: -4.372em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 2.659em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.888em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.305em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.773em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.19em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.643em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.54em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.544em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.247em; border-left: 0px solid; width: 0px; height: 4.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>cosine_similarity</mtext><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mrow><mo symmetric=\"true\">‖</mo><mi>u</mi><mo symmetric=\"true\">‖</mo></mrow><mrow><mo symmetric=\"true\">‖</mo><mi>v</mi><mo symmetric=\"true\">‖</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><mrow><msqrt><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup></msqrt><msqrt><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup></msqrt></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>where,\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-292\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-293\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">v</script> are the two vectors being compared.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x22C5;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-295\" style=\"width: 0.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.773em, 1000.32em, 2.19em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-296\"><span class=\"mo\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Regular;\">⋅</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.191em; border-left: 0px solid; width: 0px; height: 0.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⋅</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">\\cdot</script> represents the dot product.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-298\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">\\|u\\|</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mo\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">\\|v\\|</script> represent the magnitudes (or norms) of the vectors, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-308\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-309\"><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">n</script> is the number of dimensions in the vectors.</li>\n    </ul>\n  </li>\n  <li>Note that as mentioned earlier, the length normalization part (i.e., dividing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi><mo>&amp;#x22C5;</mo><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-311\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.98em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-312\"><span class=\"mi\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-315\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi><mo>⋅</mo><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">u \\cdot v</script> by the magnitude of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-316\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-317\"><span class=\"mi\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-319\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">v</script>, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-322\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mo\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\|u\\|\\|v\\|</script>) limits the range to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mo>&amp;#x2212;</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-330\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-331\"><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">[-1,1]</script>, making it scale invariant.</li>\n</ul>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">u</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-292\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-293\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">v</script> are the two vectors being compared.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x22C5;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-295\" style=\"width: 0.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.773em, 1000.32em, 2.19em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-296\"><span class=\"mo\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Regular;\">⋅</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.191em; border-left: 0px solid; width: 0px; height: 0.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⋅</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">\\cdot</script> represents the dot product.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>u</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-298\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">u</span><span class=\"mo\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>u</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">\\|u\\|</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>v</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mo\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">\\|v\\|</script> represent the magnitudes (or norms) of the vectors, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-308\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-309\"><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">n</script> is the number of dimensions in the vectors.</li>\n    </ul>\n<h4 id=\"cosine-similarity-vs-dot-product-similarity\">Cosine Similarity vs. Dot Product Similarity</h4>\n<ul>\n  <li>Cosine similarity and dot product similarity are both techniques used to determine the similarity between vectors, which can represent things like text documents, user preferences, etc. The choice between the two depends on the specific use case and desired properties. Here’s a comparison of the advantages of cosine similarity over dot product similarity:\n    <ul>\n      <li><strong>Magnitude Normalization:</strong> Cosine similarity considers only the angle between two vectors, ignoring their magnitudes. This is particularly useful when comparing documents of different lengths or vectors where magnitude isn’t representative of similarity. Dot product, on the other hand, can be affected by the magnitude of the vectors. A long document with many mentions of a particular term might have a high dot product with another document, even if the percentage of relevant content is low. Note that if you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. In other words, cosine similarity is simply dot product, normalized by magnitude (hence is a value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-338\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.92em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-339\"><span class=\"mo\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Regular;\">∈</span><span class=\"mo\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mn\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-344\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\in [0, 1]</script>). Cosine similarity is preferable because it is scale invariant and thus lends itself naturally towards diverse data samples (with say, varying length). For instance, say we have two sets of documents and we computing similarity within each set. Within each set docs are identical, but set #1 documents are shorter, than set #2 ones. Dot product would produce different numbers if the embedding/feature size is different, while in both cases cosine similarity would yield comparable results (since it is length normalized). On the other hand, plain dot product is a little bit “cheaper” (in terms of complexity and implementation), since it involves lesser operations (no length normalization).</li>\n      <li><strong>Bound Values:</strong> Cosine similarity returns values between -1 and 1 for all vectors, but it specifically returns values between 0 and 1 for vectors with non-negative dimensions (like in the case of TF-IDF representations of documents). This bounded nature can be easier to interpret.\n  Dot product values can range from negative to positive infinity, which can make normalization or thresholding harder.</li>\n      <li><strong>Robustness in High Dimensions:</strong> In high dimensional spaces, most pairs of vectors tend to be almost orthogonal, which means their dot products approach zero. However, their cosine similarities can still provide meaningful differentiation.\n  Dot product can be highly sensitive to the magnitude of individual dimensions, especially in high-dimensional spaces.</li>\n      <li><strong>Common Use Cases:</strong> Cosine similarity is extensively used in text analytics, information retrieval, and recommender systems because of its effectiveness in these domains. When representing text with models like TF-IDF, where vectors are non-negative and the magnitude might be influenced by the length of the text, cosine similarity is more appropriate. While dot product has its own strengths, it might not be as suitable for these use cases without additional normalization.</li>\n      <li><strong>Intuitiveness:</strong>  In many scenarios, thinking in terms of angles (cosine similarity) can be more intuitive than considering the raw projection (dot product). For instance, when two vectors point in the exact same direction (regardless of their magnitudes), their cosine similarity is 1, indicating perfect similarity.</li>\n      <li><strong>Centroid Calculation:</strong> When trying to calculate the centroid (average) of multiple vectors (like in clustering), the centroid remains meaningful under cosine similarity. If you average the vectors and then compare another vector using cosine similarity, you get a measure of how similar the vector is to the “average” vector. This isn’t necessarily true with dot product. Despite these advantages, it’s worth noting that in some applications, especially in neural networks and deep learning, raw dot products (sometimes followed by a normalization step) are preferred due to their computational properties and the nature of learned embeddings. Always consider the specific application and the properties of the data when choosing between these measures.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Magnitude Normalization:</strong> Cosine similarity considers only the angle between two vectors, ignoring their magnitudes. This is particularly useful when comparing documents of different lengths or vectors where magnitude isn’t representative of similarity. Dot product, on the other hand, can be affected by the magnitude of the vectors. A long document with many mentions of a particular term might have a high dot product with another document, even if the percentage of relevant content is low. Note that if you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. In other words, cosine similarity is simply dot product, normalized by magnitude (hence is a value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-338\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.92em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-339\"><span class=\"mo\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Regular;\">∈</span><span class=\"mo\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mn\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-344\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\in [0, 1]</script>). Cosine similarity is preferable because it is scale invariant and thus lends itself naturally towards diverse data samples (with say, varying length). For instance, say we have two sets of documents and we computing similarity within each set. Within each set docs are identical, but set #1 documents are shorter, than set #2 ones. Dot product would produce different numbers if the embedding/feature size is different, while in both cases cosine similarity would yield comparable results (since it is length normalized). On the other hand, plain dot product is a little bit “cheaper” (in terms of complexity and implementation), since it involves lesser operations (no length normalization).</li>\n      <li><strong>Bound Values:</strong> Cosine similarity returns values between -1 and 1 for all vectors, but it specifically returns values between 0 and 1 for vectors with non-negative dimensions (like in the case of TF-IDF representations of documents). This bounded nature can be easier to interpret.\n  Dot product values can range from negative to positive infinity, which can make normalization or thresholding harder.</li>\n      <li><strong>Robustness in High Dimensions:</strong> In high dimensional spaces, most pairs of vectors tend to be almost orthogonal, which means their dot products approach zero. However, their cosine similarities can still provide meaningful differentiation.\n  Dot product can be highly sensitive to the magnitude of individual dimensions, especially in high-dimensional spaces.</li>\n      <li><strong>Common Use Cases:</strong> Cosine similarity is extensively used in text analytics, information retrieval, and recommender systems because of its effectiveness in these domains. When representing text with models like TF-IDF, where vectors are non-negative and the magnitude might be influenced by the length of the text, cosine similarity is more appropriate. While dot product has its own strengths, it might not be as suitable for these use cases without additional normalization.</li>\n      <li><strong>Intuitiveness:</strong>  In many scenarios, thinking in terms of angles (cosine similarity) can be more intuitive than considering the raw projection (dot product). For instance, when two vectors point in the exact same direction (regardless of their magnitudes), their cosine similarity is 1, indicating perfect similarity.</li>\n      <li><strong>Centroid Calculation:</strong> When trying to calculate the centroid (average) of multiple vectors (like in clustering), the centroid remains meaningful under cosine similarity. If you average the vectors and then compare another vector using cosine similarity, you get a measure of how similar the vector is to the “average” vector. This isn’t necessarily true with dot product. Despite these advantages, it’s worth noting that in some applications, especially in neural networks and deep learning, raw dot products (sometimes followed by a normalization step) are preferred due to their computational properties and the nature of learned embeddings. Always consider the specific application and the properties of the data when choosing between these measures.</li>\n    </ul>",
    "contentMarkdown": "*   For encoder models, contextualized embeddings are obtained at the output. Arithmetic operations can be performed on the embeddings to for various tasks such as understanding the similarity between two words, identifying word analogies, etc.\n*   For the task of word similarity, the respective contextualized embeddings of the words can be used; while for sentence similarity, the output of the `[CLS]` token can be used or the word embeddings of all tokens can be averaged. For best performance on sentence similarity tasks, [Sentence BERT](https://sbert.net) variants of encoder models are preferred.\n*   Word/sentence similarity is the measure of the degree to which two words/sentences are semantically equivalent in meaning.\n*   Below are the two most common measures of word/sentence similarity (note that neither of them is a “distance metric”).\n\n#### Dot Product Similarity\n\n*   The dot product of two vectors uuu and vvv is defined as:\n\nu⋅v\\=|u||v|cosθu⋅v\\=|u||v|cos⁡θ\n\n*   It’s perhaps easiest to visualize its use as a similarity measure when ‖v‖\\=1‖v‖\\=1\\\\|v\\\\|=1, as in the diagram ([source](https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity)) below, where cosθ\\=u⋅v‖u‖‖v‖\\=u⋅v‖u‖cos⁡θ\\=u⋅v‖u‖‖v‖\\=u⋅v‖u‖\\\\cos \\\\theta=\\\\frac{u \\\\cdot v}{\\\\|u\\\\|\\\\|v\\\\|} = \\\\frac{u \\\\cdot v}{\\\\|u\\\\|}.\n\n![](../assets/LLM/dotp.png)\n\n*   Here you can see that when θ\\=0θ\\=0\\\\theta=0 and cosθ\\=1cos⁡θ\\=1\\\\cos \\\\theta=1, i.e., the vectors are colinear, the dot product is the element-wise product of the vectors. When θθ\\\\theta is a right angle, and cosθ\\=0cos⁡θ\\=0\\\\cos \\\\theta=0, i.e. the vectors are orthogonal, the dot product is 0. In general, cosθcos⁡θ\\\\cos \\\\theta tells you the similarity in terms of the direction of the vectors (it is -1 when they point in opposite directions). This holds as the number of dimensions is increased, and cosθcos⁡θ\\\\cos \\\\theta thus has important uses as a similarity measure in multidimensional space, which is why it is arguably the most commonly used similarity metric.\n\n##### Geometric Intuition\n\n*   The dot product between u,vu,vu, v can be interpreted as projecting uuu onto vvv (or vice-versa), and then taking product of projected length of uuu (‖u‖)(‖u‖)(\\\\|u\\\\|) with length of vvv (‖v‖)(‖v‖)(\\\\|v\\\\|).\n*   When uuu is orthogonal to vvv, projection of uuu onto vvv is a zero length vector, yielding a zero product. If you visualize all possible rotations of uuu while keeping vvv fixed, the dot product gives:\n    *   Zero value when uuu is orthogonal to vvv as the projection of uuu onto vvv yields a vector of zero length. This corresponds to the intuition of zero similarity.\n    *   Largest value of ‖u‖‖v‖‖u‖‖v‖\\\\|u\\\\|\\\\|v\\\\| when uuu and vvv point in the same direction.\n    *   Lowest value of −‖u‖‖v‖−‖u‖‖v‖\\-\\\\|u\\\\|\\\\|v\\\\| when uuu and vvv point in opposite direction.\n*   Dividing u⋅vu⋅vu \\\\cdot v by the magnitude of uuu and vvv, i.e., ‖u‖‖v‖‖u‖‖v‖\\\\|u\\\\|\\\\|v\\\\|, limits the range to \\[−1,1\\]\\[−1,1\\]\\[-1,1\\] making it scale invariant, which is what brings us to cosine similarity.\n\n*   Zero value when uuu is orthogonal to vvv as the projection of uuu onto vvv yields a vector of zero length. This corresponds to the intuition of zero similarity.\n*   Largest value of ‖u‖‖v‖‖u‖‖v‖\\\\|u\\\\|\\\\|v\\\\| when uuu and vvv point in the same direction.\n*   Lowest value of −‖u‖‖v‖−‖u‖‖v‖\\-\\\\|u\\\\|\\\\|v\\\\| when uuu and vvv point in opposite direction.\n\n#### Cosine Similarity\n\ncosine\\_similarity(u,v)\\=u⋅v‖u‖‖v‖\\=∑ni\\=1uivi∑ni\\=1u2i‾‾‾‾‾‾‾√∑ni\\=1v2i‾‾‾‾‾‾‾√cosine\\_similarity(u,v)\\=u⋅v‖u‖‖v‖\\=∑i\\=1nuivi∑i\\=1nui2∑i\\=1nvi2\n\n*   where,\n    *   uuu and vvv are the two vectors being compared.\n    *   ⋅⋅\\\\cdot represents the dot product.\n    *   ‖u‖‖u‖\\\\|u\\\\| and ‖v‖‖v‖\\\\|v\\\\| represent the magnitudes (or norms) of the vectors, and nnn is the number of dimensions in the vectors.\n*   Note that as mentioned earlier, the length normalization part (i.e., dividing u⋅vu⋅vu \\\\cdot v by the magnitude of uuu and vvv, i.e., ‖u‖‖v‖‖u‖‖v‖\\\\|u\\\\|\\\\|v\\\\|) limits the range to \\[−1,1\\]\\[−1,1\\]\\[-1,1\\], making it scale invariant.\n\n*   uuu and vvv are the two vectors being compared.\n*   ⋅⋅\\\\cdot represents the dot product.\n*   ‖u‖‖u‖\\\\|u\\\\| and ‖v‖‖v‖\\\\|v\\\\| represent the magnitudes (or norms) of the vectors, and nnn is the number of dimensions in the vectors.\n\n#### Cosine Similarity vs. Dot Product Similarity\n\n*   Cosine similarity and dot product similarity are both techniques used to determine the similarity between vectors, which can represent things like text documents, user preferences, etc. The choice between the two depends on the specific use case and desired properties. Here’s a comparison of the advantages of cosine similarity over dot product similarity:\n    *   **Magnitude Normalization:** Cosine similarity considers only the angle between two vectors, ignoring their magnitudes. This is particularly useful when comparing documents of different lengths or vectors where magnitude isn’t representative of similarity. Dot product, on the other hand, can be affected by the magnitude of the vectors. A long document with many mentions of a particular term might have a high dot product with another document, even if the percentage of relevant content is low. Note that if you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. In other words, cosine similarity is simply dot product, normalized by magnitude (hence is a value ∈\\[0,1\\]∈\\[0,1\\]\\\\in \\[0, 1\\]). Cosine similarity is preferable because it is scale invariant and thus lends itself naturally towards diverse data samples (with say, varying length). For instance, say we have two sets of documents and we computing similarity within each set. Within each set docs are identical, but set #1 documents are shorter, than set #2 ones. Dot product would produce different numbers if the embedding/feature size is different, while in both cases cosine similarity would yield comparable results (since it is length normalized). On the other hand, plain dot product is a little bit “cheaper” (in terms of complexity and implementation), since it involves lesser operations (no length normalization).\n    *   **Bound Values:** Cosine similarity returns values between -1 and 1 for all vectors, but it specifically returns values between 0 and 1 for vectors with non-negative dimensions (like in the case of TF-IDF representations of documents). This bounded nature can be easier to interpret. Dot product values can range from negative to positive infinity, which can make normalization or thresholding harder.\n    *   **Robustness in High Dimensions:** In high dimensional spaces, most pairs of vectors tend to be almost orthogonal, which means their dot products approach zero. However, their cosine similarities can still provide meaningful differentiation. Dot product can be highly sensitive to the magnitude of individual dimensions, especially in high-dimensional spaces.\n    *   **Common Use Cases:** Cosine similarity is extensively used in text analytics, information retrieval, and recommender systems because of its effectiveness in these domains. When representing text with models like TF-IDF, where vectors are non-negative and the magnitude might be influenced by the length of the text, cosine similarity is more appropriate. While dot product has its own strengths, it might not be as suitable for these use cases without additional normalization.\n    *   **Intuitiveness:** In many scenarios, thinking in terms of angles (cosine similarity) can be more intuitive than considering the raw projection (dot product). For instance, when two vectors point in the exact same direction (regardless of their magnitudes), their cosine similarity is 1, indicating perfect similarity.\n    *   **Centroid Calculation:** When trying to calculate the centroid (average) of multiple vectors (like in clustering), the centroid remains meaningful under cosine similarity. If you average the vectors and then compare another vector using cosine similarity, you get a measure of how similar the vector is to the “average” vector. This isn’t necessarily true with dot product. Despite these advantages, it’s worth noting that in some applications, especially in neural networks and deep learning, raw dot products (sometimes followed by a normalization step) are preferred due to their computational properties and the nature of learned embeddings. Always consider the specific application and the properties of the data when choosing between these measures.\n\n*   **Magnitude Normalization:** Cosine similarity considers only the angle between two vectors, ignoring their magnitudes. This is particularly useful when comparing documents of different lengths or vectors where magnitude isn’t representative of similarity. Dot product, on the other hand, can be affected by the magnitude of the vectors. A long document with many mentions of a particular term might have a high dot product with another document, even if the percentage of relevant content is low. Note that if you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. In other words, cosine similarity is simply dot product, normalized by magnitude (hence is a value ∈\\[0,1\\]∈\\[0,1\\]\\\\in \\[0, 1\\]). Cosine similarity is preferable because it is scale invariant and thus lends itself naturally towards diverse data samples (with say, varying length). For instance, say we have two sets of documents and we computing similarity within each set. Within each set docs are identical, but set #1 documents are shorter, than set #2 ones. Dot product would produce different numbers if the embedding/feature size is different, while in both cases cosine similarity would yield comparable results (since it is length normalized). On the other hand, plain dot product is a little bit “cheaper” (in terms of complexity and implementation), since it involves lesser operations (no length normalization).\n*   **Bound Values:** Cosine similarity returns values between -1 and 1 for all vectors, but it specifically returns values between 0 and 1 for vectors with non-negative dimensions (like in the case of TF-IDF representations of documents). This bounded nature can be easier to interpret. Dot product values can range from negative to positive infinity, which can make normalization or thresholding harder.\n*   **Robustness in High Dimensions:** In high dimensional spaces, most pairs of vectors tend to be almost orthogonal, which means their dot products approach zero. However, their cosine similarities can still provide meaningful differentiation. Dot product can be highly sensitive to the magnitude of individual dimensions, especially in high-dimensional spaces.\n*   **Common Use Cases:** Cosine similarity is extensively used in text analytics, information retrieval, and recommender systems because of its effectiveness in these domains. When representing text with models like TF-IDF, where vectors are non-negative and the magnitude might be influenced by the length of the text, cosine similarity is more appropriate. While dot product has its own strengths, it might not be as suitable for these use cases without additional normalization.\n*   **Intuitiveness:** In many scenarios, thinking in terms of angles (cosine similarity) can be more intuitive than considering the raw projection (dot product). For instance, when two vectors point in the exact same direction (regardless of their magnitudes), their cosine similarity is 1, indicating perfect similarity.\n*   **Centroid Calculation:** When trying to calculate the centroid (average) of multiple vectors (like in clustering), the centroid remains meaningful under cosine similarity. If you average the vectors and then compare another vector using cosine similarity, you get a measure of how similar the vector is to the “average” vector. This isn’t necessarily true with dot product. Despite these advantages, it’s worth noting that in some applications, especially in neural networks and deep learning, raw dot products (sometimes followed by a normalization step) are preferred due to their computational properties and the nature of learned embeddings. Always consider the specific application and the properties of the data when choosing between these measures.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 10,
    "tags": [
      "nlpllms",
      "neural network",
      "deep learning",
      "embedding",
      "bert",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 1850,
      "contentLength": 135508
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#similarity-search-with-embeddings",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-llm-training-steps-4",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "How Do LLMs Work?",
    "title": "LLM Training Steps",
    "subtitle": "How Do LLMs Work?",
    "contentHtml": "<ul>\n  <li>At a top-level, here are steps involved in training LLMs:\n    <ol>\n      <li><strong>Corpus Preparation:</strong> Gather a large corpus of text data, such as news articles, social media posts, or web documents.</li>\n      <li><strong>Tokenization:</strong> Split the text into individual words or subword units, known as tokens.</li>\n      <li><strong>Embedding Generation</strong>: Typically accomplished using a randomly initialized embedding table, via the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\"><code class=\"language-plaintext highlighter-rouge\">nn.Embedding</code></a> class in PyTorch. Pre-trained embeddings such Word2Vec, GloVe, FastText, etc. can also be used as a starting point for training. Note that these embeddings represent the non-contextualized vector form of the input token.</li>\n      <li><strong>Neural Network Training:</strong> Train a neural network model on the input tokens.\n        <ul>\n          <li>For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our <a href=\"../bert\">BERT</a> primer.</li>\n          <li>For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li><strong>Corpus Preparation:</strong> Gather a large corpus of text data, such as news articles, social media posts, or web documents.</li>\n      <li><strong>Tokenization:</strong> Split the text into individual words or subword units, known as tokens.</li>\n      <li><strong>Embedding Generation</strong>: Typically accomplished using a randomly initialized embedding table, via the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\"><code class=\"language-plaintext highlighter-rouge\">nn.Embedding</code></a> class in PyTorch. Pre-trained embeddings such Word2Vec, GloVe, FastText, etc. can also be used as a starting point for training. Note that these embeddings represent the non-contextualized vector form of the input token.</li>\n      <li><strong>Neural Network Training:</strong> Train a neural network model on the input tokens.\n        <ul>\n          <li>For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our <a href=\"../bert\">BERT</a> primer.</li>\n          <li>For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our <a href=\"../bert\">BERT</a> primer.</li>\n          <li>For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.</li>\n        </ul>",
    "contentMarkdown": "*   At a top-level, here are steps involved in training LLMs:\n    1.  **Corpus Preparation:** Gather a large corpus of text data, such as news articles, social media posts, or web documents.\n    2.  **Tokenization:** Split the text into individual words or subword units, known as tokens.\n    3.  **Embedding Generation**: Typically accomplished using a randomly initialized embedding table, via the [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) class in PyTorch. Pre-trained embeddings such Word2Vec, GloVe, FastText, etc. can also be used as a starting point for training. Note that these embeddings represent the non-contextualized vector form of the input token.\n    4.  **Neural Network Training:** Train a neural network model on the input tokens.\n        *   For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our [BERT](../bert) primer.\n        *   For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.\n\n1.  **Corpus Preparation:** Gather a large corpus of text data, such as news articles, social media posts, or web documents.\n2.  **Tokenization:** Split the text into individual words or subword units, known as tokens.\n3.  **Embedding Generation**: Typically accomplished using a randomly initialized embedding table, via the [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) class in PyTorch. Pre-trained embeddings such Word2Vec, GloVe, FastText, etc. can also be used as a starting point for training. Note that these embeddings represent the non-contextualized vector form of the input token.\n4.  **Neural Network Training:** Train a neural network model on the input tokens.\n    *   For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our [BERT](../bert) primer.\n    *   For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.\n\n*   For encoder models such as BERT and its variants, the model learns to predict the context (surrounding words) of a given word which are masked. BERT is specifically trained on the Masked Language Modeling task (known as the Cloze task) and the Next Sentence Prediction objective; described in our [BERT](../bert) primer.\n*   For decoder models such as GPT-N, LLaMA, etc., the model learns to predict the next token in the sequence, given the prior context of tokens.",
    "order": 4,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "neural network",
      "embedding",
      "bert",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 433,
      "contentLength": 3493
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llm-training-steps",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-reasoning-5",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "How Do LLMs Work?",
    "title": "Reasoning",
    "subtitle": "How Do LLMs Work?",
    "contentHtml": "<ul>\n  <li>Let’s delve into how reasoning works in LLMs; we will define reasoning as the “ability to make inferences using evidence and logic.” <a href=\"https://arxiv.org/pdf/2302.07842.pdf\">(source)</a></li>\n  <li>There are a multitude of varieties of reasoning, such as commonsense reasoning or mathematical reasoning.</li>\n  <li>Similarly, there are a variety of methods to elicit reasoning from the model, one of them being chain-of-thought prompting which can be found <a href=\"../prompt-engineering\">here</a>.</li>\n  <li>It’s important to note that the extent of how much reasoning an LLM uses in order to give its final prediction is still unknown, since teasing apart the contribution of reasoning and factual information to derive the final output is not a straightforward task.</li>\n</ul>",
    "contentMarkdown": "*   Let’s delve into how reasoning works in LLMs; we will define reasoning as the “ability to make inferences using evidence and logic.” [(source)](https://arxiv.org/pdf/2302.07842.pdf)\n*   There are a multitude of varieties of reasoning, such as commonsense reasoning or mathematical reasoning.\n*   Similarly, there are a variety of methods to elicit reasoning from the model, one of them being chain-of-thought prompting which can be found [here](../prompt-engineering).\n*   It’s important to note that the extent of how much reasoning an LLM uses in order to give its final prediction is still unknown, since teasing apart the contribution of reasoning and factual information to derive the final output is not a straightforward task.",
    "order": 5,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 110,
      "contentLength": 798
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#reasoning",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-process-6",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Retrieval/Knowledge-Augmented Generation or RAG (i.e., Providing LLMs External Knowledge)",
    "title": "Process",
    "subtitle": "Retrieval/Knowledge-Augmented Generation or RAG (i.e., Providing LLMs External Knowledge)",
    "contentHtml": "<ul>\n  <li>First step is to store the knowledge of your internal documents in a format that is suitable for querying. We do so by embedding all of your internally held knowledge using an embedding model:\n    <ol>\n      <li>Split text corpus of the entire knowledge base into chunks – a chunk represents a single piece of context available to be queried. Keep in mind that the data of interest can be coming from multiple sources of different types, e.g. documentation in Confluence supplemented by PDF reports.</li>\n      <li>Use the Embedding Model to transform each of the chunks into a vector embedding.</li>\n      <li>Store all vector embeddings in a Vector Database.</li>\n      <li>Save text that represents each of the embeddings separately together with the pointer to the embedding (we will need this later).</li>\n    </ol>\n  </li>\n  <li>\n    <p>The following flowchart (<a href=\"https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\">source</a>) illustrates the architecture of the system:</p>\n\n    <p><img src=\"/primers/ai/assets/LLM/RAG.webp\" alt=\"\"></p>\n  </li>\n  <li>Next we can start constructing the answer to a question/query of interest:\n    <ol>\n      <li>Embed a question/query you want to ask using the same Embedding Model that was used to embed the knowledge base itself.</li>\n      <li>Use the resulting Vector Embedding to run a query against the index in Vector Database. Choose how many vectors you want to retrieve from the Vector Database - it will equal the amount of context you will be  retrieving and eventually using for answering the query question.</li>\n      <li>Vector DB performs an Approximate Nearest Neighbour (ANN) search for the provided vector embedding against the index and returns previously chosen amount of context vectors. The procedure returns vectors that are most similar in a given Embedding/Latent space.</li>\n      <li>Map the returned Vector Embeddings to the text chunks that represent them.</li>\n      <li>Pass a question together with the retrieved context text chunks to the LLM via prompt. Instruct the LLM to only use the provided context to answer the given question. This does not mean that no Prompt Engineering will be needed - you will want to ensure that the answers returned by LLM fall into expected boundaries, e.g. if there is no data in the retrieved context that could be used make sure that no made up answer is provided.</li>\n    </ol>\n  </li>\n  <li>To make a real demo (for e.g., an interactive chatbot like ChatGPT), face the entire application with a Web UI that exposes a text input box to act as a chat interface. After running the provided question through steps 1. to 9. - return and display the generated answer. This is how most of the chatbots that are based on a single or multiple internal knowledge base sources are actually built nowadays.</li>\n</ul>\n<ol>\n      <li>Split text corpus of the entire knowledge base into chunks – a chunk represents a single piece of context available to be queried. Keep in mind that the data of interest can be coming from multiple sources of different types, e.g. documentation in Confluence supplemented by PDF reports.</li>\n      <li>Use the Embedding Model to transform each of the chunks into a vector embedding.</li>\n      <li>Store all vector embeddings in a Vector Database.</li>\n      <li>Save text that represents each of the embeddings separately together with the pointer to the embedding (we will need this later).</li>\n    </ol>\n<p>The following flowchart (<a href=\"https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\">source</a>) illustrates the architecture of the system:</p>\n<p><img src=\"/primers/ai/assets/LLM/RAG.webp\" alt=\"\"></p>\n<ol>\n      <li>Embed a question/query you want to ask using the same Embedding Model that was used to embed the knowledge base itself.</li>\n      <li>Use the resulting Vector Embedding to run a query against the index in Vector Database. Choose how many vectors you want to retrieve from the Vector Database - it will equal the amount of context you will be  retrieving and eventually using for answering the query question.</li>\n      <li>Vector DB performs an Approximate Nearest Neighbour (ANN) search for the provided vector embedding against the index and returns previously chosen amount of context vectors. The procedure returns vectors that are most similar in a given Embedding/Latent space.</li>\n      <li>Map the returned Vector Embeddings to the text chunks that represent them.</li>\n      <li>Pass a question together with the retrieved context text chunks to the LLM via prompt. Instruct the LLM to only use the provided context to answer the given question. This does not mean that no Prompt Engineering will be needed - you will want to ensure that the answers returned by LLM fall into expected boundaries, e.g. if there is no data in the retrieved context that could be used make sure that no made up answer is provided.</li>\n    </ol>",
    "contentMarkdown": "*   First step is to store the knowledge of your internal documents in a format that is suitable for querying. We do so by embedding all of your internally held knowledge using an embedding model:\n    1.  Split text corpus of the entire knowledge base into chunks – a chunk represents a single piece of context available to be queried. Keep in mind that the data of interest can be coming from multiple sources of different types, e.g. documentation in Confluence supplemented by PDF reports.\n    2.  Use the Embedding Model to transform each of the chunks into a vector embedding.\n    3.  Store all vector embeddings in a Vector Database.\n    4.  Save text that represents each of the embeddings separately together with the pointer to the embedding (we will need this later).\n*   The following flowchart ([source](https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to)) illustrates the architecture of the system:\n    \n    ![](/primers/ai/assets/LLM/RAG.webp)\n    \n*   Next we can start constructing the answer to a question/query of interest:\n    1.  Embed a question/query you want to ask using the same Embedding Model that was used to embed the knowledge base itself.\n    2.  Use the resulting Vector Embedding to run a query against the index in Vector Database. Choose how many vectors you want to retrieve from the Vector Database - it will equal the amount of context you will be retrieving and eventually using for answering the query question.\n    3.  Vector DB performs an Approximate Nearest Neighbour (ANN) search for the provided vector embedding against the index and returns previously chosen amount of context vectors. The procedure returns vectors that are most similar in a given Embedding/Latent space.\n    4.  Map the returned Vector Embeddings to the text chunks that represent them.\n    5.  Pass a question together with the retrieved context text chunks to the LLM via prompt. Instruct the LLM to only use the provided context to answer the given question. This does not mean that no Prompt Engineering will be needed - you will want to ensure that the answers returned by LLM fall into expected boundaries, e.g. if there is no data in the retrieved context that could be used make sure that no made up answer is provided.\n*   To make a real demo (for e.g., an interactive chatbot like ChatGPT), face the entire application with a Web UI that exposes a text input box to act as a chat interface. After running the provided question through steps 1. to 9. - return and display the generated answer. This is how most of the chatbots that are based on a single or multiple internal knowledge base sources are actually built nowadays.\n\n1.  Split text corpus of the entire knowledge base into chunks – a chunk represents a single piece of context available to be queried. Keep in mind that the data of interest can be coming from multiple sources of different types, e.g. documentation in Confluence supplemented by PDF reports.\n2.  Use the Embedding Model to transform each of the chunks into a vector embedding.\n3.  Store all vector embeddings in a Vector Database.\n4.  Save text that represents each of the embeddings separately together with the pointer to the embedding (we will need this later).\n\nThe following flowchart ([source](https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to)) illustrates the architecture of the system:\n\n![](/primers/ai/assets/LLM/RAG.webp)\n\n1.  Embed a question/query you want to ask using the same Embedding Model that was used to embed the knowledge base itself.\n2.  Use the resulting Vector Embedding to run a query against the index in Vector Database. Choose how many vectors you want to retrieve from the Vector Database - it will equal the amount of context you will be retrieving and eventually using for answering the query question.\n3.  Vector DB performs an Approximate Nearest Neighbour (ANN) search for the provided vector embedding against the index and returns previously chosen amount of context vectors. The procedure returns vectors that are most similar in a given Embedding/Latent space.\n4.  Map the returned Vector Embeddings to the text chunks that represent them.\n5.  Pass a question together with the retrieved context text chunks to the LLM via prompt. Instruct the LLM to only use the provided context to answer the given question. This does not mean that no Prompt Engineering will be needed - you will want to ensure that the answers returned by LLM fall into expected boundaries, e.g. if there is no data in the retrieved context that could be used make sure that no made up answer is provided.",
    "order": 6,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "embedding",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 738,
      "contentLength": 4965
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#process",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-summary-7",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Retrieval/Knowledge-Augmented Generation or RAG (i.e., Providing LLMs External Knowledge)",
    "title": "Summary",
    "subtitle": "Retrieval/Knowledge-Augmented Generation or RAG (i.e., Providing LLMs External Knowledge)",
    "contentHtml": "<ul>\n  <li>RAG augments the knowledge base of an LM with relevant documents. Vector databases such as <a href=\"https://www.pinecone.io/\">Pinecone</a>, <a href=\"https://www.chroma.com/\">Chroma</a>, <a href=\"https://weaviate.io/\">Weaviate</a>, etc. offer great solutions to augment LLMs. Open-soruce solutions such as <a href=\"https://milvus.io/\">Milvus</a> and <a href=\"https://github.com/jerryjliu/llama_index\">LlamaIndex</a> are great options as well.</li>\n  <li>Here is RAG step-by-step:\n    <ol>\n      <li>Chunk, embed, &amp; index documents in a vector database (VDB).</li>\n      <li>Match the query embedding of the claim advisor using (approximate) nearest neighbor techniques.</li>\n      <li>Retrieve the relevant context from the VDB.</li>\n      <li>Augment the LLM’s prompt with the retrieved content.</li>\n    </ol>\n  </li>\n  <li>As a stack recommendation, you can build prototypes with <a href=\"https://langchain.com/\">LangChain</a> or for more of an industrial flair, go with <a href=\"https://cloud.google.com/vertex-ai\">Google Vertex</a>.</li>\n  <li>Another method recent LM’s have leveraged is the search engine itself such as WebGPT does. “WebGPT learns to interact with a web-browser, which allows it to further refine the initial query or perform additional actions based on its interactions with the tool. More specifically, WebGPT can search the internet, navigate webpages, follow links, and cite sources.” <a href=\"https://arxiv.org/pdf/2302.07842.pdf\">(source)</a></li>\n</ul>\n<ol>\n      <li>Chunk, embed, &amp; index documents in a vector database (VDB).</li>\n      <li>Match the query embedding of the claim advisor using (approximate) nearest neighbor techniques.</li>\n      <li>Retrieve the relevant context from the VDB.</li>\n      <li>Augment the LLM’s prompt with the retrieved content.</li>\n    </ol>",
    "contentMarkdown": "*   RAG augments the knowledge base of an LM with relevant documents. Vector databases such as [Pinecone](https://www.pinecone.io/), [Chroma](https://www.chroma.com/), [Weaviate](https://weaviate.io/), etc. offer great solutions to augment LLMs. Open-soruce solutions such as [Milvus](https://milvus.io/) and [LlamaIndex](https://github.com/jerryjliu/llama_index) are great options as well.\n*   Here is RAG step-by-step:\n    1.  Chunk, embed, & index documents in a vector database (VDB).\n    2.  Match the query embedding of the claim advisor using (approximate) nearest neighbor techniques.\n    3.  Retrieve the relevant context from the VDB.\n    4.  Augment the LLM’s prompt with the retrieved content.\n*   As a stack recommendation, you can build prototypes with [LangChain](https://langchain.com/) or for more of an industrial flair, go with [Google Vertex](https://cloud.google.com/vertex-ai).\n*   Another method recent LM’s have leveraged is the search engine itself such as WebGPT does. “WebGPT learns to interact with a web-browser, which allows it to further refine the initial query or perform additional actions based on its interactions with the tool. More specifically, WebGPT can search the internet, navigate webpages, follow links, and cite sources.” [(source)](https://arxiv.org/pdf/2302.07842.pdf)\n\n1.  Chunk, embed, & index documents in a vector database (VDB).\n2.  Match the query embedding of the claim advisor using (approximate) nearest neighbor techniques.\n3.  Retrieve the relevant context from the VDB.\n4.  Augment the LLM’s prompt with the retrieved content.",
    "order": 7,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "embedding",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 207,
      "contentLength": 1829
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#summary",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-challenges-with-context-scaling-8",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Challenges with Context Scaling",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ol>\n  <li><strong>Fixed Maximum Length:</strong> Positional embeddings are configured for a predetermined sequence length. If a model is designed for 512 tokens, it possesses 512 distinct positional embedding vectors. Beyond this, there’s no positional information for extra tokens, making longer sequences problematic.</li>\n  <li><strong>Memory Overhead:</strong> Extending these embeddings for incredibly lengthy sequences demands more memory, especially if the embeddings are parameters that need to be learned during training.</li>\n  <li><strong>Computational Burden:</strong> The self-attention mechanism in Transformers grows in computational intensity with the length of the sequence. It’s quadratic in nature, meaning that even a modest increase in sequence length can substantially raise the computation time.</li>\n</ol>\n<h4 id=\"the-needle-in-a-haystack-test\">The “Needle in a Haystack” Test</h4>\n<ul>\n  <li>To understand the in-context retrieval ability of long-context LLMs over various parts of their prompt, a simple ‘needle in a haystack’ analysis could be conducted. This method involves embedding specific, targeted information (the ‘needle’) within a larger, more complex body of text (the ‘haystack’). The purpose is to test the LLM’s ability to identify and utilize this specific piece of information amidst a deluge of other data.</li>\n  <li>In practical terms, the analysis could involve inserting a unique fact or data point into a lengthy, seemingly unrelated text. The LLM would then be tasked with tasks or queries that require it to recall or apply this embedded information. This setup mimics real-world situations where essential details are often buried within extensive content, and the ability to retrieve such details is crucial.</li>\n  <li>The experiment could be structured to assess various aspects of the LLM’s performance. For instance, the placement of the ‘needle’ could be varied—early, middle, or late in the text—to see if the model’s retrieval ability changes based on information location. Additionally, the complexity of the surrounding ‘haystack’ can be modified to test the LLM’s performance under varying degrees of contextual difficulty. By analyzing how well the LLM performs in these scenarios, insights can be gained into its in-context retrieval capabilities and potential areas for improvement.</li>\n  <li>This can be accomplished using the <a href=\"https://github.com/gkamradt/LLMTest_NeedleInAHaystack\">Needle In A Haystack</a> library. The following plot shows OpenAI’s GPT-4-128K’s (top) and (bottom) performance with varying context length.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/RAG/GPT4_haystack.jpg\" alt=\"\"></p>\n<p><img src=\"/primers/ai/assets/RAG/Claude_haystack.jpg\" alt=\"\"></p>\n<ul>\n  <li>The following figure <a href=\"https://www.anthropic.com/index/claude-2-1\">(source)</a> shows Claude 2.1’s long context question answering errors based on the areas of the prompt context length. On an average, Claude 2.1 demonstrated a 30% reduction in incorrect answers compared to Claude 2.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/RAG/Claude_context.jpg\" alt=\"\"></p>\n<ul>\n  <li>However, in their <a href=\"https://www.anthropic.com/index/claude-2-1-prompting\">Long context prompting for Claude 2.1</a> blog, Anthropic noted that adding “Here is the most relevant sentence in the context:” to the start of Claude’s response raised the score from 27% to 98% on the original evaluation! The figure below from the blog shows that Claude 2.1’s performance when retrieving an individual sentence across its full 200K token context window. This experiment uses the aforementioned prompt technique to guide Claude in recalling the most relevant sentence.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/RAG/Claude_haystack1.jpg\" alt=\"\"></p>\n<h4 id=\"status-quo\">Status Quo</h4>\n<ul>\n  <li>Considering the fact that self-attention in the Transformer has a quadratic time and space complexity with respect to the context length, here’s how context length sizes of 100k tokens and longer are achieved in practice in the state-of-the-art open LLMs:\n    <ul>\n      <li>The LLMs are first pretrained using the exact attention (without using approximations such as Linformer, Performer, BigBird, etc.), usually with up to 4096 token contexts using FlashAttention 2 (which significantly speeds up the training process compared to a vanilla attention implementation) and Rotary Position Embedding (RoPE, which allows modeling very long sequences).</li>\n      <li>Post this step, the context size is extended via additional pre-training using techniques with various degrees of approximation as compared to the exact attention. This part is often missing in open model info cards while OpenAI and Anthropic keep this information secret. The most effective such techniques used in open models are currently YaRN, LongLoRA, and Llama 2 Long.</li>\n      <li>Inference with such long contexts requires multiple GPUs of A100 or H100 grade even for relatively small models such as 7B or 13B.</li>\n      <li>Some techniques allow extending the context size of a pretrained LLM without additional pretraining. Two such techniques proposed recently are SelfExtend and LM-Infinite.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The LLMs are first pretrained using the exact attention (without using approximations such as Linformer, Performer, BigBird, etc.), usually with up to 4096 token contexts using FlashAttention 2 (which significantly speeds up the training process compared to a vanilla attention implementation) and Rotary Position Embedding (RoPE, which allows modeling very long sequences).</li>\n      <li>Post this step, the context size is extended via additional pre-training using techniques with various degrees of approximation as compared to the exact attention. This part is often missing in open model info cards while OpenAI and Anthropic keep this information secret. The most effective such techniques used in open models are currently YaRN, LongLoRA, and Llama 2 Long.</li>\n      <li>Inference with such long contexts requires multiple GPUs of A100 or H100 grade even for relatively small models such as 7B or 13B.</li>\n      <li>Some techniques allow extending the context size of a pretrained LLM without additional pretraining. Two such techniques proposed recently are SelfExtend and LM-Infinite.</li>\n    </ul>\n<h4 id=\"rag-vs-ultra-long-context-1m-tokens\">RAG vs. Ultra Long Context (1M+ Tokens)</h4>\n<ul>\n  <li>The recent unveiling of the Gemini Pro 1.5 model, featuring a 1 million token context window, has reignited a pivotal discussion: is there still a place for RAG?</li>\n  <li>The consensus appears to affirm the continued relevance of RAG, especially considering the financial implications associated with the Gemini Pro 1.5’s extensive token usage. Specifically, each query within this model demands payment for every token in the 1 million token context, thereby accruing significant costs—approximately $7 per call. This pricing structure starkly contrasts with RAG’s cost-effective approach, where only a select number of pertinent tokens are charged, potentially reducing costs by an estimated 99%, albeit possibly at the expense of performance.</li>\n  <li>This financial consideration sharply defines viable applications for the Gemini Pro 1.5 model, particularly discouraging its use in scenarios typically suited for RAG due to the prohibitive costs involved. Nonetheless, this does not preclude the utility of long context window models in other domains. When the full breadth of the context window is leveraged, these models can provide substantial value, making even a seemingly high cost per call appear reasonable.</li>\n  <li>Optimal uses for such large context window models <a href=\"https://www.linkedin.com/in/peter-gostev/\">(source)</a> would include deriving patterns from the entire dataset, might include:\n    <ol>\n      <li>Analyzing a compilation of 1,000 customer call transcripts to generate insights on the reasons for calls, sentiment analysis, detection of anomalies, assessment of agent performance, and compliance monitoring.\n \t2. Examining extensive marketing performance data to unearth trends or optimization opportunities.</li>\n      <li>Processing the recorded dialogue from an all-day workshop to extract pivotal ideas and discussions.</li>\n      <li>Given a large codebase, resolve a particular bug.</li>\n    </ol>\n  </li>\n  <li>Prohibitive use cases would include applications that requires multi-turn or follow-up questions, which are more efficiently handled by RAG  systems (asking a coding assistant follow-up questions given a piece of code).</li>\n  <li>These examples underscore a critical principle: utilizing 1 million tokens to inquire about a fraction of that amount is inefficient and costly—RAG is better suited for such tasks. However, deploying long context models to analyze and derive patterns across the entirety of their token capacity could indeed be transformative, offering a compelling advantage where the scale and scope of the data justify the investment.</li>\n  <li>The following infographic <a href=\"https://www.linkedin.com/in/peter-gostev/\">(source)</a> illustrates this:</li>\n</ul>\n<ol>\n      <li>Analyzing a compilation of 1,000 customer call transcripts to generate insights on the reasons for calls, sentiment analysis, detection of anomalies, assessment of agent performance, and compliance monitoring.\n \t2. Examining extensive marketing performance data to unearth trends or optimization opportunities.</li>\n      <li>Processing the recorded dialogue from an all-day workshop to extract pivotal ideas and discussions.</li>\n      <li>Given a large codebase, resolve a particular bug.</li>\n    </ol>\n<p><img src=\"assets/RAG/RAGvsUltraLongContext.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "1.  **Fixed Maximum Length:** Positional embeddings are configured for a predetermined sequence length. If a model is designed for 512 tokens, it possesses 512 distinct positional embedding vectors. Beyond this, there’s no positional information for extra tokens, making longer sequences problematic.\n2.  **Memory Overhead:** Extending these embeddings for incredibly lengthy sequences demands more memory, especially if the embeddings are parameters that need to be learned during training.\n3.  **Computational Burden:** The self-attention mechanism in Transformers grows in computational intensity with the length of the sequence. It’s quadratic in nature, meaning that even a modest increase in sequence length can substantially raise the computation time.\n\n#### The “Needle in a Haystack” Test\n\n*   To understand the in-context retrieval ability of long-context LLMs over various parts of their prompt, a simple ‘needle in a haystack’ analysis could be conducted. This method involves embedding specific, targeted information (the ‘needle’) within a larger, more complex body of text (the ‘haystack’). The purpose is to test the LLM’s ability to identify and utilize this specific piece of information amidst a deluge of other data.\n*   In practical terms, the analysis could involve inserting a unique fact or data point into a lengthy, seemingly unrelated text. The LLM would then be tasked with tasks or queries that require it to recall or apply this embedded information. This setup mimics real-world situations where essential details are often buried within extensive content, and the ability to retrieve such details is crucial.\n*   The experiment could be structured to assess various aspects of the LLM’s performance. For instance, the placement of the ‘needle’ could be varied—early, middle, or late in the text—to see if the model’s retrieval ability changes based on information location. Additionally, the complexity of the surrounding ‘haystack’ can be modified to test the LLM’s performance under varying degrees of contextual difficulty. By analyzing how well the LLM performs in these scenarios, insights can be gained into its in-context retrieval capabilities and potential areas for improvement.\n*   This can be accomplished using the [Needle In A Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) library. The following plot shows OpenAI’s GPT-4-128K’s (top) and (bottom) performance with varying context length.\n\n![](/primers/ai/assets/RAG/GPT4_haystack.jpg)\n\n![](/primers/ai/assets/RAG/Claude_haystack.jpg)\n\n*   The following figure [(source)](https://www.anthropic.com/index/claude-2-1) shows Claude 2.1’s long context question answering errors based on the areas of the prompt context length. On an average, Claude 2.1 demonstrated a 30% reduction in incorrect answers compared to Claude 2.\n\n![](/primers/ai/assets/RAG/Claude_context.jpg)\n\n*   However, in their [Long context prompting for Claude 2.1](https://www.anthropic.com/index/claude-2-1-prompting) blog, Anthropic noted that adding “Here is the most relevant sentence in the context:” to the start of Claude’s response raised the score from 27% to 98% on the original evaluation! The figure below from the blog shows that Claude 2.1’s performance when retrieving an individual sentence across its full 200K token context window. This experiment uses the aforementioned prompt technique to guide Claude in recalling the most relevant sentence.\n\n![](/primers/ai/assets/RAG/Claude_haystack1.jpg)\n\n#### Status Quo\n\n*   Considering the fact that self-attention in the Transformer has a quadratic time and space complexity with respect to the context length, here’s how context length sizes of 100k tokens and longer are achieved in practice in the state-of-the-art open LLMs:\n    *   The LLMs are first pretrained using the exact attention (without using approximations such as Linformer, Performer, BigBird, etc.), usually with up to 4096 token contexts using FlashAttention 2 (which significantly speeds up the training process compared to a vanilla attention implementation) and Rotary Position Embedding (RoPE, which allows modeling very long sequences).\n    *   Post this step, the context size is extended via additional pre-training using techniques with various degrees of approximation as compared to the exact attention. This part is often missing in open model info cards while OpenAI and Anthropic keep this information secret. The most effective such techniques used in open models are currently YaRN, LongLoRA, and Llama 2 Long.\n    *   Inference with such long contexts requires multiple GPUs of A100 or H100 grade even for relatively small models such as 7B or 13B.\n    *   Some techniques allow extending the context size of a pretrained LLM without additional pretraining. Two such techniques proposed recently are SelfExtend and LM-Infinite.\n\n*   The LLMs are first pretrained using the exact attention (without using approximations such as Linformer, Performer, BigBird, etc.), usually with up to 4096 token contexts using FlashAttention 2 (which significantly speeds up the training process compared to a vanilla attention implementation) and Rotary Position Embedding (RoPE, which allows modeling very long sequences).\n*   Post this step, the context size is extended via additional pre-training using techniques with various degrees of approximation as compared to the exact attention. This part is often missing in open model info cards while OpenAI and Anthropic keep this information secret. The most effective such techniques used in open models are currently YaRN, LongLoRA, and Llama 2 Long.\n*   Inference with such long contexts requires multiple GPUs of A100 or H100 grade even for relatively small models such as 7B or 13B.\n*   Some techniques allow extending the context size of a pretrained LLM without additional pretraining. Two such techniques proposed recently are SelfExtend and LM-Infinite.\n\n#### RAG vs. Ultra Long Context (1M+ Tokens)\n\n*   The recent unveiling of the Gemini Pro 1.5 model, featuring a 1 million token context window, has reignited a pivotal discussion: is there still a place for RAG?\n*   The consensus appears to affirm the continued relevance of RAG, especially considering the financial implications associated with the Gemini Pro 1.5’s extensive token usage. Specifically, each query within this model demands payment for every token in the 1 million token context, thereby accruing significant costs—approximately $7 per call. This pricing structure starkly contrasts with RAG’s cost-effective approach, where only a select number of pertinent tokens are charged, potentially reducing costs by an estimated 99%, albeit possibly at the expense of performance.\n*   This financial consideration sharply defines viable applications for the Gemini Pro 1.5 model, particularly discouraging its use in scenarios typically suited for RAG due to the prohibitive costs involved. Nonetheless, this does not preclude the utility of long context window models in other domains. When the full breadth of the context window is leveraged, these models can provide substantial value, making even a seemingly high cost per call appear reasonable.\n*   Optimal uses for such large context window models [(source)](https://www.linkedin.com/in/peter-gostev/) would include deriving patterns from the entire dataset, might include:\n    1.  Analyzing a compilation of 1,000 customer call transcripts to generate insights on the reasons for calls, sentiment analysis, detection of anomalies, assessment of agent performance, and compliance monitoring. 2. Examining extensive marketing performance data to unearth trends or optimization opportunities.\n    2.  Processing the recorded dialogue from an all-day workshop to extract pivotal ideas and discussions.\n    3.  Given a large codebase, resolve a particular bug.\n*   Prohibitive use cases would include applications that requires multi-turn or follow-up questions, which are more efficiently handled by RAG systems (asking a coding assistant follow-up questions given a piece of code).\n*   These examples underscore a critical principle: utilizing 1 million tokens to inquire about a fraction of that amount is inefficient and costly—RAG is better suited for such tasks. However, deploying long context models to analyze and derive patterns across the entirety of their token capacity could indeed be transformative, offering a compelling advantage where the scale and scope of the data justify the investment.\n*   The following infographic [(source)](https://www.linkedin.com/in/peter-gostev/) illustrates this:\n\n1.  Analyzing a compilation of 1,000 customer call transcripts to generate insights on the reasons for calls, sentiment analysis, detection of anomalies, assessment of agent performance, and compliance monitoring. 2. Examining extensive marketing performance data to unearth trends or optimization opportunities.\n2.  Processing the recorded dialogue from an all-day workshop to extract pivotal ideas and discussions.\n3.  Given a large codebase, resolve a particular bug.\n\n![](assets/RAG/RAGvsUltraLongContext.jpeg)",
    "order": 8,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 7,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "gpt",
      "llm",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1288,
      "contentLength": 9797
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#challenges-with-context-scaling",
    "scrapedAt": "2025-12-28T11:53:26.172Z"
  },
  {
    "id": "ai-LLM-solutions-to-challenges-9",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Solutions to Challenges",
    "subtitle": "Context Length Extension",
    "contentHtml": "<h4 id=\"positional-interpolation-pi\">Positional Interpolation (PI)</h4>\n<ul>\n  <li>\n    <p><strong>Concept:</strong> Think of PI as a ‘resizing’ tool. Just as you might resize an image to fit a specific frame, PI adjusts position indices to fit within the existing context size. This is done using mathematical interpolation techniques.</p>\n  </li>\n  <li>\n    <p><strong>Functionality:</strong> Suppose you trained a model for a 512-token context, but now want it to manage 1024 tokens. PI would transform positions [0,1,2,…,1023] to something like [0,0.5,1,…,511.5] to utilize the existing 512 embeddings.</p>\n  </li>\n  <li>\n    <p><strong>Advantages:</strong> It’s like giving an old model a new pair of glasses. The model can now ‘see’ or process longer sequences without having to undergo rigorous training again from scratch.</p>\n  </li>\n  <li>\n    <p><strong>Fine-tuning:</strong> After employing PI, models often need some brushing up. This is done through fine-tuning, where the model learns to adjust to its new sequence processing capability.</p>\n  </li>\n</ul>\n<p><strong>Concept:</strong> Think of PI as a ‘resizing’ tool. Just as you might resize an image to fit a specific frame, PI adjusts position indices to fit within the existing context size. This is done using mathematical interpolation techniques.</p>\n<p><strong>Functionality:</strong> Suppose you trained a model for a 512-token context, but now want it to manage 1024 tokens. PI would transform positions [0,1,2,…,1023] to something like [0,0.5,1,…,511.5] to utilize the existing 512 embeddings.</p>\n<p><strong>Advantages:</strong> It’s like giving an old model a new pair of glasses. The model can now ‘see’ or process longer sequences without having to undergo rigorous training again from scratch.</p>\n<p><strong>Fine-tuning:</strong> After employing PI, models often need some brushing up. This is done through fine-tuning, where the model learns to adjust to its new sequence processing capability.</p>\n<h4 id=\"rotary-positional-encoding-rope\">Rotary Positional Encoding (RoPE)</h4>\n<ul>\n  <li>\n    <p><strong>Concept:</strong> Rather than adding distinct positional information, RoPE rotates the existing embeddings based on their positions. By distributing positional data across all dimensions, the essence of sequence position is captured in a more fluid manner.</p>\n  </li>\n  <li>\n    <p><strong>Functionality:</strong> RoPE employs mathematical operations to rotate the input embeddings. This allows the model to handle sequences that go beyond its original training without requiring explicit positional data for each new position.</p>\n  </li>\n  <li>\n    <p><strong>Advantages:</strong> The rotation-based mechanism is more dynamic, meaning the model can work with sequences of any length without needing distinct embeddings for every position. This offers significant flexibility, especially when dealing with texts of unpredictable lengths.</p>\n  </li>\n  <li>\n    <p><strong>Limitation:</strong> The continuous nature of RoPE’s rotation can cause some imprecision, especially when sequences become extremely lengthy.</p>\n  </li>\n</ul>\n<p><strong>Concept:</strong> Rather than adding distinct positional information, RoPE rotates the existing embeddings based on their positions. By distributing positional data across all dimensions, the essence of sequence position is captured in a more fluid manner.</p>\n<p><strong>Functionality:</strong> RoPE employs mathematical operations to rotate the input embeddings. This allows the model to handle sequences that go beyond its original training without requiring explicit positional data for each new position.</p>\n<p><strong>Advantages:</strong> The rotation-based mechanism is more dynamic, meaning the model can work with sequences of any length without needing distinct embeddings for every position. This offers significant flexibility, especially when dealing with texts of unpredictable lengths.</p>\n<p><strong>Limitation:</strong> The continuous nature of RoPE’s rotation can cause some imprecision, especially when sequences become extremely lengthy.</p>\n<h4 id=\"alibi-attention-with-linear-biases\">ALiBi (Attention with Linear Biases)</h4>\n<ul>\n  <li>Author: Ofer Press from FAIR et al.</li>\n  <li>While ALiBi does not directly increase context length, it enhances the Transformer’s adaptability to varied sequence lengths by introducing biases in the attention mechanism, optimizing its performance on extended contexts.</li>\n  <li>The original Transformer leveraged Positional Sinusoidal Encoding which did not have the ‘extrapolation’ ability, thus, it performed poorly during inference/ fine-tuning when the context length was increased.\n    <ul>\n      <li>For example, when you train a transformer model on sequences of a particular length (say 2K tokens) and later want to use it on longer sequences (like 65K tokens), this encoding does not effectively “extrapolate” to these longer sequences. This means that the model starts to perform poorly when dealing with sequences longer than what it was trained on.</li>\n    </ul>\n  </li>\n  <li>AliBi is an alternative to Positional Sinusoidal Encoding and is a modification to the attention mechanism within the Transformer architecture. Instead of adding positional information at the start (or bottom) of the model, ALiBi integrates this information within the attention mechanism itself.</li>\n  <li>In the attention mechanism, attention scores are computed between query and key pairs. ALiBi introduces a bias to these attention scores based on the distance between tokens in the sequence. Specifically, the farther apart two tokens are in the sequence, the more penalty or bias is added to their attention score. This bias ensures that the model is aware of the token positions when calculating attention.</li>\n  <li>Benefits of ALiBi:\n    <ul>\n      <li>Adaptability: Unlike Positional Sinusoidal Encoding, ALiBi is more adaptable to different sequence lengths, making it more suitable for models that need to handle varying sequence lengths during training and inference.</li>\n      <li>Training Speed: Incorporating ALiBi can speed up the training process.</li>\n    </ul>\n  </li>\n  <li>The image below depicts the constant bias added from the original ALiBi paper.</li>\n</ul>\n<ul>\n      <li>For example, when you train a transformer model on sequences of a particular length (say 2K tokens) and later want to use it on longer sequences (like 65K tokens), this encoding does not effectively “extrapolate” to these longer sequences. This means that the model starts to perform poorly when dealing with sequences longer than what it was trained on.</li>\n    </ul>\n<ul>\n      <li>Adaptability: Unlike Positional Sinusoidal Encoding, ALiBi is more adaptable to different sequence lengths, making it more suitable for models that need to handle varying sequence lengths during training and inference.</li>\n      <li>Training Speed: Incorporating ALiBi can speed up the training process.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/alibi.png\" alt=\"\"></p>\n<h4 id=\"sparse-attention\">Sparse Attention</h4>\n<ul>\n  <li>Sparse attention is another modification to self-attention and it exploits the reasoning that not all tokens within your content size are relevant to each other.</li>\n  <li>Thus, it considers only some tokens when calculating the attention score, and adds sparsity to make the computation linear not quadratic w.r.t. input token size.</li>\n  <li>There are many ways in which sparse attention can be implemented and we will look at a few below:\n    <ul>\n      <li><strong>Sliding Window Attention or Local</strong>: implements a fixed-size window attention surrounding each token. Here, each token doesn’t look at all other tokens but only a fixed number around it, defined by a window size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-355\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mi\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">w</script>. If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"mi\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">w</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">n</script>, this significantly reduces computations. However, information can still flow across the entire sequence since each token can pass its information to its neighbors, who pass it to their neighbors, and so on.</li>\n      <li><strong>BigBird Attention:</strong> Another approach, introduced in the BigBird model, combines different types of attention: some tokens attend globally (to all tokens), some attend locally (like the sliding window), and some attend to random tokens. This combination ensures efficient computation while maintaining a good flow of information across the entire sequence.</li>\n    </ul>\n  </li>\n  <li>The image below, <a href=\"https://blog.research.google/2021/03/constructing-transformers-for-longer.html\">source</a> depicts full attention and how it can be viewed as a graph.</li>\n</ul>\n<ul>\n      <li><strong>Sliding Window Attention or Local</strong>: implements a fixed-size window attention surrounding each token. Here, each token doesn’t look at all other tokens but only a fixed number around it, defined by a window size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-355\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mi\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">w</script>. If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"mi\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">w</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">n</script>, this significantly reduces computations. However, information can still flow across the entire sequence since each token can pass its information to its neighbors, who pass it to their neighbors, and so on.</li>\n      <li><strong>BigBird Attention:</strong> Another approach, introduced in the BigBird model, combines different types of attention: some tokens attend globally (to all tokens), some attend locally (like the sliding window), and some attend to random tokens. This combination ensures efficient computation while maintaining a good flow of information across the entire sequence.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/fullAttention.png\" alt=\"\"></p>\n<ul>\n  <li>In contrast, the image below, <a href=\"https://blog.research.google/2021/03/constructing-transformers-for-longer.html\">source</a> depicts sparse attention specifically from the BigBird paper.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/sparseattention.png\" alt=\"\"></p>\n<h4 id=\"flash-attention\">Flash Attention</h4>\n<ul>\n  <li>FlashAttention optimizes the attention mechanism for GPUs by breaking computations into smaller blocks, reducing memory transfer overheads and enhancing processing speed. Let’s see how below:</li>\n  <li>Background context:\n    <ul>\n      <li>Remember from earlier, transformers utilize an attention mechanism that involves several computational steps to determine how much focus each word in a sentence should have on other words. These steps include matrix multiplications, as illustrated by the operations: S = Q * K, P = softmax(S), and O = P * V.</li>\n      <li>However, when processing these operations on a GPU, there are some inefficiencies that slow down the computation:\n        <ol>\n          <li><strong>GPU Memory Hierarchy</strong>: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.</li>\n          <li><strong>Memory Access Overhead</strong>: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.</li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n  <li>Solution - FlashAttention:</li>\n  <li>\n    <p>FlashAttention was introduced to optimize these operations for GPUs. Instead of computing the attention for the entire matrices at once, FlashAttention breaks them down into smaller blocks or tiles:</p>\n\n    <ol>\n      <li><strong>Tiling</strong>: The Q, K, and V matrices are divided into smaller blocks. These blocks are then loaded from the HBM to the faster SRAM for computation.</li>\n      <li><strong>Optimized Computation</strong>: Within each block, the attention output is computed without the need to constantly shift large intermediate results between the two types of memory. This means fewer transfers between the slow and fast memories, which leads to speed improvements.</li>\n      <li><strong>Optimized for GPU</strong>: While individual operations like matrix multiplication are already efficient on GPUs, FlashAttention makes the entire attention layer more GPU-friendly by minimizing memory transfers and fusing several operations.\n        <ul>\n          <li>The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.</li>\n          <li>In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>The image below, <a href=\"https://arxiv.org/abs/2205.14135\">source</a>, depicts Flash Attention from the original paper.</li>\n</ul>\n<ul>\n      <li>Remember from earlier, transformers utilize an attention mechanism that involves several computational steps to determine how much focus each word in a sentence should have on other words. These steps include matrix multiplications, as illustrated by the operations: S = Q * K, P = softmax(S), and O = P * V.</li>\n      <li>However, when processing these operations on a GPU, there are some inefficiencies that slow down the computation:\n        <ol>\n          <li><strong>GPU Memory Hierarchy</strong>: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.</li>\n          <li><strong>Memory Access Overhead</strong>: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.</li>\n        </ol>\n      </li>\n    </ul>\n<ol>\n          <li><strong>GPU Memory Hierarchy</strong>: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.</li>\n          <li><strong>Memory Access Overhead</strong>: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.</li>\n        </ol>\n<p>FlashAttention was introduced to optimize these operations for GPUs. Instead of computing the attention for the entire matrices at once, FlashAttention breaks them down into smaller blocks or tiles:</p>\n<ol>\n      <li><strong>Tiling</strong>: The Q, K, and V matrices are divided into smaller blocks. These blocks are then loaded from the HBM to the faster SRAM for computation.</li>\n      <li><strong>Optimized Computation</strong>: Within each block, the attention output is computed without the need to constantly shift large intermediate results between the two types of memory. This means fewer transfers between the slow and fast memories, which leads to speed improvements.</li>\n      <li><strong>Optimized for GPU</strong>: While individual operations like matrix multiplication are already efficient on GPUs, FlashAttention makes the entire attention layer more GPU-friendly by minimizing memory transfers and fusing several operations.\n        <ul>\n          <li>The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.</li>\n          <li>In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.</li>\n          <li>In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/LLM/flashAttention.png\" alt=\"\"></p>\n<h4 id=\"multi-query-attention\">Multi-Query Attention</h4>\n<ul>\n  <li><strong>Background context:</strong>\n    <ul>\n      <li><strong>Multi-Head Attention (MHA) in Transformers:</strong>\n        <ul>\n          <li>In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.</li>\n        </ul>\n      </li>\n      <li><strong>The Challenge with MHA:</strong>\n        <ul>\n          <li>While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Introducing Multi-Query Attention (MQA):</strong>\n    <ul>\n      <li>MQA is an optimization over the standard MHA. Instead of each head having separate weight matrices for projecting the “key” and “value”, MQA proposes that all heads share a common weight matrix for these projections.</li>\n    </ul>\n  </li>\n  <li><strong>Advantages of MQA:</strong>\n    <ol>\n      <li><strong>Memory Efficiency:</strong> By sharing weights for the “key” and “value” projections across heads, you significantly reduce the memory required for caching during inference. For instance, a model with 96 heads, like GPT-3, can reduce its memory consumption for the key/value cache by up to 96 times.</li>\n      <li><strong>Speed in Inference:</strong> Since you’re now working with shared projections and a reduced cache size, the calculation of attention scores during inference becomes faster. This is especially beneficial when generating longer sequences of text.</li>\n      <li><strong>Maintains Training Speed:</strong> Despite these changes, the training speed remains largely unaffected. This means you get the advantages of MQA without any significant downside in terms of training time.</li>\n    </ol>\n  </li>\n</ul>\n<ul>\n      <li><strong>Multi-Head Attention (MHA) in Transformers:</strong>\n        <ul>\n          <li>In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.</li>\n        </ul>\n      </li>\n      <li><strong>The Challenge with MHA:</strong>\n        <ul>\n          <li>While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.</li>\n        </ul>\n<ul>\n          <li>While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.</li>\n        </ul>\n<ul>\n      <li>MQA is an optimization over the standard MHA. Instead of each head having separate weight matrices for projecting the “key” and “value”, MQA proposes that all heads share a common weight matrix for these projections.</li>\n    </ul>\n<ol>\n      <li><strong>Memory Efficiency:</strong> By sharing weights for the “key” and “value” projections across heads, you significantly reduce the memory required for caching during inference. For instance, a model with 96 heads, like GPT-3, can reduce its memory consumption for the key/value cache by up to 96 times.</li>\n      <li><strong>Speed in Inference:</strong> Since you’re now working with shared projections and a reduced cache size, the calculation of attention scores during inference becomes faster. This is especially beneficial when generating longer sequences of text.</li>\n      <li><strong>Maintains Training Speed:</strong> Despite these changes, the training speed remains largely unaffected. This means you get the advantages of MQA without any significant downside in terms of training time.</li>\n    </ol>",
    "contentMarkdown": "#### Positional Interpolation (PI)\n\n*   **Concept:** Think of PI as a ‘resizing’ tool. Just as you might resize an image to fit a specific frame, PI adjusts position indices to fit within the existing context size. This is done using mathematical interpolation techniques.\n    \n*   **Functionality:** Suppose you trained a model for a 512-token context, but now want it to manage 1024 tokens. PI would transform positions \\[0,1,2,…,1023\\] to something like \\[0,0.5,1,…,511.5\\] to utilize the existing 512 embeddings.\n    \n*   **Advantages:** It’s like giving an old model a new pair of glasses. The model can now ‘see’ or process longer sequences without having to undergo rigorous training again from scratch.\n    \n*   **Fine-tuning:** After employing PI, models often need some brushing up. This is done through fine-tuning, where the model learns to adjust to its new sequence processing capability.\n    \n\n**Concept:** Think of PI as a ‘resizing’ tool. Just as you might resize an image to fit a specific frame, PI adjusts position indices to fit within the existing context size. This is done using mathematical interpolation techniques.\n\n**Functionality:** Suppose you trained a model for a 512-token context, but now want it to manage 1024 tokens. PI would transform positions \\[0,1,2,…,1023\\] to something like \\[0,0.5,1,…,511.5\\] to utilize the existing 512 embeddings.\n\n**Advantages:** It’s like giving an old model a new pair of glasses. The model can now ‘see’ or process longer sequences without having to undergo rigorous training again from scratch.\n\n**Fine-tuning:** After employing PI, models often need some brushing up. This is done through fine-tuning, where the model learns to adjust to its new sequence processing capability.\n\n#### Rotary Positional Encoding (RoPE)\n\n*   **Concept:** Rather than adding distinct positional information, RoPE rotates the existing embeddings based on their positions. By distributing positional data across all dimensions, the essence of sequence position is captured in a more fluid manner.\n    \n*   **Functionality:** RoPE employs mathematical operations to rotate the input embeddings. This allows the model to handle sequences that go beyond its original training without requiring explicit positional data for each new position.\n    \n*   **Advantages:** The rotation-based mechanism is more dynamic, meaning the model can work with sequences of any length without needing distinct embeddings for every position. This offers significant flexibility, especially when dealing with texts of unpredictable lengths.\n    \n*   **Limitation:** The continuous nature of RoPE’s rotation can cause some imprecision, especially when sequences become extremely lengthy.\n    \n\n**Concept:** Rather than adding distinct positional information, RoPE rotates the existing embeddings based on their positions. By distributing positional data across all dimensions, the essence of sequence position is captured in a more fluid manner.\n\n**Functionality:** RoPE employs mathematical operations to rotate the input embeddings. This allows the model to handle sequences that go beyond its original training without requiring explicit positional data for each new position.\n\n**Advantages:** The rotation-based mechanism is more dynamic, meaning the model can work with sequences of any length without needing distinct embeddings for every position. This offers significant flexibility, especially when dealing with texts of unpredictable lengths.\n\n**Limitation:** The continuous nature of RoPE’s rotation can cause some imprecision, especially when sequences become extremely lengthy.\n\n#### ALiBi (Attention with Linear Biases)\n\n*   Author: Ofer Press from FAIR et al.\n*   While ALiBi does not directly increase context length, it enhances the Transformer’s adaptability to varied sequence lengths by introducing biases in the attention mechanism, optimizing its performance on extended contexts.\n*   The original Transformer leveraged Positional Sinusoidal Encoding which did not have the ‘extrapolation’ ability, thus, it performed poorly during inference/ fine-tuning when the context length was increased.\n    *   For example, when you train a transformer model on sequences of a particular length (say 2K tokens) and later want to use it on longer sequences (like 65K tokens), this encoding does not effectively “extrapolate” to these longer sequences. This means that the model starts to perform poorly when dealing with sequences longer than what it was trained on.\n*   AliBi is an alternative to Positional Sinusoidal Encoding and is a modification to the attention mechanism within the Transformer architecture. Instead of adding positional information at the start (or bottom) of the model, ALiBi integrates this information within the attention mechanism itself.\n*   In the attention mechanism, attention scores are computed between query and key pairs. ALiBi introduces a bias to these attention scores based on the distance between tokens in the sequence. Specifically, the farther apart two tokens are in the sequence, the more penalty or bias is added to their attention score. This bias ensures that the model is aware of the token positions when calculating attention.\n*   Benefits of ALiBi:\n    *   Adaptability: Unlike Positional Sinusoidal Encoding, ALiBi is more adaptable to different sequence lengths, making it more suitable for models that need to handle varying sequence lengths during training and inference.\n    *   Training Speed: Incorporating ALiBi can speed up the training process.\n*   The image below depicts the constant bias added from the original ALiBi paper.\n\n*   For example, when you train a transformer model on sequences of a particular length (say 2K tokens) and later want to use it on longer sequences (like 65K tokens), this encoding does not effectively “extrapolate” to these longer sequences. This means that the model starts to perform poorly when dealing with sequences longer than what it was trained on.\n\n*   Adaptability: Unlike Positional Sinusoidal Encoding, ALiBi is more adaptable to different sequence lengths, making it more suitable for models that need to handle varying sequence lengths during training and inference.\n*   Training Speed: Incorporating ALiBi can speed up the training process.\n\n![](/primers/ai/assets/LLM/alibi.png)\n\n#### Sparse Attention\n\n*   Sparse attention is another modification to self-attention and it exploits the reasoning that not all tokens within your content size are relevant to each other.\n*   Thus, it considers only some tokens when calculating the attention score, and adds sparsity to make the computation linear not quadratic w.r.t. input token size.\n*   There are many ways in which sparse attention can be implemented and we will look at a few below:\n    *   **Sliding Window Attention or Local**: implements a fixed-size window attention surrounding each token. Here, each token doesn’t look at all other tokens but only a fixed number around it, defined by a window size www. If www is much smaller than nnn, this significantly reduces computations. However, information can still flow across the entire sequence since each token can pass its information to its neighbors, who pass it to their neighbors, and so on.\n    *   **BigBird Attention:** Another approach, introduced in the BigBird model, combines different types of attention: some tokens attend globally (to all tokens), some attend locally (like the sliding window), and some attend to random tokens. This combination ensures efficient computation while maintaining a good flow of information across the entire sequence.\n*   The image below, [source](https://blog.research.google/2021/03/constructing-transformers-for-longer.html) depicts full attention and how it can be viewed as a graph.\n\n*   **Sliding Window Attention or Local**: implements a fixed-size window attention surrounding each token. Here, each token doesn’t look at all other tokens but only a fixed number around it, defined by a window size www. If www is much smaller than nnn, this significantly reduces computations. However, information can still flow across the entire sequence since each token can pass its information to its neighbors, who pass it to their neighbors, and so on.\n*   **BigBird Attention:** Another approach, introduced in the BigBird model, combines different types of attention: some tokens attend globally (to all tokens), some attend locally (like the sliding window), and some attend to random tokens. This combination ensures efficient computation while maintaining a good flow of information across the entire sequence.\n\n![](/primers/ai/assets/LLM/fullAttention.png)\n\n*   In contrast, the image below, [source](https://blog.research.google/2021/03/constructing-transformers-for-longer.html) depicts sparse attention specifically from the BigBird paper.\n\n![](/primers/ai/assets/LLM/sparseattention.png)\n\n#### Flash Attention\n\n*   FlashAttention optimizes the attention mechanism for GPUs by breaking computations into smaller blocks, reducing memory transfer overheads and enhancing processing speed. Let’s see how below:\n*   Background context:\n    *   Remember from earlier, transformers utilize an attention mechanism that involves several computational steps to determine how much focus each word in a sentence should have on other words. These steps include matrix multiplications, as illustrated by the operations: S = Q \\* K, P = softmax(S), and O = P \\* V.\n    *   However, when processing these operations on a GPU, there are some inefficiencies that slow down the computation:\n        1.  **GPU Memory Hierarchy**: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.\n        2.  **Memory Access Overhead**: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.\n*   Solution - FlashAttention:\n*   FlashAttention was introduced to optimize these operations for GPUs. Instead of computing the attention for the entire matrices at once, FlashAttention breaks them down into smaller blocks or tiles:\n    \n    1.  **Tiling**: The Q, K, and V matrices are divided into smaller blocks. These blocks are then loaded from the HBM to the faster SRAM for computation.\n    2.  **Optimized Computation**: Within each block, the attention output is computed without the need to constantly shift large intermediate results between the two types of memory. This means fewer transfers between the slow and fast memories, which leads to speed improvements.\n    3.  **Optimized for GPU**: While individual operations like matrix multiplication are already efficient on GPUs, FlashAttention makes the entire attention layer more GPU-friendly by minimizing memory transfers and fusing several operations.\n        *   The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.\n        *   In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.\n*   The image below, [source](https://arxiv.org/abs/2205.14135), depicts Flash Attention from the original paper.\n\n*   Remember from earlier, transformers utilize an attention mechanism that involves several computational steps to determine how much focus each word in a sentence should have on other words. These steps include matrix multiplications, as illustrated by the operations: S = Q \\* K, P = softmax(S), and O = P \\* V.\n*   However, when processing these operations on a GPU, there are some inefficiencies that slow down the computation:\n    1.  **GPU Memory Hierarchy**: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.\n    2.  **Memory Access Overhead**: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.\n\n1.  **GPU Memory Hierarchy**: GPUs have different types of memory. SRAM (Static Random-Access Memory) is fast but has limited size, while HBM (High Bandwidth Memory) has a much larger size but is slower. For effective GPU operations, data needs to be loaded into the quick SRAM memory. But due to SRAM’s limited size, larger intermediate results (like the matrices P, S, and O) need to be stored back into the slower HBM memory, which adds overheads.\n2.  **Memory Access Overhead**: Constantly moving these large intermediate results (P, S, and O) between the SRAM and HBM memories creates a bottleneck in performance.\n\nFlashAttention was introduced to optimize these operations for GPUs. Instead of computing the attention for the entire matrices at once, FlashAttention breaks them down into smaller blocks or tiles:\n\n1.  **Tiling**: The Q, K, and V matrices are divided into smaller blocks. These blocks are then loaded from the HBM to the faster SRAM for computation.\n2.  **Optimized Computation**: Within each block, the attention output is computed without the need to constantly shift large intermediate results between the two types of memory. This means fewer transfers between the slow and fast memories, which leads to speed improvements.\n3.  **Optimized for GPU**: While individual operations like matrix multiplication are already efficient on GPUs, FlashAttention makes the entire attention layer more GPU-friendly by minimizing memory transfers and fusing several operations.\n    *   The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.\n    *   In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.\n\n*   The result of these optimizations is a significant speedup in both training and inference times. Moreover, this optimization is now integrated into popular frameworks like PyTorch 2.0, making it easily accessible for developers.\n*   In essence, FlashAttention is a smart way to restructure and execute the attention mechanism’s computations on GPUs, minimizing the bottlenecks caused by the GPU’s memory architecture.\n\n![](/primers/ai/assets/LLM/flashAttention.png)\n\n#### Multi-Query Attention\n\n*   **Background context:**\n    *   **Multi-Head Attention (MHA) in Transformers:**\n        *   In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.\n    *   **The Challenge with MHA:**\n        *   While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.\n*   **Introducing Multi-Query Attention (MQA):**\n    *   MQA is an optimization over the standard MHA. Instead of each head having separate weight matrices for projecting the “key” and “value”, MQA proposes that all heads share a common weight matrix for these projections.\n*   **Advantages of MQA:**\n    1.  **Memory Efficiency:** By sharing weights for the “key” and “value” projections across heads, you significantly reduce the memory required for caching during inference. For instance, a model with 96 heads, like GPT-3, can reduce its memory consumption for the key/value cache by up to 96 times.\n    2.  **Speed in Inference:** Since you’re now working with shared projections and a reduced cache size, the calculation of attention scores during inference becomes faster. This is especially beneficial when generating longer sequences of text.\n    3.  **Maintains Training Speed:** Despite these changes, the training speed remains largely unaffected. This means you get the advantages of MQA without any significant downside in terms of training time.\n\n*   **Multi-Head Attention (MHA) in Transformers:**\n    *   In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.\n*   **The Challenge with MHA:**\n    *   While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.\n\n*   In the original Transformer architecture, the attention mechanism uses multiple heads. Each of these heads independently calculates its own attention scores by projecting the input into different “query”, “key”, and “value” spaces using separate weight matrices. The outputs from all heads are then concatenated and linearly transformed to produce the final result.\n\n*   While MHA allows the model to focus on different parts of the input simultaneously, it has its costs. One such cost is memory usage. During the inference stage, especially in decoders, previous tokens’ “keys” and “values” are cached so that the model doesn’t need to recompute them for each new token. However, as more tokens are processed, the cache grows, consuming more GPU memory.\n\n*   MQA is an optimization over the standard MHA. Instead of each head having separate weight matrices for projecting the “key” and “value”, MQA proposes that all heads share a common weight matrix for these projections.\n\n1.  **Memory Efficiency:** By sharing weights for the “key” and “value” projections across heads, you significantly reduce the memory required for caching during inference. For instance, a model with 96 heads, like GPT-3, can reduce its memory consumption for the key/value cache by up to 96 times.\n2.  **Speed in Inference:** Since you’re now working with shared projections and a reduced cache size, the calculation of attention scores during inference becomes faster. This is especially beneficial when generating longer sequences of text.\n3.  **Maintains Training Speed:** Despite these changes, the training speed remains largely unaffected. This means you get the advantages of MQA without any significant downside in terms of training time.",
    "order": 9,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 15,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "gpt",
      "llm",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 2901,
      "contentLength": 29181
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#solutions-to-challenges",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-comparative-analysis-10",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Comparative Analysis",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Stability:</strong> PI’s methodology, in certain settings, can be more consistent in performance than RoPE.</p>\n  </li>\n  <li>\n    <p><strong>Approach:</strong> While PI essentially ‘squeezes’ or ‘stretches’ positional indices to align with existing embeddings, RoPE modifies the very nature of how embeddings encapsulate position information.</p>\n  </li>\n  <li>\n    <p><strong>Training Dynamics:</strong> Post-PI models often crave some refinement to accommodate the interpolated positions, whereas RoPE’s intrinsic design means it’s already geared up for variable sequence lengths without additional training.</p>\n  </li>\n  <li>\n    <p><strong>Flexibility:</strong> RoPE’s absence of dependency on fixed embeddings gives it an edge, as it can gracefully handle sequences of any conceivable length.</p>\n  </li>\n</ul>\n<p><strong>Stability:</strong> PI’s methodology, in certain settings, can be more consistent in performance than RoPE.</p>\n<p><strong>Approach:</strong> While PI essentially ‘squeezes’ or ‘stretches’ positional indices to align with existing embeddings, RoPE modifies the very nature of how embeddings encapsulate position information.</p>\n<p><strong>Training Dynamics:</strong> Post-PI models often crave some refinement to accommodate the interpolated positions, whereas RoPE’s intrinsic design means it’s already geared up for variable sequence lengths without additional training.</p>\n<p><strong>Flexibility:</strong> RoPE’s absence of dependency on fixed embeddings gives it an edge, as it can gracefully handle sequences of any conceivable length.</p>",
    "contentMarkdown": "*   **Stability:** PI’s methodology, in certain settings, can be more consistent in performance than RoPE.\n    \n*   **Approach:** While PI essentially ‘squeezes’ or ‘stretches’ positional indices to align with existing embeddings, RoPE modifies the very nature of how embeddings encapsulate position information.\n    \n*   **Training Dynamics:** Post-PI models often crave some refinement to accommodate the interpolated positions, whereas RoPE’s intrinsic design means it’s already geared up for variable sequence lengths without additional training.\n    \n*   **Flexibility:** RoPE’s absence of dependency on fixed embeddings gives it an edge, as it can gracefully handle sequences of any conceivable length.\n    \n\n**Stability:** PI’s methodology, in certain settings, can be more consistent in performance than RoPE.\n\n**Approach:** While PI essentially ‘squeezes’ or ‘stretches’ positional indices to align with existing embeddings, RoPE modifies the very nature of how embeddings encapsulate position information.\n\n**Training Dynamics:** Post-PI models often crave some refinement to accommodate the interpolated positions, whereas RoPE’s intrinsic design means it’s already geared up for variable sequence lengths without additional training.\n\n**Flexibility:** RoPE’s absence of dependency on fixed embeddings gives it an edge, as it can gracefully handle sequences of any conceivable length.",
    "order": 10,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 184,
      "contentLength": 1602
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#comparative-analysis",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-dynamically-scaled-rope-11",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Dynamically Scaled RoPE",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ul>\n  <li>Static RoPE can sometimes force a compromise between catering to very long sequences and maintaining efficacy on shorter ones. Enter the dynamic variant of RoPE, which seeks to fluidly adjust scaling based on the sequence’s length, offering the best of both worlds.</li>\n</ul>\n<h4 id=\"approach\">Approach</h4>\n<ul>\n  <li>\n    <p><strong>Adaptivity:</strong> Instead of a one-size-fits-all scaling, this method tailors the scaling based on the present sequence length. It’s like adjusting the focus of a camera lens based on the subject’s distance.</p>\n  </li>\n  <li>\n    <p><strong>Scale Dynamics:</strong> The model starts with precise position values for the initial context (up to the first 2k tokens). Beyond that, it recalibrates the scaling factor in real-time, relative to how the sequence length evolves.</p>\n  </li>\n</ul>\n<p><strong>Adaptivity:</strong> Instead of a one-size-fits-all scaling, this method tailors the scaling based on the present sequence length. It’s like adjusting the focus of a camera lens based on the subject’s distance.</p>\n<p><strong>Scale Dynamics:</strong> The model starts with precise position values for the initial context (up to the first 2k tokens). Beyond that, it recalibrates the scaling factor in real-time, relative to how the sequence length evolves.</p>\n<h4 id=\"key-benefits\">Key Benefits</h4>\n<ul>\n  <li>\n    <p><strong>Performance Boost:</strong> Dynamic scaling typically exhibits enhanced efficacy compared to its static counterpart and other techniques like NTK-Aware.</p>\n  </li>\n  <li>\n    <p><strong>Versatility:</strong> The real-time adaptability ensures that the model remains effective across a wide spectrum of sequence lengths.</p>\n  </li>\n</ul>\n<p><strong>Performance Boost:</strong> Dynamic scaling typically exhibits enhanced efficacy compared to its static counterpart and other techniques like NTK-Aware.</p>\n<p><strong>Versatility:</strong> The real-time adaptability ensures that the model remains effective across a wide spectrum of sequence lengths.</p>\n<h4 id=\"ntk-aware-method-perspective\">NTK-Aware Method Perspective</h4>\n<ul>\n  <li>\n    <p><strong>Performance Metrics:</strong> NTK-Aware may falter a bit with shorter sequences, but it tends to flourish as sequences grow.</p>\n  </li>\n  <li>\n    <p><strong>Parameter Dynamics:</strong> Both dynamic RoPE and NTK-Aware possess parameters influencing their performance over different sequence lengths. The distinction lies in dynamic RoPE’s ability to adjust these parameters on-the-fly based on the current sequence length, enhancing its responsiveness.</p>\n  </li>\n</ul>\n<p><strong>Performance Metrics:</strong> NTK-Aware may falter a bit with shorter sequences, but it tends to flourish as sequences grow.</p>\n<p><strong>Parameter Dynamics:</strong> Both dynamic RoPE and NTK-Aware possess parameters influencing their performance over different sequence lengths. The distinction lies in dynamic RoPE’s ability to adjust these parameters on-the-fly based on the current sequence length, enhancing its responsiveness.</p>\n<h4 id=\"summary-1\">Summary</h4>\n<ul>\n  <li>Dynamically Scaled RoPE, with its adaptive nature, represents a promising direction in the quest for more versatile and efficient language models. By dynamically adjusting to varying sequence lengths, it ensures that models like LLaMA maintain optimal performance across diverse contexts.</li>\n</ul>",
    "contentMarkdown": "*   Static RoPE can sometimes force a compromise between catering to very long sequences and maintaining efficacy on shorter ones. Enter the dynamic variant of RoPE, which seeks to fluidly adjust scaling based on the sequence’s length, offering the best of both worlds.\n\n#### Approach\n\n*   **Adaptivity:** Instead of a one-size-fits-all scaling, this method tailors the scaling based on the present sequence length. It’s like adjusting the focus of a camera lens based on the subject’s distance.\n    \n*   **Scale Dynamics:** The model starts with precise position values for the initial context (up to the first 2k tokens). Beyond that, it recalibrates the scaling factor in real-time, relative to how the sequence length evolves.\n    \n\n**Adaptivity:** Instead of a one-size-fits-all scaling, this method tailors the scaling based on the present sequence length. It’s like adjusting the focus of a camera lens based on the subject’s distance.\n\n**Scale Dynamics:** The model starts with precise position values for the initial context (up to the first 2k tokens). Beyond that, it recalibrates the scaling factor in real-time, relative to how the sequence length evolves.\n\n#### Key Benefits\n\n*   **Performance Boost:** Dynamic scaling typically exhibits enhanced efficacy compared to its static counterpart and other techniques like NTK-Aware.\n    \n*   **Versatility:** The real-time adaptability ensures that the model remains effective across a wide spectrum of sequence lengths.\n    \n\n**Performance Boost:** Dynamic scaling typically exhibits enhanced efficacy compared to its static counterpart and other techniques like NTK-Aware.\n\n**Versatility:** The real-time adaptability ensures that the model remains effective across a wide spectrum of sequence lengths.\n\n#### NTK-Aware Method Perspective\n\n*   **Performance Metrics:** NTK-Aware may falter a bit with shorter sequences, but it tends to flourish as sequences grow.\n    \n*   **Parameter Dynamics:** Both dynamic RoPE and NTK-Aware possess parameters influencing their performance over different sequence lengths. The distinction lies in dynamic RoPE’s ability to adjust these parameters on-the-fly based on the current sequence length, enhancing its responsiveness.\n    \n\n**Performance Metrics:** NTK-Aware may falter a bit with shorter sequences, but it tends to flourish as sequences grow.\n\n**Parameter Dynamics:** Both dynamic RoPE and NTK-Aware possess parameters influencing their performance over different sequence lengths. The distinction lies in dynamic RoPE’s ability to adjust these parameters on-the-fly based on the current sequence length, enhancing its responsiveness.\n\n#### Summary\n\n*   Dynamically Scaled RoPE, with its adaptive nature, represents a promising direction in the quest for more versatile and efficient language models. By dynamically adjusting to varying sequence lengths, it ensures that models like LLaMA maintain optimal performance across diverse contexts.",
    "order": 11,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 413,
      "contentLength": 3401
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#dynamically-scaled-rope",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-related-traditional-dbs-vs-vector-dbs-12",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Related: Traditional DBs V/s Vector DBs",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ul>\n  <li>The below infographic <a href=\"https://www.linkedin.com/in/svonava\">(source)</a> performs a comparative analysis between traditional databases and vector databases.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/dbca.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "*   The below infographic [(source)](https://www.linkedin.com/in/svonava) performs a comparative analysis between traditional databases and vector databases.\n\n![](/primers/ai/assets/LLM/dbca.jpeg)",
    "order": 12,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 16,
      "contentLength": 245
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#related:-traditional-dbs-v/s-vector-dbs",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-when-not-to-use-vector-dbs-13",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "When Not to Use Vector DBs?",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ul>\n  <li>Credits for this section go to <a href=\"https://www.linkedin.com/in/prithivirajdamodaran/\">Prithivi Da</a>.</li>\n  <li>Vector DBs are “leaky abstractions”.</li>\n</ul>\n<blockquote>\n  <p>A leaky abstraction is an abstraction that exposes details and limitations of its underlying implementation to its users that should ideally be hidden away</p>\n</blockquote>\n<p>A leaky abstraction is an abstraction that exposes details and limitations of its underlying implementation to its users that should ideally be hidden away</p>\n<ul>\n  <li>Encoders are usecase specific, so you need to know which encoder and hidden dimension size will yield the best representation. For some-cases / domains you have to train your own instead of using a pre-trained one.</li>\n  <li>Default similarity and distance functions for relevance may not be good for all usecase. Cosine is default for most VdBs (which tools like langchain blindly keep). For instance if the indexed data is noisy, <a href=\"https://www.learndatasci.com/glossary/jaccard-similarity\">Jaccard similarity</a> is robust to noise, while cosine similarity is not.</li>\n  <li>LSH isn’t a good option for multi billion record scale hence Milvus skipped it keeping only <a href=\"https://towardsdatascience.com/similarity-search-with-ivfpq-9c6348fd4db3\">IVF-PQ</a> and HSNW; you should know when to use what.</li>\n  <li>If your case requires “read-your-own-writes” the latency of encoding and indexing cannot tolerate your needs.</li>\n  <li>Total Cost of Ownership (TCO) is higher compared to traditional and hybrid data stores.</li>\n  <li>Backfilling can be very slow if your historical dataset is huge.</li>\n</ul>",
    "contentMarkdown": "*   Credits for this section go to [Prithivi Da](https://www.linkedin.com/in/prithivirajdamodaran/).\n*   Vector DBs are “leaky abstractions”.\n\n> A leaky abstraction is an abstraction that exposes details and limitations of its underlying implementation to its users that should ideally be hidden away\n\nA leaky abstraction is an abstraction that exposes details and limitations of its underlying implementation to its users that should ideally be hidden away\n\n*   Encoders are usecase specific, so you need to know which encoder and hidden dimension size will yield the best representation. For some-cases / domains you have to train your own instead of using a pre-trained one.\n*   Default similarity and distance functions for relevance may not be good for all usecase. Cosine is default for most VdBs (which tools like langchain blindly keep). For instance if the indexed data is noisy, [Jaccard similarity](https://www.learndatasci.com/glossary/jaccard-similarity) is robust to noise, while cosine similarity is not.\n*   LSH isn’t a good option for multi billion record scale hence Milvus skipped it keeping only [IVF-PQ](https://towardsdatascience.com/similarity-search-with-ivfpq-9c6348fd4db3) and HSNW; you should know when to use what.\n*   If your case requires “read-your-own-writes” the latency of encoding and indexing cannot tolerate your needs.\n*   Total Cost of Ownership (TCO) is higher compared to traditional and hybrid data stores.\n*   Backfilling can be very slow if your historical dataset is huge.",
    "order": 13,
    "orderInChapter": 6,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 217,
      "contentLength": 1666
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#when-not-to-use-vector-dbs?",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-knowledge-graphs-with-llms-best-of-both-worlds-14",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Context Length Extension",
    "title": "Knowledge Graphs with LLMs: Best of Both Worlds",
    "subtitle": "Context Length Extension",
    "contentHtml": "<ul>\n  <li>Credits to the following section go to <a href=\"https://www.linkedin.com/in/tonyseale/\">Tony Seale</a>.</li>\n  <li>The recent increasing significance on LLMs within organisations is not just a fleeting fad but part of a transformative shift that all forward-thinking organisations must come to terms with. However, for an organisation to succeed in this transition, effectively leveraging ontologies (of which knowledge graphs are a popular instantiation) is a crucial factor.</li>\n  <li>LLMs possess remarkable AI capabilities, allowing them to comprehend and generate human-like text by learning intricate patterns from vast volumes of training data. These powerful models are capable of crafting eloquent letters, analysing data, generating code, orchestrating workflows, and performing a myriad of other complex tasks. Their potential seems increasingly disruptive, with Microsoft even ‘betting the house’ on them.</li>\n  <li>However, when deploying LLMs within an enterprise context, reliability, trustworthiness, and understandability are vital concerns for those running and governing these systems. Hallucination is simply not an option.</li>\n  <li>Ontologies offer structured and formal representations of knowledge, defining relationships between concepts within specific domains. These structures enable computers to comprehend and reason in a logical, consistent, and comprehensible manner. Yet, designing and maintaining ontologies requires substantial effort. Before LLMs came along, they were the ‘top dog in town’ when it came to a semantic understanding, but now they seem relatively inflexible, incomplete and slow to change.</li>\n  <li>Enter the intriguing and powerful synergy created by the convergence of LLMs AND Ontologies. The ability of LLMs to generate and extend ontologies is a game-changer. Although you still need a ‘human-in-the-loop,’ the top LLMs demonstrate surprising effectiveness. Simultaneously, ontologies provide vital context to the prompts given to LLMs, enriching the accuracy and relevance of the LLM’s responses. Ontologies can also be used to validate the consistency of those responses.</li>\n  <li>LLMs can help discover new knowledge, and the ontologies compile that knowledge down for future use.</li>\n  <li>This collaborative partnership between LLMs and ontologies establishes a reinforcing feedback loop of continuous improvement. As LLMs help generate better ontologies faster and more dynamically, the ontologies, in turn, elevate the performance of LLMs by offering a more comprehensive context of the data and text they analyse. This positive feedback loop has the potential to catalyse an exponential leap in the capabilities of AI applications within organisations, streamlining processes, adding intelligence, and enhancing customer experiences like never before.</li>\n</ul>\n<h4 id=\"continuous-vs-discrete-knowledge-representation\">Continuous V/s Discrete Knowledge Representation</h4>\n<ul>\n  <li>Credits to the following section go to <a href=\"https://www.linkedin.com/in/tonyseale/\">Tony Seale</a>.</li>\n  <li>We can think of information existing in a continuous stream or in discrete chunks. LLMs fall under the category of continuous knowledge representation, while Knowledge Graphs belong to the discrete realm. Each approach has its merits, and understanding the implications of their differences is essential.</li>\n  <li>LLM embeddings are dense, continuous real-valued vectors existing in a high-dimensional space. Think of them as coordinates on a map: just as longitude and latitude can pinpoint a location on a two-dimensional map, embeddings guide us to rough positions in a multi-dimensional ‘semantic space’ made up of the connections between the words on the internet. Since the embedding vectors are continuous, they allow for an infinite range of values within a given interval, making the embeddings’ coordinates ‘fuzzy’.</li>\n  <li>An LLM embedding for ‘Jennifer Aniston’ will be a several-thousand-dimensional continuous vector that leads to a location in a several-billion-parameter ‘word-space’. If we add the ‘TV series’ embedding to this vector then I will be pulled towards the position of the ‘Friends’ vector. Magic! But this magic comes with a price: you can never quite trust the answers. Hallucination and creativity are two sides of the same coin.</li>\n  <li>On the other hand, Knowledge Graphs embrace a discrete representation approach, where each entity is associated with a unique URL. For example, the Wikidata URL for Jennifer Aniston is <code class=\"language-plaintext highlighter-rouge\">https://www.wikidata.org/wiki/Q32522</code>. This represents a discrete location in ‘DNS + IP space’. Humans have carefully structured data that is reliable, editable, and explainable. However, the discrete nature of Knowledge Graphs also comes with its own price. There is no magical internal animation here; just static facts.</li>\n</ul>",
    "contentMarkdown": "*   Credits to the following section go to [Tony Seale](https://www.linkedin.com/in/tonyseale/).\n*   The recent increasing significance on LLMs within organisations is not just a fleeting fad but part of a transformative shift that all forward-thinking organisations must come to terms with. However, for an organisation to succeed in this transition, effectively leveraging ontologies (of which knowledge graphs are a popular instantiation) is a crucial factor.\n*   LLMs possess remarkable AI capabilities, allowing them to comprehend and generate human-like text by learning intricate patterns from vast volumes of training data. These powerful models are capable of crafting eloquent letters, analysing data, generating code, orchestrating workflows, and performing a myriad of other complex tasks. Their potential seems increasingly disruptive, with Microsoft even ‘betting the house’ on them.\n*   However, when deploying LLMs within an enterprise context, reliability, trustworthiness, and understandability are vital concerns for those running and governing these systems. Hallucination is simply not an option.\n*   Ontologies offer structured and formal representations of knowledge, defining relationships between concepts within specific domains. These structures enable computers to comprehend and reason in a logical, consistent, and comprehensible manner. Yet, designing and maintaining ontologies requires substantial effort. Before LLMs came along, they were the ‘top dog in town’ when it came to a semantic understanding, but now they seem relatively inflexible, incomplete and slow to change.\n*   Enter the intriguing and powerful synergy created by the convergence of LLMs AND Ontologies. The ability of LLMs to generate and extend ontologies is a game-changer. Although you still need a ‘human-in-the-loop,’ the top LLMs demonstrate surprising effectiveness. Simultaneously, ontologies provide vital context to the prompts given to LLMs, enriching the accuracy and relevance of the LLM’s responses. Ontologies can also be used to validate the consistency of those responses.\n*   LLMs can help discover new knowledge, and the ontologies compile that knowledge down for future use.\n*   This collaborative partnership between LLMs and ontologies establishes a reinforcing feedback loop of continuous improvement. As LLMs help generate better ontologies faster and more dynamically, the ontologies, in turn, elevate the performance of LLMs by offering a more comprehensive context of the data and text they analyse. This positive feedback loop has the potential to catalyse an exponential leap in the capabilities of AI applications within organisations, streamlining processes, adding intelligence, and enhancing customer experiences like never before.\n\n#### Continuous V/s Discrete Knowledge Representation\n\n*   Credits to the following section go to [Tony Seale](https://www.linkedin.com/in/tonyseale/).\n*   We can think of information existing in a continuous stream or in discrete chunks. LLMs fall under the category of continuous knowledge representation, while Knowledge Graphs belong to the discrete realm. Each approach has its merits, and understanding the implications of their differences is essential.\n*   LLM embeddings are dense, continuous real-valued vectors existing in a high-dimensional space. Think of them as coordinates on a map: just as longitude and latitude can pinpoint a location on a two-dimensional map, embeddings guide us to rough positions in a multi-dimensional ‘semantic space’ made up of the connections between the words on the internet. Since the embedding vectors are continuous, they allow for an infinite range of values within a given interval, making the embeddings’ coordinates ‘fuzzy’.\n*   An LLM embedding for ‘Jennifer Aniston’ will be a several-thousand-dimensional continuous vector that leads to a location in a several-billion-parameter ‘word-space’. If we add the ‘TV series’ embedding to this vector then I will be pulled towards the position of the ‘Friends’ vector. Magic! But this magic comes with a price: you can never quite trust the answers. Hallucination and creativity are two sides of the same coin.\n*   On the other hand, Knowledge Graphs embrace a discrete representation approach, where each entity is associated with a unique URL. For example, the Wikidata URL for Jennifer Aniston is `https://www.wikidata.org/wiki/Q32522`. This represents a discrete location in ‘DNS + IP space’. Humans have carefully structured data that is reliable, editable, and explainable. However, the discrete nature of Knowledge Graphs also comes with its own price. There is no magical internal animation here; just static facts.",
    "order": 14,
    "orderInChapter": 7,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 673,
      "contentLength": 4936
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#knowledge-graphs-with-llms:-best-of-both-worlds",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-rag-for-limiting-hallucination-15",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "The “Context Stuffing” Problem",
    "title": "RAG for Limiting Hallucination",
    "subtitle": "The “Context Stuffing” Problem",
    "contentHtml": "<ul>\n  <li>Hallucination is typically caused due to imperfections in training data, lack of access to external, real-world knowledge, and limited contextual understanding from prompts.</li>\n  <li>RAG (using either an agent or an external data-source such as a Vector DB) can serve as a means to alleviate model hallucination and improve accuracy.</li>\n  <li>Furthermore, augmenting the prompt using examples is another effective strategy to reduce hallucination.</li>\n  <li>Another approach which has recently gained traction is plan-and-execute where the model is asked to first plan and then solve the problem step-by-step while paying attention to calculations.</li>\n  <li>Lastly, as contaminated training data can cause hallucinations, cleaning up the data and fine-tuning your model can also help reduce hallucinations. However, as most models are large to train or even fine-tune, this approach should be used while taking the cost-vs-accuracy tradeoff into consideration.</li>\n</ul>",
    "contentMarkdown": "*   Hallucination is typically caused due to imperfections in training data, lack of access to external, real-world knowledge, and limited contextual understanding from prompts.\n*   RAG (using either an agent or an external data-source such as a Vector DB) can serve as a means to alleviate model hallucination and improve accuracy.\n*   Furthermore, augmenting the prompt using examples is another effective strategy to reduce hallucination.\n*   Another approach which has recently gained traction is plan-and-execute where the model is asked to first plan and then solve the problem step-by-step while paying attention to calculations.\n*   Lastly, as contaminated training data can cause hallucinations, cleaning up the data and fine-tuning your model can also help reduce hallucinations. However, as most models are large to train or even fine-tune, this approach should be used while taking the cost-vs-accuracy tradeoff into consideration.",
    "order": 15,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "attention",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 139,
      "contentLength": 989
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#rag-for-limiting-hallucination",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-fine-tuning-vs-prompting-16",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Methods to Knowledge-Augment LLMs",
    "title": "Fine-tuning vs. Prompting",
    "subtitle": "Methods to Knowledge-Augment LLMs",
    "contentHtml": "<ul>\n  <li>You can think of fine-tuning as a more-powerful form of prompting, where instead of writing your instructions in text you actually encode them in the weights of the model itself. You do this by training an existing model on example input/output pairs that demonstrate the task you want your fine-tuned model to learn. Fine-tuning can work with as few as 50 examples but offers optimal performance with thousands to tens of thousands  if possible.</li>\n  <li>Prompting has some big advantages over fine-tuning, as follows:\n    <ul>\n      <li>It’s way easier/faster to iterate on your instructions than label data and re-train a model.</li>\n      <li>Operationally, it’s easier to deploy one big model and just adjust its behavior as necessary vs deploying many small fine-tuned models that will likely each get lower utilization.</li>\n    </ul>\n  </li>\n  <li>On the other hand, the benefits of fine-tuning are as follows:\n    <ul>\n      <li>The biggest advantage is that it it is far more effective at guiding a model’s behavior than prompting (leading to better performance), so you can often get away with a much smaller model. This enables faster responses and lower inference costs. For e.g., a fine-tuned Llama 7B model is 50x cheaper than GPT-3.5 on a per-token basis, and for many use cases can produce results that are as good or better!</li>\n      <li>Fine-tuning enables check-pointing of your model with relevant data; while prompting requires stuffing up your prompt every single time with relevant data (which is an exercise that needs to be repeated per inference run).</li>\n      <li>Fine-tuning costs can be categorized as NRE costs; they’re one-off while with prompt tuning, per-token costs with a stuffed prompt can accumulate with every run (so the apt choice of technique for a particular use-case depends on the amount of inference runs are planned).</li>\n      <li>With LoRA-based schemes (such as QLoRA), you can fine-tune a minimal (sub-1%) fraction of parameters and still be able to render great performance levels compared to prompting.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>It’s way easier/faster to iterate on your instructions than label data and re-train a model.</li>\n      <li>Operationally, it’s easier to deploy one big model and just adjust its behavior as necessary vs deploying many small fine-tuned models that will likely each get lower utilization.</li>\n    </ul>\n<ul>\n      <li>The biggest advantage is that it it is far more effective at guiding a model’s behavior than prompting (leading to better performance), so you can often get away with a much smaller model. This enables faster responses and lower inference costs. For e.g., a fine-tuned Llama 7B model is 50x cheaper than GPT-3.5 on a per-token basis, and for many use cases can produce results that are as good or better!</li>\n      <li>Fine-tuning enables check-pointing of your model with relevant data; while prompting requires stuffing up your prompt every single time with relevant data (which is an exercise that needs to be repeated per inference run).</li>\n      <li>Fine-tuning costs can be categorized as NRE costs; they’re one-off while with prompt tuning, per-token costs with a stuffed prompt can accumulate with every run (so the apt choice of technique for a particular use-case depends on the amount of inference runs are planned).</li>\n      <li>With LoRA-based schemes (such as QLoRA), you can fine-tune a minimal (sub-1%) fraction of parameters and still be able to render great performance levels compared to prompting.</li>\n    </ul>",
    "contentMarkdown": "*   You can think of fine-tuning as a more-powerful form of prompting, where instead of writing your instructions in text you actually encode them in the weights of the model itself. You do this by training an existing model on example input/output pairs that demonstrate the task you want your fine-tuned model to learn. Fine-tuning can work with as few as 50 examples but offers optimal performance with thousands to tens of thousands if possible.\n*   Prompting has some big advantages over fine-tuning, as follows:\n    *   It’s way easier/faster to iterate on your instructions than label data and re-train a model.\n    *   Operationally, it’s easier to deploy one big model and just adjust its behavior as necessary vs deploying many small fine-tuned models that will likely each get lower utilization.\n*   On the other hand, the benefits of fine-tuning are as follows:\n    *   The biggest advantage is that it it is far more effective at guiding a model’s behavior than prompting (leading to better performance), so you can often get away with a much smaller model. This enables faster responses and lower inference costs. For e.g., a fine-tuned Llama 7B model is 50x cheaper than GPT-3.5 on a per-token basis, and for many use cases can produce results that are as good or better!\n    *   Fine-tuning enables check-pointing of your model with relevant data; while prompting requires stuffing up your prompt every single time with relevant data (which is an exercise that needs to be repeated per inference run).\n    *   Fine-tuning costs can be categorized as NRE costs; they’re one-off while with prompt tuning, per-token costs with a stuffed prompt can accumulate with every run (so the apt choice of technique for a particular use-case depends on the amount of inference runs are planned).\n    *   With LoRA-based schemes (such as QLoRA), you can fine-tune a minimal (sub-1%) fraction of parameters and still be able to render great performance levels compared to prompting.\n\n*   It’s way easier/faster to iterate on your instructions than label data and re-train a model.\n*   Operationally, it’s easier to deploy one big model and just adjust its behavior as necessary vs deploying many small fine-tuned models that will likely each get lower utilization.\n\n*   The biggest advantage is that it it is far more effective at guiding a model’s behavior than prompting (leading to better performance), so you can often get away with a much smaller model. This enables faster responses and lower inference costs. For e.g., a fine-tuned Llama 7B model is 50x cheaper than GPT-3.5 on a per-token basis, and for many use cases can produce results that are as good or better!\n*   Fine-tuning enables check-pointing of your model with relevant data; while prompting requires stuffing up your prompt every single time with relevant data (which is an exercise that needs to be repeated per inference run).\n*   Fine-tuning costs can be categorized as NRE costs; they’re one-off while with prompt tuning, per-token costs with a stuffed prompt can accumulate with every run (so the apt choice of technique for a particular use-case depends on the amount of inference runs are planned).\n*   With LoRA-based schemes (such as QLoRA), you can fine-tune a minimal (sub-1%) fraction of parameters and still be able to render great performance levels compared to prompting.",
    "order": 16,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 547,
      "contentLength": 3573
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#fine-tuning-vs.-prompting",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-rag-17",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Methods to Knowledge-Augment LLMs",
    "title": "RAG",
    "subtitle": "Methods to Knowledge-Augment LLMs",
    "contentHtml": "<ul>\n  <li>Please refer the <a href=\"../RAG\">RAG primer</a> for a detailed discourse on RAG.</li>\n</ul>\n<h4 id=\"rag-vs-fine-tuning\">RAG vs. Fine-tuning</h4>\n<ul>\n  <li>The table below <a href=\"https://arxiv.org/pdf/2312.10997v1.pdf\">(source)</a> compares RAG vs. fine-tuning.</li>\n</ul>\n<p><img src=\"../../../images/papers/RAGvsFT.jpg\" alt=\"\"></p>\n<ul>\n  <li>To summarize the above table:\n    <ol>\n      <li>RAG engages retrieval systems with LLMs to offer access to factual, access-controlled, timely information. Fine tuning <em>can not do this</em>, so there’s no competition.</li>\n      <li>Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style</li>\n      <li>All in all, focus on RAG first. A successful LLM application <em>must</em> connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li>RAG engages retrieval systems with LLMs to offer access to factual, access-controlled, timely information. Fine tuning <em>can not do this</em>, so there’s no competition.</li>\n      <li>Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style</li>\n      <li>All in all, focus on RAG first. A successful LLM application <em>must</em> connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.</li>\n    </ol>",
    "contentMarkdown": "*   Please refer the [RAG primer](../RAG) for a detailed discourse on RAG.\n\n#### RAG vs. Fine-tuning\n\n*   The table below [(source)](https://arxiv.org/pdf/2312.10997v1.pdf) compares RAG vs. fine-tuning.\n\n![](../../../images/papers/RAGvsFT.jpg)\n\n*   To summarize the above table:\n    1.  RAG engages retrieval systems with LLMs to offer access to factual, access-controlled, timely information. Fine tuning _can not do this_, so there’s no competition.\n    2.  Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style\n    3.  All in all, focus on RAG first. A successful LLM application _must_ connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.\n\n1.  RAG engages retrieval systems with LLMs to offer access to factual, access-controlled, timely information. Fine tuning _can not do this_, so there’s no competition.\n2.  Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style\n3.  All in all, focus on RAG first. A successful LLM application _must_ connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.",
    "order": 17,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 246,
      "contentLength": 1808
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#rag",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-augmenting-llms-with-knowledge-graphs-18",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Methods to Knowledge-Augment LLMs",
    "title": "Augmenting LLMs with Knowledge Graphs",
    "subtitle": "Methods to Knowledge-Augment LLMs",
    "contentHtml": "<h4 id=\"motivation\">Motivation</h4>\n<ul>\n  <li>Per <a href=\"https://www.linkedin.com/in/tonyseale\">Tony Seale</a>,\n    <ul>\n      <li>The butcher-on-the-bus is a rhetorical device that sheds light on human memory processes. Imagine recognising someone on a bus but struggling to place their identity. Without a doubt, you know them, but it takes a moment of reflection before it hits you … a-ha! They’re the local butcher!</li>\n      <li>This scenario illustrates how our memory seemingly comprises two types: one that is flexible, fuzzy, generalisable, and gradually learned, and another that is specific, precise, and acquired in a single shot.</li>\n      <li>Could this dualistic model enhance AI systems? LLMs learn statistical approximations from text corpora, granting them generalisation, flexibility, and creativity. However, they also suffer from hallucinations, unreliability, and staleness. On the other hand, databases offer accuracy, speed, and reliability but lack adaptability and intelligence.</li>\n      <li>Perhaps the key lies in bridging these two worlds, and that’s where graphs come into play. By integrating LLMs with internal data through Knowledge Graphs (KGs), we can create a Working Memory Graph (WMG) that combines the strengths of both approaches in order to achieve a given task.</li>\n      <li>To build a WMG, the LLM processes a question and returns a graph of nodes using URLs as identifiers, these URLs link to ground truths stored in the organisation’s Knowledge Graph. The WMG can also incorporate nodes representing conceptual understanding, establishing connections between the LLM’s numerical vectors and the KG’s ontological classes.</li>\n    </ul>\n  </li>\n  <li>Thus, combining the best of both worlds (LLMs with their reasoning capabilities along with KGs with their structured, static ontology) can yield a system with the structured knowledge capabilities of knowledge graphs as well as the reasoning capabilities of LLMs. This will enable unleashing the true potential of your organisation’s knowledge assets. Combining the power of LLMs with the reliability of knowledge graphs can be a game-changer. However, bridging the gap between these two representations has been an ongoing challenge. More on this in the next section.</li>\n</ul>\n<ul>\n      <li>The butcher-on-the-bus is a rhetorical device that sheds light on human memory processes. Imagine recognising someone on a bus but struggling to place their identity. Without a doubt, you know them, but it takes a moment of reflection before it hits you … a-ha! They’re the local butcher!</li>\n      <li>This scenario illustrates how our memory seemingly comprises two types: one that is flexible, fuzzy, generalisable, and gradually learned, and another that is specific, precise, and acquired in a single shot.</li>\n      <li>Could this dualistic model enhance AI systems? LLMs learn statistical approximations from text corpora, granting them generalisation, flexibility, and creativity. However, they also suffer from hallucinations, unreliability, and staleness. On the other hand, databases offer accuracy, speed, and reliability but lack adaptability and intelligence.</li>\n      <li>Perhaps the key lies in bridging these two worlds, and that’s where graphs come into play. By integrating LLMs with internal data through Knowledge Graphs (KGs), we can create a Working Memory Graph (WMG) that combines the strengths of both approaches in order to achieve a given task.</li>\n      <li>To build a WMG, the LLM processes a question and returns a graph of nodes using URLs as identifiers, these URLs link to ground truths stored in the organisation’s Knowledge Graph. The WMG can also incorporate nodes representing conceptual understanding, establishing connections between the LLM’s numerical vectors and the KG’s ontological classes.</li>\n    </ul>\n<h4 id=\"process-1\">Process</h4>\n<ul>\n  <li>Per <a href=\"https://www.linkedin.com/in/tonyseale\">Tony Seale</a>, a simple and pragmatic technique to connect your knowledge graph to LLMs effectively is as follows:\n    <ul>\n      <li><strong>Extract Relevant Nodes:</strong> Begin by pulling all the nodes that you wish to index from your Knowledge Graph, including their descriptions:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">  rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Generate Embedding Vectors:</strong> Employ your large language model to create an embedding vector for the description of each node:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">  node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Build a Vector Store:</strong> Store the generated embedding vectors in a dedicated vector store:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">  index = faiss.IndexFlatL2(len(embedding))\n  index.add(embedding)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Query with Natural Language:</strong> When a user poses a question in natural language, convert the query into an embedding vector using the same language model. Then, leverage the vector store to find the nodes with the lowest cosine similarity to the query vector:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">  question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']\n  d, i = index.search(question_embedding, 100)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Semantic Post-processing:</strong> To further enhance the user experience, apply post-processing techniques to the retrieved related nodes. This step refines the results and presents information in a way that best provides users with actionable insights.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Extract Relevant Nodes:</strong> Begin by pulling all the nodes that you wish to index from your Knowledge Graph, including their descriptions:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">  rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Generate Embedding Vectors:</strong> Employ your large language model to create an embedding vector for the description of each node:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">  node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Build a Vector Store:</strong> Store the generated embedding vectors in a dedicated vector store:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">  index = faiss.IndexFlatL2(len(embedding))\n  index.add(embedding)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Query with Natural Language:</strong> When a user poses a question in natural language, convert the query into an embedding vector using the same language model. Then, leverage the vector store to find the nodes with the lowest cosine similarity to the query vector:\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">  question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']\n  d, i = index.search(question_embedding, 100)\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Semantic Post-processing:</strong> To further enhance the user experience, apply post-processing techniques to the retrieved related nodes. This step refines the results and presents information in a way that best provides users with actionable insights.</li>\n    </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">  rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)\n</code></pre>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">  node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']\n</code></pre>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">  index = faiss.IndexFlatL2(len(embedding))\n  index.add(embedding)\n</code></pre>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">  question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']\n  d, i = index.search(question_embedding, 100)\n</code></pre>",
    "contentMarkdown": "#### Motivation\n\n*   Per [Tony Seale](https://www.linkedin.com/in/tonyseale),\n    *   The butcher-on-the-bus is a rhetorical device that sheds light on human memory processes. Imagine recognising someone on a bus but struggling to place their identity. Without a doubt, you know them, but it takes a moment of reflection before it hits you … a-ha! They’re the local butcher!\n    *   This scenario illustrates how our memory seemingly comprises two types: one that is flexible, fuzzy, generalisable, and gradually learned, and another that is specific, precise, and acquired in a single shot.\n    *   Could this dualistic model enhance AI systems? LLMs learn statistical approximations from text corpora, granting them generalisation, flexibility, and creativity. However, they also suffer from hallucinations, unreliability, and staleness. On the other hand, databases offer accuracy, speed, and reliability but lack adaptability and intelligence.\n    *   Perhaps the key lies in bridging these two worlds, and that’s where graphs come into play. By integrating LLMs with internal data through Knowledge Graphs (KGs), we can create a Working Memory Graph (WMG) that combines the strengths of both approaches in order to achieve a given task.\n    *   To build a WMG, the LLM processes a question and returns a graph of nodes using URLs as identifiers, these URLs link to ground truths stored in the organisation’s Knowledge Graph. The WMG can also incorporate nodes representing conceptual understanding, establishing connections between the LLM’s numerical vectors and the KG’s ontological classes.\n*   Thus, combining the best of both worlds (LLMs with their reasoning capabilities along with KGs with their structured, static ontology) can yield a system with the structured knowledge capabilities of knowledge graphs as well as the reasoning capabilities of LLMs. This will enable unleashing the true potential of your organisation’s knowledge assets. Combining the power of LLMs with the reliability of knowledge graphs can be a game-changer. However, bridging the gap between these two representations has been an ongoing challenge. More on this in the next section.\n\n*   The butcher-on-the-bus is a rhetorical device that sheds light on human memory processes. Imagine recognising someone on a bus but struggling to place their identity. Without a doubt, you know them, but it takes a moment of reflection before it hits you … a-ha! They’re the local butcher!\n*   This scenario illustrates how our memory seemingly comprises two types: one that is flexible, fuzzy, generalisable, and gradually learned, and another that is specific, precise, and acquired in a single shot.\n*   Could this dualistic model enhance AI systems? LLMs learn statistical approximations from text corpora, granting them generalisation, flexibility, and creativity. However, they also suffer from hallucinations, unreliability, and staleness. On the other hand, databases offer accuracy, speed, and reliability but lack adaptability and intelligence.\n*   Perhaps the key lies in bridging these two worlds, and that’s where graphs come into play. By integrating LLMs with internal data through Knowledge Graphs (KGs), we can create a Working Memory Graph (WMG) that combines the strengths of both approaches in order to achieve a given task.\n*   To build a WMG, the LLM processes a question and returns a graph of nodes using URLs as identifiers, these URLs link to ground truths stored in the organisation’s Knowledge Graph. The WMG can also incorporate nodes representing conceptual understanding, establishing connections between the LLM’s numerical vectors and the KG’s ontological classes.\n\n#### Process\n\n*   Per [Tony Seale](https://www.linkedin.com/in/tonyseale), a simple and pragmatic technique to connect your knowledge graph to LLMs effectively is as follows:\n    *   **Extract Relevant Nodes:** Begin by pulling all the nodes that you wish to index from your Knowledge Graph, including their descriptions:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)`\n        \n    *   **Generate Embedding Vectors:** Employ your large language model to create an embedding vector for the description of each node:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']`\n        \n    *   **Build a Vector Store:** Store the generated embedding vectors in a dedicated vector store:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `index = faiss.IndexFlatL2(len(embedding))   index.add(embedding)`\n        \n    *   **Query with Natural Language:** When a user poses a question in natural language, convert the query into an embedding vector using the same language model. Then, leverage the vector store to find the nodes with the lowest cosine similarity to the query vector:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']   d, i = index.search(question_embedding, 100)`\n        \n    *   **Semantic Post-processing:** To further enhance the user experience, apply post-processing techniques to the retrieved related nodes. This step refines the results and presents information in a way that best provides users with actionable insights.\n\n*   **Extract Relevant Nodes:** Begin by pulling all the nodes that you wish to index from your Knowledge Graph, including their descriptions:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)`\n    \n*   **Generate Embedding Vectors:** Employ your large language model to create an embedding vector for the description of each node:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']`\n    \n*   **Build a Vector Store:** Store the generated embedding vectors in a dedicated vector store:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `index = faiss.IndexFlatL2(len(embedding))   index.add(embedding)`\n    \n*   **Query with Natural Language:** When a user poses a question in natural language, convert the query into an embedding vector using the same language model. Then, leverage the vector store to find the nodes with the lowest cosine similarity to the query vector:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']   d, i = index.search(question_embedding, 100)`\n    \n*   **Semantic Post-processing:** To further enhance the user experience, apply post-processing techniques to the retrieved related nodes. This step refines the results and presents information in a way that best provides users with actionable insights.\n\n![](https://aman.ai/images/copy.png)\n\n  `rows = rdflib_graph.query(‘SELECT * WHERE {?uri dc:description ?desc}’)`\n\n![](https://aman.ai/images/copy.png)\n\n  `node_embedding = openai.Embedding.create(input = row.desc, model=model) ['data'][0]['embedding']`\n\n![](https://aman.ai/images/copy.png)\n\n  `index = faiss.IndexFlatL2(len(embedding))   index.add(embedding)`\n\n![](https://aman.ai/images/copy.png)\n\n  `question_embedding = openai.Embedding.create(input = question, model=model) ['data'][0]['embedding']   d, i = index.search(question_embedding, 100)`",
    "order": 18,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 5,
    "tags": [
      "nlpllms",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 949,
      "contentLength": 11617
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#augmenting-llms-with-knowledge-graphs",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-open-llm-leaderboard-19",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Open LLM Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>With the plethora of LLMs and chatbots being released week upon week, often with grandiose claims of their performance, it can be hard to filter out the genuine progress that is being made by the open-source community and which model is the current state of the art. The <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\">Open LLM Leaderboard</a> aims to track, rank and evaluate LLMs and chatbots as they are released.</li>\n</ul>",
    "contentMarkdown": "*   With the plethora of LLMs and chatbots being released week upon week, often with grandiose claims of their performance, it can be hard to filter out the genuine progress that is being made by the open-source community and which model is the current state of the art. The [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) aims to track, rank and evaluate LLMs and chatbots as they are released.",
    "order": 19,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 65,
      "contentLength": 465
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#open-llm-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-lmsys-chatbot-arena-leaderboard-20",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "LMSYS Chatbot Arena Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>The <a href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\">LMSYS Chatbot Arena Leaderboard</a> brings a human touch to model evaluation, utilizing  crowdsourced user votes to rank models based on real user preferences. It features open and closed models like Mistral, Gemini, etc. Put simply, LMSYS Chatbot Arena is thus a crowdsourced open platform for LLM evaluation.</li>\n  <li>Chatbot Arena ranks AI models based on three benchmarks: <a href=\"https://chat.lmsys.org/?arena\">Chatbot Arena</a>, which uses over 200,000 collected human preference votes to rank LLMs with the Elo ranking system, <a href=\"https://arxiv.org/abs/2306.05685\">MT-Bench</a>, a set of multi-turn challenging questions graded by GPT-4, and <a href=\"https://arxiv.org/abs/2009.03300\">MMLU</a> (5-shot), a multitask accuracy test covering 57 different tasks.</li>\n  <li>It spun out of <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> presented at NeurIPS 2023 by Zheng et al. from UC Berkeley, UC San Diego, Carnegie Mellon University, Stanford, and MBZUAI, which introduces an innovative approach for evaluating LLMs used as chat assistants. The authors propose using strong LLMs as judges to assess the performance of other LLMs in handling open-ended questions.</li>\n  <li>The study introduces two benchmarks: MT-Bench, a series of multi-turn questions designed to test conversational and instruction-following abilities, and Chatbot Arena, a crowdsourced battle platform for user interaction and model evaluation.</li>\n  <li>A key focus of the research is exploring the use of LLMs, like GPT-4, as automated judges in these benchmarks, to approximate human preferences. This approach, termed “LLM-as-a-judge”, is tested for its alignment with human preferences and its practicality as a scalable, cost-effective alternative to human evaluations.</li>\n  <li>The authors address several biases and limitations inherent in LLM judges. Position bias, where the order of answers affects judgment, is mitigated by randomizing answer order. Verbosity bias, the tendency to favor longer answers, is countered by length normalization. Self-enhancement bias, where LLMs might prefer answers similar to their own style, is reduced through style normalization. Limited reasoning ability in math questions is addressed by introducing chain-of-thought and reference-guided judging methods.</li>\n  <li>The figure below from the paper shows multi-turn dialogues between a user and two AI assistants—LLaMA-13B (Assistant A) and Vicuna-13B (Assistant B)—initiated by a question from the MMLU benchmark and a follow-up instruction. GPT-4 is then presented with the context to determine which assistant answers better.</li>\n</ul>\n<p><img src=\"../../../images/papers/MT-Bench.jpg\" alt=\"\"></p>\n<ul>\n  <li>In their empirical evaluations, the authors demonstrate that strong LLM judges like GPT-4 can achieve over 80% agreement with human preferences, matching the level of agreement among humans. This highlights the potential of LLMs as effective judges in the automated evaluation of chatbots.</li>\n  <li>The study also evaluates the performance of various LLM models, including Vicuna and LLaMA variants, using MT-Bench and Chatbot Arena, underscoring the effectiveness of these benchmarks in differentiating the capabilities of chatbots.</li>\n  <li>The research contributes to the field by offering a systematic study of the LLM-as-a-judge approach and by providing publicly available datasets from MT-bench and Chatbot Arena for future exploration. The paper argues for a hybrid evaluation framework that combines capability-based benchmarks with preference-based benchmarks for comprehensive model assessment.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\"><img src=\"/primers/ai/assets/LLM/ChatbotArena.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The [LMSYS Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) brings a human touch to model evaluation, utilizing crowdsourced user votes to rank models based on real user preferences. It features open and closed models like Mistral, Gemini, etc. Put simply, LMSYS Chatbot Arena is thus a crowdsourced open platform for LLM evaluation.\n*   Chatbot Arena ranks AI models based on three benchmarks: [Chatbot Arena](https://chat.lmsys.org/?arena), which uses over 200,000 collected human preference votes to rank LLMs with the Elo ranking system, [MT-Bench](https://arxiv.org/abs/2306.05685), a set of multi-turn challenging questions graded by GPT-4, and [MMLU](https://arxiv.org/abs/2009.03300) (5-shot), a multitask accuracy test covering 57 different tasks.\n*   It spun out of [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) presented at NeurIPS 2023 by Zheng et al. from UC Berkeley, UC San Diego, Carnegie Mellon University, Stanford, and MBZUAI, which introduces an innovative approach for evaluating LLMs used as chat assistants. The authors propose using strong LLMs as judges to assess the performance of other LLMs in handling open-ended questions.\n*   The study introduces two benchmarks: MT-Bench, a series of multi-turn questions designed to test conversational and instruction-following abilities, and Chatbot Arena, a crowdsourced battle platform for user interaction and model evaluation.\n*   A key focus of the research is exploring the use of LLMs, like GPT-4, as automated judges in these benchmarks, to approximate human preferences. This approach, termed “LLM-as-a-judge”, is tested for its alignment with human preferences and its practicality as a scalable, cost-effective alternative to human evaluations.\n*   The authors address several biases and limitations inherent in LLM judges. Position bias, where the order of answers affects judgment, is mitigated by randomizing answer order. Verbosity bias, the tendency to favor longer answers, is countered by length normalization. Self-enhancement bias, where LLMs might prefer answers similar to their own style, is reduced through style normalization. Limited reasoning ability in math questions is addressed by introducing chain-of-thought and reference-guided judging methods.\n*   The figure below from the paper shows multi-turn dialogues between a user and two AI assistants—LLaMA-13B (Assistant A) and Vicuna-13B (Assistant B)—initiated by a question from the MMLU benchmark and a follow-up instruction. GPT-4 is then presented with the context to determine which assistant answers better.\n\n![](../../../images/papers/MT-Bench.jpg)\n\n*   In their empirical evaluations, the authors demonstrate that strong LLM judges like GPT-4 can achieve over 80% agreement with human preferences, matching the level of agreement among humans. This highlights the potential of LLMs as effective judges in the automated evaluation of chatbots.\n*   The study also evaluates the performance of various LLM models, including Vicuna and LLaMA variants, using MT-Bench and Chatbot Arena, underscoring the effectiveness of these benchmarks in differentiating the capabilities of chatbots.\n*   The research contributes to the field by offering a systematic study of the LLM-as-a-judge approach and by providing publicly available datasets from MT-bench and Chatbot Arena for future exploration. The paper argues for a hybrid evaluation framework that combines capability-based benchmarks with preference-based benchmarks for comprehensive model assessment.\n\n[![](/primers/ai/assets/LLM/ChatbotArena.jpg)](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)",
    "order": 20,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 488,
      "contentLength": 3904
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#lmsys-chatbot-arena-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-massive-text-embedding-benchmark-mteb-leaderboard-21",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Massive Text Embedding Benchmark (MTEB) Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation.</li>\n  <li>To solve this problem, Muennighoff et al. from HuggingFace and cohere.ai introduce the <a href=\"https://huggingface.co/spaces/mteb/leaderboard\">Massive Text Embedding Benchmark (MTEB) Leaderboard</a>. MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages.</li>\n  <li>Through the benchmarking of 33 models on MTEB, they establish the most comprehensive benchmark of text embeddings to date. The following figure from the paper shows an overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/MTEB.jpg\" alt=\"\"></p>\n<ul>\n  <li>They find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks.</li>\n  <li><a href=\"https://arxiv.org/abs/2210.07316\">Paper</a>.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/mteb/leaderboard\"><img src=\"/primers/ai/assets/LLM/MTEB2.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation.\n*   To solve this problem, Muennighoff et al. from HuggingFace and cohere.ai introduce the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages.\n*   Through the benchmarking of 33 models on MTEB, they establish the most comprehensive benchmark of text embeddings to date. The following figure from the paper shows an overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade\n\n![](/primers/ai/assets/LLM/MTEB.jpg)\n\n*   They find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks.\n*   [Paper](https://arxiv.org/abs/2210.07316).\n\n[![](/primers/ai/assets/LLM/MTEB2.jpg)](https://huggingface.co/spaces/mteb/leaderboard)",
    "order": 21,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 188,
      "contentLength": 1534
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#massive-text-embedding-benchmark-(mteb)-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-big-code-models-leaderboard-22",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Big Code Models Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Inspired from the <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\">Open LLM Leaderboard</a> and <a href=\"https://huggingface.co/spaces/optimum/llm-perf-leaderboard\">Open LLM-Perf Leaderboard</a>, the <a href=\"https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\">Big Code Models Leaderboard</a> compares performance of base multilingual code generation models on <a href=\"https://huggingface.co/datasets/openai_humaneval\">HumanEval</a> benchmark and <a href=\"https://huggingface.co/datasets/nuprl/MultiPL-E\">MultiPL-E</a>.</li>\n  <li>Note that <a href=\"https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\">Big Code Models Leaderboard</a> only compares open pre-trained multilingual code models (and not fine-tuned models).</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\"><img src=\"/primers/ai/assets/LLM/BigCode.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Inspired from the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) and [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard), the [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) compares performance of base multilingual code generation models on [HumanEval](https://huggingface.co/datasets/openai_humaneval) benchmark and [MultiPL-E](https://huggingface.co/datasets/nuprl/MultiPL-E).\n*   Note that [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) only compares open pre-trained multilingual code models (and not fine-tuned models).\n\n[![](/primers/ai/assets/LLM/BigCode.jpg)](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)",
    "order": 22,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 48,
      "contentLength": 935
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#big-code-models-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-open-vlm-leaderboard-23",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Open VLM Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Based on <a href=\"https://github.com/open-compass/VLMEvalKit\">VLMEvalKit: A Toolkit for Evaluating Large Vision-Language Models</a> which is an open-source evaluation toolkit for VLMs.</li>\n  <li>As of this writing, the Open VLM Leaderboard covers 54 different VLMs (including GPT-4V, Gemini, QwenVL-Plus, LLaVA, etc.) and 22 different multi-modal benchmarks.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"><img src=\"/primers/ai/assets/LLM/VLMEvalKit.jpeg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Based on [VLMEvalKit: A Toolkit for Evaluating Large Vision-Language Models](https://github.com/open-compass/VLMEvalKit) which is an open-source evaluation toolkit for VLMs.\n*   As of this writing, the Open VLM Leaderboard covers 54 different VLMs (including GPT-4V, Gemini, QwenVL-Plus, LLaVA, etc.) and 22 different multi-modal benchmarks.\n\n[![](/primers/ai/assets/LLM/VLMEvalKit.jpeg)](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)",
    "order": 23,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 44,
      "contentLength": 523
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#open-vlm-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-berkeley-function-calling-leaderboard-24",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Berkeley Function-Calling Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Berkeley Function-Calling Leaderboard (BFCL) assesses the function-calling capabilities of various LLMs. It consists of 2,000 question-function-answer pairs across multiple programming languages (Python, Java, JavaScript, REST API, SQL).</li>\n  <li>The evaluation covers complex use cases, including simple, multiple, and parallel function calls, requiring the selection and simultaneous execution of functions.</li>\n  <li>BFCL tests function relevance detection to see how models handle irrelevant functions, expecting them to return an error message.</li>\n  <li>Both proprietary and open-source models perform similarly in simple scenarios, but GPT-series models excel in more complex function-calling tasks.</li>\n  <li>The Gorilla OpenFunctions dataset has expanded from 100 to 2,000 data points, increasing diversity and complexity in evaluations. The dataset includes functions from varied fields such as Mathematics, Sports, Finance, and more, covering 40 sub-domains.</li>\n  <li>Evaluations are divided into Python (simple, multiple, parallel, parallel multiple functions) and Non-Python (chatting capability, function relevance, REST API, SQL, Java, JavaScript) categories.</li>\n  <li>Python evaluations cover scenarios from single function calls to complex parallel multiple function calls.</li>\n  <li>Non-Python evaluations test models on general-purpose chat, relevance detection, and specific API and language scenarios.</li>\n  <li>Function relevance detection is a key focus, evaluating whether models avoid using irrelevant functions and highlighting their potential for hallucination.</li>\n  <li>REST API testing involves real-world GET requests with parameters in URLs and headers, assessing models’ ability to generate executable API calls.</li>\n  <li>SQL evaluation includes basic SQL queries, while Java and JavaScript testing focus on language-specific function-calling abilities.</li>\n  <li>BFCL uses AST evaluation to check syntax and structural accuracy, and executable evaluation to verify real-world function execution.</li>\n  <li>AST evaluation ensures function matching, parameter consistency, and type/value accuracy.</li>\n  <li>Executable function evaluation runs generated functions to verify response accuracy and consistency, particularly for REST APIs.</li>\n  <li>The evaluation approach requires complete matching of model outputs to expected results; partial matches are considered failures.</li>\n  <li>Ongoing development includes continuous updates and community feedback to refine evaluation methods, especially for SQL and chat capabilities.</li>\n</ul>\n<p><a href=\"https://gorilla.cs.berkeley.edu/leaderboard.html\"><img src=\"/primers/ai/assets/LLM/BFCL.png\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Berkeley Function-Calling Leaderboard (BFCL) assesses the function-calling capabilities of various LLMs. It consists of 2,000 question-function-answer pairs across multiple programming languages (Python, Java, JavaScript, REST API, SQL).\n*   The evaluation covers complex use cases, including simple, multiple, and parallel function calls, requiring the selection and simultaneous execution of functions.\n*   BFCL tests function relevance detection to see how models handle irrelevant functions, expecting them to return an error message.\n*   Both proprietary and open-source models perform similarly in simple scenarios, but GPT-series models excel in more complex function-calling tasks.\n*   The Gorilla OpenFunctions dataset has expanded from 100 to 2,000 data points, increasing diversity and complexity in evaluations. The dataset includes functions from varied fields such as Mathematics, Sports, Finance, and more, covering 40 sub-domains.\n*   Evaluations are divided into Python (simple, multiple, parallel, parallel multiple functions) and Non-Python (chatting capability, function relevance, REST API, SQL, Java, JavaScript) categories.\n*   Python evaluations cover scenarios from single function calls to complex parallel multiple function calls.\n*   Non-Python evaluations test models on general-purpose chat, relevance detection, and specific API and language scenarios.\n*   Function relevance detection is a key focus, evaluating whether models avoid using irrelevant functions and highlighting their potential for hallucination.\n*   REST API testing involves real-world GET requests with parameters in URLs and headers, assessing models’ ability to generate executable API calls.\n*   SQL evaluation includes basic SQL queries, while Java and JavaScript testing focus on language-specific function-calling abilities.\n*   BFCL uses AST evaluation to check syntax and structural accuracy, and executable evaluation to verify real-world function execution.\n*   AST evaluation ensures function matching, parameter consistency, and type/value accuracy.\n*   Executable function evaluation runs generated functions to verify response accuracy and consistency, particularly for REST APIs.\n*   The evaluation approach requires complete matching of model outputs to expected results; partial matches are considered failures.\n*   Ongoing development includes continuous updates and community feedback to refine evaluation methods, especially for SQL and chat capabilities.\n\n[![](/primers/ai/assets/LLM/BFCL.png)](https://gorilla.cs.berkeley.edu/leaderboard.html)",
    "order": 24,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 326,
      "contentLength": 2723
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#berkeley-function-calling-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-llm-safety-leaderboard-25",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "LLM Safety Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>The LLM Safety Leaderboard aims to provide a unified evaluation for LLM safety and help researchers and practitioners better understand the capabilities, limitations, and potential risks of LLMs.</li>\n  <li>The leaderboard is generated based on the trustworthiness evaluation platform <a href=\"https://decodingtrust.github.io/\">DecodingTrust</a>. DecodingTrust provides a multifaceted evaluation framework covering eight trustworthiness perspectives: toxicity, stereotype bias, adversarial robustness, OOD robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. In particular, DecodingTrust 1) offers comprehensive trustworthiness perspectives for a holistic trustworthiness evaluation, 2) provides novel red-teaming algorithms tailored for each perspective, enabling in-depth testing of LLMs, 3) supports easy installation across various cloud environments, 4) provides a comprehensive leaderboard for both open and closed models based on their trustworthiness, 5) provides failure example studies to enhance transparency and understanding, 6) provides an end-to-end demonstration as well as detailed model evaluation reports for practical usage.</li>\n  <li>Hugging Face <a href=\"https://huggingface.co/blog/leaderboard-decodingtrust\">Post</a></li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard\"><img src=\"/primers/ai/assets/LLM/LLMSafety.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The LLM Safety Leaderboard aims to provide a unified evaluation for LLM safety and help researchers and practitioners better understand the capabilities, limitations, and potential risks of LLMs.\n*   The leaderboard is generated based on the trustworthiness evaluation platform [DecodingTrust](https://decodingtrust.github.io/). DecodingTrust provides a multifaceted evaluation framework covering eight trustworthiness perspectives: toxicity, stereotype bias, adversarial robustness, OOD robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. In particular, DecodingTrust 1) offers comprehensive trustworthiness perspectives for a holistic trustworthiness evaluation, 2) provides novel red-teaming algorithms tailored for each perspective, enabling in-depth testing of LLMs, 3) supports easy installation across various cloud environments, 4) provides a comprehensive leaderboard for both open and closed models based on their trustworthiness, 5) provides failure example studies to enhance transparency and understanding, 6) provides an end-to-end demonstration as well as detailed model evaluation reports for practical usage.\n*   Hugging Face [Post](https://huggingface.co/blog/leaderboard-decodingtrust)\n\n[![](/primers/ai/assets/LLM/LLMSafety.jpg)](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard)",
    "order": 25,
    "orderInChapter": 7,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 147,
      "contentLength": 1448
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llm-safety-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-alpacaeval-leaderboard-26",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "AlpacaEval Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>The <a href=\"https://tatsu-lab.github.io/alpaca_eval/\">AlpacaEval Leaderboard</a> is an automatic evaluator for instruction-following language models.</li>\n</ul>\n<p><a href=\"https://tatsu-lab.github.io/alpaca_eval/\"><img src=\"/primers/ai/assets/LLM/AlpacaEval.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) is an automatic evaluator for instruction-following language models.\n\n[![](/primers/ai/assets/LLM/AlpacaEval.jpg)](https://tatsu-lab.github.io/alpaca_eval/)",
    "order": 26,
    "orderInChapter": 8,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 13,
      "contentLength": 291
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#alpacaeval-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-hallucination-leaderboard-27",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Hallucination Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>The <a href=\"https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard\">Hallucination Leaderboard</a> tracks, ranks and evaluates hallucinations in LLMs. It evaluates the propensity for hallucination in LLMs across a diverse array of tasks, including Closed-book Open-domain QA, Summarization, Reading Comprehension, Instruction Following, Fact-Checking, and Hallucination Detection.</li>\n  <li>The evaluation encompasses a wide range of datasets such as NQ Open, TriviaQA, TruthfulQA, XSum, CNN/DM, RACE, SQuADv2, MemoTrap, IFEval, FEVER, FaithDial, True-False, HaluEval, NQ-Swap, and PopQA, offering a comprehensive assessment of each model’s performance in generating accurate and contextually relevant content.</li>\n  <li><a href=\"https://huggingface.co/blog/leaderboard-hallucinations\">Blog</a></li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard\"><img src=\"/primers/ai/assets/LLM/HallucinationL2.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The [Hallucination Leaderboard](https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard) tracks, ranks and evaluates hallucinations in LLMs. It evaluates the propensity for hallucination in LLMs across a diverse array of tasks, including Closed-book Open-domain QA, Summarization, Reading Comprehension, Instruction Following, Fact-Checking, and Hallucination Detection.\n*   The evaluation encompasses a wide range of datasets such as NQ Open, TriviaQA, TruthfulQA, XSum, CNN/DM, RACE, SQuADv2, MemoTrap, IFEval, FEVER, FaithDial, True-False, HaluEval, NQ-Swap, and PopQA, offering a comprehensive assessment of each model’s performance in generating accurate and contextually relevant content.\n*   [Blog](https://huggingface.co/blog/leaderboard-hallucinations)\n\n[![](/primers/ai/assets/LLM/HallucinationL2.jpg)](https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard)",
    "order": 27,
    "orderInChapter": 9,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "cnn",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 84,
      "contentLength": 984
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#hallucination-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-llm-perf-leaderboard-28",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "LLM-Perf Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>The LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) tracks the decoding speed (tokens/sec), memory usage (MB), and energy consumption (tokens/kWh) of LLMs.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/optimum/llm-perf-leaderboard\"><img src=\"/primers/ai/assets/LLM/LLM-Perf.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The LLM-Perf Leaderboard\\](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) tracks the decoding speed (tokens/sec), memory usage (MB), and energy consumption (tokens/kWh) of LLMs.\n\n[![](/primers/ai/assets/LLM/LLM-Perf.jpg)](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)",
    "order": 28,
    "orderInChapter": 10,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 19,
      "contentLength": 346
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llm-perf-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-vectaras-hallucination-leaderboard-29",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Vectara’s Hallucination Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/vectara/hallucination-leaderboard/\">Vectara’s Hallucination Leaderboard</a> is computed using Vectara’s Hallucination Evaluation Model. This evaluates how often an LLM introduces hallucinations when summarizing a document.</li>\n</ul>\n<p><a href=\"https://github.com/vectara/hallucination-leaderboard/\"><img src=\"/primers/ai/assets/LLM/HallucinationL.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [Vectara’s Hallucination Leaderboard](https://github.com/vectara/hallucination-leaderboard/) is computed using Vectara’s Hallucination Evaluation Model. This evaluates how often an LLM introduces hallucinations when summarizing a document.\n\n[![](/primers/ai/assets/LLM/HallucinationL.jpg)](https://github.com/vectara/hallucination-leaderboard/)",
    "order": 29,
    "orderInChapter": 11,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 24,
      "contentLength": 408
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#vectara’s-hallucination-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-yall---yet-another-llm-leaderboard-30",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "YALL - yet Another LLM Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li><a href=\"https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard\">YALL</a> is a leaderboard made with <a href=\"https://github.com/mlabonne/llm-autoeval\">LLM AutoEval</a> using <a href=\"https://huggingface.co/NousResearch\">Nous</a> benchmark suite.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard\"><img src=\"/primers/ai/assets/LLM/YALL.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [YALL](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard) is a leaderboard made with [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) using [Nous](https://huggingface.co/NousResearch) benchmark suite.\n\n[![](/primers/ai/assets/LLM/YALL.jpg)](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard)",
    "order": 30,
    "orderInChapter": 12,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 14,
      "contentLength": 419
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#yall---yet-another-llm-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-artificial-analysis-leaderboard-31",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Artificial Analysis Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li><a href=\"https://artificialanalysis.ai/leaderboards/models\">Artificial Analysis Leaderboard</a> presents daily updated metrics to assess the performance and cost-efficiency of language models and their hosting providers (Together, Fireworks, etc.).</li>\n</ul>\n<p><a href=\"https://artificialanalysis.ai/leaderboards/models\"><img src=\"/primers/ai/assets/LLM/AA.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [Artificial Analysis Leaderboard](https://artificialanalysis.ai/leaderboards/models) presents daily updated metrics to assess the performance and cost-efficiency of language models and their hosting providers (Together, Fireworks, etc.).\n\n[![](/primers/ai/assets/LLM/AA.jpg)](https://artificialanalysis.ai/leaderboards/models)",
    "order": 31,
    "orderInChapter": 13,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 25,
      "contentLength": 390
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#artificial-analysis-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-martians-provider-leaderboard-32",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Martian’s Provider Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li><a href=\"https://leaderboard.withmartian.com/\">Martian’s Provider Leaderboard</a> collects metrics daily and tracks them over time to evaluate the performance of LLM inference providers on common LLMs.</li>\n</ul>\n<p><a href=\"https://leaderboard.withmartian.com/\"><img src=\"/primers/ai/assets/LLM/Martian.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [Martian’s Provider Leaderboard](https://leaderboard.withmartian.com/) collects metrics daily and tracks them over time to evaluate the performance of LLM inference providers on common LLMs.\n\n[![](/primers/ai/assets/LLM/Martian.jpg)](https://leaderboard.withmartian.com/)",
    "order": 32,
    "orderInChapter": 14,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 24,
      "contentLength": 335
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#martian’s-provider-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-enterprise-scenarios-leaderboard-33",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Enterprise Scenarios Leaderboard",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li><a href=\"https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard\">Enterprise Scenarios Leaderboard</a> from <a href=\"https://huggingface.co/PatronusAI\">Patronus AI</a> assesses models on real-world enterprise use cases, from finance to customer support, offering insights into their performance on tasks critical to business operations.</li>\n  <li>Hugging Face <a href=\"https://huggingface.co/blog/leaderboards-on-the-hub-patronus\">Blog</a></li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard\"><img src=\"/primers/ai/assets/LLM/Enterprise.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [Enterprise Scenarios Leaderboard](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard) from [Patronus AI](https://huggingface.co/PatronusAI) assesses models on real-world enterprise use cases, from finance to customer support, offering insights into their performance on tasks critical to business operations.\n*   Hugging Face [Blog](https://huggingface.co/blog/leaderboards-on-the-hub-patronus)\n\n[![](/primers/ai/assets/LLM/Enterprise.jpg)](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard)",
    "order": 33,
    "orderInChapter": 15,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 35,
      "contentLength": 632
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#enterprise-scenarios-leaderboard",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-thefastestai-34",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "TheFastest.AI",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Human conversations are fast, typically around 200ms between turns. This site provides reliable measurements for the inference latency of popular models using the following metrics:\n    <ul>\n      <li><strong>TTFT:</strong> Time To First Token.</li>\n      <li><strong>TPS:</strong> Tokens Per Second.</li>\n      <li><strong>Total Time:</strong> From request to final token.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>TTFT:</strong> Time To First Token.</li>\n      <li><strong>TPS:</strong> Tokens Per Second.</li>\n      <li><strong>Total Time:</strong> From request to final token.</li>\n    </ul>\n<p><a href=\"https://thefastest.ai\"><img src=\"/primers/ai/assets/LLM/TheFastest.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Human conversations are fast, typically around 200ms between turns. This site provides reliable measurements for the inference latency of popular models using the following metrics:\n    *   **TTFT:** Time To First Token.\n    *   **TPS:** Tokens Per Second.\n    *   **Total Time:** From request to final token.\n\n*   **TTFT:** Time To First Token.\n*   **TPS:** Tokens Per Second.\n*   **Total Time:** From request to final token.\n\n[![](/primers/ai/assets/LLM/TheFastest.jpg)](https://thefastest.ai)",
    "order": 34,
    "orderInChapter": 16,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 65,
      "contentLength": 716
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#thefastest.ai",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-ai-model-review-35",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "AI Model Review",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>While there are information sources about model performance and model rankings, AI Model Review aims to provide a more qualitative perspective, by asking the models to respond to a standardized set of questions. This way we can directly compare how the models respond and get an intuition about the strengths and weaknesses of different models.</li>\n</ul>\n<p><a href=\"https://aimodelreview.com/\"><img src=\"assets/LLM/AIModelReview.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   While there are information sources about model performance and model rankings, AI Model Review aims to provide a more qualitative perspective, by asking the models to respond to a standardized set of questions. This way we can directly compare how the models respond and get an intuition about the strengths and weaknesses of different models.\n\n[![](assets/LLM/AIModelReview.jpg)](https://aimodelreview.com/)",
    "order": 35,
    "orderInChapter": 17,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 57,
      "contentLength": 462
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#ai-model-review",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-judge-arena-benchmarking-llms-as-evaluators-36",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Leaderboards",
    "title": "Judge Arena: Benchmarking LLMs As Evaluators",
    "subtitle": "Leaderboards",
    "contentHtml": "<ul>\n  <li>Judge Arena is a platform designed to benchmark LLMs as evaluators by allowing users to compare models’ judging abilities side-by-side. Users vote on which judge aligns best with their preferences, contributing to a leaderboard ranking.</li>\n  <li>The system generates random or custom user input/AI response pairs for evaluation by two LLM judges, who score and critique the responses. Votes and critiques are used to rank 18 state-of-the-art LLMs, including both proprietary and open-source models. To judge the LLM judge and ensure your LLM judge is reliable, the benchmark includes a dataset annotated with scores provided by human judges, and compares how LLM-judge scores correlate with human judge scores.</li>\n  <li>Preliminary results show a competitive mix of top proprietary (e.g., GPT-4 Turbo) and open-source models (e.g., Llama 3.1), with smaller models like Qwen 2.5 7B also performing strongly. Findings align with research suggesting certain models, like Llama, excel as evaluation bases.</li>\n  <li><a href=\"https://huggingface.co/blog/arena-atla\">Blog</a></li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/AtlaAI/judge-arena\"><img src=\"/primers/ai/assets/LLM/JudgeArena.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Judge Arena is a platform designed to benchmark LLMs as evaluators by allowing users to compare models’ judging abilities side-by-side. Users vote on which judge aligns best with their preferences, contributing to a leaderboard ranking.\n*   The system generates random or custom user input/AI response pairs for evaluation by two LLM judges, who score and critique the responses. Votes and critiques are used to rank 18 state-of-the-art LLMs, including both proprietary and open-source models. To judge the LLM judge and ensure your LLM judge is reliable, the benchmark includes a dataset annotated with scores provided by human judges, and compares how LLM-judge scores correlate with human judge scores.\n*   Preliminary results show a competitive mix of top proprietary (e.g., GPT-4 Turbo) and open-source models (e.g., Llama 3.1), with smaller models like Qwen 2.5 7B also performing strongly. Findings align with research suggesting certain models, like Llama, excel as evaluation bases.\n*   [Blog](https://huggingface.co/blog/arena-atla)\n\n[![](/primers/ai/assets/LLM/JudgeArena.jpg)](https://huggingface.co/spaces/AtlaAI/judge-arena)",
    "order": 36,
    "orderInChapter": 18,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 154,
      "contentLength": 1223
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#judge-arena:-benchmarking-llms-as-evaluators",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-motivation-37",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Extending Prompt Context",
    "title": "Motivation",
    "subtitle": "Extending Prompt Context",
    "contentHtml": "<ul>\n  <li>As LLMs become ubiquitous, their applications to long sequences have been a key focus, especially for applications like summarizing text (potentially interleaved with other data sources like tables and images), writing code, and predicting protein sequences, which require the model to effectively consider long distance structural dependencies. A large context allows a pre-trained LLM to look at customer data (e.g., documents the LLM did not use in training) and responds to useful information seeking queries.</li>\n  <li>Yet, most open-source LLMs (e.g., LLaMA, MPT, Falcon) have been trained with a maximum of 2K token sequence length, which is a key limitation in modeling long sequences. Inference time solutions such as ALiBi have yet to be evaluated for larger models (e.g., MPT-7b-StoryWriter-65k+). Recent work on model scaling has shown that for a given compute budget, the best performances are not necessarily achieved by the largest models, but by smaller models trained on more data (measured by number of tokens). A smaller model is also generally preferred for inference efficiency during serving including on-device serving.</li>\n  <li>In light of this, LLMs with extended prompt contexts offer various advantages especially for use-cases centered around extracting long-range dependencies such as long-document question answering and summarization.</li>\n</ul>",
    "contentMarkdown": "*   As LLMs become ubiquitous, their applications to long sequences have been a key focus, especially for applications like summarizing text (potentially interleaved with other data sources like tables and images), writing code, and predicting protein sequences, which require the model to effectively consider long distance structural dependencies. A large context allows a pre-trained LLM to look at customer data (e.g., documents the LLM did not use in training) and responds to useful information seeking queries.\n*   Yet, most open-source LLMs (e.g., LLaMA, MPT, Falcon) have been trained with a maximum of 2K token sequence length, which is a key limitation in modeling long sequences. Inference time solutions such as ALiBi have yet to be evaluated for larger models (e.g., MPT-7b-StoryWriter-65k+). Recent work on model scaling has shown that for a given compute budget, the best performances are not necessarily achieved by the largest models, but by smaller models trained on more data (measured by number of tokens). A smaller model is also generally preferred for inference efficiency during serving including on-device serving.\n*   In light of this, LLMs with extended prompt contexts offer various advantages especially for use-cases centered around extracting long-range dependencies such as long-document question answering and summarization.",
    "order": 37,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 201,
      "contentLength": 1390
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#motivation",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-status-quo-38",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Extending Prompt Context",
    "title": "Status Quo",
    "subtitle": "Extending Prompt Context",
    "contentHtml": "<ul>\n  <li>Open-source LLMs are behind commercial models when it comes to context length. For example, OpenAI’s GPT-3.5 now has a context length of 16k, GPT-4 of 32k, and Anthropic’s Claude up 100k, while Meta’s LLaMA or Technology Innovation Institute’s Falcon only have a context length of 2k.</li>\n  <li>But it is possible to extend the context length of open-source models like LLaMa either post-pre-training or during pre-training; here are two amazing blog posts:\n    <ul>\n      <li><a href=\"https://kaiokendev.github.io/context\">Extending Context is Hard… but not Impossible</a> explores and tests how to extend LLaMa to 8k.</li>\n      <li><a href=\"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\">The Secret Sauce behind 100K context window in LLMs: all tricks in one place</a> explores the main pain points and tricks to extend the context length.</li>\n    </ul>\n  </li>\n  <li>The problem we’re looking to address is the quadratic time and space complexity of attention layer computations w.r.t. the number of input tokens <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-405\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-406\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">n</script>.</li>\n  <li>The second problem occurs when the embedding size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-408\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-411\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">d > n</script>, in which case the linear layers exhibit a quadratic time complexity w.r.t. embedding size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-413\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-414\"><span class=\"mi\" id=\"MathJax-Span-415\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">d</script>.</li>\n  <li>The third problem is Absolute/Sinusoidal Positional Embedding used in the original architecture.</li>\n  <li>In Transformer architecture, the shapes of learnable matrix weights are agnostic to the number of input tokens <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-416\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-417\"><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">n</script>.</li>\n  <li>Thus, a Transformer trained with a 2K context length can consume tokens of any length, even 100K. But the model will not produce meaningful results on 100K tokens during inference if it isn’t trained with a 100K context length.</li>\n  <li>“Training the vanilla Transformer on a giant corpus and only on a large context length is unfeasibly expensive due to the quadratic complexity w.r.t. to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-419\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-420\"><span class=\"mi\" id=\"MathJax-Span-421\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-422\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-423\"><span class=\"mi\" id=\"MathJax-Span-424\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">d</script>. LLaMA on 2K context length was <a href=\"https://matt-rickard.com/commoditization-of-large-language-models-part-3\">estimated</a> to be trained for ~$3M. Thus, LLaMA on 100K would cost ~$150M.” (<a href=\"(https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)\">source</a>)</li>\n</ul>\n<ul>\n      <li><a href=\"https://kaiokendev.github.io/context\">Extending Context is Hard… but not Impossible</a> explores and tests how to extend LLaMa to 8k.</li>\n      <li><a href=\"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\">The Secret Sauce behind 100K context window in LLMs: all tricks in one place</a> explores the main pain points and tricks to extend the context length.</li>\n    </ul>\n<h4 id=\"extending-context-length-via-rope-scaling\">Extending Context Length Via RoPE Scaling</h4>\n<ul>\n  <li>The technique was originally proposed by <a href=\"https://www.reddit.com/user/emozilla/\">u/emozilla on Reddit</a> as <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/\">“Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning”</a> and allows us to scale out the context length of models without fine-tuning by dynamically interpolating RoPE to represent longer sequences while preserving performance.</li>\n  <li>While it works well out of the box, performance can be further improved by additional fine-tuning. With RoPE scaling, companies can now easily extend open-source LLMs to the context lengths which work for their given use case.</li>\n  <li>From the Reddit <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/\">post</a>:\n    <ul>\n      <li>“When <a href=\"https://www.reddit.com/user/kaiokendev/\">u/kaiokendev</a> first posted about linearly interpolating RoPE for longer sequences, I (and a few others) had wondered if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. My idea was to use the exact position values for the first 2k context (after all, why mess with a good thing?) and then re-calculate the position vector for every new sequence length as the model generates token by token. Essentially, set scale to original model context length / current sequence length. This has the effect of slowly increasing scale as the sequence length increases.</li>\n      <li>I did some experiments and found that this has very strong performance, much better than simple linear interpolation. When <a href=\"https://www.reddit.com/user/bloc97/\">u/bloc97</a> posted his NTK-Aware method, it was much closer to this dynamic linear scaling in terms of performance. Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. Unfortunately, it also suffers from catastrophic perplexity blowup, just like regular RoPE and static linear scaling.</li>\n      <li>The main hyperparamter of NTK-Aware is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-425\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-426\"><span class=\"mi\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">\\alpha</script>. Like static linear scaling, it represents a tradeoff between short/long sequence performance. So I thought, why not use the same dynamic scaling method with NTK-Aware? For Dynamic NTK, the scaling of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-428\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-429\"><span class=\"mi\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">\\alpha</script> is set to (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-431\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-432\"><span class=\"mi\" id=\"MathJax-Span-433\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">\\alpha</script> * current sequence length / original model context length) - (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-434\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-435\"><span class=\"mi\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">\\alpha</script> - 1). The idea again is to dynamically scale the hyperparameter as the sequence length increases.</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/LLM/Ropescale.webp\" alt=\"\"></p>\n\n    <ul>\n      <li>This uses the same methodology as NTK-Aware (perplexity on GovReport test). You can check out all the code on <a href=\"https://github.com/jquesnelle/scaled-rope\">GitHub</a>.”</li>\n    </ul>\n  </li>\n  <li>Hugging Face <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling\">Transformers</a> now supports RoPE-scaling (rotary position embeddings) to extend the context length of large language models like LLaMA, GPT-NeoX, or Falcon.</li>\n  <li>So in essence, RoPE scaling dynamically rescales relative position differences based on the input length, analogous to a rope stretching and contracting.</li>\n</ul>\n<ul>\n      <li>“When <a href=\"https://www.reddit.com/user/kaiokendev/\">u/kaiokendev</a> first posted about linearly interpolating RoPE for longer sequences, I (and a few others) had wondered if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. My idea was to use the exact position values for the first 2k context (after all, why mess with a good thing?) and then re-calculate the position vector for every new sequence length as the model generates token by token. Essentially, set scale to original model context length / current sequence length. This has the effect of slowly increasing scale as the sequence length increases.</li>\n      <li>I did some experiments and found that this has very strong performance, much better than simple linear interpolation. When <a href=\"https://www.reddit.com/user/bloc97/\">u/bloc97</a> posted his NTK-Aware method, it was much closer to this dynamic linear scaling in terms of performance. Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. Unfortunately, it also suffers from catastrophic perplexity blowup, just like regular RoPE and static linear scaling.</li>\n      <li>The main hyperparamter of NTK-Aware is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-425\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-426\"><span class=\"mi\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">\\alpha</script>. Like static linear scaling, it represents a tradeoff between short/long sequence performance. So I thought, why not use the same dynamic scaling method with NTK-Aware? For Dynamic NTK, the scaling of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-428\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-429\"><span class=\"mi\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">\\alpha</script> is set to (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-431\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-432\"><span class=\"mi\" id=\"MathJax-Span-433\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">\\alpha</script> * current sequence length / original model context length) - (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-434\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-435\"><span class=\"mi\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">\\alpha</script> - 1). The idea again is to dynamically scale the hyperparameter as the sequence length increases.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/Ropescale.webp\" alt=\"\"></p>\n<ul>\n      <li>This uses the same methodology as NTK-Aware (perplexity on GovReport test). You can check out all the code on <a href=\"https://github.com/jquesnelle/scaled-rope\">GitHub</a>.”</li>\n    </ul>",
    "contentMarkdown": "*   Open-source LLMs are behind commercial models when it comes to context length. For example, OpenAI’s GPT-3.5 now has a context length of 16k, GPT-4 of 32k, and Anthropic’s Claude up 100k, while Meta’s LLaMA or Technology Innovation Institute’s Falcon only have a context length of 2k.\n*   But it is possible to extend the context length of open-source models like LLaMa either post-pre-training or during pre-training; here are two amazing blog posts:\n    *   [Extending Context is Hard… but not Impossible](https://kaiokendev.github.io/context) explores and tests how to extend LLaMa to 8k.\n    *   [The Secret Sauce behind 100K context window in LLMs: all tricks in one place](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c) explores the main pain points and tricks to extend the context length.\n*   The problem we’re looking to address is the quadratic time and space complexity of attention layer computations w.r.t. the number of input tokens nnn.\n*   The second problem occurs when the embedding size d\\>nd\\>nd > n, in which case the linear layers exhibit a quadratic time complexity w.r.t. embedding size ddd.\n*   The third problem is Absolute/Sinusoidal Positional Embedding used in the original architecture.\n*   In Transformer architecture, the shapes of learnable matrix weights are agnostic to the number of input tokens nnn.\n*   Thus, a Transformer trained with a 2K context length can consume tokens of any length, even 100K. But the model will not produce meaningful results on 100K tokens during inference if it isn’t trained with a 100K context length.\n*   “Training the vanilla Transformer on a giant corpus and only on a large context length is unfeasibly expensive due to the quadratic complexity w.r.t. to nnn and ddd. LLaMA on 2K context length was [estimated](https://matt-rickard.com/commoditization-of-large-language-models-part-3) to be trained for ~$3M. Thus, LLaMA on 100K would cost ~$150M.” ([source](\\(https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\\)))\n\n*   [Extending Context is Hard… but not Impossible](https://kaiokendev.github.io/context) explores and tests how to extend LLaMa to 8k.\n*   [The Secret Sauce behind 100K context window in LLMs: all tricks in one place](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c) explores the main pain points and tricks to extend the context length.\n\n#### Extending Context Length Via RoPE Scaling\n\n*   The technique was originally proposed by [u/emozilla on Reddit](https://www.reddit.com/user/emozilla/) as [“Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning”](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/) and allows us to scale out the context length of models without fine-tuning by dynamically interpolating RoPE to represent longer sequences while preserving performance.\n*   While it works well out of the box, performance can be further improved by additional fine-tuning. With RoPE scaling, companies can now easily extend open-source LLMs to the context lengths which work for their given use case.\n*   From the Reddit [post](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/):\n    \n    *   “When [u/kaiokendev](https://www.reddit.com/user/kaiokendev/) first posted about linearly interpolating RoPE for longer sequences, I (and a few others) had wondered if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. My idea was to use the exact position values for the first 2k context (after all, why mess with a good thing?) and then re-calculate the position vector for every new sequence length as the model generates token by token. Essentially, set scale to original model context length / current sequence length. This has the effect of slowly increasing scale as the sequence length increases.\n    *   I did some experiments and found that this has very strong performance, much better than simple linear interpolation. When [u/bloc97](https://www.reddit.com/user/bloc97/) posted his NTK-Aware method, it was much closer to this dynamic linear scaling in terms of performance. Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. Unfortunately, it also suffers from catastrophic perplexity blowup, just like regular RoPE and static linear scaling.\n    *   The main hyperparamter of NTK-Aware is αα\\\\alpha. Like static linear scaling, it represents a tradeoff between short/long sequence performance. So I thought, why not use the same dynamic scaling method with NTK-Aware? For Dynamic NTK, the scaling of αα\\\\alpha is set to (αα\\\\alpha \\* current sequence length / original model context length) - (αα\\\\alpha - 1). The idea again is to dynamically scale the hyperparameter as the sequence length increases.\n    \n    ![](/primers/ai/assets/LLM/Ropescale.webp)\n    \n    *   This uses the same methodology as NTK-Aware (perplexity on GovReport test). You can check out all the code on [GitHub](https://github.com/jquesnelle/scaled-rope).”\n*   Hugging Face [Transformers](https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling) now supports RoPE-scaling (rotary position embeddings) to extend the context length of large language models like LLaMA, GPT-NeoX, or Falcon.\n*   So in essence, RoPE scaling dynamically rescales relative position differences based on the input length, analogous to a rope stretching and contracting.\n\n*   “When [u/kaiokendev](https://www.reddit.com/user/kaiokendev/) first posted about linearly interpolating RoPE for longer sequences, I (and a few others) had wondered if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. My idea was to use the exact position values for the first 2k context (after all, why mess with a good thing?) and then re-calculate the position vector for every new sequence length as the model generates token by token. Essentially, set scale to original model context length / current sequence length. This has the effect of slowly increasing scale as the sequence length increases.\n*   I did some experiments and found that this has very strong performance, much better than simple linear interpolation. When [u/bloc97](https://www.reddit.com/user/bloc97/) posted his NTK-Aware method, it was much closer to this dynamic linear scaling in terms of performance. Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. Unfortunately, it also suffers from catastrophic perplexity blowup, just like regular RoPE and static linear scaling.\n*   The main hyperparamter of NTK-Aware is αα\\\\alpha. Like static linear scaling, it represents a tradeoff between short/long sequence performance. So I thought, why not use the same dynamic scaling method with NTK-Aware? For Dynamic NTK, the scaling of αα\\\\alpha is set to (αα\\\\alpha \\* current sequence length / original model context length) - (αα\\\\alpha - 1). The idea again is to dynamically scale the hyperparameter as the sequence length increases.\n\n![](/primers/ai/assets/LLM/Ropescale.webp)\n\n*   This uses the same methodology as NTK-Aware (perplexity on GovReport test). You can check out all the code on [GitHub](https://github.com/jquesnelle/scaled-rope).”",
    "order": 38,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 6,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "gpt",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 1055,
      "contentLength": 25507
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#status-quo",
    "scrapedAt": "2025-12-28T11:53:26.173Z"
  },
  {
    "id": "ai-LLM-summary-of-tricks-to-optimize-attentionmemory-usag-39",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Extending Prompt Context",
    "title": "Summary of Tricks to Optimize Attention/memory Usage for Extending Prompt Context",
    "subtitle": "Extending Prompt Context",
    "contentHtml": "<ul>\n  <li>From <a href=\"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\">The Secret Sauce behind 100K context window in LLMs: all tricks in one place</a>, we can summarize all the tricks to extend prompt context as follows:\n    <ol>\n      <li>One option is to train the model on 2K tokens context and then fine-tune it in longer contexts (for example, 65K). But it won’t work with the original Transformer because of the <strong>Absolute/Sinusoidal Positional Encoding.</strong> To address this, remove Absolute/Sinusoidal Positional Encoding and use <a href=\"../../../papers/#train-short-test-long-attention-with-linear-biases-enables-input-length-extrapolation\">ALiBi</a>, a simple and elegant positional embedding that doesn’t hurt accuracy. <strong>Then you can train on 2K and fine-tune on 100K.</strong></li>\n      <li>You don’t need to calculate attention scores between all tokens. Some tokens are more important than others, so Sparse Attention can be used. <strong>It will speed up both training and inference.</strong></li>\n      <li><a href=\"../../../papers/#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness\">Flash Attention</a> efficiently implements the attention layer for GPU. It uses tiling and avoids materialization of big intermediate matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>,</mo><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-437\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-438\"><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">n</span><span class=\"mo\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">(n, n)</script> that do not fit into GPU SRAM. <strong>It will speed up both training and inference.</strong></li>\n      <li><a href=\"../../../papers/#fast-transformer-decoding-one-write-head-is-all-you-need\">Multi-Query attention</a> instead of Multi-Head attention. That means you share weights across all heads when linearly projecting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-444\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-447\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-448\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">V</script>. <strong>It dramatically speeds up incremental inference.</strong></li>\n      <li><strong>Conditional computation</strong> avoids applying all model parameters to all tokens from the input sequence. <a href=\"https://arxiv.org/abs/2303.09752\">CoLT5</a> applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. <strong>It will speed up both training and inference.</strong></li>\n      <li>To fit a large context, you need a lot of RAM in GPU, so people use 80GB A100 GPUs.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li>One option is to train the model on 2K tokens context and then fine-tune it in longer contexts (for example, 65K). But it won’t work with the original Transformer because of the <strong>Absolute/Sinusoidal Positional Encoding.</strong> To address this, remove Absolute/Sinusoidal Positional Encoding and use <a href=\"../../../papers/#train-short-test-long-attention-with-linear-biases-enables-input-length-extrapolation\">ALiBi</a>, a simple and elegant positional embedding that doesn’t hurt accuracy. <strong>Then you can train on 2K and fine-tune on 100K.</strong></li>\n      <li>You don’t need to calculate attention scores between all tokens. Some tokens are more important than others, so Sparse Attention can be used. <strong>It will speed up both training and inference.</strong></li>\n      <li><a href=\"../../../papers/#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness\">Flash Attention</a> efficiently implements the attention layer for GPU. It uses tiling and avoids materialization of big intermediate matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>,</mo><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-437\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-438\"><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">n</span><span class=\"mo\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">(n, n)</script> that do not fit into GPU SRAM. <strong>It will speed up both training and inference.</strong></li>\n      <li><a href=\"../../../papers/#fast-transformer-decoding-one-write-head-is-all-you-need\">Multi-Query attention</a> instead of Multi-Head attention. That means you share weights across all heads when linearly projecting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-444\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-447\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-448\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">V</script>. <strong>It dramatically speeds up incremental inference.</strong></li>\n      <li><strong>Conditional computation</strong> avoids applying all model parameters to all tokens from the input sequence. <a href=\"https://arxiv.org/abs/2303.09752\">CoLT5</a> applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. <strong>It will speed up both training and inference.</strong></li>\n      <li>To fit a large context, you need a lot of RAM in GPU, so people use 80GB A100 GPUs.</li>\n    </ol>",
    "contentMarkdown": "*   From [The Secret Sauce behind 100K context window in LLMs: all tricks in one place](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c), we can summarize all the tricks to extend prompt context as follows:\n    1.  One option is to train the model on 2K tokens context and then fine-tune it in longer contexts (for example, 65K). But it won’t work with the original Transformer because of the **Absolute/Sinusoidal Positional Encoding.** To address this, remove Absolute/Sinusoidal Positional Encoding and use [ALiBi](../../../papers/#train-short-test-long-attention-with-linear-biases-enables-input-length-extrapolation), a simple and elegant positional embedding that doesn’t hurt accuracy. **Then you can train on 2K and fine-tune on 100K.**\n    2.  You don’t need to calculate attention scores between all tokens. Some tokens are more important than others, so Sparse Attention can be used. **It will speed up both training and inference.**\n    3.  [Flash Attention](../../../papers/#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness) efficiently implements the attention layer for GPU. It uses tiling and avoids materialization of big intermediate matrices (n,n)(n,n)(n, n) that do not fit into GPU SRAM. **It will speed up both training and inference.**\n    4.  [Multi-Query attention](../../../papers/#fast-transformer-decoding-one-write-head-is-all-you-need) instead of Multi-Head attention. That means you share weights across all heads when linearly projecting KKK and VVV. **It dramatically speeds up incremental inference.**\n    5.  **Conditional computation** avoids applying all model parameters to all tokens from the input sequence. [CoLT5](https://arxiv.org/abs/2303.09752) applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. **It will speed up both training and inference.**\n    6.  To fit a large context, you need a lot of RAM in GPU, so people use 80GB A100 GPUs.\n\n1.  One option is to train the model on 2K tokens context and then fine-tune it in longer contexts (for example, 65K). But it won’t work with the original Transformer because of the **Absolute/Sinusoidal Positional Encoding.** To address this, remove Absolute/Sinusoidal Positional Encoding and use [ALiBi](../../../papers/#train-short-test-long-attention-with-linear-biases-enables-input-length-extrapolation), a simple and elegant positional embedding that doesn’t hurt accuracy. **Then you can train on 2K and fine-tune on 100K.**\n2.  You don’t need to calculate attention scores between all tokens. Some tokens are more important than others, so Sparse Attention can be used. **It will speed up both training and inference.**\n3.  [Flash Attention](../../../papers/#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness) efficiently implements the attention layer for GPU. It uses tiling and avoids materialization of big intermediate matrices (n,n)(n,n)(n, n) that do not fit into GPU SRAM. **It will speed up both training and inference.**\n4.  [Multi-Query attention](../../../papers/#fast-transformer-decoding-one-write-head-is-all-you-need) instead of Multi-Head attention. That means you share weights across all heads when linearly projecting KKK and VVV. **It dramatically speeds up incremental inference.**\n5.  **Conditional computation** avoids applying all model parameters to all tokens from the input sequence. [CoLT5](https://arxiv.org/abs/2303.09752) applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. **It will speed up both training and inference.**\n6.  To fit a large context, you need a lot of RAM in GPU, so people use 80GB A100 GPUs.",
    "order": 39,
    "orderInChapter": 3,
    "difficulty": 5,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 484,
      "contentLength": 12746
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#summary-of-tricks-to-optimize-attention/memory-usage-for-extending-prompt-context",
    "scrapedAt": "2025-12-28T11:53:26.174Z"
  },
  {
    "id": "ai-LLM-large-prompt-context-models-40",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Extending Prompt Context",
    "title": "Large Prompt Context Models",
    "subtitle": "Extending Prompt Context",
    "contentHtml": "<ul>\n  <li>Anthropic AI <a href=\"https://www.anthropic.com/index/100k-context-windows\">announced</a> that they are expanding Claude’s context window to 100k tokens, tripling GPT-4’s maximum of 32k.</li>\n  <li>For scale: The first Harry Potter book has 76,944 words, which is ~100k tokens after tokenization.</li>\n  <li>Larger context windows significantly elevate LLMs’ capabilities across a wide range of applications:\n    <ol>\n      <li><strong>Improved comprehension of lengthy and complex texts:</strong> by accessing a greater portion of the text, LLMs can generate responses and create content that is contextually relevant, more accurate, comprehensive, and coherent. This opens the door for processing extensive documents such as academic articles or legal contracts with more accuracy.</li>\n      <li><strong>Reduced need for fine-tuning:</strong> longer prompts can support advanced prompting techniques such as Chain of Thought and Few-Shot Learning, improving the LLM’s performance at inference time.</li>\n      <li><strong>Enhanced ability to summarize and synthesize information:</strong> with a greater understanding of entire documents, LLMs can generate summaries that encapsulate the key findings and more accurately.</li>\n      <li><strong>Improved context:</strong> conversational AI systems often struggle to maintain context during extended interactions. A larger context window can store more significant portions of the conversation history, leading to more coherent and contextually appropriate responses.</li>\n    </ol>\n  </li>\n  <li>Over time, this could gradually diminish the need for vector store approaches for external knowledge retrieval in LLMs because you could now include the information as regular input.</li>\n  <li>It will likely make LLMs more efficient few-shot learners as well since more examples can now be provided via the context. However, this will likely not be a replacement for fine-tuning yet. Fine-tuning not only optimizes LLMs for domain-specific datasets, but it also helps to optimize them for a target task.</li>\n  <li>As an analogy, a person who specifically studied for a math exam will perform better than a random person who is only given past exams as examples without studying. Moreover, you can combine the two: apply in-context learning to finetuned models (a person who studied the exam subject and also uses past exams as examples).</li>\n  <li>MosaicML also announced <a href=\"https://huggingface.co/mosaicml/mpt-7b-storywriter\">MPT-65K</a>, an LLM that can handle 65k tokens.</li>\n</ul>\n<ol>\n      <li><strong>Improved comprehension of lengthy and complex texts:</strong> by accessing a greater portion of the text, LLMs can generate responses and create content that is contextually relevant, more accurate, comprehensive, and coherent. This opens the door for processing extensive documents such as academic articles or legal contracts with more accuracy.</li>\n      <li><strong>Reduced need for fine-tuning:</strong> longer prompts can support advanced prompting techniques such as Chain of Thought and Few-Shot Learning, improving the LLM’s performance at inference time.</li>\n      <li><strong>Enhanced ability to summarize and synthesize information:</strong> with a greater understanding of entire documents, LLMs can generate summaries that encapsulate the key findings and more accurately.</li>\n      <li><strong>Improved context:</strong> conversational AI systems often struggle to maintain context during extended interactions. A larger context window can store more significant portions of the conversation history, leading to more coherent and contextually appropriate responses.</li>\n    </ol>\n<h4 id=\"scaling-transformer-to-1m-tokens-and-beyond-with-rmt\"><a href=\"https://arxiv.org/abs/2304.11062\">Scaling Transformer to 1M Tokens and Beyond with RMT</a></h4>\n<ul>\n  <li>This technical report by Bulatov et al. from DeepPavlov, Artificial Intelligence Research Institute (AIRI), and London Institute for Mathematical Sciences presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing.</li>\n  <li>By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model’s effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy.</li>\n  <li>Their method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. - Their experiments demonstrate the effectiveness of RMT, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.</li>\n  <li>The following figure from the paper shows memory-intensive synthetic tasks. Synthetic tasks and the required RMT operations to solve them are presented. In the Memorize task, a fact statement is placed at the start of the sequence. In the Detect and Memorize task, a fact is randomly placed within a text sequence, making its detection more challenging. In the Reasoning task, two facts required to provide an answer are randomly placed within the text. For all tasks, the question is at the end of the sequence. ’mem’ denotes memory tokens, ’Q’ represents the question, and ’A’ signifies the answer.</li>\n</ul>\n<p><img src=\"../../../images/papers/RMT1T.jpg\" alt=\"\"></p>\n<h4 id=\"hyena-hierarchy-towards-larger-convolutional-language-models\"><a href=\"https://arxiv.org/abs/2302.10866\">Hyena Hierarchy: Towards Larger Convolutional Language Models</a></h4>\n<ul>\n  <li>Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability.</li>\n  <li>This paper by Poli et al. from proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. Guided by these findings, we introduce the Hyena hierarchy, an operator defined by a recurrence of two efficient subquadratic primitives: a long convolution and element-wise multiplicative gating (see figure below from the paper). A specified depth (i.e., number of steps) of the recurrence controls the size of the operator. For short recurrences, existing models are recovered as special cases. By mapping each step in the Hyena recurrence to its corresponding matrix form, we reveal Hyena operators to be equivalently defined as a decomposition of a data-controlled matrix i.e., a matrix whose entries are functions of the input. Furthermore, we show how Hyena operators can be evaluated efficiently without materializing the full matrix, by leveraging fast convolution algorithms. Empirically, Hyena operators are able to significantly shrink the quality gap with attention at\nscale, reaching similar perplexity and downstream performance with a smaller computational budget and without hybridization of attention.</li>\n  <li>The following figure from the paper illustrates the Hyena operator is defined as a recurrence of two efficient subquadratic primitives: an implicit long convolution <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-450\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"mi\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">h</script> (i.e. Hyena filters parameterized by a feed-forward network) and multiplicative elementwise gating of the (projected) input. The depth of the recurrence specifies the size of the operator. Hyena can equivalently be expressed as a multiplication with data-controlled (conditioned by the input <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-453\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-454\"><span class=\"mi\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">u</script>) diagonal matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>D</mi></mrow><mi>x</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-456\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-457\"><span class=\"msubsup\" id=\"MathJax-Span-458\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-459\"><span class=\"mrow\" id=\"MathJax-Span-460\"><span class=\"mi\" id=\"MathJax-Span-461\" style=\"font-family: STIXGeneral-Regular;\">D</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">D</mi></mrow><mi>x</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">\\mathrm{D}_x</script> and Toeplitz matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>S</mi></mrow><mi>h</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-463\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-464\"><span class=\"msubsup\" id=\"MathJax-Span-465\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Regular;\">S</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">S</mi></mrow><mi>h</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">\\mathrm{S}_h</script>. In addition, Hyena exhibits sublinear parameter scaling (in sequence length) and unrestricted context, similar to attention, while having lower time complexity.</li>\n</ul>\n<p><img src=\"../../../images/papers/Hyena.jpg\" alt=\"\"></p>\n<ul>\n  <li>In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.</li>\n</ul>\n<h4 id=\"longnet-scaling-transformers-to-1000000000-tokens\"><a href=\"https://arxiv.org/abs/2307.02486\">LongNet: Scaling Transformers to 1,000,000,000 Tokens</a></h4>\n<ul>\n  <li>Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted.</li>\n  <li>This paper by Ding et al. from Furu Wei’s group at MSR introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, they propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages:\n    <ol>\n      <li>It has a linear computation complexity and a logarithm dependency between tokens;</li>\n      <li>It can be served as a distributed trainer for extremely long sequences;</li>\n      <li>Its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.</li>\n    </ol>\n  </li>\n  <li>The following figure from the paper illustrates the trend of Transformer sequence lengths over time.</li>\n</ul>\n<ol>\n      <li>It has a linear computation complexity and a logarithm dependency between tokens;</li>\n      <li>It can be served as a distributed trainer for extremely long sequences;</li>\n      <li>Its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.</li>\n    </ol>\n<p><img src=\"../../../images/papers/LongNet.jpg\" alt=\"\"></p>\n<ul>\n  <li>Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.</li>\n  <li><a href=\"https://aka.ms/LongNet\">Code</a></li>\n</ul>\n<h4 id=\"extending-context-window-of-large-language-models-via-positional-interpolation\"><a href=\"https://arxiv.org/abs//2306.15595\">Extending Context Window of Large Language Models Via Positional Interpolation</a></h4>\n<ul>\n  <li>This paper by Chen et al. from Meta AI in 2023 presents Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B.</li>\n  <li>Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism.</li>\n  <li>They present a theoretical study which shows that the upper bound of interpolation is at least ∼600x smaller than that of extrapolation, further demonstrating its stability.</li>\n  <li>Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.</li>\n  <li>The following figure from the paper illustrates the Position Interpolation method. Consider a Llama model pre-trained with a 2048 context window length. Upper left illustrates the normal usage of an LLM model: input position indices (blue dots) are within the pre-trained range. Upper right illustrates length extrapolation where models are required to operate unseen positions (red dots) up to 4096. Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range.</li>\n</ul>\n<p><img src=\"../../../images/papers/PI.jpg\" alt=\"\"></p>\n<ul>\n  <li>From <a href=\"https://www.linkedin.com/in/damienbenveniste/\">Damien Benveniste</a>’s post:\n    <ul>\n      <li>The typical Transformer architecture is composed of Embeddings to encode the text input, multiple transformer blocks, and a prediction head specific to the learning task the LLM is used for.</li>\n      <li>To encode the text, we use a text embedding matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-470\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-471\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">T</script> that has the size of the token vocabulary and a positional embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-473\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"mi\" id=\"MathJax-Span-475\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">P</script> that encodes the position of the token in the input sequence. That position embedding size defines the context size. That embedding can be learned or it can be a simple sin function of the position index. Typically they are added together <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mo>+</mo><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-476\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-477\"><span class=\"mi\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-479\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-480\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi><mo>+</mo><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">T + P</script> such that the same word is encoded differently at positions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-481\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-482\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">i</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-484\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">j</script>.</li>\n      <li>The great thing about Llama 2 is that it uses Rotary Positional Embeddings (RoPE) as opposed to the typical sin function encoding. Each Attention layer is modified using that embedding and it ensures the computed attention between input tokens to be only dependent on the distance between those tokens. If token <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>T</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-487\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-488\"><span class=\"msubsup\" id=\"MathJax-Span-489\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>T</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">T_1</script> is at position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-492\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-493\"><span class=\"mi\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">i</script> and a token <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>T</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-495\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-496\"><span class=\"msubsup\" id=\"MathJax-Span-497\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-499\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>T</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">T_2</script> at position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-500\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-501\"><span class=\"mi\" id=\"MathJax-Span-502\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">j</script>, the attention <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>j</mi><mo>&amp;#x2212;</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-503\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.76em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-504\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-507\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-509\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-511\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-514\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-516\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">i</span><span class=\"mo\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>j</mi><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">A(T_1, T_2) = f(j - i)</script> is a function of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi><mo>&amp;#x2212;</mo><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-522\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.67em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-523\"><span class=\"mi\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-526\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi><mo>−</mo><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">j - i</script>. The attention is not dependent on the specific token’s locations but on their relative positions.</li>\n      <li>The technique used by the <a href=\"https://arxiv.org/abs//2306.15595\">paper</a> to extend the context window is to interpolate at non-integer positions. Basically, if the original window size is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-527\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-528\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">L</script>, you can extend it to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-530\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-531\"><span class=\"msup\" id=\"MathJax-Span-532\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-534\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>L</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">L'</script> (with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;gt;</mo><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-535\" style=\"width: 3.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.76em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-536\"><span class=\"msup\" id=\"MathJax-Span-537\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>L</mi><mo>′</mo></msup><mo>&gt;</mo><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">L' > L</script>) by rescaling the integer positions as: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>i</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>i</mi><mo>&amp;#x2217;</mo><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-542\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.95em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-543\"><span class=\"msup\" id=\"MathJax-Span-544\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-548\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">i</span><span class=\"mo\" id=\"MathJax-Span-549\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-551\"><span class=\"mrow\" id=\"MathJax-Span-552\"><span class=\"mo\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"msup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-556\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>i</mi><mo>′</mo></msup><mo>=</mo><mi>i</mi><mo>∗</mo><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><msup><mi>L</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">i' = i * L / L'</script>.</li>\n      <li>As an example, if you wanted to have a text input of 16,384 tokens (so 4x the window size of Llama 2) into Llama 2, you would just need to divide every integer position by 4: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>i</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mfrac><mi>i</mi><mn>4</mn></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-557\" style=\"width: 2.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.45em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-558\"><span class=\"msup\" id=\"MathJax-Span-559\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-561\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-562\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-563\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.102em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-565\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>i</mi><mo>′</mo></msup><mo>=</mo><mfrac><mi>i</mi><mn>4</mn></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">i' = \\frac{i}{4}</script>. To be clear, if you look at the implementation of Llama 2 available on GitHub (line 50 in <a href=\"https://github.com/facebookresearch/llama/blob/main/llama/model.py#L50\">model.py</a>), you would just need to replace the following line of code:</li>\n    </ul>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\">  <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">end</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">freqs</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">)</span> \n</code></pre></div>    </div>\n    <p>by</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">  <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">end</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">freqs</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mi\">4</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li>How simple is that? Because the model was not trained for that position embedding, you would need to fine-tune a bit the model to adapt it to that new context window and position embedding. When we think that LLama 2 will most likely be used to be fine-tuned on private data, that is the icing on the cake to be able to dynamically adapt the context window to our needs as we fine-tune it.</li>\n      <li>They were able to extend LLama’s context window by 16 times while keeping the performance at the same level!</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/LLM/cw.jpeg\" alt=\"\"></p>\n  </li>\n</ul>\n<ul>\n      <li>The typical Transformer architecture is composed of Embeddings to encode the text input, multiple transformer blocks, and a prediction head specific to the learning task the LLM is used for.</li>\n      <li>To encode the text, we use a text embedding matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-470\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-471\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">T</script> that has the size of the token vocabulary and a positional embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-473\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"mi\" id=\"MathJax-Span-475\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">P</script> that encodes the position of the token in the input sequence. That position embedding size defines the context size. That embedding can be learned or it can be a simple sin function of the position index. Typically they are added together <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mo>+</mo><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-476\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-477\"><span class=\"mi\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-479\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-480\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi><mo>+</mo><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">T + P</script> such that the same word is encoded differently at positions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-481\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-482\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">i</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-484\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">j</script>.</li>\n      <li>The great thing about Llama 2 is that it uses Rotary Positional Embeddings (RoPE) as opposed to the typical sin function encoding. Each Attention layer is modified using that embedding and it ensures the computed attention between input tokens to be only dependent on the distance between those tokens. If token <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>T</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-487\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-488\"><span class=\"msubsup\" id=\"MathJax-Span-489\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>T</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">T_1</script> is at position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-492\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-493\"><span class=\"mi\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">i</script> and a token <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>T</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-495\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-496\"><span class=\"msubsup\" id=\"MathJax-Span-497\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-499\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>T</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">T_2</script> at position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-500\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-501\"><span class=\"mi\" id=\"MathJax-Span-502\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">j</script>, the attention <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>j</mi><mo>&amp;#x2212;</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-503\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.76em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-504\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-507\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-509\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-511\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-514\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-516\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">i</span><span class=\"mo\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>j</mi><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">A(T_1, T_2) = f(j - i)</script> is a function of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi><mo>&amp;#x2212;</mo><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-522\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.67em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-523\"><span class=\"mi\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-526\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi><mo>−</mo><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">j - i</script>. The attention is not dependent on the specific token’s locations but on their relative positions.</li>\n      <li>The technique used by the <a href=\"https://arxiv.org/abs//2306.15595\">paper</a> to extend the context window is to interpolate at non-integer positions. Basically, if the original window size is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-527\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-528\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">L</script>, you can extend it to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-530\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-531\"><span class=\"msup\" id=\"MathJax-Span-532\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-534\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>L</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">L'</script> (with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;gt;</mo><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-535\" style=\"width: 3.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.76em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-536\"><span class=\"msup\" id=\"MathJax-Span-537\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>L</mi><mo>′</mo></msup><mo>&gt;</mo><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">L' > L</script>) by rescaling the integer positions as: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>i</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>i</mi><mo>&amp;#x2217;</mo><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><msup><mi>L</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-542\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.95em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-543\"><span class=\"msup\" id=\"MathJax-Span-544\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-548\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">i</span><span class=\"mo\" id=\"MathJax-Span-549\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-551\"><span class=\"mrow\" id=\"MathJax-Span-552\"><span class=\"mo\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"msup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-556\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>i</mi><mo>′</mo></msup><mo>=</mo><mi>i</mi><mo>∗</mo><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><msup><mi>L</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">i' = i * L / L'</script>.</li>\n      <li>As an example, if you wanted to have a text input of 16,384 tokens (so 4x the window size of Llama 2) into Llama 2, you would just need to divide every integer position by 4: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>i</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mfrac><mi>i</mi><mn>4</mn></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-557\" style=\"width: 2.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.45em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-558\"><span class=\"msup\" id=\"MathJax-Span-559\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-561\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-562\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-563\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.102em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-565\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>i</mi><mo>′</mo></msup><mo>=</mo><mfrac><mi>i</mi><mn>4</mn></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">i' = \\frac{i}{4}</script>. To be clear, if you look at the implementation of Llama 2 available on GitHub (line 50 in <a href=\"https://github.com/facebookresearch/llama/blob/main/llama/model.py#L50\">model.py</a>), you would just need to replace the following line of code:</li>\n    </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\">  <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">end</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">freqs</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">)</span> \n</code></pre>\n<p>by</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">  <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">end</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">freqs</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mi\">4</span>\n</code></pre>\n<ul>\n      <li>How simple is that? Because the model was not trained for that position embedding, you would need to fine-tune a bit the model to adapt it to that new context window and position embedding. When we think that LLama 2 will most likely be used to be fine-tuned on private data, that is the icing on the cake to be able to dynamically adapt the context window to our needs as we fine-tune it.</li>\n      <li>They were able to extend LLama’s context window by 16 times while keeping the performance at the same level!</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/cw.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "*   Anthropic AI [announced](https://www.anthropic.com/index/100k-context-windows) that they are expanding Claude’s context window to 100k tokens, tripling GPT-4’s maximum of 32k.\n*   For scale: The first Harry Potter book has 76,944 words, which is ~100k tokens after tokenization.\n*   Larger context windows significantly elevate LLMs’ capabilities across a wide range of applications:\n    1.  **Improved comprehension of lengthy and complex texts:** by accessing a greater portion of the text, LLMs can generate responses and create content that is contextually relevant, more accurate, comprehensive, and coherent. This opens the door for processing extensive documents such as academic articles or legal contracts with more accuracy.\n    2.  **Reduced need for fine-tuning:** longer prompts can support advanced prompting techniques such as Chain of Thought and Few-Shot Learning, improving the LLM’s performance at inference time.\n    3.  **Enhanced ability to summarize and synthesize information:** with a greater understanding of entire documents, LLMs can generate summaries that encapsulate the key findings and more accurately.\n    4.  **Improved context:** conversational AI systems often struggle to maintain context during extended interactions. A larger context window can store more significant portions of the conversation history, leading to more coherent and contextually appropriate responses.\n*   Over time, this could gradually diminish the need for vector store approaches for external knowledge retrieval in LLMs because you could now include the information as regular input.\n*   It will likely make LLMs more efficient few-shot learners as well since more examples can now be provided via the context. However, this will likely not be a replacement for fine-tuning yet. Fine-tuning not only optimizes LLMs for domain-specific datasets, but it also helps to optimize them for a target task.\n*   As an analogy, a person who specifically studied for a math exam will perform better than a random person who is only given past exams as examples without studying. Moreover, you can combine the two: apply in-context learning to finetuned models (a person who studied the exam subject and also uses past exams as examples).\n*   MosaicML also announced [MPT-65K](https://huggingface.co/mosaicml/mpt-7b-storywriter), an LLM that can handle 65k tokens.\n\n1.  **Improved comprehension of lengthy and complex texts:** by accessing a greater portion of the text, LLMs can generate responses and create content that is contextually relevant, more accurate, comprehensive, and coherent. This opens the door for processing extensive documents such as academic articles or legal contracts with more accuracy.\n2.  **Reduced need for fine-tuning:** longer prompts can support advanced prompting techniques such as Chain of Thought and Few-Shot Learning, improving the LLM’s performance at inference time.\n3.  **Enhanced ability to summarize and synthesize information:** with a greater understanding of entire documents, LLMs can generate summaries that encapsulate the key findings and more accurately.\n4.  **Improved context:** conversational AI systems often struggle to maintain context during extended interactions. A larger context window can store more significant portions of the conversation history, leading to more coherent and contextually appropriate responses.\n\n#### [Scaling Transformer to 1M Tokens and Beyond with RMT](https://arxiv.org/abs/2304.11062)\n\n*   This technical report by Bulatov et al. from DeepPavlov, Artificial Intelligence Research Institute (AIRI), and London Institute for Mathematical Sciences presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing.\n*   By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model’s effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy.\n*   Their method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. - Their experiments demonstrate the effectiveness of RMT, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.\n*   The following figure from the paper shows memory-intensive synthetic tasks. Synthetic tasks and the required RMT operations to solve them are presented. In the Memorize task, a fact statement is placed at the start of the sequence. In the Detect and Memorize task, a fact is randomly placed within a text sequence, making its detection more challenging. In the Reasoning task, two facts required to provide an answer are randomly placed within the text. For all tasks, the question is at the end of the sequence. ’mem’ denotes memory tokens, ’Q’ represents the question, and ’A’ signifies the answer.\n\n![](../../../images/papers/RMT1T.jpg)\n\n#### [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)\n\n*   Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability.\n*   This paper by Poli et al. from proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. Guided by these findings, we introduce the Hyena hierarchy, an operator defined by a recurrence of two efficient subquadratic primitives: a long convolution and element-wise multiplicative gating (see figure below from the paper). A specified depth (i.e., number of steps) of the recurrence controls the size of the operator. For short recurrences, existing models are recovered as special cases. By mapping each step in the Hyena recurrence to its corresponding matrix form, we reveal Hyena operators to be equivalently defined as a decomposition of a data-controlled matrix i.e., a matrix whose entries are functions of the input. Furthermore, we show how Hyena operators can be evaluated efficiently without materializing the full matrix, by leveraging fast convolution algorithms. Empirically, Hyena operators are able to significantly shrink the quality gap with attention at scale, reaching similar perplexity and downstream performance with a smaller computational budget and without hybridization of attention.\n*   The following figure from the paper illustrates the Hyena operator is defined as a recurrence of two efficient subquadratic primitives: an implicit long convolution hhh (i.e. Hyena filters parameterized by a feed-forward network) and multiplicative elementwise gating of the (projected) input. The depth of the recurrence specifies the size of the operator. Hyena can equivalently be expressed as a multiplication with data-controlled (conditioned by the input uuu) diagonal matrices DxDx\\\\mathrm{D}\\_x and Toeplitz matrices ShSh\\\\mathrm{S}\\_h. In addition, Hyena exhibits sublinear parameter scaling (in sequence length) and unrestricted context, similar to attention, while having lower time complexity.\n\n![](../../../images/papers/Hyena.jpg)\n\n*   In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.\n\n#### [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)\n\n*   Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted.\n*   This paper by Ding et al. from Furu Wei’s group at MSR introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, they propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages:\n    1.  It has a linear computation complexity and a logarithm dependency between tokens;\n    2.  It can be served as a distributed trainer for extremely long sequences;\n    3.  Its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.\n*   The following figure from the paper illustrates the trend of Transformer sequence lengths over time.\n\n1.  It has a linear computation complexity and a logarithm dependency between tokens;\n2.  It can be served as a distributed trainer for extremely long sequences;\n3.  Its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.\n\n![](../../../images/papers/LongNet.jpg)\n\n*   Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.\n*   [Code](https://aka.ms/LongNet)\n\n#### [Extending Context Window of Large Language Models Via Positional Interpolation](https://arxiv.org/abs//2306.15595)\n\n*   This paper by Chen et al. from Meta AI in 2023 presents Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B.\n*   Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism.\n*   They present a theoretical study which shows that the upper bound of interpolation is at least ∼600x smaller than that of extrapolation, further demonstrating its stability.\n*   Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.\n*   The following figure from the paper illustrates the Position Interpolation method. Consider a Llama model pre-trained with a 2048 context window length. Upper left illustrates the normal usage of an LLM model: input position indices (blue dots) are within the pre-trained range. Upper right illustrates length extrapolation where models are required to operate unseen positions (red dots) up to 4096. Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from \\[0, 4096\\] to \\[0, 2048\\] to force them to reside in the pretrained range.\n\n![](../../../images/papers/PI.jpg)\n\n*   From [Damien Benveniste](https://www.linkedin.com/in/damienbenveniste/)’s post:\n    \n    *   The typical Transformer architecture is composed of Embeddings to encode the text input, multiple transformer blocks, and a prediction head specific to the learning task the LLM is used for.\n    *   To encode the text, we use a text embedding matrix TTT that has the size of the token vocabulary and a positional embedding PPP that encodes the position of the token in the input sequence. That position embedding size defines the context size. That embedding can be learned or it can be a simple sin function of the position index. Typically they are added together T+PT+PT + P such that the same word is encoded differently at positions iii and jjj.\n    *   The great thing about Llama 2 is that it uses Rotary Positional Embeddings (RoPE) as opposed to the typical sin function encoding. Each Attention layer is modified using that embedding and it ensures the computed attention between input tokens to be only dependent on the distance between those tokens. If token T1T1T\\_1 is at position iii and a token T2T2T\\_2 at position jjj, the attention A(T1,T2)\\=f(j−i)A(T1,T2)\\=f(j−i)A(T\\_1, T\\_2) = f(j - i) is a function of j−ij−ij - i. The attention is not dependent on the specific token’s locations but on their relative positions.\n    *   The technique used by the [paper](https://arxiv.org/abs//2306.15595) to extend the context window is to interpolate at non-integer positions. Basically, if the original window size is LLL, you can extend it to L′L′L' (with L′\\>LL′\\>LL' > L) by rescaling the integer positions as: i′\\=i∗L/L′i′\\=i∗L/L′i' = i \\* L / L'.\n    *   As an example, if you wanted to have a text input of 16,384 tokens (so 4x the window size of Llama 2) into Llama 2, you would just need to divide every integer position by 4: i′\\=i4i′\\=i4i' = \\\\frac{i}{4}. To be clear, if you look at the implementation of Llama 2 available on GitHub (line 50 in [model.py](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L50)), you would just need to replace the following line of code:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `t = torch.arange(end, device=freqs.device)` \n    \n    by\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `t = torch.arange(end, device=freqs.device) / 4`\n    \n    *   How simple is that? Because the model was not trained for that position embedding, you would need to fine-tune a bit the model to adapt it to that new context window and position embedding. When we think that LLama 2 will most likely be used to be fine-tuned on private data, that is the icing on the cake to be able to dynamically adapt the context window to our needs as we fine-tune it.\n    *   They were able to extend LLama’s context window by 16 times while keeping the performance at the same level!\n    \n    ![](/primers/ai/assets/LLM/cw.jpeg)\n    \n\n*   The typical Transformer architecture is composed of Embeddings to encode the text input, multiple transformer blocks, and a prediction head specific to the learning task the LLM is used for.\n*   To encode the text, we use a text embedding matrix TTT that has the size of the token vocabulary and a positional embedding PPP that encodes the position of the token in the input sequence. That position embedding size defines the context size. That embedding can be learned or it can be a simple sin function of the position index. Typically they are added together T+PT+PT + P such that the same word is encoded differently at positions iii and jjj.\n*   The great thing about Llama 2 is that it uses Rotary Positional Embeddings (RoPE) as opposed to the typical sin function encoding. Each Attention layer is modified using that embedding and it ensures the computed attention between input tokens to be only dependent on the distance between those tokens. If token T1T1T\\_1 is at position iii and a token T2T2T\\_2 at position jjj, the attention A(T1,T2)\\=f(j−i)A(T1,T2)\\=f(j−i)A(T\\_1, T\\_2) = f(j - i) is a function of j−ij−ij - i. The attention is not dependent on the specific token’s locations but on their relative positions.\n*   The technique used by the [paper](https://arxiv.org/abs//2306.15595) to extend the context window is to interpolate at non-integer positions. Basically, if the original window size is LLL, you can extend it to L′L′L' (with L′\\>LL′\\>LL' > L) by rescaling the integer positions as: i′\\=i∗L/L′i′\\=i∗L/L′i' = i \\* L / L'.\n*   As an example, if you wanted to have a text input of 16,384 tokens (so 4x the window size of Llama 2) into Llama 2, you would just need to divide every integer position by 4: i′\\=i4i′\\=i4i' = \\\\frac{i}{4}. To be clear, if you look at the implementation of Llama 2 available on GitHub (line 50 in [model.py](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L50)), you would just need to replace the following line of code:\n\n![](https://aman.ai/images/copy.png)\n\n  `t = torch.arange(end, device=freqs.device)` \n\nby\n\n![](https://aman.ai/images/copy.png)\n\n  `t = torch.arange(end, device=freqs.device) / 4`\n\n*   How simple is that? Because the model was not trained for that position embedding, you would need to fine-tune a bit the model to adapt it to that new context window and position embedding. When we think that LLama 2 will most likely be used to be fine-tuned on private data, that is the icing on the cake to be able to dynamically adapt the context window to our needs as we fine-tune it.\n*   They were able to extend LLama’s context window by 16 times while keeping the performance at the same level!\n\n![](/primers/ai/assets/LLM/cw.jpeg)",
    "order": 40,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 13,
    "tags": [
      "nlpllms",
      "deep learning",
      "transformer",
      "attention",
      "embedding",
      "convolution",
      "bert",
      "gpt"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 2567,
      "contentLength": 89491
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#large-prompt-context-models",
    "scrapedAt": "2025-12-28T11:53:26.174Z"
  },
  {
    "id": "ai-LLM-popular-foundation-llms-41",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Popular LLMs",
    "title": "Popular Foundation LLMs",
    "subtitle": "Popular LLMs",
    "contentHtml": "<ul>\n  <li>The following descriptions of models are from their respective project pages.</li>\n</ul>\n<h4 id=\"llama\">Llama</h4>\n<h5 id=\"llama-1\"><a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\">LLaMA</a></h5>\n<ul>\n  <li>Read our LLaMA primer <a href=\"../llama\">here</a>.</li>\n</ul>\n<h5 id=\"llama-2\"><a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\">Llama 2</a></h5>\n<ul>\n  <li><a href=\"https://arxiv.org/abs/2307.09288\">Llama 2</a> is a collection of pretrained and fine-tuned LLMs from Meta AI ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Their models outperform open-source chat models on most benchmarks we tested, and based on their human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</li>\n  <li>Llama 2 is powered by Ghost Attention (GAtt), introduced in the paper, which improves multi-turn memory. From section 3.3 in the <a href=\"https://arxiv.org/abs/2307.09288\">technical report</a>:\n    <ul>\n      <li>“In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in the below figure (left) which shows that issues with multi-turn memory (left) can be improved with GAtt (right).</li>\n    </ul>\n\n    <p><img src=\"../../../images/papers/GAtt.jpg\" alt=\"\"></p>\n\n    <ul>\n      <li>To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation <a href=\"#constitutional-ai-harmlessness-from-ai-feedback\">(Bai et al., 2022)</a> that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in the figure above (right).</li>\n      <li>GAtt Method: Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>u</mi><mi>n</mi></msub><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-566\" style=\"width: 9.013em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.4em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-567\"><span class=\"mrow\" id=\"MathJax-Span-568\"><span class=\"mo\" id=\"MathJax-Span-569\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-570\"><span class=\"msubsup\" id=\"MathJax-Span-571\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-573\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-575\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-576\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-581\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-585\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>u</mi><mi>n</mi></msub><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">\\left[u_1, a_1, \\ldots, u_n, a_n\\right]</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>u</mi><mi>n</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-589\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-590\"><span class=\"msubsup\" id=\"MathJax-Span-591\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-592\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>u</mi><mi>n</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">u_n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>a</mi><mi>n</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-594\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"msubsup\" id=\"MathJax-Span-596\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>a</mi><mi>n</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">a_n</script> correspond to the user and assistant messages for turn <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-599\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-600\"><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">n</script>, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be “act as.” We can then synthetically concatenate this instruction to all the user messages of the conversation.</li>\n      <li>Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.</li>\n      <li>For the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy e.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists of hobbies and public figures, we asked Llama 2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also modify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”-&gt; “Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.</li>\n      <li>GAtt Evaluation: We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5 in the paper). We tried to set constraints not present in the training of GAtt at inference time, for instance “Always answer with Haiku,” for which the model was found to remain consistent.</li>\n      <li>To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message (“Act as Oscar Wilde”). From the figure above, we can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left).</li>\n      <li>Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.”</li>\n    </ul>\n  </li>\n  <li>Another important aspect that is highlighted in the report is the effect of RLHF on Llama 2, and this graph from Meta’s paper shows how high-quality human preferences data (obtained from <a href=\"https://www.surgehq.ai/\">Surge AI</a>) keeps on improving Llama 2 – without saturation.</li>\n</ul>\n<ul>\n      <li>“In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in the below figure (left) which shows that issues with multi-turn memory (left) can be improved with GAtt (right).</li>\n    </ul>\n<p><img src=\"../../../images/papers/GAtt.jpg\" alt=\"\"></p>\n<ul>\n      <li>To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation <a href=\"#constitutional-ai-harmlessness-from-ai-feedback\">(Bai et al., 2022)</a> that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in the figure above (right).</li>\n      <li>GAtt Method: Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>u</mi><mi>n</mi></msub><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-566\" style=\"width: 9.013em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.4em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-567\"><span class=\"mrow\" id=\"MathJax-Span-568\"><span class=\"mo\" id=\"MathJax-Span-569\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-570\"><span class=\"msubsup\" id=\"MathJax-Span-571\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-573\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-575\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-576\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-581\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-585\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>u</mi><mi>n</mi></msub><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">\\left[u_1, a_1, \\ldots, u_n, a_n\\right]</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>u</mi><mi>n</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-589\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-590\"><span class=\"msubsup\" id=\"MathJax-Span-591\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-592\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>u</mi><mi>n</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">u_n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>a</mi><mi>n</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-594\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"msubsup\" id=\"MathJax-Span-596\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>a</mi><mi>n</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">a_n</script> correspond to the user and assistant messages for turn <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-599\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-600\"><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">n</script>, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be “act as.” We can then synthetically concatenate this instruction to all the user messages of the conversation.</li>\n      <li>Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.</li>\n      <li>For the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy e.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists of hobbies and public figures, we asked Llama 2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also modify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”-&gt; “Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.</li>\n      <li>GAtt Evaluation: We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5 in the paper). We tried to set constraints not present in the training of GAtt at inference time, for instance “Always answer with Haiku,” for which the model was found to remain consistent.</li>\n      <li>To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message (“Act as Oscar Wilde”). From the figure above, we can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left).</li>\n      <li>Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.”</li>\n    </ul>\n<p><img src=\"../../../images/papers/Llama2RLHF.jpeg\" alt=\"\"></p>\n<ul>\n  <li>They also call out the importance of supervised fine-tuning (SFT) data quality (in the “quality is all you need” section) – it’s not about volume, but diversity and quality.</li>\n  <li>From <a href=\"https://www.linkedin.com/in/drjimfan/\">Linxi Fan</a>’s notes:\n    <ul>\n      <li>Llama-2 likely costed $20M+ to train. Meta has done an incredible service to the community by releasing the model with a commercially-friendly license. AI researchers from big companies were wary of Llama-1 due to licensing issues, but now many of them will jump on the ship and contribute their firepower.</li>\n      <li>Meta’s team did a human study on 4K prompts to evaluate Llama-2’s helpfulness. They use “win rate” as a metric to compare models, in similar spirit as the Vicuna benchmark. 70B model roughly ties with GPT-3.5-0301, and performs noticeably stronger than Falcon, MPT, and Vicuna. These real human ratings should be trusted more than academic benchmarks, because they typically capture the “in-the-wild vibe” better.</li>\n      <li>Llama-2 is not yet at GPT-3.5 level, mainly because of its weak coding abilities. On “HumanEval” (standard coding benchmark), it isn’t nearly as good as StarCoder or many other models specifically designed for coding. That being said, I have little doubt that Llama-2 will improve significantly thanks to its open weights.</li>\n      <li>Meta’s team goes above and beyond on AI safety issues. In fact, almost half of the paper is talking about safety guardrails, red-teaming, and evaluations. A round of applause for such responsible efforts!</li>\n      <li>In prior works, there’s a thorny trade-ff between helpfulness and safety. Meta mitigates this by training 2 separate reward models. They aren’t open-source yet, but would be extremely valuable to the community.</li>\n      <li>Llama-2 will dramatically boost multimodal AI and robotics research. These fields need more than just blackbox access to an API.</li>\n      <li>So far, we have to convert the complex sensory signals (video, audio, 3D perception) to text description and then feed to an LLM, which is awkward and leads to huge information loss. It’d be much more effective to graft sensory modules directly on a strong LLM backbone.</li>\n      <li>The <a href=\"https://arxiv.org/abs/2307.09288\">whitepaper</a> itself is a masterpiece. Unlike GPT-4’s paper that shared very little info, Llama-2 spelled out the entire recipe, including model details, training stages, hardware, data pipeline, and annotation process. For example, there’s a systematic analysis on the effect of RLHF with nice visualizations. Quote sec 5.1: “We posit that the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF.”</li>\n    </ul>\n  </li>\n  <li>The following figure from the paper shows the training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly available online sources. Following this, they create an initial version of Llama 2-Chat through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.</li>\n</ul>\n<ul>\n      <li>Llama-2 likely costed $20M+ to train. Meta has done an incredible service to the community by releasing the model with a commercially-friendly license. AI researchers from big companies were wary of Llama-1 due to licensing issues, but now many of them will jump on the ship and contribute their firepower.</li>\n      <li>Meta’s team did a human study on 4K prompts to evaluate Llama-2’s helpfulness. They use “win rate” as a metric to compare models, in similar spirit as the Vicuna benchmark. 70B model roughly ties with GPT-3.5-0301, and performs noticeably stronger than Falcon, MPT, and Vicuna. These real human ratings should be trusted more than academic benchmarks, because they typically capture the “in-the-wild vibe” better.</li>\n      <li>Llama-2 is not yet at GPT-3.5 level, mainly because of its weak coding abilities. On “HumanEval” (standard coding benchmark), it isn’t nearly as good as StarCoder or many other models specifically designed for coding. That being said, I have little doubt that Llama-2 will improve significantly thanks to its open weights.</li>\n      <li>Meta’s team goes above and beyond on AI safety issues. In fact, almost half of the paper is talking about safety guardrails, red-teaming, and evaluations. A round of applause for such responsible efforts!</li>\n      <li>In prior works, there’s a thorny trade-ff between helpfulness and safety. Meta mitigates this by training 2 separate reward models. They aren’t open-source yet, but would be extremely valuable to the community.</li>\n      <li>Llama-2 will dramatically boost multimodal AI and robotics research. These fields need more than just blackbox access to an API.</li>\n      <li>So far, we have to convert the complex sensory signals (video, audio, 3D perception) to text description and then feed to an LLM, which is awkward and leads to huge information loss. It’d be much more effective to graft sensory modules directly on a strong LLM backbone.</li>\n      <li>The <a href=\"https://arxiv.org/abs/2307.09288\">whitepaper</a> itself is a masterpiece. Unlike GPT-4’s paper that shared very little info, Llama-2 spelled out the entire recipe, including model details, training stages, hardware, data pipeline, and annotation process. For example, there’s a systematic analysis on the effect of RLHF with nice visualizations. Quote sec 5.1: “We posit that the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF.”</li>\n    </ul>\n<p><img src=\"../../../images/papers/Llama2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Summary:\n    <ul>\n      <li>Llama 2 is available for free (including commercial license).</li>\n      <li>Llama 2 can be accessed via managed services in Azure and AWS.</li>\n      <li>Llama is trained on 2B tokens, with 4 variants, ranging from 7-70B parameters.</li>\n      <li>Llama is intended to be used in English, with almost 90% of the pre-training data being in English.</li>\n      <li>The commercial license specifies a number of harmful use cases that violate the license, including spam!</li>\n      <li>Llama 2 is very comparable to ChatGPT 3.5 in most benchmarks (particularly, it beats ChatGPT in human evaluation on helpfulness: Win 36%; Tie 32%; Loss 32%) other than coding, looking at the data mix coding data is still quite small (classified under the - unknown language category)</li>\n      <li>Llama 2 outperforms all other open-source models including Falcon and MPT, and has three variants including 7B, 13B, and 70B; the 70B variant achieves top performance across the board.</li>\n      <li>Benchmarks were done both on standardized ones (like MMLU) and head to head competition against other models, including PaLM-2 Bison and ChatGPT 3.5.</li>\n      <li>A large portion of the paper focuses on RLHF improvements and objectives which is super neat.</li>\n      <li>Model toxicity and evaluation is another large focus, including evaluations like red-teaming which were found in the Claude 2 model card. Generally Llama 2 performed very well with fewer safety violations than ChatGPT in human evaluations.</li>\n      <li>The tokenizer is the same as Llama 1 which is interesting, but the context length is now 4k, double the original 2k!</li>\n      <li>There’s both a regular and chat variation, as has been the trend in recent papers.</li>\n      <li>Llama 2 (with fine tuning) offers better domain-specificity via fine-tuning at lower cost, and better guardrails.</li>\n      <li>Llama 2 is trained on 40% more data than Llama 1 and performs well against benchmarks.</li>\n      <li>In short: companies can create their own enterprise “ChatGPT” (without sharing any data with OpenAI).</li>\n    </ul>\n  </li>\n  <li>\n    <p>Quantized Llama 2 weights are available for local inference <a href=\"https://huggingface.co/TheBloke\">here</a>.</p>\n  </li>\n  <li>The following diagram presents summarizes the key graphs/tables of the Llama 2 paper:</li>\n</ul>\n<ul>\n      <li>Llama 2 is available for free (including commercial license).</li>\n      <li>Llama 2 can be accessed via managed services in Azure and AWS.</li>\n      <li>Llama is trained on 2B tokens, with 4 variants, ranging from 7-70B parameters.</li>\n      <li>Llama is intended to be used in English, with almost 90% of the pre-training data being in English.</li>\n      <li>The commercial license specifies a number of harmful use cases that violate the license, including spam!</li>\n      <li>Llama 2 is very comparable to ChatGPT 3.5 in most benchmarks (particularly, it beats ChatGPT in human evaluation on helpfulness: Win 36%; Tie 32%; Loss 32%) other than coding, looking at the data mix coding data is still quite small (classified under the - unknown language category)</li>\n      <li>Llama 2 outperforms all other open-source models including Falcon and MPT, and has three variants including 7B, 13B, and 70B; the 70B variant achieves top performance across the board.</li>\n      <li>Benchmarks were done both on standardized ones (like MMLU) and head to head competition against other models, including PaLM-2 Bison and ChatGPT 3.5.</li>\n      <li>A large portion of the paper focuses on RLHF improvements and objectives which is super neat.</li>\n      <li>Model toxicity and evaluation is another large focus, including evaluations like red-teaming which were found in the Claude 2 model card. Generally Llama 2 performed very well with fewer safety violations than ChatGPT in human evaluations.</li>\n      <li>The tokenizer is the same as Llama 1 which is interesting, but the context length is now 4k, double the original 2k!</li>\n      <li>There’s both a regular and chat variation, as has been the trend in recent papers.</li>\n      <li>Llama 2 (with fine tuning) offers better domain-specificity via fine-tuning at lower cost, and better guardrails.</li>\n      <li>Llama 2 is trained on 40% more data than Llama 1 and performs well against benchmarks.</li>\n      <li>In short: companies can create their own enterprise “ChatGPT” (without sharing any data with OpenAI).</li>\n    </ul>\n<p>Quantized Llama 2 weights are available for local inference <a href=\"https://huggingface.co/TheBloke\">here</a>.</p>\n<p><img src=\"../../../images/papers/Llama_overv.jpeg\" alt=\"\"></p>\n<ul>\n  <li>The following infographic <a href=\"https://www.linkedin.com/in/sebastianraschka/\">(source)</a> presents an overview of Llama 2:</li>\n</ul>\n<p><img src=\"../../../images/papers/LLama2Summ.jpeg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p><a href=\"https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI\">Demo</a>; HuggingFace <a href=\"https://huggingface.co/meta-llama\">repo</a>; <a href=\"https://ai.meta.com/resources/models-and-libraries/llama/\">Project page</a>.</p>\n  </li>\n  <li>Related: <a href=\"https://github.com/karpathy/llama2.c\">llama2.c</a>\n    <ul>\n      <li>The quest for running LLMs on a single computer landed <a href=\"https://www.linkedin.com/in/andrej-karpathy-9a650716/\">Andrej Karpathy</a> to embark on a weekend project to create a simplified version of the Llama 2 model, informally called TinyLlama or BabyLlama.</li>\n      <li>Based on <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>, this is a bare-bones project with the Llama 2 architecture hard-coded, FP32 precision, one inference file of pure C with no dependencies.</li>\n      <li>“With the code in this repo you can train the Llama 2 LLM architecture from scratch in PyTorch, then export the weights to a binary file, and load that into one ~simple 500-line C file (<a href=\"https://github.com/karpathy/llama2.c/blob/master/run.c\">run.c</a>) that inferences the model. You might think that you need many billion parameter LLMs to do anything useful, but in fact very small LLMs can have surprisingly strong performance if you make the domain narrow enough.” <a href=\"https://github.com/karpathy/llama2.c\">(source)</a></li>\n      <li>Basically, it is nanoGPT, tuned to implement the Llama 2 architecture instead of GPT-2, and the meat of it was writing the C inference engine in run.c,” explained Karpathy in Llama2.c GitHub repository. His objective was to implement nanoGPT into Llama 2 architecture, instead of GPT within C programming language. The repository has already got 2.2K stars.</li>\n      <li>The success of Karpathy’s approach lies in its ability to achieve highly interactive rates, even with reasonably sized models containing a few million parameters and trained on a 15 million parameter model of the TinyStories dataset.</li>\n      <li>On a M1 MacBook Air, the Llama 2 model with ~15 million parameters can infer at around 100 tokens per second in fp32, all through the C code he developed.</li>\n      <li>This surprising result demonstrates the feasibility of running complex models on resource-constrained devices with a straightforward implementation.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper\">Llama 2 is about as factually accurate as GPT-4 for summaries and is 30X cheaper\n</a> evaluates Llama 2 for summarization and obtains stellar results compared to GPT-4, with investigations around the issues of LLMs not following instructions and ordering bias.</li>\n</ul>\n<p><a href=\"https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI\">Demo</a>; HuggingFace <a href=\"https://huggingface.co/meta-llama\">repo</a>; <a href=\"https://ai.meta.com/resources/models-and-libraries/llama/\">Project page</a>.</p>\n<ul>\n      <li>The quest for running LLMs on a single computer landed <a href=\"https://www.linkedin.com/in/andrej-karpathy-9a650716/\">Andrej Karpathy</a> to embark on a weekend project to create a simplified version of the Llama 2 model, informally called TinyLlama or BabyLlama.</li>\n      <li>Based on <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>, this is a bare-bones project with the Llama 2 architecture hard-coded, FP32 precision, one inference file of pure C with no dependencies.</li>\n      <li>“With the code in this repo you can train the Llama 2 LLM architecture from scratch in PyTorch, then export the weights to a binary file, and load that into one ~simple 500-line C file (<a href=\"https://github.com/karpathy/llama2.c/blob/master/run.c\">run.c</a>) that inferences the model. You might think that you need many billion parameter LLMs to do anything useful, but in fact very small LLMs can have surprisingly strong performance if you make the domain narrow enough.” <a href=\"https://github.com/karpathy/llama2.c\">(source)</a></li>\n      <li>Basically, it is nanoGPT, tuned to implement the Llama 2 architecture instead of GPT-2, and the meat of it was writing the C inference engine in run.c,” explained Karpathy in Llama2.c GitHub repository. His objective was to implement nanoGPT into Llama 2 architecture, instead of GPT within C programming language. The repository has already got 2.2K stars.</li>\n      <li>The success of Karpathy’s approach lies in its ability to achieve highly interactive rates, even with reasonably sized models containing a few million parameters and trained on a 15 million parameter model of the TinyStories dataset.</li>\n      <li>On a M1 MacBook Air, the Llama 2 model with ~15 million parameters can infer at around 100 tokens per second in fp32, all through the C code he developed.</li>\n      <li>This surprising result demonstrates the feasibility of running complex models on resource-constrained devices with a straightforward implementation.</li>\n    </ul>\n<h5 id=\"llama-3\"><a href=\"https://ai.meta.com/blog/meta-llama-3/\">Llama 3</a></h5>\n<ul>\n  <li><a href=\"https://ai.meta.com/blog/meta-llama-3/\">Llama 3</a> by Meta offers substantial enhancements and novelties in the capabilities of the model. An analysis of its development illustrates a significant advance over its predecessors in multiple aspects, reflecting a sustained effort towards refining language model technology.</li>\n  <li><strong>Tokenizer Enhancements</strong>: Llama 3 has seen a notable expansion in its tokenizer capacity, increasing from 32,000 tokens in Llama 2 to 128,000 tokens. This enlargement allows for more efficient sequence compression, with a reduction in sequence length by approximately 15%, thus potentially enhancing downstream task performance due to a denser information representation.</li>\n  <li><strong>Architectural Developments</strong>: Despite no radical changes in the overall architecture from Llama 2, all variants of Llama 3 now incorporate Grouped Query Attention (GQA), a scheme previously reserved for larger models. GQA facilitates a more compact representation of the keys/values in the Attention mechanism, significantly reducing the footprint of the Key-Value (KV) cache during inference, thus optimizing computational efficiency.</li>\n  <li><strong>Sequence Length Capacity</strong>: The context window for Llama 3 has been increased to 8,192 tokens, up from 4,096 in Llama 2 and 2,048 in Llama 1. While this expansion is modest compared to the capabilities of models like GPT-4, which supports up to 128,000 tokens, it marks a progressive improvement, with potential future enhancements in subsequent versions.</li>\n  <li><strong>Training Data Scope</strong>: The training dataset size for Llama 3 has escalated dramatically to 15 trillion tokens, a substantial increment from the 2 trillion tokens used for Llama 2. This dataset not only focuses on English but also includes a 5% representation from over 30 different languages, incorporating a richer diversity in data, albeit still predominantly English-centric.</li>\n  <li><strong>Scaling Laws and Efficiency</strong>: The utilization of a 15 trillion token dataset to train a model with 8 billion parameters represents an unconventional approach by current standards, where such large datasets are typically reserved for much larger models. Meta’s approach indicates a shift towards maximizing model capability and efficiency beyond traditional compute-to-performance ratios, as indicated by scaling laws such as those outlined in the Chinchilla study.</li>\n  <li><strong>Systems and Infrastructure</strong>: Llama 3’s training was executed on a system of 16,000 GPUs, achieving an observed throughput of 400 TFLOPS. This figure suggests approximately 40% utilization of the peak theoretical output based on NVIDIA’s stated capabilities for the H100 GPUs at fp16 precision, acknowledging the adjustments required for realistic sparsity conditions.</li>\n  <li><strong>Model “Strength”</strong>: Incorporating insights from the model card for Llama 3, the performance comparison between the 8 billion parameter version (Llama 3 8B) and the larger 70 billion parameter version (Llama 2 70B) reveals intriguing nuances. Notably, Llama 3 8B, which was trained with a staggering 15 trillion tokens, exhibits comparable performance to Llama 2 70B, which was trained with just 2 trillion tokens. This discrepancy in training data volume underscores the significant impact of extensive training on model performance.\n    <ul>\n      <li><strong>Performance Metrics Based on Computational Training</strong>: The metrics defining the strength of Llama 3 8B highlight its computational intensity. The model accrued approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1.8</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-602\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-603\"><span class=\"mn\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Regular;\">1.8</span><span class=\"mo\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-606\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-608\"><span class=\"mrow\" id=\"MathJax-Span-609\"><span class=\"mn\" id=\"MathJax-Span-610\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1.8</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">1.8 \\times 10^{24}</script> floating point operations (FLOPs) over 1.3 million GPU hours, assuming a throughput of 400 TFLOPS. In contrast, an alternative calculation method estimating FLOPs as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6</mn><mi>N</mi><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-611\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-612\"><span class=\"mn\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6</mn><mi>N</mi><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">6ND</script> (where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-616\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-617\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">N</script> is the number of parameters and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-619\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-620\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">D</script> is the number of tokens) yields approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>7.2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>23</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"mn\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Regular;\">7.2</span><span class=\"mo\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-626\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-627\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-628\"><span class=\"mrow\" id=\"MathJax-Span-629\"><span class=\"mn\" id=\"MathJax-Span-630\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">23</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>7.2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>23</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">7.2 \\times 10^{23}</script> FLOPs, suggesting some variability in these estimates. Prioritizing the more comprehensive GPU hours calculation, Llama 3 8B’s total computational input stands around <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-631\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-632\"><span class=\"mn\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-635\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-637\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"mn\" id=\"MathJax-Span-639\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">2 \\times 10^{24}</script> FLOPs.</li>\n      <li><strong>Comparative Analysis with Llama 3 70B and 400B Models</strong>: For Llama 3 70B, the computational input is substantially higher, reaching approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9.2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mn\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">9.2</span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-644\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-646\"><span class=\"mrow\" id=\"MathJax-Span-647\"><span class=\"mn\" id=\"MathJax-Span-648\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>9.2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">9.2 \\times 10^{24}</script> FLOPs calculated over 6.4 million GPU hours, which aligns closely with the alternative method’s estimate of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6.3</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-649\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mn\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Regular;\">6.3</span><span class=\"mo\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-653\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-655\"><span class=\"mrow\" id=\"MathJax-Span-656\"><span class=\"mn\" id=\"MathJax-Span-657\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6.3</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">6.3 \\times 10^{24}</script> FLOPs. Should the 400 billion parameter model train on the same dataset, the expected computational investment would scale up to approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>25</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mn\" id=\"MathJax-Span-660\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-662\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-664\"><span class=\"mrow\" id=\"MathJax-Span-665\"><span class=\"mn\" id=\"MathJax-Span-666\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">25</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>4</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>25</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">4 \\times 10^{25}</script> FLOPs. This projection places it just below the threshold outlined in regulatory frameworks such as the recent Biden Executive Order, which sets a reporting requirement at <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>26</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-667\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-668\"><span class=\"mn\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-671\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-673\"><span class=\"mrow\" id=\"MathJax-Span-674\"><span class=\"mn\" id=\"MathJax-Span-675\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">26</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>26</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">1 \\times 10^{26}</script> FLOPs.</li>\n      <li><strong>The Significance of Data Quality and Comprehensive Model Evaluation</strong>: Beyond raw computational power, the quality of training data plays a critical role in shaping a model’s effectiveness. The integration of diverse and high-quality data can significantly enhance model performance, emphasizing the importance of not reducing the model’s capability to merely its computational input. However, when simplifying the comparison across models, total FLOPs provide a useful measure, amalgamating the scale of the model and the extent of its training into a singular metric indicative of its overall ‘strength.’</li>\n    </ul>\n  </li>\n  <li>In conclusion, Llama 3’s architecture and training regimen illustrate Meta’s strategic emphasis on maximizing model efficiency and performance through both scaled parameter counts and extensive training, setting new benchmarks in the landscape of language models. This approach not only boosts performance but also extends the model’s applicability and utility across a wider range of tasks and scenarios.</li>\n  <li><strong>Conclusion</strong>: The advancements in Llama 3 underscore Meta’s commitment to pushing the boundaries of what small yet powerfully trained models can achieve. This strategy not only enhances the capabilities of such models but also broadens their applicability in real-world scenarios, paving the way for future innovations in machine learning landscapes. Moreover, the anticipation surrounding the potential release of a 400 billion parameter model highlights the community’s eagerness for more robust, accessible AI tools, reflecting a growing trend towards democratizing high-performance computational models.</li>\n  <li><a href=\"https://ai.meta.com/blog/meta-llama-3/\">Blog</a>; <a href=\"https://meta.ai\">Model Demo</a>; <a href=\"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md\">Model Card</a>; <a href=\"https://github.com/pytorch/torchtune\">TorchTune</a></li>\n</ul>\n<ul>\n      <li><strong>Performance Metrics Based on Computational Training</strong>: The metrics defining the strength of Llama 3 8B highlight its computational intensity. The model accrued approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1.8</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-602\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-603\"><span class=\"mn\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Regular;\">1.8</span><span class=\"mo\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-606\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-608\"><span class=\"mrow\" id=\"MathJax-Span-609\"><span class=\"mn\" id=\"MathJax-Span-610\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1.8</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">1.8 \\times 10^{24}</script> floating point operations (FLOPs) over 1.3 million GPU hours, assuming a throughput of 400 TFLOPS. In contrast, an alternative calculation method estimating FLOPs as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6</mn><mi>N</mi><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-611\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-612\"><span class=\"mn\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6</mn><mi>N</mi><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">6ND</script> (where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-616\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-617\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">N</script> is the number of parameters and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-619\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-620\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">D</script> is the number of tokens) yields approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>7.2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>23</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"mn\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Regular;\">7.2</span><span class=\"mo\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-626\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-627\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-628\"><span class=\"mrow\" id=\"MathJax-Span-629\"><span class=\"mn\" id=\"MathJax-Span-630\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">23</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>7.2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>23</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">7.2 \\times 10^{23}</script> FLOPs, suggesting some variability in these estimates. Prioritizing the more comprehensive GPU hours calculation, Llama 3 8B’s total computational input stands around <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-631\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-632\"><span class=\"mn\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-635\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-637\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"mn\" id=\"MathJax-Span-639\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">2 \\times 10^{24}</script> FLOPs.</li>\n      <li><strong>Comparative Analysis with Llama 3 70B and 400B Models</strong>: For Llama 3 70B, the computational input is substantially higher, reaching approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9.2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mn\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">9.2</span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-644\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-646\"><span class=\"mrow\" id=\"MathJax-Span-647\"><span class=\"mn\" id=\"MathJax-Span-648\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>9.2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">9.2 \\times 10^{24}</script> FLOPs calculated over 6.4 million GPU hours, which aligns closely with the alternative method’s estimate of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6.3</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-649\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mn\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Regular;\">6.3</span><span class=\"mo\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-653\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-655\"><span class=\"mrow\" id=\"MathJax-Span-656\"><span class=\"mn\" id=\"MathJax-Span-657\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6.3</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">6.3 \\times 10^{24}</script> FLOPs. Should the 400 billion parameter model train on the same dataset, the expected computational investment would scale up to approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>25</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mn\" id=\"MathJax-Span-660\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-662\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-664\"><span class=\"mrow\" id=\"MathJax-Span-665\"><span class=\"mn\" id=\"MathJax-Span-666\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">25</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>4</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>25</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">4 \\times 10^{25}</script> FLOPs. This projection places it just below the threshold outlined in regulatory frameworks such as the recent Biden Executive Order, which sets a reporting requirement at <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>26</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-667\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.39em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-668\"><span class=\"mn\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-671\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-673\"><span class=\"mrow\" id=\"MathJax-Span-674\"><span class=\"mn\" id=\"MathJax-Span-675\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">26</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>26</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">1 \\times 10^{26}</script> FLOPs.</li>\n      <li><strong>The Significance of Data Quality and Comprehensive Model Evaluation</strong>: Beyond raw computational power, the quality of training data plays a critical role in shaping a model’s effectiveness. The integration of diverse and high-quality data can significantly enhance model performance, emphasizing the importance of not reducing the model’s capability to merely its computational input. However, when simplifying the comparison across models, total FLOPs provide a useful measure, amalgamating the scale of the model and the extent of its training into a singular metric indicative of its overall ‘strength.’</li>\n    </ul>\n<h5 id=\"llama-31\"><a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">Llama 3.1</a></h5>\n<ul>\n  <li><a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">Llama 3.1</a> by the Llama Team at Meta introduces the Llama 3 foundation models, which are dense Transformers designed to support multilinguality, coding, reasoning, and tool usage. The largest model contains 405B parameters and can process up to 128K tokens. Llama 3 models are publicly released, including pre-trained and post-trained versions and a Llama Guard model for input and output safety. The models integrate image, video, and speech capabilities using a compositional approach.</li>\n  <li>Llama 3 development optimizes data quality and diversity, scaling, and managing complexity. The model is trained on a corpus of approximately 15T multilingual tokens, which is a significant increase from Llama 2’s 1.8T tokens. The flagship model is pre-trained using 3.8×10^25 FLOPs, nearly 50 times more than the largest Llama 2 model. The architecture uses a dense Transformer model with grouped query attention (GQA) for improved inference speed and reduced key-value cache size during decoding.</li>\n  <li><strong>Pre-Training</strong>:\n    <ul>\n      <li><strong>Data Curation</strong>: Pre-training data is curated from various sources, filtered for quality, and de-duplicated at multiple levels. PII and unsafe content are removed, and a custom parser extracts high-quality text from web data. Multilingual data processing involves fasttext-based language identification and document de-duplication. The data mix includes 50% general knowledge, 25% mathematical and reasoning data, 17% code, and 8% multilingual data, optimized through scaling law experiments.</li>\n      <li><strong>Model Architecture</strong>: Llama 3 uses a standard dense Transformer architecture with enhancements like GQA and a vocabulary size of 128K tokens. The RoPE (Rotary Position Embedding) base frequency is increased to 500,000 to support longer contexts, enabling the model to better handle sequences of up to 128K tokens.</li>\n      <li><strong>Preprocessing and Filtering</strong>: The data preprocessing pipeline is highly sophisticated and involves several key steps to ensure high-quality input for the model. Roberta and DistilRoberta models are employed to classify data quality, providing a robust initial screening. The system also utilizes fasttext for language identification, categorizing documents into 176 languages, which aids in the precise removal of non-relevant content. Extensive de-duplication processes are applied at URL, document, and line levels to eliminate redundant information. This includes:\n        <ul>\n          <li><strong>URL-level de-duplication</strong>: Removing duplicate URLs to keep the most recent versions.</li>\n          <li><strong>Document-level de-duplication</strong>: Utilizing MinHash techniques to remove near-duplicate documents.</li>\n          <li><strong>Line-level de-duplication</strong>: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.</li>\n          <li><strong>Heuristic Filtering</strong>: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.</li>\n          <li><strong>Model-Based Quality Filtering</strong>: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.</li>\n          <li><strong>Quality Scoring</strong>: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.</li>\n          <li><strong>Difficulty Scoring</strong>: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Training Details</strong>: The model training employs a combination of pipeline parallelism and Fully Sharded Data Parallelism (FSDP). Pipeline parallelism partitions the model into stages across GPUs, while FSDP shards the model weights and optimizer states, allowing efficient training across multiple GPUs. The training process also includes a model averaging technique across the Reward Model (RM), Supervised Finetuning (SFT), and Direct Preference Optimization (DPO) stages, ensuring consistency and performance optimization.</li>\n  <li><strong>Post-Training</strong>:\n    <ul>\n      <li><strong>Supervised Finetuning and Direct Preference Optimization (DPO)</strong>: The model undergoes multiple rounds of post-training involving supervised finetuning and DPO. A reward model is trained on human-annotated preference data, and the language model is finetuned on a mixture of human-curated and synthetic data.</li>\n      <li><strong>Data Quality Control</strong>: Extensive data cleaning, pruning, and quality control measures ensure high-quality training samples. Data is categorized and scored for quality using both reward model and Llama-based signals.</li>\n      <li><strong>Multimodal Capabilities</strong>: Separate encoders for images and speech are trained and integrated into the language model using adapters. This approach enables the model to handle image, video, and speech inputs effectively.</li>\n      <li><strong>Float8 Quantization</strong>: The Llama 3 models utilize Float8 (fp8) quantization, where both weights and inputs are quantized to fp8. This quantization is followed by multiplication with \tscaling factors, resulting in outputs in bf16 format. This approach reduces VRAM usage and speeds up inference, making the models more efficient for deployment.</li>\n    </ul>\n  </li>\n  <li>The experimental results show that Llama 3 models perform competitively with state-of-the-art models like GPT-4 across various tasks, including language understanding, coding, and reasoning. The models also demonstrate robustness and scalability, with improvements in safety and alignment with human preferences.</li>\n  <li><a href=\"https://ai.meta.com/blog/meta-llama-3-1/\">Blog</a>; <a href=\"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\">Model card</a>; <a href=\"https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f\">Hugging Face</a></li>\n</ul>\n<ul>\n      <li><strong>Data Curation</strong>: Pre-training data is curated from various sources, filtered for quality, and de-duplicated at multiple levels. PII and unsafe content are removed, and a custom parser extracts high-quality text from web data. Multilingual data processing involves fasttext-based language identification and document de-duplication. The data mix includes 50% general knowledge, 25% mathematical and reasoning data, 17% code, and 8% multilingual data, optimized through scaling law experiments.</li>\n      <li><strong>Model Architecture</strong>: Llama 3 uses a standard dense Transformer architecture with enhancements like GQA and a vocabulary size of 128K tokens. The RoPE (Rotary Position Embedding) base frequency is increased to 500,000 to support longer contexts, enabling the model to better handle sequences of up to 128K tokens.</li>\n      <li><strong>Preprocessing and Filtering</strong>: The data preprocessing pipeline is highly sophisticated and involves several key steps to ensure high-quality input for the model. Roberta and DistilRoberta models are employed to classify data quality, providing a robust initial screening. The system also utilizes fasttext for language identification, categorizing documents into 176 languages, which aids in the precise removal of non-relevant content. Extensive de-duplication processes are applied at URL, document, and line levels to eliminate redundant information. This includes:\n        <ul>\n          <li><strong>URL-level de-duplication</strong>: Removing duplicate URLs to keep the most recent versions.</li>\n          <li><strong>Document-level de-duplication</strong>: Utilizing MinHash techniques to remove near-duplicate documents.</li>\n          <li><strong>Line-level de-duplication</strong>: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.</li>\n          <li><strong>Heuristic Filtering</strong>: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.</li>\n          <li><strong>Model-Based Quality Filtering</strong>: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.</li>\n          <li><strong>Quality Scoring</strong>: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.</li>\n          <li><strong>Difficulty Scoring</strong>: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>URL-level de-duplication</strong>: Removing duplicate URLs to keep the most recent versions.</li>\n          <li><strong>Document-level de-duplication</strong>: Utilizing MinHash techniques to remove near-duplicate documents.</li>\n          <li><strong>Line-level de-duplication</strong>: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.</li>\n          <li><strong>Heuristic Filtering</strong>: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.</li>\n          <li><strong>Model-Based Quality Filtering</strong>: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.</li>\n          <li><strong>Quality Scoring</strong>: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.</li>\n          <li><strong>Difficulty Scoring</strong>: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.</li>\n        </ul>\n<ul>\n      <li><strong>Supervised Finetuning and Direct Preference Optimization (DPO)</strong>: The model undergoes multiple rounds of post-training involving supervised finetuning and DPO. A reward model is trained on human-annotated preference data, and the language model is finetuned on a mixture of human-curated and synthetic data.</li>\n      <li><strong>Data Quality Control</strong>: Extensive data cleaning, pruning, and quality control measures ensure high-quality training samples. Data is categorized and scored for quality using both reward model and Llama-based signals.</li>\n      <li><strong>Multimodal Capabilities</strong>: Separate encoders for images and speech are trained and integrated into the language model using adapters. This approach enables the model to handle image, video, and speech inputs effectively.</li>\n      <li><strong>Float8 Quantization</strong>: The Llama 3 models utilize Float8 (fp8) quantization, where both weights and inputs are quantized to fp8. This quantization is followed by multiplication with \tscaling factors, resulting in outputs in bf16 format. This approach reduces VRAM usage and speeds up inference, making the models more efficient for deployment.</li>\n    </ul>\n<h5 id=\"llama-32\"><a href=\"https://huggingface.co/meta-llama\">Llama 3.2</a></h5>\n<ul>\n  <li>\n    <p>The Llama 3.2 heard of models were created from bigger Llama 3.1 models (8B and 70B) using pruning and distillation as described below.</p>\n  </li>\n  <li><strong>Pruning:</strong>\n    <ul>\n      <li>Structured pruning was used in a single shot manner from the Llama 3.1 8B.</li>\n      <li>It involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network</li>\n    </ul>\n  </li>\n  <li><strong>Knowledge distillation:</strong>\n    <ul>\n      <li>For the 1B and 3B in Llama 3.2, incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets.</li>\n      <li>It was used after pruning to recover performance.</li>\n    </ul>\n  </li>\n  <li><strong>Vision models:</strong>\n    <ul>\n      <li>As the first Llama models to support vision tasks, Llama 3.2 11B and 90B required an entirely new model architecture that supports image reasoning.</li>\n      <li>To add image input support, a set of adapter weights were trained that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. The adapter was trained on text-image pairs to align the image representations with the language representations. During adapter training, the parameters of the image encoder were also updated, but intentionally did not update the language-model parameters. By doing that, all the text-only capabilities were kept intact, providing developers a drop-in replacement for Llama 3.1 models.</li>\n    </ul>\n  </li>\n  <li><strong>Post-training:</strong>\n    <ul>\n      <li>Used similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model</li>\n      <li>Each round involved supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).</li>\n      <li>Scaled context length support to 128K tokens, while maintaining the same quality as the pre-trained model.</li>\n      <li>Filtered synthetic data was also used to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.</li>\n    </ul>\n  </li>\n</ul>\n<p>The Llama 3.2 heard of models were created from bigger Llama 3.1 models (8B and 70B) using pruning and distillation as described below.</p>\n<ul>\n      <li>Structured pruning was used in a single shot manner from the Llama 3.1 8B.</li>\n      <li>It involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network</li>\n    </ul>\n<ul>\n      <li>For the 1B and 3B in Llama 3.2, incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets.</li>\n      <li>It was used after pruning to recover performance.</li>\n    </ul>\n<ul>\n      <li>As the first Llama models to support vision tasks, Llama 3.2 11B and 90B required an entirely new model architecture that supports image reasoning.</li>\n      <li>To add image input support, a set of adapter weights were trained that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. The adapter was trained on text-image pairs to align the image representations with the language representations. During adapter training, the parameters of the image encoder were also updated, but intentionally did not update the language-model parameters. By doing that, all the text-only capabilities were kept intact, providing developers a drop-in replacement for Llama 3.1 models.</li>\n    </ul>\n<ul>\n      <li>Used similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model</li>\n      <li>Each round involved supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).</li>\n      <li>Scaled context length support to 128K tokens, while maintaining the same quality as the pre-trained model.</li>\n      <li>Filtered synthetic data was also used to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.</li>\n    </ul>\n<p><img src=\"assets/LLM/Llama32.png\" alt=\"\"></p>\n<h4 id=\"gpt\">GPT</h4>\n<h5 id=\"gpt-35-turbo\"><a href=\"https://platform.openai.com/docs/models/gpt-3-5-turbo\">GPT-3.5 Turbo</a></h5>\n<ul>\n  <li>OpenAI’s GPT-3.5 Turbo is based on the transformer architecture, with a context window of 16K tokens.</li>\n  <li>While the exact parameters of GPT-3.5 Turbo haven’t been publicly detailed as specifically distinct from other GPT-3 models, it generally falls within the range of the larger GPT-3 models, which can have up to 175 billion parameters.</li>\n</ul>\n<h5 id=\"gpt-4\"><a href=\"https://openai.com/research/gpt-4\">GPT-4</a></h5>\n<ul>\n  <li>Read our GPT-4 primer <a href=\"../GPT-4\">here</a>.</li>\n  <li>Per a <a href=\"https://www.reddit.com/r/mlscaling/comments/14eowmw/gpt4_rumors_a_mixtureofexperts_w8_gpt3220bs/\">rumor</a>, GPT-4 might be an 8-way Mixture-of-Experts (MoE) model with 8 220B parameters (a total of 1.76T parameters).</li>\n  <li>A Mixture of Experts (MoE) model essentially revolves around a router that directs questions to the appropriate expert. If GPT-4 does adopt the MoE approach, it would consist of eight specialist models each trained in a specific domain, like mathematics, history, storytelling, etc. When a question is posed, the router analyses it and seamlessly forwards it to the most suitable expert.</li>\n  <li>The concept of MoE is quite prevalent (refer <a href=\"../../../papers/#outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer\">Outrageously Large Neural Networks: the Sparsely-Gated Mixture-of-Experts Layer</a>), with Langchain’s high-level implementation of an <a href=\"https://python.langchain.com/docs/modules/chains/foundational/router\">LLMRouterChain</a>, and notable low-level integrated examples like Google’s Switch Transformer (refer <a href=\"../../../papers/#switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsity\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>).</li>\n  <li>Per yet another <a href=\"https://archive.is/2RQ8X#selection-449.1-1067.86\">rumor</a>, here are the specifics:\n    <ul>\n      <li><strong>Parameter count:</strong> GPT-4 is more than 10x the size of GPT-3; with a total of ~1.8 trillion parameters across 120 layers.</li>\n      <li><strong>Architecture:</strong> GPT-4 uses an MoE architecture; the main idea behind used an MoE model was to keep costs training/inference reasonable while ensuring great performance. In other words, it is not a dense transformer like, for instance, PaLM (or GPT-3). They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed per forward pass. There roughly ~55B shared parameters for attention.</li>\n      <li><strong>MoE routing:</strong> While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.</li>\n      <li><strong>Inference:</strong> Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model (vs. the MoE architecture that’s used).</li>\n      <li><strong>Dataset:</strong> GPT-4 is trained on ~13T tokens. These are not unique tokens, but the total amount of tokens seen over all epochs. There are millions of instruction fine-tuning data samples from ScaleAI &amp; internally (probably acquired through ChatGPT + their API before they changed the policy).</li>\n      <li><strong>Training epochs:</strong> 2 epochs for text-based data and 4 for code-based data.</li>\n      <li><strong>Training paradigm:</strong> For pre-training GPT-4 32K, they utilized an 8K context length. The 32K context version of GPT-4 was based on fine-tuning of the 8K after the pre-training. <a href=\"https://kaiokendev.github.io/context\">Extending context is hard… but not impossible</a> is a good reference on how to achieve this.</li>\n      <li><strong>Batch size:</strong> The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens. For the real batch size:** Divide this number by the context width to get the real batch size.</li>\n      <li><strong>Parallelism strategies:</strong> To parallelize across all their A100s GPUs, they utilized 8-way tensor parallelism as that is the limit for NVLink. Beyond that, they used 15-way pipeline parallelism. Also apparently they used DeepSpeed ZeRo Stage 1 or block-level FSDP.</li>\n      <li><strong>Training cost</strong>: OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU. Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from. If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million. Had H100s been used, pre-training could be done with ~8,192 H100s in ~55 days for $21.5 million at $2 per H100 hour.</li>\n      <li><strong>MoE tradeoffs</strong>: There are multiple MoE tradeoffs taken; for example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation. This means some parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates. Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research. There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with. With such a large training run, OpenAI instead chose to be more conservative on the number of experts.</li>\n      <li><strong>GPT-4 inference cost</strong>: GPT-4 costs 3x that of the 175B parameter DaVinci. This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved. An estimate of it’s costs is $0.0049 cents per 1K tokens for 128 A100s to inference GPT-4 8K context width and $0.0021 cents per 1K tokens for 128 H100s to inference GPT-4 8K context width. It should be noted that they assume decent high utilization and keep batch sizes large.</li>\n      <li><strong>Multi-Query Attention</strong>: GPT-4 uses MQA instead of MHA (MQA is a classic choice at this point). Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32K context width GPT-4 definitely cannot run on 40GB A100s, and the 8K is capped on max batch size.</li>\n      <li><strong>Continuous batching</strong>: OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.</li>\n      <li><strong>Vision multi-modal</strong>: They have a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Google DeepMind’s <a href=\"../../../papers/#flamingo-a-visual-language-model-for-few-shot-learning\">Flamingo</a>. This adds more parameters on top of the 1.8T text-only GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training. On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text. One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video. Some of the data they train on is joint data (rendered LaTeX/text), screenshots of web pages, YouTube videos: sampling frames, and run Whisper around it to get transcript.</li>\n      <li><strong>Speculative decoding</strong>: OpenAI might be using speculative decoding on GPT-4’s inference. The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch. If the small model was right about its predictions (i.e., the larger model agrees), we can decode several tokens in a single batch. But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model. <strong>The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.</strong>\n        <ul>\n          <li>Per <a href=\"https://twitter.com/karpathy/status/1697318534555336961\">Andrej Karpathy</a>, speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-676\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-677\"><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">K</script> input tokens in a batch (for larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-679\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-680\"><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">K</script> than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n            <ul>\n              <li>At batch_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.</li>\n              <li>Let’s take a look:\n                <ul>\n                  <li>A100: 1935 GB/s memory bandwidth, 1248 TOPS</li>\n                  <li>MacBook M2: 100 GB/s, 7 TFLOPS</li>\n                </ul>\n              </li>\n              <li>The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.</li>\n              <li>The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.</li>\n              <li>In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.</li>\n            </ul>\n          </li>\n          <li>The reason we can’t naively use this fact to sample in chunks of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-682\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-683\"><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">K</script> tokens at a time is that every <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-685\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-686\"><span class=\"msubsup\" id=\"MathJax-Span-687\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"texatom\" id=\"MathJax-Span-689\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-692\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>N</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">N^{th}</script> token depends on what token we sample at time at step <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x2212;</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-696\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>−</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">N-1</script>. There is a serial dependency, so the baseline implementation just goes one by one left to right.</li>\n          <li>Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-698\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-699\"><span class=\"mi\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">K</script> tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).</li>\n          <li>The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.</li>\n          <li>In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.</li>\n          <li>More on this here: <a href=\"https://arxiv.org/abs/1811.03115\">Blockwise Parallel Decoding for Deep Autoregressive Models</a>, <a href=\"https://arxiv.org/abs/2302.01318\">Accelerating Large Language Model Decoding with Speculative Sampling</a>, and <a href=\"https://arxiv.org/abs/2211.17192\">Fast Inference from Transformers via Speculative Decoding</a></li>\n        </ul>\n      </li>\n      <li><strong>Inference architecture</strong>: The inference runs on a cluster of 128 GPUs. There are multiple of these clusters in multiple datacenters in different locations. It is done in 8-way tensor parallelism and 16-way pipeline parallelism. Each node of 8 GPUs has only ~130B parameters, or less than 30GB per GPU at FP16 and less than 15GB at FP8/int8. The model has 120 layers, so it fits in 15 different nodes. (Possibly the there are less layers on the first node since it needs to also compute the embeddings). According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by Chinchilla’s optimal. This goes to show that they are struggling to get high quality data.</li>\n      <li><strong>Why no Fully Sharded Data Parallel (FSDP)?</strong> A possible reason for this could be that some of the hardware infra they secured is of an older generation. This is pretty common at local compute clusters as the organisation usually upgrade the infra in several “waves” to avoid a complete pause of operation. With such a high amount of pipeline parallelism it is very likely that they suffer from the “batch bubble”: slight idle time between batches.</li>\n      <li><strong>Dataset mixture</strong>: They trained on 13T tokens. CommonCrawl &amp; RefinedWeb are both 5T. Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of “unaccounted for” tokens: the “secret” data – parts of it probably came from Twitter, Reddit, and YouTube. Some speculations are: LibGen (4M+ books), Sci-Hub (80M+ papers), all of GitHub. Part of the missing dataset could also be custom dataset of college textbooks collected by hand for as much courses as possible. This is very easy to convert to text form and than use <a href=\"../../../papers/#self-instruct-aligning-language-model-with-self-generated-instructions\">Self-Instruct</a> to transform it into instruction form. This creates the “illusion” that GPT-4 “is smart” no matter who uses it: for computer scientists, it can help you with your questions about P!=NP; for a philosophy major, it can totally talk to you about epistemology. There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. There are some books it knows so well that it had seen them for sure. Moreover, it even knows the unique ids of project Euler problems.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Parameter count:</strong> GPT-4 is more than 10x the size of GPT-3; with a total of ~1.8 trillion parameters across 120 layers.</li>\n      <li><strong>Architecture:</strong> GPT-4 uses an MoE architecture; the main idea behind used an MoE model was to keep costs training/inference reasonable while ensuring great performance. In other words, it is not a dense transformer like, for instance, PaLM (or GPT-3). They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed per forward pass. There roughly ~55B shared parameters for attention.</li>\n      <li><strong>MoE routing:</strong> While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.</li>\n      <li><strong>Inference:</strong> Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model (vs. the MoE architecture that’s used).</li>\n      <li><strong>Dataset:</strong> GPT-4 is trained on ~13T tokens. These are not unique tokens, but the total amount of tokens seen over all epochs. There are millions of instruction fine-tuning data samples from ScaleAI &amp; internally (probably acquired through ChatGPT + their API before they changed the policy).</li>\n      <li><strong>Training epochs:</strong> 2 epochs for text-based data and 4 for code-based data.</li>\n      <li><strong>Training paradigm:</strong> For pre-training GPT-4 32K, they utilized an 8K context length. The 32K context version of GPT-4 was based on fine-tuning of the 8K after the pre-training. <a href=\"https://kaiokendev.github.io/context\">Extending context is hard… but not impossible</a> is a good reference on how to achieve this.</li>\n      <li><strong>Batch size:</strong> The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens. For the real batch size:** Divide this number by the context width to get the real batch size.</li>\n      <li><strong>Parallelism strategies:</strong> To parallelize across all their A100s GPUs, they utilized 8-way tensor parallelism as that is the limit for NVLink. Beyond that, they used 15-way pipeline parallelism. Also apparently they used DeepSpeed ZeRo Stage 1 or block-level FSDP.</li>\n      <li><strong>Training cost</strong>: OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU. Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from. If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million. Had H100s been used, pre-training could be done with ~8,192 H100s in ~55 days for $21.5 million at $2 per H100 hour.</li>\n      <li><strong>MoE tradeoffs</strong>: There are multiple MoE tradeoffs taken; for example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation. This means some parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates. Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research. There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with. With such a large training run, OpenAI instead chose to be more conservative on the number of experts.</li>\n      <li><strong>GPT-4 inference cost</strong>: GPT-4 costs 3x that of the 175B parameter DaVinci. This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved. An estimate of it’s costs is $0.0049 cents per 1K tokens for 128 A100s to inference GPT-4 8K context width and $0.0021 cents per 1K tokens for 128 H100s to inference GPT-4 8K context width. It should be noted that they assume decent high utilization and keep batch sizes large.</li>\n      <li><strong>Multi-Query Attention</strong>: GPT-4 uses MQA instead of MHA (MQA is a classic choice at this point). Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32K context width GPT-4 definitely cannot run on 40GB A100s, and the 8K is capped on max batch size.</li>\n      <li><strong>Continuous batching</strong>: OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.</li>\n      <li><strong>Vision multi-modal</strong>: They have a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Google DeepMind’s <a href=\"../../../papers/#flamingo-a-visual-language-model-for-few-shot-learning\">Flamingo</a>. This adds more parameters on top of the 1.8T text-only GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training. On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text. One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video. Some of the data they train on is joint data (rendered LaTeX/text), screenshots of web pages, YouTube videos: sampling frames, and run Whisper around it to get transcript.</li>\n      <li><strong>Speculative decoding</strong>: OpenAI might be using speculative decoding on GPT-4’s inference. The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch. If the small model was right about its predictions (i.e., the larger model agrees), we can decode several tokens in a single batch. But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model. <strong>The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.</strong>\n        <ul>\n          <li>Per <a href=\"https://twitter.com/karpathy/status/1697318534555336961\">Andrej Karpathy</a>, speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-676\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-677\"><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">K</script> input tokens in a batch (for larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-679\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-680\"><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">K</script> than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n            <ul>\n              <li>At batch_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.</li>\n              <li>Let’s take a look:\n                <ul>\n                  <li>A100: 1935 GB/s memory bandwidth, 1248 TOPS</li>\n                  <li>MacBook M2: 100 GB/s, 7 TFLOPS</li>\n                </ul>\n              </li>\n              <li>The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.</li>\n              <li>The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.</li>\n              <li>In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.</li>\n            </ul>\n          </li>\n          <li>The reason we can’t naively use this fact to sample in chunks of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-682\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-683\"><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">K</script> tokens at a time is that every <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-685\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-686\"><span class=\"msubsup\" id=\"MathJax-Span-687\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"texatom\" id=\"MathJax-Span-689\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-692\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>N</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">N^{th}</script> token depends on what token we sample at time at step <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x2212;</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-696\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>−</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">N-1</script>. There is a serial dependency, so the baseline implementation just goes one by one left to right.</li>\n          <li>Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-698\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-699\"><span class=\"mi\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">K</script> tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).</li>\n          <li>The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.</li>\n          <li>In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.</li>\n          <li>More on this here: <a href=\"https://arxiv.org/abs/1811.03115\">Blockwise Parallel Decoding for Deep Autoregressive Models</a>, <a href=\"https://arxiv.org/abs/2302.01318\">Accelerating Large Language Model Decoding with Speculative Sampling</a>, and <a href=\"https://arxiv.org/abs/2211.17192\">Fast Inference from Transformers via Speculative Decoding</a></li>\n        </ul>\n      </li>\n      <li><strong>Inference architecture</strong>: The inference runs on a cluster of 128 GPUs. There are multiple of these clusters in multiple datacenters in different locations. It is done in 8-way tensor parallelism and 16-way pipeline parallelism. Each node of 8 GPUs has only ~130B parameters, or less than 30GB per GPU at FP16 and less than 15GB at FP8/int8. The model has 120 layers, so it fits in 15 different nodes. (Possibly the there are less layers on the first node since it needs to also compute the embeddings). According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by Chinchilla’s optimal. This goes to show that they are struggling to get high quality data.</li>\n      <li><strong>Why no Fully Sharded Data Parallel (FSDP)?</strong> A possible reason for this could be that some of the hardware infra they secured is of an older generation. This is pretty common at local compute clusters as the organisation usually upgrade the infra in several “waves” to avoid a complete pause of operation. With such a high amount of pipeline parallelism it is very likely that they suffer from the “batch bubble”: slight idle time between batches.</li>\n      <li><strong>Dataset mixture</strong>: They trained on 13T tokens. CommonCrawl &amp; RefinedWeb are both 5T. Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of “unaccounted for” tokens: the “secret” data – parts of it probably came from Twitter, Reddit, and YouTube. Some speculations are: LibGen (4M+ books), Sci-Hub (80M+ papers), all of GitHub. Part of the missing dataset could also be custom dataset of college textbooks collected by hand for as much courses as possible. This is very easy to convert to text form and than use <a href=\"../../../papers/#self-instruct-aligning-language-model-with-self-generated-instructions\">Self-Instruct</a> to transform it into instruction form. This creates the “illusion” that GPT-4 “is smart” no matter who uses it: for computer scientists, it can help you with your questions about P!=NP; for a philosophy major, it can totally talk to you about epistemology. There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. There are some books it knows so well that it had seen them for sure. Moreover, it even knows the unique ids of project Euler problems.</li>\n    </ul>\n<ul>\n          <li>Per <a href=\"https://twitter.com/karpathy/status/1697318534555336961\">Andrej Karpathy</a>, speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-676\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-677\"><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">K</script> input tokens in a batch (for larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-679\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-680\"><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">K</script> than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n            <ul>\n              <li>At batch_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.</li>\n              <li>Let’s take a look:\n                <ul>\n                  <li>A100: 1935 GB/s memory bandwidth, 1248 TOPS</li>\n                  <li>MacBook M2: 100 GB/s, 7 TFLOPS</li>\n                </ul>\n              </li>\n              <li>The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.</li>\n              <li>The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.</li>\n              <li>In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.</li>\n            </ul>\n          </li>\n          <li>The reason we can’t naively use this fact to sample in chunks of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-682\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-683\"><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">K</script> tokens at a time is that every <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-685\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-686\"><span class=\"msubsup\" id=\"MathJax-Span-687\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"texatom\" id=\"MathJax-Span-689\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-692\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>N</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">N^{th}</script> token depends on what token we sample at time at step <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;#x2212;</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-696\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>−</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">N-1</script>. There is a serial dependency, so the baseline implementation just goes one by one left to right.</li>\n          <li>Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-698\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-699\"><span class=\"mi\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">K</script> tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).</li>\n          <li>The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.</li>\n          <li>In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.</li>\n          <li>More on this here: <a href=\"https://arxiv.org/abs/1811.03115\">Blockwise Parallel Decoding for Deep Autoregressive Models</a>, <a href=\"https://arxiv.org/abs/2302.01318\">Accelerating Large Language Model Decoding with Speculative Sampling</a>, and <a href=\"https://arxiv.org/abs/2211.17192\">Fast Inference from Transformers via Speculative Decoding</a></li>\n        </ul>\n<ul>\n              <li>At batch_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.</li>\n              <li>Let’s take a look:\n                <ul>\n                  <li>A100: 1935 GB/s memory bandwidth, 1248 TOPS</li>\n                  <li>MacBook M2: 100 GB/s, 7 TFLOPS</li>\n                </ul>\n              </li>\n              <li>The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.</li>\n              <li>The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.</li>\n              <li>In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.</li>\n            </ul>\n<ul>\n                  <li>A100: 1935 GB/s memory bandwidth, 1248 TOPS</li>\n                  <li>MacBook M2: 100 GB/s, 7 TFLOPS</li>\n                </ul>\n<h5 id=\"gpt-4-turbo\"><a href=\"https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\">GPT-4 Turbo</a></h5>\n<ul>\n  <li>GPT-4 Turbo utilizes the transformer architecture, with a context window of 128K tokens. It features improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.</li>\n</ul>\n<h5 id=\"gpt-4o\"><a href=\"https://openai.com/index/hello-gpt-4o/\">GPT-4o</a></h5>\n<ul>\n  <li>GPT-4o is a multimodal model just like GPT-4, which accepts text, audio, image, and video inputs, and generating text, audio, and image outputs. It boasts response times similar to human conversation, matches GPT-4 Turbo’s performance in text and coding, excels in non-English languages, and is significantly faster and cheaper.</li>\n  <li>Unlike previous models, GPT-4o processes inputs and outputs through a single unified model, improving vision and audio understanding.</li>\n  <li><a href=\"https://arxiv.org/abs/2410.21276\">System Card</a></li>\n</ul>\n<h5 id=\"gpt-4o-mini\"><a href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\">GPT-4o Mini</a></h5>\n<ul>\n  <li>OpenAI’s GPT-4o mini is a highly cost-efficient small model designed to broaden AI application access by offering intelligence at a significantly reduced cost. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it surpasses GPT-3.5 Turbo and other small models in performance on several academic benchmarks, including reasoning, math, and coding.</li>\n  <li>Supporting text and vision, with future expansions to other media, it boasts a 128K token context window and superior non-English text handling. Enhanced safety measures, informed by expert evaluations and new techniques, ensure reliable and secure usage.</li>\n</ul>\n<h4 id=\"bard-api\"><a href=\"https://github.com/dsdanielpark/Bard-API\">Bard API</a></h4>\n<ul>\n  <li>Bard is a conversational generative artificial intelligence chatbot developed by Google, based initially on the LaMDA family of LLMs(Large Language Models) and later the PaLM LLM.</li>\n  <li>Bard API is a python package that returns response of Google Bard through value of cookie.</li>\n</ul>\n<h4 id=\"claude\"><a href=\"https://claude.ai/\">Claude</a></h4>\n<ul>\n  <li>Claude 2 is Anthropic’s second-gen AI chatbot that’s cheaper, stronger, faster, can handle multiple PDFs, and supports longer conversations. It’s basically Anthropic’s answer to OpenAI’s GPT-4:\n    <ul>\n      <li>Claude 2 has 100K context. GPT-4 has 32K. So 3x more context.</li>\n      <li>Claude 2 is 4-5x cheaper than GPT-4-32k.</li>\n      <li>Claude 2’s knowledge cutoff is early 2023, while GPT-4 is late 2021. So more fresher knowledge.</li>\n    </ul>\n  </li>\n  <li>You can easily upload large files (say, an 80-page 2022 Apple annual report) and ask for a summary, key takeaways, provide financial projections (not 100% there yet), and more.</li>\n  <li>Furthermore, you can import several documents into Claude 2 and perform conceptual blending by asking about the relationship between the concepts found in each document.</li>\n</ul>\n<ul>\n      <li>Claude 2 has 100K context. GPT-4 has 32K. So 3x more context.</li>\n      <li>Claude 2 is 4-5x cheaper than GPT-4-32k.</li>\n      <li>Claude 2’s knowledge cutoff is early 2023, while GPT-4 is late 2021. So more fresher knowledge.</li>\n    </ul>\n<h5 id=\"claude-21\"><a href=\"https://www.anthropic.com/index/claude-2-1\">Claude 2.1</a></h5>\n<ul>\n  <li>Claude 2.1 boasts significant enhancements including a 200K token context window, reduced hallucination rates, system prompts, and a beta feature for tool use. Concurrently, pricing updates aim to enhance cost efficiency for customers. Unique features include:\n    <ul>\n      <li><strong>200K Token Context Window</strong>: Allows processing of up to 200,000 tokens, equivalent to about 150,000 words or over 500 pages, suitable for handling extensive documents like full codebases, financial statements, or lengthy literary works.</li>\n      <li><strong>Reduced Hallucination Rates</strong>: Achieves a 50% reduction in false statements, enhancing reliability and honesty in outputs, essential for enterprise AI applications.</li>\n      <li><strong>Improved Comprehension and Summarization</strong>: Demonstrates a 30% reduction in incorrect answers and significantly lower rates of mistakenly supporting false claims in complex documents like legal texts and financial reports.</li>\n      <li><strong>Tool Use (Beta Feature)</strong>: Integrates with users’ existing processes, products, and APIs, enabling functionalities like using calculators, translating requests into API calls, searching databases or the web, and interacting with software or product datasets.</li>\n      <li><strong>Enhanced Developer Experience</strong>: Includes a new Workbench product for easier testing and iteration of prompts, with capabilities to create, navigate, and save multiple prompts for different projects and generate code snippets for SDK integration.</li>\n      <li><strong>System Prompts</strong>: Allows custom instructions for Claude to adopt specific personalities, roles, or structured responses, enhancing performance and user experience.</li>\n      <li><strong>Availability</strong>: Offered for both free and Pro tiers on claude.ai, with exclusive features like the 200K token context window reserved for Pro users.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>200K Token Context Window</strong>: Allows processing of up to 200,000 tokens, equivalent to about 150,000 words or over 500 pages, suitable for handling extensive documents like full codebases, financial statements, or lengthy literary works.</li>\n      <li><strong>Reduced Hallucination Rates</strong>: Achieves a 50% reduction in false statements, enhancing reliability and honesty in outputs, essential for enterprise AI applications.</li>\n      <li><strong>Improved Comprehension and Summarization</strong>: Demonstrates a 30% reduction in incorrect answers and significantly lower rates of mistakenly supporting false claims in complex documents like legal texts and financial reports.</li>\n      <li><strong>Tool Use (Beta Feature)</strong>: Integrates with users’ existing processes, products, and APIs, enabling functionalities like using calculators, translating requests into API calls, searching databases or the web, and interacting with software or product datasets.</li>\n      <li><strong>Enhanced Developer Experience</strong>: Includes a new Workbench product for easier testing and iteration of prompts, with capabilities to create, navigate, and save multiple prompts for different projects and generate code snippets for SDK integration.</li>\n      <li><strong>System Prompts</strong>: Allows custom instructions for Claude to adopt specific personalities, roles, or structured responses, enhancing performance and user experience.</li>\n      <li><strong>Availability</strong>: Offered for both free and Pro tiers on claude.ai, with exclusive features like the 200K token context window reserved for Pro users.</li>\n    </ul>\n<h5 id=\"claude-3\"><a href=\"https://www.anthropic.com/news/claude-3-family\">Claude 3</a></h5>\n<ul>\n  <li>Anthropic announced Claude 3 family of LLMs that are competitive compared to OpenAI’s GPT4.</li>\n  <li>Based on the comparison table below, Claude 3 Haiku is a GPT3.5 alternative as it performs better and it is cheaper (I: $0.25/M tok; O:$1.25/M tok) in comparison to GPT3.5 (I:$0.50/M tok; O:$1.50/M tok). On the other hand, while Claude 3 Opus is much better than GPT4 in performance, GPT4 is cheaper (I: $30/M tok; O: $60/M tok) for long output generation tasks while Opus is better for long context tasks. (I: $15/M tok; O: $75/M tok).</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/claude3.webp\" alt=\"\"></p>\n<h5 id=\"claude-35\"><a href=\"https://www.anthropic.com/news/claude-3-5-sonnet\">Claude 3.5</a></h5>\n<ul>\n  <li>At launch, Claude 3.5 Sonnet set new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.</li>\n  <li>Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.</li>\n  <li><a href=\"https://cdn.sanity.io/files/4zrzovbb/website/fed9cc193a14b84131812372d8d5857f8f304c52.pdf\">Model card</a></li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/Claude-3.5.webp\" alt=\"\"></p>\n<h4 id=\"alpaca\"><a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">Alpaca</a></h4>\n<ul>\n  <li>Stanford’s Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On their preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce.</li>\n</ul>\n<h4 id=\"vicuna\"><a href=\"https://lmsys.org/blog/2023-03-30-vicuna/\">Vicuna</a></h4>\n<ul>\n  <li>Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90% quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90% of cases. The cost of training Vicuna-13B was around $300.</li>\n  <li>Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.</li>\n  <li>Their training recipe builds on top of Stanford’s <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">Alpaca</a> with the following improvements.\n    <ul>\n      <li><strong>Memory Optimizations:</strong> To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing <a href=\"https://arxiv.org/abs/1604.06174\">gradient checkpointing</a> and <a href=\"https://arxiv.org/abs/2205.14135\">flash attention</a>.</li>\n      <li><strong>Multi-round conversations:</strong> We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.</li>\n      <li><strong>Cost Reduction via Spot Instance:</strong> The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ <a href=\"https://github.com/skypilot-org/skypilot\">SkyPilot</a> <a href=\"https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html\">managed spot</a> to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Memory Optimizations:</strong> To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing <a href=\"https://arxiv.org/abs/1604.06174\">gradient checkpointing</a> and <a href=\"https://arxiv.org/abs/2205.14135\">flash attention</a>.</li>\n      <li><strong>Multi-round conversations:</strong> We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.</li>\n      <li><strong>Cost Reduction via Spot Instance:</strong> The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ <a href=\"https://github.com/skypilot-org/skypilot\">SkyPilot</a> <a href=\"https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html\">managed spot</a> to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.</li>\n    </ul>\n<h4 id=\"stablevicuna\"><a href=\"https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot\">StableVicuna</a></h4>\n<ul>\n  <li>StableVicuna is the first large-scale open source chatbot trained via reinforced learning from human feedback (RLHF). StableVicuna is a further instruction fine tuned and RLHF trained version of Vicuna v0 13b, which is an instruction fine tuned <a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\">LLaMA</a> 13B model.</li>\n</ul>\n<h4 id=\"dolly-20\"><a href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\">Dolly 2.0</a></h4>\n<ul>\n  <li>Dolly 2.0 is the world’s first open source, instruction-tuned LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.</li>\n  <li>Dolly 2.0 is a 12B parameter language model based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees.</li>\n  <li>They have also released the <code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> dataset:\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning large language models. Under the licensing terms for <code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> (<a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>), anyone can use, modify, or extend this dataset for any purpose, including commercial applications.</li>\n      <li>This dataset is the first open source, human-generated instruction dataset specifically designed to make large language models exhibit the magical interactivity of ChatGPT. <code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> was authored by more than 5,000 Databricks employees during March and April of 2023. These training records are natural, expressive and designed to represent a wide range of the behaviors, from brainstorming and content generation to information extraction and summarization.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning large language models. Under the licensing terms for <code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> (<a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>), anyone can use, modify, or extend this dataset for any purpose, including commercial applications.</li>\n      <li>This dataset is the first open source, human-generated instruction dataset specifically designed to make large language models exhibit the magical interactivity of ChatGPT. <code class=\"language-plaintext highlighter-rouge\">databricks-dolly-15k</code> was authored by more than 5,000 Databricks employees during March and April of 2023. These training records are natural, expressive and designed to represent a wide range of the behaviors, from brainstorming and content generation to information extraction and summarization.</li>\n    </ul>\n<h4 id=\"stablelm\"><a href=\"https://github.com/Stability-AI/StableLM\">StableLM</a></h4>\n<ul>\n  <li>Stability AI’s StableLM series of language models. StableLM comes in two variants: StableVicuna and StableLM-Alpha.</li>\n  <li>StableVicuna is an RLHF fine-tune of <a href=\"https://huggingface.co/lmsys/vicuna-13b-delta-v0\">Vicuna-13B v0</a>, which itself is a fine-tune of <a href=\"https://github.com/facebookresearch/llama\">LLaMA-13B</a>. It is our attempt at creating an open-source RLHF LLM Chatbot.</li>\n  <li>StableLM-Alpha models are trained on the new dataset that build on <a href=\"https://pile.eleuther.ai/\">The Pile</a>, which contains 1.5 trillion tokens, roughly 3x the size of The Pile. These models will be trained on up to 1.5 trillion tokens. The context length for these models is 4096 tokens. The models range from 3B to 175B parameters. As a proof-of-concept, we also fine-tuned the model with <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">Stanford Alpaca</a>’s procedure using a combination of five recent datasets for conversational agents: Stanford’s <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">Alpaca</a>, Nomic-AI’s <a href=\"https://github.com/nomic-ai/gpt4all\">gpt4all</a>, RyokoAI’s <a href=\"https://huggingface.co/datasets/RyokoAI/ShareGPT52K\">ShareGPT52K</a> datasets, Databricks labs’ <a href=\"https://github.com/databrickslabs/dolly\">Dolly</a>, and Anthropic’s <a href=\"https://github.com/anthropics/hh-rlhf\">HH</a>. We will be releasing these models as StableLM-Tuned-Alpha.</li>\n  <li><a href=\"https://github.com/Stability-AI/StableLM\">Code</a></li>\n</ul>\n<h4 id=\"openllama\"><a href=\"https://github.com/openlm-research/open_llama\">OpenLLaMA</a></h4>\n<ul>\n  <li>OpenLLaMA is a permissively licensed open source reproduction of Meta AI’s <a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\">LLaMA</a> large language model. They have released a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. They provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models.</li>\n</ul>\n<h4 id=\"mpt\"><a href=\"https://huggingface.co/mosaicml/mpt-7b\">MPT</a></h4>\n<ul>\n  <li>MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by <a href=\"https://www.mosaicml.com/\">MosaicML</a>.</li>\n  <li>MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.</li>\n  <li>These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing positional embeddings with Attention with Linear Biases (ALiBi). Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA’s FasterTransformer.</li>\n  <li>MPT-7B is licensed for the possibility of commercial use (unlike <a href=\"https://arxiv.org/abs/2302.13971\">LLaMA</a>).</li>\n  <li>MPT-7B is trained on a large amount of data (1T tokens like <a href=\"https://arxiv.org/abs/2302.13971\">LLaMA</a> vs. 300B for <a href=\"https://github.com/EleutherAI/pythia\">Pythia</a>, 300B for <a href=\"https://github.com/openlm-research/open_llama\">OpenLLaMA</a>, and 800B for <a href=\"https://github.com/Stability-AI/StableLM\">StableLM</a>).</li>\n  <li>MPT-7B is prepared to handle extremely long inputs, due to <a href=\"https://arxiv.org/abs/2108.12409\">ALiBi</a> (they finetuned <a href=\"https://huggingface.co/mosaicml/mpt-7b-storywriter\">MPT-7B-StoryWriter-65k+</a> on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).</li>\n  <li>MPT-7B is capable of fast training and inference (via <a href=\"https://arxiv.org/pdf/2205.14135.pdf\">FlashAttention</a> and <a href=\"https://github.com/NVIDIA/FasterTransformer\">FasterTransformer</a>)</li>\n  <li>MPT-7B is equipped with highly efficient open-source training code via the <a href=\"https://github.com/mosaicml/llm-foundry\">llm-foundry repository</a>.</li>\n</ul>\n<h4 id=\"falcon\"><a href=\"https://huggingface.co/tiiuae\">Falcon</a></h4>\n<ul>\n  <li>Falcon is a causal decoder-only model built by <a href=\"https://www.tii.ae/\">TII</a> and trained on 1,000B tokens of <a href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\">RefinedWeb</a> enhanced with curated corpora. It is made available under the <a href=\"https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt\">TII Falcon LLM License</a>.</li>\n  <li>It was the best open-source model available at the time of release. Falcon-40B outperforms <a href=\"https://github.com/facebookresearch/llama\">LLaMA</a>, <a href=\"https://github.com/Stability-AI/StableLM\">StableLM</a>, <a href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\">RedPajama</a>, <a href=\"https://huggingface.co/mosaicml/mpt-7b\">MPT</a>, etc. See the <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\">OpenLLM Leaderboard</a>.</li>\n  <li>It features an architecture optimized for inference, with FlashAttention (<a href=\"https://arxiv.org/abs/2205.14135\">Dao et al., 2022</a>) and multiquery attention (<a href=\"https://arxiv.org/abs/1911.02150\">Shazeer et al., 2019</a>).</li>\n  <li>Unlike LLaMA, it is made available under a license allowing commercial use, see the details of the <a href=\"https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt\">TII Falcon LLM License</a>.</li>\n  <li>There are two variants: <a href=\"https://huggingface.co/tiiuae/falcon-7b\">Falcon-7B</a>, <a href=\"https://huggingface.co/tiiuae/falcon-40b\">Falcon-40B</a>, and <a href=\"https://huggingface.co/tiiuae/falcon-180b\">Falcon-180B</a>.</li>\n</ul>\n<h5 id=\"falcon-2\"><a href=\"https://falconllm.tii.ae/falcon-2.html\">Falcon 2</a></h5>\n<ul>\n  <li>Falcon 2 includes two powerful versions: Falcon 2 11B, an efficient LLM with 11 billion parameters, and Falcon 2 11B VLM, which adds vision-to-language (VLM) capabilities.</li>\n  <li>Falcon 2 11B VLM is TII’s first multimodal model, enabling seamless image-to-text conversion, a groundbreaking feature in the AI market.</li>\n  <li>Both models are open-source under the TII Falcon License 2.0, encouraging responsible and unrestricted AI usage globally.</li>\n  <li>Falcon 2 11B outperforms Meta’s Llama 3 (8B) and closely matches Google’s Gemma 7B in performance evaluations.</li>\n  <li>The models support multiple languages, including English, French, Spanish, and more, making them versatile across diverse applications.</li>\n  <li>Falcon 2 models are scalable, capable of running efficiently on a single GPU, and applicable across various industries, from healthcare to education, with potential uses in document management, visual impairment support, and more.</li>\n  <li><a href=\"https://falconllm.tii.ae/falcon-2.html\">Blog</a></li>\n</ul>\n<h5 id=\"falcon-mamba\"><a href=\"https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\">Falcon Mamba</a></h5>\n<ul>\n  <li>Falcon Mamba 7B release is a groundbreaking leap in Open-Source AI, setting new standards and redefining the capabilities of language models.</li>\n  <li>Falcon Mamba 7B outperforms leading Transformer-based LLMs of its size, Meta’s Llama 3.1 8B and Mistral’s 7B, on the newly introduced benchmarks by Hugging Face and it is officially the most capable open-source SSLM out there.</li>\n  <li><strong>Background</strong>:\n    <ul>\n      <li>Transformers have worked great leveraging “Self-Attention” to determine the relationships and dependencies that give language its meaning. But the elegance of self-attention comes at a cost, it requires extensive computations, particularly as the sequence length increases. Hence, there has been a growing interest in models that use a latent state that does not depend on the sequence length, referred to as “State Space Models” SSMs. This architecture can handle long sequences achieving linear complexity scaling with sequence length, removing the quadratic bottleneck in the Attention mechanism.</li>\n      <li>Falcon Mamba is an advanced State Space Language Model (SSLM), outperforming both Transformer and SSM-based LLMs of its size, as independently verified by Hugging Face. Falcon Mamba has a low memory cost and can generate long sequences without any increase in memory.</li>\n      <li>Falcon Mamba was trained with ~6 Trillion tokens, mostly from Refined-Web</li>\n      <li>Falcon Mamba-7B was trained on 256 H100 80GB GPUs, using a 3D parallelism strategy combined with ZeRO</li>\n      <li>Mamba was trained leveraging a multi-stage training strategy to increase context-length from 2k to 8k</li>\n      <li>Model training took roughly two months</li>\n    </ul>\n  </li>\n  <li><a href=\"https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\">Blog</a></li>\n</ul>\n<ul>\n      <li>Transformers have worked great leveraging “Self-Attention” to determine the relationships and dependencies that give language its meaning. But the elegance of self-attention comes at a cost, it requires extensive computations, particularly as the sequence length increases. Hence, there has been a growing interest in models that use a latent state that does not depend on the sequence length, referred to as “State Space Models” SSMs. This architecture can handle long sequences achieving linear complexity scaling with sequence length, removing the quadratic bottleneck in the Attention mechanism.</li>\n      <li>Falcon Mamba is an advanced State Space Language Model (SSLM), outperforming both Transformer and SSM-based LLMs of its size, as independently verified by Hugging Face. Falcon Mamba has a low memory cost and can generate long sequences without any increase in memory.</li>\n      <li>Falcon Mamba was trained with ~6 Trillion tokens, mostly from Refined-Web</li>\n      <li>Falcon Mamba-7B was trained on 256 H100 80GB GPUs, using a 3D parallelism strategy combined with ZeRO</li>\n      <li>Mamba was trained leveraging a multi-stage training strategy to increase context-length from 2k to 8k</li>\n      <li>Model training took roughly two months</li>\n    </ul>\n<h5 id=\"the-refinedweb-dataset-for-falcon-llm\"><a href=\"https://arxiv.org/abs/2306.01116\">The RefinedWeb Dataset for Falcon LLM</a></h5>\n<ul>\n  <li>Proposed in The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116).</li>\n  <li>Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon.</li>\n  <li>This paper by Penedo et al. from the Falcon LLM team shows that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.</li>\n  <li>The following figure from the paper shows subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall. Rates measured in % of documents in the document preparation phase, then in tokens.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/RefinedWeb.jpg\" alt=\"\"></p>\n<ul>\n  <li>The following figure from the paper shows models trained on REFINEDWEB alone outperform models trained on curated corpora. Zero-shot performance on our <code class=\"language-plaintext highlighter-rouge\">main-agg</code> task aggregate. At equivalent compute budgets, our models significantly outperform publicly available models trained on The Pile, and match the performance of the GPT-3 models when tested within our evaluation setup.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/RefinedWeb2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Related: Falcon LLM details –\n    <ol>\n      <li>Data is key! As noted in the abstract and benchmarks, Falcon performs very well due to the data refinement techniques used. A key theme through the paper</li>\n      <li>Falcon follows a very close scaling law to GPT-3, with the authors of the paper testing Babbage and Currie (two smaller variants). They measure it using the Eleuther AI evaluation harness.</li>\n      <li>Data Filtering and deduplication is key. Starting with Common Crawl, they apply a 7 part pipeline including URL deduplication, website specific actions, document deduplication and line by line deduplication.</li>\n      <li>The final dataset is only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mfrac><mn>1</mn><mn>9</mn></mfrac><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-701\" style=\"width: 1.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.888em, 1001.36em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-702\"><span class=\"msubsup\" id=\"MathJax-Span-703\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.73em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mfrac\" id=\"MathJax-Span-704\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-705\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-706\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">9</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.633em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-707\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"mi\" id=\"MathJax-Span-709\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-710\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mfrac><mn>1</mn><mn>9</mn></mfrac><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-115\">\\frac{1}{9}^{th}</script> of the Common Crawl original!</li>\n      <li>The team conducts several tests on 1B and 3B param models to validate their data cleaning hypothesis. C4 is still an excellent dataset, but Refined web outperforms The Pile and Oscar, which have duplications.</li>\n      <li>After blocking NSFW urls, the dataset/model toxicity matches that of The Pile, indicating that more work can be done to further decrease it. Minimal work was done on investigating social and other biases.</li>\n      <li>The team open sourced a 600B subset from their 5000B Token dataset.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li>Data is key! As noted in the abstract and benchmarks, Falcon performs very well due to the data refinement techniques used. A key theme through the paper</li>\n      <li>Falcon follows a very close scaling law to GPT-3, with the authors of the paper testing Babbage and Currie (two smaller variants). They measure it using the Eleuther AI evaluation harness.</li>\n      <li>Data Filtering and deduplication is key. Starting with Common Crawl, they apply a 7 part pipeline including URL deduplication, website specific actions, document deduplication and line by line deduplication.</li>\n      <li>The final dataset is only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mfrac><mn>1</mn><mn>9</mn></mfrac><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-701\" style=\"width: 1.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.888em, 1001.36em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-702\"><span class=\"msubsup\" id=\"MathJax-Span-703\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.73em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mfrac\" id=\"MathJax-Span-704\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-705\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-706\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">9</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.633em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-707\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"mi\" id=\"MathJax-Span-709\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-710\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mfrac><mn>1</mn><mn>9</mn></mfrac><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-115\">\\frac{1}{9}^{th}</script> of the Common Crawl original!</li>\n      <li>The team conducts several tests on 1B and 3B param models to validate their data cleaning hypothesis. C4 is still an excellent dataset, but Refined web outperforms The Pile and Oscar, which have duplications.</li>\n      <li>After blocking NSFW urls, the dataset/model toxicity matches that of The Pile, indicating that more work can be done to further decrease it. Minimal work was done on investigating social and other biases.</li>\n      <li>The team open sourced a 600B subset from their 5000B Token dataset.</li>\n    </ol>\n<p><img src=\"/primers/ai/assets/LLM/falcon.jpeg\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://falconllm.tii.ae/\">Project page</a>; <a href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\">download</a>.</li>\n</ul>\n<h4 id=\"redpajama\"><a href=\"https://www.together.xyz/blog/redpajama-7b\">RedPajama</a></h4>\n<ul>\n  <li>The RedPajama project aims to create a set of leading open-source models and to rigorously understand the ingredients that yield good performance. In April 2023, they released the RedPajama base dataset based on the LLaMA paper, which has worked to kindle rapid innovation in open-source AI.</li>\n  <li>The 5 terabyte dataset has been downloaded thousands of times and used to train over 100 models!</li>\n  <li>They’ve trained 3B and 7B models on the Summit supercomputer, in collaboration with AAI CERC lab at Université de Montréal, EleutherAI &amp; LAION for compute time on Summit within the INCITE program award “Scalable Foundation Models for Transferable Generalist AI”.</li>\n  <li>They have released the v1 versions of the RedPajama-INCITE family of models, including instruct-tuned and chat versions under the Apache 2.0 license.\n    <ul>\n      <li><strong>RedPajama-INCITE-7B-Instruct</strong> is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.</li>\n      <li><strong>RedPajama-INCITE-7B-Chat</strong> is available in OpenChatKit, including a training script for easily fine-tuning the model and is available to try now! The chat model is built on fully open-source data and does not use distilled data from closed models like OpenAI’s – ensuring it is clean for use in open or commercial applications.</li>\n      <li><strong>RedPajama-INCITE-7B-Base</strong> was trained on 1T tokens of the RedPajama-1T dataset and releases with 10 checkpoints from training and open data generation scripts allowing full reproducibility of the model. This model is 4 points behind LLaMA-7B, and 1.3 points behind Falcon-7B/MPT-7B on HELM.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://www.together.xyz/blog/redpajama-7b\">Project page</a>.</li>\n</ul>\n<ul>\n      <li><strong>RedPajama-INCITE-7B-Instruct</strong> is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.</li>\n      <li><strong>RedPajama-INCITE-7B-Chat</strong> is available in OpenChatKit, including a training script for easily fine-tuning the model and is available to try now! The chat model is built on fully open-source data and does not use distilled data from closed models like OpenAI’s – ensuring it is clean for use in open or commercial applications.</li>\n      <li><strong>RedPajama-INCITE-7B-Base</strong> was trained on 1T tokens of the RedPajama-1T dataset and releases with 10 checkpoints from training and open data generation scripts allowing full reproducibility of the model. This model is 4 points behind LLaMA-7B, and 1.3 points behind Falcon-7B/MPT-7B on HELM.</li>\n    </ul>\n<h4 id=\"pythia\"><a href=\"https://arxiv.org/abs/2304.01373\">Pythia</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2304.01373\">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a> by Biderman et al. from EleutherAI, Booz Allen Hamilton, Yale University, IIIT Delhi, Stability AI, Datasaur.ai, and University of Amsterdam. This paper introduces Pythia, a suite of 16 LLMs, ranging from 70M to 12B parameters, all trained on public data in the same order, aimed at understanding the development and evolution of LLMs across training and scaling.</li>\n  <li>The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two models: one trained on the Pile, and one trained on the Pile after the dataset has been globally deduplicated. All 8 model sizes are trained on the exact same data, in the exact same order.</li>\n  <li>Pythia allows public access to 154 checkpoints for each model, with tools to download and reconstruct their exact training data, offering insights into memorization, term frequency effects on few-shot performance, and reducing gender bias.</li>\n  <li>The Pythia model suite was deliberately designed to promote scientific research on large language models, especially interpretability research. Despite not centering downstream performance as a design goal, they find the models <a href=\"https://huggingface.co/EleutherAI/pythia-1.4b#evaluations\">match or exceed</a> the performance of similar and same-sized models, such as those in the OPT and GPT-Neo suites.</li>\n  <li>The following table from the paper shows commonly used model suites and how they rate according to theirmerg requirements.</li>\n</ul>\n<p><img src=\"../../../images/papers/Pythia.jpg\" alt=\"\"></p>\n<ul>\n  <li>The suite’s consistent setup across models is used to analyze gender bias mitigation by modifying training data’s gendered terms, demonstrating reduced bias measures in larger models.</li>\n  <li>Another focus is memorization dynamics, where memorization is modeled as a Poisson point process, indicating that memorization occurrences are uniformly distributed throughout training, contrary to the theory that later training data is memorized more.</li>\n  <li>The study also explores the impact of term frequency in pretraining data on model performance, finding a correlation between term frequency and task accuracy in larger models, an emergent property not observed in smaller models.</li>\n  <li>The paper, presented at the International Conference on Machine Learning (ICML) 2023, emphasizes the utility of Pythia for detailed analysis and research on LLM behaviors, offering a new perspective on how pretraining data affects model development.</li>\n  <li><a href=\"https://huggingface.co/EleutherAI/pythia-1.4b\">Hugging Face</a>.</li>\n</ul>\n<h4 id=\"orca\"><a href=\"https://arxiv.org/pdf/2306.02707\">Orca</a></h4>\n<ul>\n  <li>Orca 13B, a small yet mighty AI model developed by Microsoft that’s making waves in the AI community. Despite its size, Orca 13B is proving that it can stand toe-to-toe with the giants, demonstrating capabilities that rival even the larger foundation models (LFMs) like ChatGPT and GPT-4.</li>\n  <li>Orca 13B’s progressive learning approach is a cornerstone of its success. By learning from rich signals from GPT-4, including explanation traces, step-by-step thought processes, and other complex instructions, Orca is able to develop a deeper understanding of the reasoning process. This is a significant departure from traditional AI models, which often focus on imitating the style of LFMs but fail to capture their reasoning process.</li>\n  <li>The use of explanation traces, for instance, allows Orca to understand the underlying logic behind the responses generated by GPT-4. This not only enhances Orca’s ability to generate accurate responses, but also enables it to understand the context and nuances of different scenarios, thereby improving its overall performance.</li>\n  <li>Furthermore, the role of ChatGPT as a teacher assistant is crucial in providing a supportive learning environment for Orca. By providing guidance and feedback, ChatGPT helps Orca refine its learning process and improve its understanding of complex instructions. This teacher-student dynamic is a key factor in Orca’s ability to imitate the reasoning process of LFMs.</li>\n  <li>Orca’s performance in various benchmarks is a testament to its capabilities. In complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and AGIEval, Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% and 42% respectively. This is a significant achievement, considering that these benchmarks are designed to test the model’s ability to reason and make decisions in complex scenarios.</li>\n  <li>One of the most remarkable aspects of Orca is its size. Despite being a smaller AI model compared to giants like ChatGPT, Orca manages to perform at the same level. This is a significant breakthrough in technology as it demonstrates that powerful AI models can be built by smaller teams, making AI development more accessible.</li>\n  <li>The size of Orca also has implications for its efficiency and scalability. Being a smaller model, Orca requires less computational resources to train and operate, making it a more sustainable and cost-effective solution for AI development. Furthermore, its smaller size makes it easier to scale and adapt to different applications, thereby increasing its versatility and utility.</li>\n</ul>\n<h4 id=\"xgen\"><a href=\"https://blog.salesforceairesearch.com/xgen/\">XGen</a></h4>\n<ul>\n  <li>A series of 7B LLMs named XGen-7B from Salesforce with standard dense attention on up to 8K sequence length for up to 1.5T tokens. We also fine tune the models on public-domain instructional data. The main take-aways are:\n    <ul>\n      <li>On standard NLP benchmarks, XGen achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size.</li>\n      <li>Our targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models.</li>\n      <li>XGen-7B archives equally strong results both in text (e.g., MMLU, QA) and code (HumanEval) tasks.</li>\n      <li>Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://blog.salesforceairesearch.com/xgen/\">Project page</a>; <a href=\"https://github.com/salesforce/xGen?ref=blog.salesforceairesearch.com\">Code</a>; Model <a href=\"https://huggingface.co/Salesforce/xgen-7b-8k-base?ref=blog.salesforceairesearch.com\">checkpoint</a>.</li>\n</ul>\n<ul>\n      <li>On standard NLP benchmarks, XGen achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size.</li>\n      <li>Our targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models.</li>\n      <li>XGen-7B archives equally strong results both in text (e.g., MMLU, QA) and code (HumanEval) tasks.</li>\n      <li>Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.</li>\n    </ul>\n<h4 id=\"openllms\"><a href=\"https://github.com/imoneoi/openchat\">OpenLLMs</a></h4>\n<ul>\n  <li>OpenLLMs is a series of open-source language models fine-tuned on a small, yet diverse and high-quality dataset of multi-round conversations. Specifically, we utilize only ~6K GPT-4 conversations directly filtered from the ~90K ShareGPT conversations. Despite the small size of the dataset, OpenLLMs has demonstrated remarkable performance.</li>\n  <li><strong>Generic Models:</strong>\n    <ul>\n      <li><a href=\"https://huggingface.co/openchat/openchat\">OpenChat</a>: based on LLaMA-13B with a context length of 2048.\n        <ul>\n          <li>Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 80.9% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n      <li><a href=\"https://huggingface.co/openchat/openchat_8192\">OpenChat-8192</a>: based on LLaMA-13B, with an extended context length of 8192.\n        <ul>\n          <li>Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 79.5% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Code Models:</strong>\n    <ul>\n      <li><a href=\"https://huggingface.co/openchat/opencoderplus\">OpenCoderPlus</a>: based on StarCoderPlus with a native context length of 8192.\n        <ul>\n          <li>Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves a 78.7% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Dataset:</strong>\n    <ul>\n      <li><a href=\"https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset\">openchat_sharegpt4_dataset</a>: ~6k cleaned and filtered GPT-4 data from ShareGPT.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><a href=\"https://huggingface.co/openchat/openchat\">OpenChat</a>: based on LLaMA-13B with a context length of 2048.\n        <ul>\n          <li>Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 80.9% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n      <li><a href=\"https://huggingface.co/openchat/openchat_8192\">OpenChat-8192</a>: based on LLaMA-13B, with an extended context length of 8192.\n        <ul>\n          <li>Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 79.5% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 80.9% win-rate on AlpacaEval.</li>\n        </ul>\n<ul>\n          <li>Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves 79.5% win-rate on AlpacaEval.</li>\n        </ul>\n<ul>\n      <li><a href=\"https://huggingface.co/openchat/opencoderplus\">OpenCoderPlus</a>: based on StarCoderPlus with a native context length of 8192.\n        <ul>\n          <li>Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves a 78.7% win-rate on AlpacaEval.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.</li>\n          <li>Achieves a 78.7% win-rate on AlpacaEval.</li>\n        </ul>\n<ul>\n      <li><a href=\"https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset\">openchat_sharegpt4_dataset</a>: ~6k cleaned and filtered GPT-4 data from ShareGPT.</li>\n    </ul>\n<h4 id=\"llongma-2\"><a href=\"https://huggingface.co/conceptofmind/LLongMA-2-13b\">LlongMA-2</a></h4>\n<ul>\n  <li>LlongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with @theemozilla of NousResearch and Kaiokendev1, by extending the context length of the Llama-2 7b model through fine-tuning. The models maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.</li>\n  <li>The model has similar performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with <code class=\"language-plaintext highlighter-rouge\">trust_remote_code</code> for &lt;= 4.30.</li>\n</ul>\n<h4 id=\"qwen\"><a href=\"https://github.com/QwenLM/Qwen-7B/\">Qwen</a></h4>\n<ul>\n  <li>Qwen is a Transformer-based LLM by Alibaba, opensourced as two variants: <a href=\"https://huggingface.co/Qwen/Qwen-7B\">Qwen-7B</a> and <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">Qwen-7B-Chat</a>. Qwen-7B is pre-trained on self-constructed large-scale high-quality dataset of over 2.2 trillion tokens created using web texts, books, codes, etc.</li>\n  <li>In comparison with similar-sized models, Qwen outperforms the competitors on a series of benchmarks that evaluates natural language understanding, mathematics, coding, etc. Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.\nQwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent!</li>\n  <li>The features of the Qwen-7B series include:\n    <ul>\n      <li><strong>Trained with high-quality pretraining data.</strong> Qwen-7B is pretrained on a self-constructed large-scale high-quality dataset of over 2.2 trillion tokens. The dataset includes plain texts and codes, and it covers a wide range of domains, including general domain data and professional domain data.</li>\n      <li><strong>Strong performance.</strong> In comparison with the models of the similar model size, we outperform the competitors on a series of benchmark datasets, which evaluates natural language understanding, mathematics, coding, etc.</li>\n      <li><strong>Better support of languages.</strong> Qwen’s tokenizer, based on a large vocabulary of over 150K tokens, is a more efficient one compared with other tokenizers. It is friendly to many languages, and it is helpful for users to further finetune Qwen-7B for the extension of understanding a certain language.</li>\n      <li><strong>Support of 8K Context Length.</strong> Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.</li>\n      <li><strong>Support of Plugins.</strong> Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Trained with high-quality pretraining data.</strong> Qwen-7B is pretrained on a self-constructed large-scale high-quality dataset of over 2.2 trillion tokens. The dataset includes plain texts and codes, and it covers a wide range of domains, including general domain data and professional domain data.</li>\n      <li><strong>Strong performance.</strong> In comparison with the models of the similar model size, we outperform the competitors on a series of benchmark datasets, which evaluates natural language understanding, mathematics, coding, etc.</li>\n      <li><strong>Better support of languages.</strong> Qwen’s tokenizer, based on a large vocabulary of over 150K tokens, is a more efficient one compared with other tokenizers. It is friendly to many languages, and it is helpful for users to further finetune Qwen-7B for the extension of understanding a certain language.</li>\n      <li><strong>Support of 8K Context Length.</strong> Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.</li>\n      <li><strong>Support of Plugins.</strong> Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent.</li>\n    </ul>\n<h5 id=\"qwen-2\"><a href=\"https://qwenlm.github.io/blog/qwen2/\">Qwen 2</a></h5>\n<ul>\n  <li>Slightly behind Llama-3-70b-Instruct (#11:1208) on LMSYS Chatbot Arena Leaderboard, Qwen2-72B-Instruct ranks at #15 with a score of 1187.</li>\n  <li><strong>Model size:</strong> 0.5B, 1.5B, 7B, 57B-14B (MoE), 72B as Base &amp; Instruct versions</li>\n  <li><strong>Context length:</strong>\n    <ul>\n      <li>32k for 0.5B &amp; 1.5B</li>\n      <li>64k for 57B MoE</li>\n      <li>128k for 7B and 72B</li>\n    </ul>\n  </li>\n  <li><strong>Formats available at launch:</strong> AWQ, GPTQ &amp; GGUFs</li>\n  <li><strong>Compatibility:</strong> Works with Ollama and vLLM</li>\n  <li><strong>Supported languages:</strong> Arabic, Bengali, Burmese, Cebuano, Czech, Dutch, English, French, German, Hebrew, Hindi, Indonesian, Italian, Japanese, Khmer, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Spanish, Tagalog, Thai, Turkish, Urdu, Vietnamese</li>\n  <li><strong>Post-training improvements:</strong> Utilizes SFT &amp; DPO with merging for alignment.</li>\n  <li><strong>Token vocabulary:</strong> 151k tokens with ChatML format.</li>\n  <li><strong>Performance:</strong>\n    <ul>\n      <li>The 7B model outperforms Llama3 8B and GLM 4.</li>\n      <li>The 72B model surpasses Llama 3 70B and Mixtral.</li>\n    </ul>\n  </li>\n  <li><strong>Licensing:</strong>\n    <ul>\n      <li>Qianwen license for the 72B model (free for usage below 100M monthly users)</li>\n      <li>Apache 2.0 for all other models</li>\n    </ul>\n  </li>\n  <li><a href=\"https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f\">Hugging Face</a></li>\n</ul>\n<ul>\n      <li>32k for 0.5B &amp; 1.5B</li>\n      <li>64k for 57B MoE</li>\n      <li>128k for 7B and 72B</li>\n    </ul>\n<ul>\n      <li>The 7B model outperforms Llama3 8B and GLM 4.</li>\n      <li>The 72B model surpasses Llama 3 70B and Mixtral.</li>\n    </ul>\n<ul>\n      <li>Qianwen license for the 72B model (free for usage below 100M monthly users)</li>\n      <li>Apache 2.0 for all other models</li>\n    </ul>\n<h5 id=\"qwen25-omni\"><a href=\"https://qwenlm.github.io/blog/qwen2.5-omni/\">Qwen2.5 Omni</a></h5>\n<ul>\n  <li>Qwen2.5-Omni is a multimodal model from the Qwen series, capable of processing and generating text, audio, image, and video in real time. It introduces the Thinker-Talker architecture, where the Thinker handles input understanding and the Talker produces natural speech output. The model supports real-time video and voice chat with chunked input and streaming response capabilities.</li>\n  <li>It uses a novel TMRoPE positional embedding for synchronized audio-video input and delivers high-quality, robust speech synthesis. Qwen2.5-Omni outperforms or matches specialized models like Qwen2-Audio and Qwen2.5-VL-7B in benchmarks. It excels in tasks across multiple modalities such as speech recognition, translation, image reasoning, and video understanding. Future updates will enhance voice command handling and multimodal collaboration.</li>\n  <li><a href=\"https://github.com/QwenLM/Qwen2.5-Omni\">Code</a>; <a href=\"https://huggingface.co/Qwen/Qwen2.5-Omni-7B\">Hugging Face</a></li>\n</ul>\n<h4 id=\"mistral-7b\"><a href=\"https://mistral.ai/news/announcing-mistral-7b/\">Mistral 7B</a></h4>\n<ul>\n  <li>Mistral 7B is a 7.3B parameter model that:\n    <ul>\n      <li>Outperforms Llama 2 13B on all benchmarks.</li>\n      <li>Outperforms Llama 1 34B on many benchmarks.</li>\n      <li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks.</li>\n      <li>Uses Grouped-query attention (GQA) for faster inference.</li>\n      <li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost.</li>\n    </ul>\n  </li>\n  <li>Mistral 7B uses a sliding window attention (SWA) mechanism (<a href=\"https://arxiv.org/pdf/1904.10509.pdf\">Child et al.</a>, <a href=\"https://arxiv.org/pdf/2004.05150v2.pdf\">Beltagy et al.</a>), in which each layer attends to the previous 4,096 hidden states. The main improvement, and reason for which this was initially investigated, is a linear compute cost of <code class=\"language-plaintext highlighter-rouge\">O(sliding_window.seq_len)</code>. In practice, changes made to <a href=\"https://github.com/Dao-AILab/flash-attention\">FlashAttention</a> and <a href=\"https://facebookresearch.github.io/xformers\">xFormers</a> yield a 2x speed improvement for sequence length of 16k with a window of 4k.</li>\n  <li>Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token <code class=\"language-plaintext highlighter-rouge\">i</code> at layer <code class=\"language-plaintext highlighter-rouge\">k</code> attends to tokens <code class=\"language-plaintext highlighter-rouge\">[i-sliding_window, i]</code> at layer <code class=\"language-plaintext highlighter-rouge\">k-1</code>. These tokens attended to tokens <code class=\"language-plaintext highlighter-rouge\">[i-2*sliding_window, i]</code>. Higher layers have access to information further in the past than what the attention patterns seems to entail.</li>\n</ul>\n<ul>\n      <li>Outperforms Llama 2 13B on all benchmarks.</li>\n      <li>Outperforms Llama 1 34B on many benchmarks.</li>\n      <li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks.</li>\n      <li>Uses Grouped-query attention (GQA) for faster inference.</li>\n      <li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost.</li>\n    </ul>\n<p><img src=\"../assets/LLM/attention_local.webp\" align=\"center\" style=\"background-color: #fff; margin: 10px auto\"></p>\n<ul>\n  <li>The following image from the paper illustrates sliding window attention. The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-711\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-712\"><span class=\"mi\" id=\"MathJax-Span-713\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">W</script> tokens from the previous layer (here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-714\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-715\"><span class=\"mi\" id=\"MathJax-Span-716\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">W</script> = 3). Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-717\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-718\"><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">k</script> attention layers, information can move forward by up to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi><mo>&amp;#x00D7;</mo><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-720\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-721\"><span class=\"mi\" id=\"MathJax-Span-722\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-723\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-724\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi><mo>×</mo><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">k \\times W</script> tokens.</li>\n</ul>\n<p><img src=\"../../../images/papers/SlidingWindowAttention.jpg\" alt=\"\"></p>\n<ul>\n  <li>Finally, a fixed attention span means we can limit our cache to a size of sliding_window tokens, using rotating buffers (read more in their <a href=\"https://github.com/mistralai/mistral-src\">reference implementation</a> repo). This saves half of the cache memory for inference on sequence length of 8192, without impacting model quality.</li>\n  <li>Mistral 7B has been released under the Apache 2.0 license, it can be used without restrictions.</li>\n  <li><a href=\"https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar\">Download it</a> and use it anywhere (including locally) with their <a href=\"https://github.com/mistralai/mistral-src\">reference implementation</a>.</li>\n  <li>Deploy it on any cloud (AWS/GCP/Azure), using vLLM <a href=\"https://docs.mistral.ai/cloud-deployment/skypilot\">inference server and skypilot</a>.</li>\n  <li>Use it on <a href=\"https://huggingface.co/mistralai\">HuggingFace</a>.</li>\n  <li>Mistral 7B is easy to fine-tune on any task. As a demonstration, they’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.</li>\n</ul>\n<h5 id=\"mixtral-8x7b-moe\">Mixtral 8x7B MoE</h5>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/pdf/2401.04088\">Mixtral of Experts</a> by Jiang et al. from Mistral.AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) language model that notably enhances the architecture of its predecessor, Mistral 7B, by incorporating 8 feedforward blocks per layer, termed “experts”. Unlike conventional models where each token is processed by the entire network, Mixtral uses a router network to select two experts per token per layer, allowing for a dynamic engagement of 47B parameters while maintaining 13B active parameters during inference.</li>\n  <li>The architecture employs a novel gating mechanism where a softmax function is applied over the top-K logits of a linear layer to dynamically allocate computational resources per token, ensuring efficient processing without engaging all available parameters. This approach significantly reduces computational costs while maintaining or enhancing performance metrics compared to larger models like Llama 2 70B and GPT-3.5.</li>\n  <li>Mixtral is pretrained using multilingual data and demonstrates superior performance in mathematics, code generation, and multilingual tasks. The model’s unique capability to handle large context sizes (up to 32k tokens) allows it to effectively manage long-range dependencies and complex queries, showcasing its robustness in retrieving contextually relevant information across varied sequence lengths and information densities.</li>\n  <li>They perform a routing analysis (i.e., a study on expert specialization) which indicated showed no significant patterns in expert assignment across different topics such as biology, philosophy, or mathematics within The Pile validation dataset, suggesting a mostly syntactic rather than semantic specialization. However, a notable syntactic specialization was observed, where specific tokens in different domains consistently mapped to the same experts, indicating structured syntactic behavior that impacts the model’s training and inference efficiency. Proportion of tokens assigned to each expert on different domains from The Pile dataset for layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform sampling. Here, they consider experts that are either selected as a first or second choice by the router.</li>\n</ul>\n<p><img src=\"../../../images/papers/Mixtral.jpg\" alt=\"\"></p>\n<ul>\n  <li>The paper also discusses Mixtral 8x7B – Instruct, a variant fine-tuned to follow instructions more precisely, using techniques such as supervised fine-tuning and Direct Preference Optimization. This version surpasses other leading models on human evaluation benchmarks and exhibits reduced biases and a balanced sentiment profile across diverse datasets.</li>\n  <li>Despite its expansive parameter space, Mixtral is optimized for efficiency, using only a fraction of its parameters per inference, which allows for faster computation speeds and lower operational costs. Both the base and instruct models are released under the Apache 2.0 license, promoting widespread use and adaptation in both academic and commercial settings.</li>\n  <li>The model’s integration with the open-source vLLM project and its compatibility with Megablocks CUDA kernels for enhanced execution speeds illustrate a commitment to community-driven improvements and accessibility. The provided modifications ensure that Mixtral can be deployed efficiently across different computing environments, including cloud-based platforms via Skypilot.</li>\n  <li>Extensive benchmarks reveal that Mixtral matches or outperforms Llama 2 70B across a spectrum of tasks, with particular strengths in code synthesis and mathematical reasoning. Detailed results highlight its efficacy in multilingual settings and its capability to handle extensive context lengths without performance degradation.</li>\n  <li>The paper positions Mixtral 8x7B as a state-of-the-art model in the landscape of sparse mixture of experts architectures, providing substantial improvements over existing models in terms of scalability, efficiency, and performance, while maintaining lower computational and memory costs.</li>\n</ul>\n<h6 id=\"summary-2\">Summary</h6>\n<ul>\n  <li>Mixtral 8x7B (56B params) from Mistral follows a Mixture of Experts (MoE) architecture, consisting of 8x 7B experts. With 8 experts and a router network that selects two of them at every layer for the inference of each token, it looks directly inspired from rumors about GPT-4’s architecture. This information can be derived from the model metadata:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"p\">{</span><span class=\"s\">\"dim\"</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"s\">\"n_layers\"</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s\">\"head_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s\">\"hidden_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">14336</span><span class=\"p\">,</span> <span class=\"s\">\"n_heads\"</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s\">\"n_kv_heads\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s\">\"norm_eps\"</span><span class=\"p\">:</span> <span class=\"mf\">1e-05</span><span class=\"p\">,</span> <span class=\"s\">\"vocab_size\"</span><span class=\"p\">:</span> <span class=\"mi\">32000</span><span class=\"p\">,</span> <span class=\"s\">\"moe\"</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">\"num_experts_per_tok\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s\">\"num_experts\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">}}</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"p\">{</span><span class=\"s\">\"dim\"</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"s\">\"n_layers\"</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s\">\"head_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s\">\"hidden_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">14336</span><span class=\"p\">,</span> <span class=\"s\">\"n_heads\"</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s\">\"n_kv_heads\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s\">\"norm_eps\"</span><span class=\"p\">:</span> <span class=\"mf\">1e-05</span><span class=\"p\">,</span> <span class=\"s\">\"vocab_size\"</span><span class=\"p\">:</span> <span class=\"mi\">32000</span><span class=\"p\">,</span> <span class=\"s\">\"moe\"</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">\"num_experts_per_tok\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s\">\"num_experts\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">}}</span>\n</code></pre>\n<ul>\n  <li>Based on leaks associated with GPT-4, we can speculate that GPT-4 is a MoE model with 8 experts, each with 111B parameters of their own and 55B shared attention parameters (166B parameters per model). For the inference of each token, also only 2 experts are used.</li>\n  <li>Since the model size (87GB) is smaller than 8x Mistral 7B (8*15GB=120GB), we could assume that the new model uses the same architecture as Mistral 7B but the attention parameters are shared, reducing the naïve 8x7B model size estimation.</li>\n  <li>The conclusion is that Mistral 8x7B uses a very similar architecture to that of GPT-4, but scaled down:\n    <ul>\n      <li>8 total experts instead of 16 (2x reduction).</li>\n      <li>7B parameters per expert instead of 166B (24x reduction).</li>\n      <li>42B total parameters (estimated) instead of 1.8T (42x reduction).</li>\n      <li>Free to use under Apache 2.0 license</li>\n      <li>Outperforms Llama 2 70B with 6x faster inference.</li>\n      <li>Matches or outperforms GPT-3.5</li>\n      <li>Multilingual: vastly outperforms LLaMA 2 70B on French, Italian, German and Spanish</li>\n      <li>Same 32K context as the original GPT-4.</li>\n    </ul>\n  </li>\n  <li>Each layer in a 8x MoE model has its FFN split into 8 chunks and a router picks 2 of them, while the attention weights are always used in full for each token. This means that if the new mistral model uses 5B parameters for the attention, you will use 5+(42-5)/4 = 14.25B params per forward pass.</li>\n  <li>They’ve also released Mixtral 8x7B Instruct v0.1 trained using supervised fine-tuning and direct preference optimization (DPO). It scores 8.3 on MT-Bench making it the best open-source model, with performance comparable to GPT3.5.</li>\n  <li>They offer three chat endpoints with competitive pricing via Mistral AI La Plateforme:\n    <ul>\n      <li>Mistral-tiny: Mistral 7B Instruct v0.2, upgraded base model with higher context length 8K <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-725\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-726\"><span class=\"mo\" id=\"MathJax-Span-727\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">\\rightarrow</script> 32K and better fine-tuning, 6.84 <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-728\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-729\"><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">\\rightarrow</script> 7.61 on MT Bench.</li>\n      <li>Mistral-small: Mistral 8x7B Instruct v0.1, matches or exceeds GPT-3.5 performance, multilingual.</li>\n      <li>Mistral-medium: Outperforms GPT-3.5 on all metrics, multilingual.</li>\n    </ul>\n  </li>\n  <li>They’ve also announced Mistral-embed, an embedding model with a 1024 embedding dimension, which achieves 55.26 on MTEB.</li>\n  <li>Refer <a href=\"https://youtu.be/ccBMRryxGog?si=QPmlkNMIDFnRTJGR&amp;t=1038\">MoE Explanation</a>.</li>\n  <li><a href=\"https://mistral.ai/news/mixtral-of-experts/\">Blog</a>; <a href=\"https://mistral.ai/news/la-plateforme/\">La Plateforme</a>; <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\">Mixtral-8x7B-v0.1 Base model</a>; <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\">Mixtral-8x7B-v0.1 Instruct model</a>.</li>\n</ul>\n<ul>\n      <li>8 total experts instead of 16 (2x reduction).</li>\n      <li>7B parameters per expert instead of 166B (24x reduction).</li>\n      <li>42B total parameters (estimated) instead of 1.8T (42x reduction).</li>\n      <li>Free to use under Apache 2.0 license</li>\n      <li>Outperforms Llama 2 70B with 6x faster inference.</li>\n      <li>Matches or outperforms GPT-3.5</li>\n      <li>Multilingual: vastly outperforms LLaMA 2 70B on French, Italian, German and Spanish</li>\n      <li>Same 32K context as the original GPT-4.</li>\n    </ul>\n<ul>\n      <li>Mistral-tiny: Mistral 7B Instruct v0.2, upgraded base model with higher context length 8K <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-725\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-726\"><span class=\"mo\" id=\"MathJax-Span-727\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">\\rightarrow</script> 32K and better fine-tuning, 6.84 <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-728\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-729\"><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">\\rightarrow</script> 7.61 on MT Bench.</li>\n      <li>Mistral-small: Mistral 8x7B Instruct v0.1, matches or exceeds GPT-3.5 performance, multilingual.</li>\n      <li>Mistral-medium: Outperforms GPT-3.5 on all metrics, multilingual.</li>\n    </ul>\n<h5 id=\"mixtral-8x22b-moe\">Mixtral 8x22B MoE</h5>\n<ul>\n  <li>A <a href=\"https://twitter.com/MistralAI/status/1777869263778291896\">262 GB open 8x22B MoE</a> from Mistral with 65k token context and 2 active experts (overall 44B active params).</li>\n</ul>\n<h5 id=\"mistral-nemo\"><a href=\"https://mistral.ai/news/mistral-nemo/\">Mistral NeMo</a></h5>\n<ul>\n  <li>Mistral NeMo is a 12B model built by Mistal in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.</li>\n  <li>Mistral NeMo is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.</li>\n  <li>Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. In particular, it is ~30% more efficient at compressing source code, Chinese, Italian, French, German, Spanish, and Russian. It is also 2x and 3x more efficient at compressing Korean and Arabic, respectively. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages.</li>\n  <li>Hugging Face: <a href=\"https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\">Base</a>; <a href=\"https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\">Instruct</a></li>\n  <li><a href=\"https://unsloth.ai/blog/mistral-nemo\">Unsloth.AI Blog</a></li>\n</ul>\n<h5 id=\"mistral-large\"><a href=\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407\">Mistral Large</a></h5>\n<ul>\n  <li>Mistral-Large-Instruct-2407 is a dense LLM of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.</li>\n  <li><strong>Key features:</strong>\n    <ul>\n      <li><strong>Multi-lingual by design:</strong> Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.</li>\n      <li><strong>Proficient in coding:</strong> Trained on 80+ coding languages such as Python, Java, C, C++, Javacsript, and Bash. Also trained on more specific languages such as Swift and Fortran.</li>\n      <li><strong>Agentic-centric:</strong> Best-in-class agentic capabilities with native function calling and JSON outputting.</li>\n      <li><strong>Advanced Reasoning:</strong> State-of-the-art mathematical and reasoning capabilities.</li>\n      <li><strong>Mistral Research License:</strong> Allows usage and modification for research and non-commercial usages.</li>\n      <li><strong>Large Context:</strong> A large 128k context window.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://mistral.ai/news/mistral-large-2407/\">Blog</a></li>\n</ul>\n<ul>\n      <li><strong>Multi-lingual by design:</strong> Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.</li>\n      <li><strong>Proficient in coding:</strong> Trained on 80+ coding languages such as Python, Java, C, C++, Javacsript, and Bash. Also trained on more specific languages such as Swift and Fortran.</li>\n      <li><strong>Agentic-centric:</strong> Best-in-class agentic capabilities with native function calling and JSON outputting.</li>\n      <li><strong>Advanced Reasoning:</strong> State-of-the-art mathematical and reasoning capabilities.</li>\n      <li><strong>Mistral Research License:</strong> Allows usage and modification for research and non-commercial usages.</li>\n      <li><strong>Large Context:</strong> A large 128k context window.</li>\n    </ul>\n<h4 id=\"zephyr\"><a href=\"https://arxiv.org/abs/2310.16944\">Zephyr</a></h4>\n<ul>\n  <li>Introduced by Tunstall et al. from Huggingface in <a href=\"https://arxiv.org/abs/2310.16944\">Zephyr: Direct Distillation of LM Alignment</a>. Tunstall et al. introduce a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    <ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n  </li>\n  <li>They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.</li>\n  <li>Results:\n    <ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n  </li>\n  <li>The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.</li>\n  <li>The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.</li>\n  <li>Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.</li>\n  <li>The image below <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\">(source)</a> gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.</li>\n</ul>\n<ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n<ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/zephyr.png\" alt=\"\"></p>\n<h5 id=\"huggingfaces-alignment-handbook\">HuggingFace’s Alignment Handbook</h5>\n<ul>\n  <li><a href=\"https://github.com/huggingface/alignment-handbook\">The Alignment Handbook</a> contains robust recipes to align language models with human and AI preferences. It also contains code to train your very own Zephyr models:\n    <ul>\n      <li>Full fine-tuning with Microsoft’s DeepSpeed ZeRO-3 on A100s</li>\n      <li>LoRA or QLoRA fine-tuning on consumer GPUs</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Full fine-tuning with Microsoft’s DeepSpeed ZeRO-3 on A100s</li>\n      <li>LoRA or QLoRA fine-tuning on consumer GPUs</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/Alignment.jpg\" alt=\"\"></p>\n<ul>\n  <li>Dataset from HuggingFace called <a href=\"https://huggingface.co/datasets/HuggingFaceH4/no_robots\">No Robots</a> of 10k instructions and demonstrations to train instruct models. This is based on the SFT dataset from OpenAI’s InstructGPT paper. 100% organic and written entirely by skilled human annotators.</li>\n</ul>\n<h4 id=\"yi\"><a href=\"https://01.ai/\">Yi</a></h4>\n<ul>\n  <li><a href=\"https://01.ai\">01.AI</a> offers two new opensource LLMs Yi-34B and Yi-6B trained on 3 trillion tokens with an extraordinarily long 200K context window.</li>\n  <li>It outperforms Llama-2 70B and Falcon-180B in most of the benchmarks and comes with a free commercial license, based on global open-source LLM English/Chinese benchmarks:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/YI.png\" alt=\"\"></p>\n<h5 id=\"yi-15\"><a href=\"https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8\">Yi 1.5</a></h5>\n<ul>\n  <li>Yi 1.5 from <a href=\"https://01.ai\">01.AI</a> has been significantly improved compared to Yi 1.0 for coding, math and reasoning, closely matching or exceeding Llama 3 and Mixtral 8x22B.</li>\n  <li>Features:\n    <ul>\n      <li>Further pre-trained Yi 1.0 on 500B tokens</li>\n      <li>Additionally fine-tuned on “3M diverse samples”</li>\n      <li>Available as 6B, 9B, and 34B checkpoints</li>\n      <li>Base + Chat models</li>\n      <li>Context = 4K</li>\n      <li>9B beats Llama 3 8B on MMLU, GSM8K, HumanEval and AlpacaEval2.0</li>\n      <li>34B closely matches or beats Mixtral 8x22B on benchmarks</li>\n      <li>Apache 2.0</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Further pre-trained Yi 1.0 on 500B tokens</li>\n      <li>Additionally fine-tuned on “3M diverse samples”</li>\n      <li>Available as 6B, 9B, and 34B checkpoints</li>\n      <li>Base + Chat models</li>\n      <li>Context = 4K</li>\n      <li>9B beats Llama 3 8B on MMLU, GSM8K, HumanEval and AlpacaEval2.0</li>\n      <li>34B closely matches or beats Mixtral 8x22B on benchmarks</li>\n      <li>Apache 2.0</li>\n    </ul>\n<h4 id=\"effi\"><a href=\"https://huggingface.co/aiplanet/effi-13b\">effi</a></h4>\n<ul>\n  <li>effi-13B parameters is a causal decoder-only model built by AI Planet based on Llama-2-13b-chat-hf and fine tuned using the 1.8 Million coversations from CoT dataset available in huggingface datasets. The model is made available under the Apache 2.0 license.</li>\n  <li><strong>Why use effi-13B-Instruct?</strong>\n    <ul>\n      <li>This is a ready to use chat/instruct model based on Llama-2-13b-chat-hf, which provides a rationale for the context provided.</li>\n      <li>Llama-2 is the best open-source model available. This is an instruct model, which may not be ideal for further fine-tuning. If you are interested in building your own instruct/chat model, we recommend starting from Llama-2-13b-chat-hf.</li>\n      <li>You will need at least 85-100GB of memory to swiftly run inference with effi-13b.</li>\n    </ul>\n  </li>\n  <li><strong>Model Description:</strong>\n    <ul>\n      <li>This model has been fine-tuned on Chain of Thought datasets, which has context from mixed sources with corresponding rationale. The final finetuned Large Language Model(LLM) have shown enhanced capabilities of solving novel tasks by providing a reasoning.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>This is a ready to use chat/instruct model based on Llama-2-13b-chat-hf, which provides a rationale for the context provided.</li>\n      <li>Llama-2 is the best open-source model available. This is an instruct model, which may not be ideal for further fine-tuning. If you are interested in building your own instruct/chat model, we recommend starting from Llama-2-13b-chat-hf.</li>\n      <li>You will need at least 85-100GB of memory to swiftly run inference with effi-13b.</li>\n    </ul>\n<ul>\n      <li>This model has been fine-tuned on Chain of Thought datasets, which has context from mixed sources with corresponding rationale. The final finetuned Large Language Model(LLM) have shown enhanced capabilities of solving novel tasks by providing a reasoning.</li>\n    </ul>\n<h4 id=\"starling\"><a href=\"https://starling.cs.berkeley.edu/\">Starling</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://starling.cs.berkeley.edu/\">Starling-7B: Increasing LLM Helpfulness &amp; Harmlessness with RLAIF</a>, this report by Zhu et al. from UC Berkeley introduces Starling-7B, a large language model enhanced by Reinforcement Learning from AI Feedback (RLAIF). It utilizes a new GPT-4 labeled ranking dataset, <a href=\"https://huggingface.co/datasets/berkeley-nest/Nectar\">Nectar</a>, and a novel reward training and policy tuning pipeline.</li>\n  <li>Starling-7B-alpha achieves a score of 8.09 on MT Bench, evaluated by GPT-4, surpassing most models except GPT-4 and GPT-4 Turbo. The model and its components, including the ranking dataset Nectar and the reward model <a href=\"https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha\">Starling-RM-7B-alpha</a>, are available on HuggingFace and as an online demo in LMSYS Chatbot Arena.</li>\n</ul>\n<p><img src=\"../../../images/papers/Starling.jpg\" alt=\"\"></p>\n<ul>\n  <li>The report discusses the effectiveness of Supervised Fine-Tuning (SFT) in chatbot systems, contrasting it with Reinforcement Learning from Human Feedback (RLHF) and AI feedback (RLAIF). It emphasizes the need for high-quality ranking datasets for chat, leading to the creation of Nectar, which includes 183K chat prompts and 3.8M pairwise comparisons.</li>\n  <li>Starling-7B is fine-tuned using the Starling-RM-7B-alpha reward model, improving its MT-Bench and AlpacaEval scores, reflecting increased helpfulness.</li>\n  <li>The model’s evaluation involves MT-Bench and AlpacaEval, with results indicating improvements in helpfulness and safety but minor regressions in areas like QA, math, and coding.</li>\n  <li>The report details the dataset creation process, particularly the efforts to mitigate positional bias in GPT-4-based rankings, resulting in the Nectar dataset.</li>\n  <li>Training of the reward model involves the K-wise maximum likelihood estimator under the Plackett-Luce Model. Policy fine-tuning experiments are conducted using different RL methods, with APA being the most effective.</li>\n  <li>The report highlights challenges in RLHF evaluation and discusses limitations, including Starling-7B’s struggles with reasoning tasks and susceptibility to jailbreaking prompts.</li>\n  <li>The research is subject to licenses and terms from various sources, including LLaMA, OpenAI, and ShareGPT, and acknowledges contributions from the broader research community.</li>\n  <li><a href=\"https://starling.cs.berkeley.edu/\">Project page</a>.</li>\n</ul>\n<h4 id=\"nexusraven-v2\"><a href=\"https://nexusflow.ai/blogs/ravenv2\">NexusRaven-V2</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://nexusflow.ai/blogs/ravenv2\">NexusRaven-V2: Surpassing GPT-4 for Zero-shot Function Calling</a>, this blog by Nexusflow introduces the open-source NexusRaven-V2, a 13B LLM that excels in zero-shot function calling, surpassing GPT-4’s capabilities. This model is pivotal in converting natural language instructions into executable code, integral to the OpenAI Assistants API. It’s a major stride in enhancing copilots and agents for using software tools, emphasizing open-source models’ role in technology and society.</li>\n  <li>NexusRaven-V2 achieves up to 7% higher function calling success rates than GPT-4, particularly in complex cases involving nested and composite functions. This is notable considering NexusRaven-V2 was never trained on these functions.</li>\n  <li>The model is instruction-tuned on Meta’s CodeLlama-13B-instruct, utilizing data solely from open-code corpora. Its open-source nature and commercial permissiveness cater to both community developers and enterprises.</li>\n  <li>NexusRaven-V2 is designed for easy integration into existing software workflows, replacing mainstream proprietary function calling APIs. It includes open-source utility artifacts, online demos, and Colab notebooks for seamless onboarding.</li>\n  <li>The following figure from the blog shows that NexusRaven-V2 provides the function calling capability to enable copilots and agents to use software tools. Given human instruction prompts and software documentations, the function calling capability generates executable code to run the functions/APIs.</li>\n</ul>\n<p><img src=\"../../../images/papers/NexusRaven1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The team introduces the Nexus-Function-Calling benchmark and a Hugging Face <a href=\"https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard\">leaderboard</a>, featuring a wide array of real-life, human-curated function-calling examples. This benchmark, with 8 out of 9 tasks open-sourced, aims to standardize evaluations in function calling.</li>\n  <li>The following figure from the blog shows NexusRaven-V2 evaluation with their human-curated Benchmark.</li>\n</ul>\n<p><img src=\"../../../images/papers/NexusRaven2.jpg\" alt=\"\"></p>\n<ul>\n  <li>The model’s robustness is evident in its handling of various descriptions of functions by developers, indicating its potential to match or surpass proprietary LLM APIs in accuracy and robustness.</li>\n  <li><a href=\"https://nexusflow.ai/blogs/ravenv2\">Project page</a>.</li>\n</ul>\n<h4 id=\"llama-guard\"><a href=\"https://scontent-sin6-4.xx.fbcdn.net/v/t39.2365-6/408725049_3688557441468029_8103913771964668529_n.pdf?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=_gWcFD1K96gAX9rRWN9&amp;_nc_ht=scontent-sin6-4.xx&amp;oh=00_AfD8Cg11tI6b7yw1YXftHQpKdigclsUWye_Cxn51IhAkog&amp;oe=65773519\">Llama Guard</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://scontent-sin6-4.xx.fbcdn.net/v/t39.2365-6/408725049_3688557441468029_8103913771964668529_n.pdf?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=_gWcFD1K96gAX9rRWN9&amp;_nc_ht=scontent-sin6-4.xx&amp;oh=00_AfD8Cg11tI6b7yw1YXftHQpKdigclsUWye_Cxn51IhAkog&amp;oe=65773519\">Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a>.</li>\n  <li>The Purple Llama initiative by Meta, aimed at promoting responsible and safe development in generative AI, encompasses a variety of tools and evaluations, including the notable Llama-Guard and the Llama 7B model for content moderation.</li>\n  <li>Llama Guard, a component of Meta’s Purple Llama Initiative, is a 7B parameter model based on Llama2, designed to classify content in LLM prompts and responses, enhancing trust and safety in AI applications.</li>\n  <li>It uses a safety risk taxonomy for content moderation, detecting policy violations and indicating the safety level of text, with detailed subcategory violations when necessary.</li>\n  <li>The model is instruction-tuned on a dataset comprising about 13,000 examples, including prompts and responses annotated for safety, with training inputs from the Anthropic <a href=\"https://github.com/anthropics/hh-rlhf\">dataset</a> and in-house redteaming examples.</li>\n  <li>Llama Guard outperforms existing moderation tools in benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, and is adept at detecting harmful content across various categories.</li>\n  <li>Its functionality includes evaluating probabilities for classifying text as safe or unsafe, and it can generate outputs indicating safety status and policy violations.</li>\n  <li>The following figure from the blog shows example task instructions for the Llama Guard prompt and response classification tasks. A task consists of four main components. Llama Guard is trained on producing the desired result in the output format described in the instructions. It acts as an LLM: it generates text in its output that indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories. Here is an example:</li>\n</ul>\n<p><img src=\"../../../images/papers/Llama-Guard.jpg\" alt=\"\"></p>\n<ul>\n  <li>Customizable for different use cases, Llama Guard is adaptable for chatbots and digital assistants, offering flexibility without compromising safety.</li>\n  <li>Part of the broader Purple Llama ecosystem, which includes industry collaborations and is available on Hugging Face, Llama Guard’s model weights are released for public use, licensed permissively for research and commercial applications.</li>\n  <li>In summary, Meta’s Purple Llama initiative represents a major advancement in ensuring safe and responsible development in generative AI. By providing a suite of tools, including the Llama-Guard and the Llama 7B model for content moderation, the initiative addresses the need for comprehensive safety measures in AI applications, fostering an open environment for ongoing innovation in the field.</li>\n  <li><a href=\"https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/\">Blog</a>; <a href=\"https://huggingface.co/meta-llama/LlamaGuard-7b\">Model</a>; <a href=\"https://colab.research.google.com/drive/16s0tlCSEDtczjPzdIK3jq0Le5LlnSYGf?usp=sharing\">Notebook</a>; <a href=\"https://huggingface.co/spaces/facebook/CyberSecEval\">Benchmark</a>.</li>\n</ul>\n<h4 id=\"notus\"><a href=\"https://huggingface.co/argilla/notus-7b-v1\">Notus</a></h4>\n<ul>\n  <li>Argilla’s Notus-7B-v1 is an open-source LLM developed using Direct Preference Optimization (DPO) and related Reinforcement Learning from Human Feedback (RLHF) techniques. This model represents the first version, fine-tuned with DPO over <a href=\"https://huggingface.co/alignment-handbook/zephyr-7b-sft-full\">alignment-handbook/zephyr-7b-sft-full</a>, which is the SFT model used to create zephyr-7b-beta.</li>\n  <li>Notus-7B-v1 differentiates itself from Zephyr-7B-beta primarily in the preference dataset used for fine-tuning. Argilla identified and rectified issues in the original UltraFeedback dataset by OpenBMB, which were leading to high scores for bad responses. They proposed a modified version of the UltraFeedback dataset by binarizing the dataset using preference ratings instead of the original overall critique score. This was accomplished by taking the mean of preference ratings for each of the different preference aspects, namely: helpfulness, honesty, instruction-following, and truthfulness. This change led to a significant difference in chosen responses, approximately 50% different from the original dataset. The new dataset version can be found at <a href=\"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences\">Argilla’s UltraFeedback Binarized Preferences</a>.</li>\n  <li>Notus-7B-v1, with its refined dataset, surpasses both Zephyr-7B-beta and Claude 2 in the AlpacaEval benchmark. It’s evaluated using both Chat (MT-Bench, AlpacaEval) and Academic (Open LLM Leaderboard) benchmarks, providing a direct comparison with the original Zephyr dDPO model and other 7B models.</li>\n  <li>This model’s success owes much to resources like the Alignment Handbook and OpenBMB’s release of the UltraFeedback dataset. Discussions with the HuggingFace H4 team and the utilization of zephyr-7b-beta’s recipe were instrumental in its development.</li>\n  <li>Intended for use in chat-like applications as an assistant, Notus models are a testament to high-quality data focus. An open question remains regarding the efficacy of using critique scores versus preference ratings post dataset correction, a comparison Argilla is excited to explore in the future.</li>\n  <li><a href=\"https://huggingface.co/argilla/notus-7b-v1\">Hugging Face</a>.</li>\n</ul>\n<h4 id=\"openchat\"><a href=\"https://arxiv.org/abs/2309.11235\">OpenChat</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2309.11235\">OpenChat: Advancing Open-Source Language Models with Mixed-Quality Data</a> by Wang et al. from Tsinghua University and Shanghai Artificial Intelligence Laboratory, OpenChat is a novel framework designed to advance open-source language models using mixed-quality data. It explores the integration of supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to enhance language model performance with diverse data qualities.</li>\n  <li>OpenChat introduces a new approach, Conditioned-RLFT (C-RLFT), which utilizes coarse-grained reward labels to distinguish between different data sources, like GPT-4 or GPT-3.5, within the general SFT training data. This method effectively leverages the mixed-quality training data, comprising a small amount of expert data and a larger proportion of sub-optimal data, without requiring preference labels.</li>\n  <li>An intuitive approach to align LLMs is through RL via RLHF, which models rewards according to human preference feedback and fine-tune LLMs to maximize the reward. The reward either modeled explicitly (by training a separate model) or implicitly (via the LLM itself, as in DPO), assigns high values on desirable responses and low values on bad ones to guide the alignment of the finetuned LLM. A popular RL framework for fine-tuning LLMs is the KL-regularized RL as in DPO, which adds an additional KL penalty to constrain the fine-tuned LLM <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03C0;</mi><mi>&amp;#x03B8;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mo>&amp;#x2223;</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-731\" style=\"width: 4.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.44em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-732\"><span class=\"msubsup\" id=\"MathJax-Span-733\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-734\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-735\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-736\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-738\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-739\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">\\pi_\\theta(y \\mid x)</script> to stay close to the base pre-trained LLM <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03C0;</mi><mn>0</mn></msub><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mo>&amp;#x2223;</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-741\" style=\"width: 4.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.44em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-742\"><span class=\"msubsup\" id=\"MathJax-Span-743\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-744\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-745\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-746\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-747\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-749\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-750\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>π</mi><mn>0</mn></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">\\pi_0(y \\mid x)</script>. This has been shown beneficial to avoid distribution collapse as compared to naïvely maximize reward using RL.</li>\n  <li>C-RLFT, based on the KL-regularized RL framework, focuses on maximizing reward while minimizing the difference between the fine-tuned policy and a reference policy. Uniquely, it employs single-stage supervised learning (which implies that reinforcement learning step, e.g., based on PPO, is not required similar to DPO), which is both lightweight and free from the need for costly human preference labeling. C-RLFT learns to distinguish expert and sub-optimal data. To this end, mixed-quality data from different sources is sufficient. The reward label can be as simple as a relative value differentiating different classes of data sources, i.e., GPT-4 or GPT-3.5.</li>\n  <li>Note that regular SFT treats all training data uniformly but that’s not the case for C-RLFT. The paper used a dataset of a small amount of expert data and a large proportion of easily accessible sub-optimal data using coarse-grained reward labels.</li>\n  <li>The figure below from the paper shows the proposed framework OpenChat with Conditioned-RLFT to advance the open-source language model fine-tuning with mixed-quality data, comparing to previous supervised fine-tuning (SFT) method and reinforcement learning fine-tuning (RLFT) method. MLE and RL denote maximum likelihood estimates and reinforcement learning, respectively.</li>\n</ul>\n<p><img src=\"../../../images/papers/OpenChat.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Implementation details:</strong>\n    <ol>\n      <li>Collect mixed-quality data from different sources (e.g. GPT-4 and GPT-3.5 conversations) and assign coarse-grained rewards based on data source quality, e.g., GPT-4=1.0 GPT-3.5=0.1.</li>\n      <li>\n        <table>\n          <tbody>\n            <tr>\n              <td>Construct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}&lt;</td>\n              <td>end_of_turn</td>\n              <td>&gt;GPT4 Assistant: {RESPONSE}”</td>\n            </tr>\n          </tbody>\n        </table>\n      </li>\n      <li>Train LLM using C-RLFT regularizing the class-conditioned references for the optimal the policy.</li>\n    </ol>\n  </li>\n  <li>The paper demonstrates that OpenChat, particularly when fine-tuned with C-RLFT on the ShareGPT dataset, significantly outperforms other 13b open-source language models in terms of average performance across standard benchmarks. Notably, OpenChat-13b excels in AGIEval, surpassing the base model in terms of generalization performance.</li>\n  <li>The implementation details include using the llama-2-13b as the base model and fine-tuning it for 5 epochs on the ShareGPT dataset with AdamW optimizer, a sequence length of 4096 tokens, and an effective batch size of 200k tokens. The training involves class-conditioning and assigning weights to different data types, with a cosine learning rate schedule.</li>\n  <li>Extensive ablation studies and analyses were conducted to validate the contributions of different components, like coarse-grained rewards and class-conditioned policy, and performance consistency. These studies shed light on the effectiveness and robustness of OpenChat.</li>\n  <li>The paper suggests future research directions, including refining the coarse-grained rewards to better reflect the actual quality of data points and exploring applications of OpenChat to enhance reasoning abilities in language models.</li>\n  <li><a href=\"https://github.com/imoneoi/openchat\">Code</a>; <a href=\"https://huggingface.co/openchat/openchat-3.5-1210\">HuggingFace</a></li>\n</ul>\n<ol>\n      <li>Collect mixed-quality data from different sources (e.g. GPT-4 and GPT-3.5 conversations) and assign coarse-grained rewards based on data source quality, e.g., GPT-4=1.0 GPT-3.5=0.1.</li>\n      <li>\n        <table>\n          <tbody>\n            <tr>\n              <td>Construct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}&lt;</td>\n              <td>end_of_turn</td>\n              <td>&gt;GPT4 Assistant: {RESPONSE}”</td>\n            </tr>\n          </tbody>\n        </table>\n      </li>\n      <li>Train LLM using C-RLFT regularizing the class-conditioned references for the optimal the policy.</li>\n    </ol>\n<table>\n          <tbody>\n            <tr>\n              <td>Construct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}&lt;</td>\n              <td>end_of_turn</td>\n              <td>&gt;GPT4 Assistant: {RESPONSE}”</td>\n            </tr>\n          </tbody>\n        </table>\n<h4 id=\"decilm\"><a href=\"https://huggingface.co/Deci/DeciLM-7B\">DeciLM</a></h4>\n<ul>\n  <li><a href=\"https://deci.ai/\">Deci AI</a> has released DeciLM-7B, a new state-of-the-art 7B pretrained LLM with a permissive Apache-2.0 license, outperforming the excellent Mistral-7B.</li>\n  <li>DeciLM-7B is Apache 2.0-licensed, and as of this writing, it’s officially #1 on the Open LLM Leaderboard for the 7B text generation category.</li>\n  <li>DeciLM-7B’s throughput is also 1.83x and 2.39x faster than Mistral 7B and Llama 2 7B, respectively.</li>\n  <li><a href=\"https://huggingface.co/Deci/DeciLM-7B\">Base</a>; <a href=\"https://huggingface.co/Deci/DeciLM-7B-instruct\">Instruct</a>; <a href=\"https://console.deci.ai/infery-llm-demo?utm_source=organic-social&amp;utm_medium=post&amp;utm_campaign=decilm-7b\">Demo</a>; <a href=\"https://deci.ai/blog/introducing-DeciLM-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date?utm_campaign=DeciLM%207B%20Launch&amp;utm_source=twitter&amp;utm_medium=social\">Technical blog</a>; [Notebooks: <a href=\"https://colab.research.google.com/drive/1VU98ezHJr1-ry6LDkhayu5-6SZKTsnsX\">DeciLM-7B-Instruct</a>; <a href=\"https://colab.research.google.com/drive/1kEV6i96AQ94xTCvSd11TxkEaksTb5o3U?usp=sharing\">Fine-tune DeciLM-7B for Translation</a></li>\n</ul>\n<h4 id=\"llm360\"><a href=\"https://arxiv.org/abs/2312.06550\">LLM360</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2312.06550\">LLM360: Towards Fully Transparent Open-Source LLMs</a> by Liu et al. from Petuum, MBZUAI, and CMU, LLM360 is a framework aimed at enhancing the transparency and reproducibility of LLMs. It emphasizes the importance of fully open-sourcing LLMs, including training code, data, model checkpoints, and intermediate results.</li>\n  <li>The paper introduces two 7B parameter LLMs, AMBER and CRYSTALCODER. AMBER is an English LLM pre-trained on 1.3 trillion tokens, while CRYSTALCODER is an English and code LLM pre-trained on 1.4 trillion tokens. Both models are notable for their transparency, with the release of all training components.</li>\n  <li>For AMBER, data preparation entailed combining RefinedWeb, StarCoder, and RedPajama-v1 datasets, with no further cleaning or sub-sampling, resulting in 1.26 trillion tokens. CRYSTALCODER’s pre-training dataset blended SlimPajama and StarCoder data, totaling around 1382 billion tokens. The training for CRYSTALCODER was divided into three stages, with a unique mix of English and coding data.</li>\n  <li>The following figure from the paper shows a summary of notable open-source LLMs. We note a trend of progressively less disclosure of important pretraining details over time: (1) availability of pretraining code, (2) disclosure of training configurations and hyperparameters, (3) intermediate checkpoints of model weights, (4) intermediate checkpoints of optimizer states, (5) disclosure of data mixture and sources, (6) reproducibility of pretraining data sequence, and (7) availability (or reconstruction scripts) of the pretraining data.</li>\n</ul>\n<p><img src=\"../../../images/papers/LLM360.jpg\" alt=\"\"></p>\n<ul>\n  <li>In terms of infrastructure, AMBER was trained on an in-house GPU cluster, utilizing 56 DGX A100 nodes, with a throughput of approximately 582.4k tokens per second. CRYSTALCODER was trained on the Cerebras Condor Galaxy 1, a 4 exaFLOPS supercomputer.</li>\n  <li>The paper discusses challenges encountered during pre-training, such as NaN loss on specific data chunks and missing optimizer states. It also highlights the importance of data cleaning and mixing ratios in LLM pre-training.</li>\n  <li>One of the key contributions of LLM360 is the release of training code, hyperparameters, configurations, model checkpoints, and evaluation metrics, all aimed at fostering a collaborative and transparent research environment.</li>\n  <li>The paper concludes with insights into future work, including a more detailed analysis of AMBER and CRYSTALCODER, exploration of optimal data mixing ratios, and the pre-training of a larger LLM.</li>\n  <li><a href=\"https://www.llm360.ai/\">Project page</a></li>\n</ul>\n<h4 id=\"olmo\"><a href=\"https://arxiv.org/pdf/2402.00838\">OLMo</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/pdf/2402.00838\">OLMo: Accelerating the Science of Language Models</a> by Groeneveld et al. from the Allen AI, UW, Yale, NYU, and CMU.</li>\n  <li>OLMo is a state-of-the-art open language model and framework. It aims to advance the science of language modeling by providing open access to model weights, training, and evaluation code, in response to the proprietary nature of recent powerful models. This initiative seeks to foster innovation and scientific study of language models, including their biases and potential risks.</li>\n  <li>Unlike previous efforts that provided limited access to model components, OLMo releases a comprehensive framework encompassing the training data Dolma, hundreds of intermediate checkpoints, training logs, and evaluation tools like Catwalk for downstream evaluation and Paloma for perplexity-based evaluation. This release supports the examination of the impact of training data, design choices, and optimization methods on model performance.</li>\n  <li>OLMo adopts a decoder-only transformer architecture with several enhancements for stability and efficiency, such as the exclusion of bias terms and the use of SwiGLU activation functions. It is available in 1B and 7B variants, with a 65B version in progress, all trained on a diverse, multi-source corpus of 3T tokens across 5B documents.</li>\n  <li>The pretraining dataset, Dolma, is a significant contribution to open research, comprising 3T tokens from various sources with detailed documentation. It underwent extensive curation, including language and quality filtering, deduplication, and mixing from multiple sources, to support the study of training data’s impact on model capabilities.</li>\n  <li>OLMo’s evaluation showcases competitive performance across various metrics compared to other models, underscoring its potential as a base model for further research and application. Additionally, the adaptation of OLMo using instruction tuning and Direct Preference Optimization demonstrates its versatility for creating safer and more effective language models.</li>\n  <li>The release also emphasizes the environmental impact of training large models, providing detailed estimates of power consumption and carbon footprint to highlight the cost of developing state-of-the-art models and encourage the use of open models to reduce redundant efforts and emissions.</li>\n  <li>OLMo is part of a broader effort to push the boundaries of open language models, with plans for future releases that include larger models, more modalities, and enhanced safety measures. This endeavor aims to empower the research community and drive forward innovation in language modeling.</li>\n  <li><a href=\"https://github.com/allenai/OLMo\">Code</a>; <a href=\"https://huggingface.co/datasets/allenai/dolma\">Data</a>; <a href=\"https://huggingface.co/allenai/OLMo-7B\">Weights</a></li>\n</ul>\n<h5 id=\"olmoe\"><a href=\"https://arxiv.org/abs/2409.02060\">OLMOE</a></h5>\n<ul>\n  <li>Proposed in OLMOE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) by Muennighoff et al. from Allen AI, Contextual AI, UW, and Princeton, OLMOE is an open, state-of-the-art language model that leverages sparse Mixture-of-Experts (MoE) architecture. OLMOE-1B-7B has a total of 7 billion parameters, but activates only 1 billion for each input token, thus making it more efficient compared to dense models of similar size. The model is pre-trained on 5.1 trillion tokens and further adapted to create OLMOE-1B-7B-INSTRUCT, which outperforms models with similar active parameters, even surpassing larger models such as Llama2-13B-Chat and DeepSeekMoE-16B.</li>\n  <li>OLMOE is a decoder-only language model consisting of NL transformer layers, and the dense feedforward network (FFN) modules typically found in such architectures are replaced with smaller, MoE-based FFN modules. Only a subset of these experts is activated per token, reducing computational requirements. Specifically, OLMOE-1B-7B employs 64 experts per layer, with 8 activated per input token. The routing of tokens is handled by a dropless token-based routing mechanism, which consistently chooses experts for each token.</li>\n  <li>Key aspects of the implementation include:\n    <ul>\n      <li><strong>Pretraining:</strong> OLMOE-1B-7B is trained from scratch using a combination of load balancing loss and router z-loss. The load balancing loss penalizes the model if too many tokens are routed to a few experts, encouraging more equal distribution across experts. The router z-loss helps stabilize the routing mechanism by limiting the size of the logits used in routing.</li>\n      <li><strong>Dataset:</strong> The model was pretrained on a mixture of datasets, including Common Crawl, arXiv, Wikipedia, and StarCoder, referred to as OLMOE-MIX. This dataset was filtered for quality, including the removal of low-quality GitHub repositories and documents with high-frequency repetitive n-grams.</li>\n      <li><strong>Sparse Upcycling:</strong> Although some prior works recommend upcycling dense models into sparse MoEs, the authors found that training from scratch outperforms sparse upcycling when using larger compute budgets and more training tokens. Thus, OLMOE-1B-7B was not upcycled from a dense model.</li>\n      <li><strong>Adaptation:</strong> The model is fine-tuned for instruction and preference tuning. This adaptation improves performance on math and code-related tasks due to the inclusion of math and code datasets. Fine-tuning with preference data (DPO) particularly enhances tasks like HumanEval and AlpacaEval.</li>\n    </ul>\n  </li>\n  <li>The following figure from the paper shows a comparison of the architecture of dense LMs and MoE models like OLMOE. The figure excludes some details, e.g., OLMOE-1B-7B also uses QK-Norm.</li>\n</ul>\n<ul>\n      <li><strong>Pretraining:</strong> OLMOE-1B-7B is trained from scratch using a combination of load balancing loss and router z-loss. The load balancing loss penalizes the model if too many tokens are routed to a few experts, encouraging more equal distribution across experts. The router z-loss helps stabilize the routing mechanism by limiting the size of the logits used in routing.</li>\n      <li><strong>Dataset:</strong> The model was pretrained on a mixture of datasets, including Common Crawl, arXiv, Wikipedia, and StarCoder, referred to as OLMOE-MIX. This dataset was filtered for quality, including the removal of low-quality GitHub repositories and documents with high-frequency repetitive n-grams.</li>\n      <li><strong>Sparse Upcycling:</strong> Although some prior works recommend upcycling dense models into sparse MoEs, the authors found that training from scratch outperforms sparse upcycling when using larger compute budgets and more training tokens. Thus, OLMOE-1B-7B was not upcycled from a dense model.</li>\n      <li><strong>Adaptation:</strong> The model is fine-tuned for instruction and preference tuning. This adaptation improves performance on math and code-related tasks due to the inclusion of math and code datasets. Fine-tuning with preference data (DPO) particularly enhances tasks like HumanEval and AlpacaEval.</li>\n    </ul>\n<p><img src=\"../../../images/papers/OLMOE.jpg\" alt=\"\"></p>\n<ul>\n  <li>The results demonstrate that OLMOE-1B-7B achieves higher performance while being more efficient in terms of compute compared to dense models like Llama2-7B, and even performs on par with some larger models. It achieves this by optimizing the trade-offs between active and total parameters, routing methods, and expert specialization, all within a fully open framework that shares training data, code, and logs.</li>\n  <li>The authors also share insights from various design experiments, highlighting:\n    <ul>\n      <li><strong>Expert Granularity:</strong> Using more granular experts (smaller but more numerous experts) yields better performance, though there are diminishing returns after a certain threshold.</li>\n      <li><strong>Routing Mechanism:</strong> Token choice routing, where tokens select their experts, performs better than expert choice routing in this setup.</li>\n      <li><strong>Loss Functions:</strong> The combination of load balancing loss and router z-loss is crucial to maintaining stability and ensuring expert specialization.</li>\n    </ul>\n  </li>\n  <li>OLMOE is an important step in making advanced MoE-based language models accessible to the open-source community by releasing all aspects of the model development, from code and data to intermediate training checkpoints.</li>\n  <li><a href=\"https://hf.co/allenai/OLMoE-1B-7B-0924\">Hugging Face</a></li>\n</ul>\n<ul>\n      <li><strong>Expert Granularity:</strong> Using more granular experts (smaller but more numerous experts) yields better performance, though there are diminishing returns after a certain threshold.</li>\n      <li><strong>Routing Mechanism:</strong> Token choice routing, where tokens select their experts, performs better than expert choice routing in this setup.</li>\n      <li><strong>Loss Functions:</strong> The combination of load balancing loss and router z-loss is crucial to maintaining stability and ensuring expert specialization.</li>\n    </ul>\n<h4 id=\"deepseek\"><a href=\"https://arxiv.org/abs/2401.02954v1\">DeepSeek</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2401.02954v1\">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</a> by Bi et al. from the DeepSeek-AI team, DeepSeek LLM is a project aimed at advancing the capabilities of open-source language models with a long-term perspective. Building upon the foundations laid by previous literature which presented varied conclusions on scaling LLMs, this paper presents novel findings that facilitate the scaling of large-scale models in two commonly used open-source configurations: 7B and 67B parameters.</li>\n  <li>At the core of their approach is the development of a dataset comprising 2 trillion tokens, which supports the pre-training phase. Additionally, supervised fine-tuning (SFT) and direct preference optimization (DPO) are conducted on the base models to create the DeepSeek Chat models. Through rigorous evaluation, DeepSeek LLM 67B demonstrates its superiority over LLaMA-2 70B across various benchmarks, especially in code, mathematics, and reasoning, and exhibits enhanced performance in open-ended evaluations against GPT-3.5.</li>\n  <li>The architecture of DeepSeek LLM adheres closely to the LLaMA design, utilizing a Pre-Norm structure with RMSNorm, SwiGLU for the Feed-Forward Network (FFN), and incorporating Rotary Embedding for positional encoding. Modifications include a 30-layer network for the 7B model and a 95-layer network for the 67B model, differing in layer adjustments to optimize training and inference efficiency.</li>\n  <li>A critical contribution of this study is the exploration of scaling laws for hyperparameters, where optimal values for batch size and learning rate are identified based on extensive experimentation. This leads to a significant revelation: the quality of training data critically impacts the optimal scaling strategy between model size and data volume. The higher the data quality, the more a scaling budget should lean towards model scaling.</li>\n  <li>The paper also delves into alignment strategies through SFT and DPO, employing a dataset with 1.5 million instances to enhance the model’s helpfulness and harmlessness. The evaluation framework spans across a wide array of public benchmarks in both English and Chinese, addressing various domains such as language understanding, reasoning, and coding.</li>\n  <li>Safety evaluation forms a pivotal part of the study, ensuring that the models adhere to ethical guidelines and are devoid of harmful outputs. The results across multiple safety evaluation metrics underscore the model’s reliability and safe interaction capabilities.</li>\n  <li>The DeepSeek LLM initiative not only pushes the envelope in the open-source landscape of LLMs but also sets a new benchmark for future research in scaling, safety, and alignment of language models, driving forward the quest towards Artificial General Intelligence (AGI).</li>\n</ul>\n<h5 id=\"deepseek-v3\"><a href=\"https://arxiv.org/abs/2412.19437\">DeepSeek-V3</a></h5>\n<ul>\n  <li><strong>Overview</strong>:\n    <ul>\n      <li>Proposed in <a href=\"https://arxiv.org/abs/2412.19437\">DeepSeek-V3</a> by the DeepSeek team, DeepSeek-V3 is an open-source Mixture-of-Experts (MoE) language model with a massive 671B total parameters, activating 37B parameters per token. The model prioritizes efficient inference and training cost-effectiveness through key innovations in architecture and training strategies.</li>\n    </ul>\n  </li>\n  <li><strong>Key Contributions</strong>:\n    <ol>\n      <li><strong>Architecture Enhancements</strong>:\n        <ul>\n          <li><strong>Multi-Head Latent Attention (MLA)</strong> and <strong>DeepSeekMoE</strong>:\n            <ul>\n              <li>Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.</li>\n              <li>DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.</li>\n            </ul>\n          </li>\n          <li><strong>Multi-Token Prediction (MTP)</strong>:\n            <ul>\n              <li>Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.</li>\n            </ul>\n          </li>\n          <li>The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/DeepSeekV3.jpg\" alt=\"\"></p>\n      </li>\n      <li><strong>Pre-Training Efficiency</strong>:\n        <ul>\n          <li>Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.</li>\n          <li>Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.</li>\n          <li>Training completed within 2.788M H800 GPU hours, costing only $5.576M.</li>\n        </ul>\n      </li>\n      <li><strong>Post-Training Innovations</strong>:\n        <ul>\n          <li>Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.</li>\n          <li>Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.</li>\n        </ul>\n      </li>\n      <li><strong>Evaluation and Performance</strong>:\n        <ul>\n          <li>Benchmarked as the strongest open-source base model, excelling in:\n            <ul>\n              <li>Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).</li>\n              <li>Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.</li>\n              <li>Multilingual capabilities, especially strong in Chinese factual knowledge.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li><strong>Inference and Deployment</strong>:\n        <ul>\n          <li>Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.</li>\n          <li>Implements redundancy and dynamic load balancing among experts to optimize inference throughput.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Technical Implementation</strong>:\n    <ol>\n      <li><strong>Architectural Details</strong>:\n        <ul>\n          <li>MLA compresses key-value and query tensors for efficient attention.</li>\n          <li>DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.</li>\n        </ul>\n      </li>\n      <li><strong>Training Framework</strong>:\n        <ul>\n          <li>Uses <strong>DualPipe</strong> for overlapping computation and communication during pipeline parallelism.</li>\n          <li>Efficient memory management via recomputation, shared embeddings, and low-precision caching.</li>\n        </ul>\n      </li>\n      <li><strong>FP8 Training</strong>:\n        <ul>\n          <li>Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.</li>\n          <li>Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Limitations and Future Directions</strong>:\n    <ul>\n      <li>While performing well, challenges remain in fine-grained task specialization and scaling inference strategies for broader applications.</li>\n      <li>Future research could explore advanced dynamic expert routing and further reducing training costs with emergent hardware.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/deepseek-ai/DeepSeek-V3\">Code</a></li>\n</ul>\n<ul>\n      <li>Proposed in <a href=\"https://arxiv.org/abs/2412.19437\">DeepSeek-V3</a> by the DeepSeek team, DeepSeek-V3 is an open-source Mixture-of-Experts (MoE) language model with a massive 671B total parameters, activating 37B parameters per token. The model prioritizes efficient inference and training cost-effectiveness through key innovations in architecture and training strategies.</li>\n    </ul>\n<ol>\n      <li><strong>Architecture Enhancements</strong>:\n        <ul>\n          <li><strong>Multi-Head Latent Attention (MLA)</strong> and <strong>DeepSeekMoE</strong>:\n            <ul>\n              <li>Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.</li>\n              <li>DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.</li>\n            </ul>\n          </li>\n          <li><strong>Multi-Token Prediction (MTP)</strong>:\n            <ul>\n              <li>Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.</li>\n            </ul>\n          </li>\n          <li>The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.</li>\n        </ul>\n\n        <p><img src=\"../../../images/papers/DeepSeekV3.jpg\" alt=\"\"></p>\n      </li>\n      <li><strong>Pre-Training Efficiency</strong>:\n        <ul>\n          <li>Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.</li>\n          <li>Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.</li>\n          <li>Training completed within 2.788M H800 GPU hours, costing only $5.576M.</li>\n        </ul>\n      </li>\n      <li><strong>Post-Training Innovations</strong>:\n        <ul>\n          <li>Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.</li>\n          <li>Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.</li>\n        </ul>\n      </li>\n      <li><strong>Evaluation and Performance</strong>:\n        <ul>\n          <li>Benchmarked as the strongest open-source base model, excelling in:\n            <ul>\n              <li>Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).</li>\n              <li>Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.</li>\n              <li>Multilingual capabilities, especially strong in Chinese factual knowledge.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li><strong>Inference and Deployment</strong>:\n        <ul>\n          <li>Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.</li>\n          <li>Implements redundancy and dynamic load balancing among experts to optimize inference throughput.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li><strong>Multi-Head Latent Attention (MLA)</strong> and <strong>DeepSeekMoE</strong>:\n            <ul>\n              <li>Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.</li>\n              <li>DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.</li>\n            </ul>\n          </li>\n          <li><strong>Multi-Token Prediction (MTP)</strong>:\n            <ul>\n              <li>Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.</li>\n            </ul>\n          </li>\n          <li>The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.</li>\n        </ul>\n<ul>\n              <li>Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.</li>\n              <li>DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.</li>\n            </ul>\n<ul>\n              <li>Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.</li>\n            </ul>\n<p><img src=\"../../../images/papers/DeepSeekV3.jpg\" alt=\"\"></p>\n<ul>\n          <li>Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.</li>\n          <li>Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.</li>\n          <li>Training completed within 2.788M H800 GPU hours, costing only $5.576M.</li>\n        </ul>\n<ul>\n          <li>Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.</li>\n          <li>Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.</li>\n        </ul>\n<ul>\n          <li>Benchmarked as the strongest open-source base model, excelling in:\n            <ul>\n              <li>Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).</li>\n              <li>Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.</li>\n              <li>Multilingual capabilities, especially strong in Chinese factual knowledge.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li>Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).</li>\n              <li>Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.</li>\n              <li>Multilingual capabilities, especially strong in Chinese factual knowledge.</li>\n            </ul>\n<ul>\n          <li>Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.</li>\n          <li>Implements redundancy and dynamic load balancing among experts to optimize inference throughput.</li>\n        </ul>\n<ol>\n      <li><strong>Architectural Details</strong>:\n        <ul>\n          <li>MLA compresses key-value and query tensors for efficient attention.</li>\n          <li>DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.</li>\n        </ul>\n      </li>\n      <li><strong>Training Framework</strong>:\n        <ul>\n          <li>Uses <strong>DualPipe</strong> for overlapping computation and communication during pipeline parallelism.</li>\n          <li>Efficient memory management via recomputation, shared embeddings, and low-precision caching.</li>\n        </ul>\n      </li>\n      <li><strong>FP8 Training</strong>:\n        <ul>\n          <li>Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.</li>\n          <li>Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>MLA compresses key-value and query tensors for efficient attention.</li>\n          <li>DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.</li>\n        </ul>\n<ul>\n          <li>Uses <strong>DualPipe</strong> for overlapping computation and communication during pipeline parallelism.</li>\n          <li>Efficient memory management via recomputation, shared embeddings, and low-precision caching.</li>\n        </ul>\n<ul>\n          <li>Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.</li>\n          <li>Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.</li>\n        </ul>\n<ul>\n      <li>While performing well, challenges remain in fine-grained task specialization and scaling inference strategies for broader applications.</li>\n      <li>Future research could explore advanced dynamic expert routing and further reducing training costs with emergent hardware.</li>\n    </ul>\n<h4 id=\"liberated-qwen15\"><a href=\"https://huggingface.co/abacusai/Liberated-Qwen1.5-72B\">Liberated-Qwen1.5</a></h4>\n<ul>\n  <li><a href=\"https://abacus.ai/\">Abacus.AI</a> has released an intriguing new open-source large language model called Liberated-Qwen1.5-72B. What makes this LLM unique is its strict adherence to system prompts, even when those prompts conflict with the user’s instructions.</li>\n  <li>As enterprises adopt LLMs for uses like customer service chatbots, getting models to reliably follow set guidelines is crucial. Too often, existing LLMs can veer off in unexpected directions when conversing with users over multiple turns. Liberated-Qwen aims to solve this with its system prompt compliance.</li>\n  <li>The team at Abacus fine-tuned the Alibaba Qwen1.5-72B model on a new 7K SystemChat dataset specifically designed to train obedience to system messages, even when contradicting user input. The result is an uncensored model that will execute directives like answering only in capitals.</li>\n  <li>Initial tests show Liberated-Qwen performing slightly better than the original Qwen model on areas like coding (HumanEval) while maintaining strong general capabilities (MMLU scores). However, it lacks safety guardrails, so caution is advised before deployment.</li>\n  <li>The team plans to further improve the model’s coding performance and release enhanced versions blending the SystemChat data with the training used for their previous Smaug release.</li>\n  <li>This innovative approach could make Liberated-Qwen and its successors compelling options for businesses needing LLMs that prioritize obedience to rules and guidelines, but getting safe deployment right will be key.</li>\n</ul>\n<h4 id=\"dolphin-llama\"><a href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b\">Dolphin Llama</a></h4>\n<ul>\n  <li>Dolphin is a model with 8B and 70B sizes by Eric Hartford based on Llama that has a variety of instruction, conversational, and coding skills.</li>\n  <li>Dolphin has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.</li>\n  <li>Dolphin is uncensored. The dataset has been filtered to remove alignment and bias. This makes the model more compliant.</li>\n</ul>\n<h4 id=\"command-r\"><a href=\"https://txt.cohere.com/command-r/\">Command-R</a></h4>\n<ul>\n  <li>Cohere’s Command-R is optimized for long context tasks such as retrieval augmented generation (RAG), using external APIs and tools. It boasts low latency, high accuracy, supports 10 languages, and has 128k context length.</li>\n  <li>The model excels at 10 major languages of global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.</li>\n  <li><a href=\"https://huggingface.co/CohereForAI/c4ai-command-r-v01\">Hugging Face</a></li>\n</ul>\n<h5 id=\"command-r-1\"><a href=\"https://huggingface.co/CohereForAI/c4ai-command-r-plus\">Command R+</a></h5>\n<ul>\n  <li>C4AI Command R+ is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks.</li>\n  <li>C4AI Command R+ is a multilingual model evaluated in 10 languages for performance: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Arabic, and Simplified Chinese. Command R+ is optimized for a variety of use cases including reasoning, summarization, and question answering.</li>\n  <li><a href=\"https://huggingface.co/CohereForAI/c4ai-command-r-plus\">Hugging Face</a>; <a href=\"https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus\">Demo</a>;</li>\n</ul>\n<h4 id=\"eaglex\"><a href=\"https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b\">EagleX</a></h4>\n<ul>\n  <li><a href=\"https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b\">EagleX 1.7T</a> is a early research release of a 7.52B parameter model training that is\n    <ul>\n      <li>Built on the <a href=\"https://wiki.rwkv.com/\">RWKV-v5 architecture</a>, a linear transformer with 10-100x+ lower inference cost.</li>\n      <li>Is continuation based on the original <a href=\"https://substack.recursal.ai/cp/141146731\">Eagle 7B</a> model</li>\n      <li>Ranks as the world’s greenest 7B model (per token)</li>\n      <li>Trained on 1.7 Trillion tokens across 100+ languages</li>\n      <li>Outperforms all 7B class models in multi-lingual benchmarks</li>\n      <li>Passes LLaMA2 (2T) in multiple English evals, approaches Mistral (&gt;2T?)</li>\n      <li>All while being an “Attention-Free Transformer”</li>\n    </ul>\n  </li>\n  <li><a href=\"https://huggingface.co/recursal/EagleX_1-7T\">Hugging Face</a>; <a href=\"https://huggingface.co/spaces/recursal/EagleX-7B-1.7T-Gradio-Demo\">Hugging Face Demo</a></li>\n</ul>\n<ul>\n      <li>Built on the <a href=\"https://wiki.rwkv.com/\">RWKV-v5 architecture</a>, a linear transformer with 10-100x+ lower inference cost.</li>\n      <li>Is continuation based on the original <a href=\"https://substack.recursal.ai/cp/141146731\">Eagle 7B</a> model</li>\n      <li>Ranks as the world’s greenest 7B model (per token)</li>\n      <li>Trained on 1.7 Trillion tokens across 100+ languages</li>\n      <li>Outperforms all 7B class models in multi-lingual benchmarks</li>\n      <li>Passes LLaMA2 (2T) in multiple English evals, approaches Mistral (&gt;2T?)</li>\n      <li>All while being an “Attention-Free Transformer”</li>\n    </ul>\n<h4 id=\"grok\"><a href=\"https://x.ai/blog/grok-os\">Grok</a></h4>\n<h5 id=\"grok-1\"><a href=\"https://x.ai/blog/grok-os\">Grok-1</a></h5>\n<ul>\n  <li><a href=\"https://x.ai\">xAI</a> has released the base model weights and network architecture of Grok-1, a 314 billion parameter Mixture-of-Experts model trained from scratch by xAI using a custom training stack on top of JAX and Rust in October 2023.</li>\n  <li>This is the raw base model checkpoint from the Grok-1 pre-training phase, which concluded in October 2023. This means that the model is not fine-tuned for any specific application, such as dialogue.</li>\n  <li><a href=\"https://github.com/xai-org/grok\">Code</a></li>\n</ul>\n<h5 id=\"grok-15\"><a href=\"https://x.ai/blog/grok-1.5\">Grok-1.5</a></h5>\n<ul>\n  <li>Grok-1.5 is introduced as the latest advancement in long context understanding and advanced reasoning, promising availability to early testers and existing users on the X platform shortly. Following the release of Grok-1’s model weights and network architecture, xAI’s Grok-1.5 showcases enhanced reasoning and problem-solving capabilities, particularly highlighted in coding and math-related tasks.</li>\n  <li>In performance benchmarks, Grok-1.5 demonstrated significant improvements by achieving a 50.6% score on the MATH benchmark, a 90% score on the GSM8K benchmark, and a 74.1% score on the HumanEval benchmark, showcasing its prowess in a wide range of mathematical and coding challenges.</li>\n  <li>A notable feature of Grok-1.5 is its ability to process contexts up to 128,000 tokens, expanding its memory capacity significantly to handle information from longer documents. This is visualized through a graph indicating a 100% recall rate across varying context lengths, emphasizing the model’s robust information retrieval capacity even with extensive contexts.</li>\n  <li>Grok-1.5’s infrastructure is based on a custom distributed training framework integrating JAX, Rust, and Kubernetes, designed for the demanding requirements of LLM research. This framework addresses challenges in training LLMs on large compute clusters by optimizing reliability, uptime, and efficient resource management through a custom training orchestrator and improvements in checkpointing, data loading, and training job restarts.</li>\n</ul>\n<h4 id=\"saullm\"><a href=\"https://arxiv.org/abs/2403.03883\">SaulLM</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2403.03883\">SaulLM-7B: A pioneering Large Language Model for Law</a>, SaulLM-7B, introduced by Colombo et al., is the first LLM with 7 billion parameters, designed specifically for the legal domain, based on the Mistral 7B architecture. It is trained on over 30 billion tokens from an English legal corpus, showing state-of-the-art proficiency in legal document comprehension and generation. Additionally, the paper introduces an instructional fine-tuning method using legal datasets to enhance SaulLM-7B’s performance on legal tasks, released under the MIT License.</li>\n  <li>The creation of SaulLM-7B addresses the gap in specialized LLM applications within the legal field, marked by complex document volumes and unique linguistic challenges. The model’s pretraining incorporates extensive legal corpora from English-speaking jurisdictions, including the USA, Canada, the UK, and Europe, aiming to comprehend and adapt to the evolving legal discourse. This focus targets the needs of legal practitioners, representing a significant step towards integrating artificial intelligence within legal applications.</li>\n  <li>SaulLM-7B’s family includes SaulLM-7B-Instruct, an instruction-tuned variant that outperforms models like Mistral and Llama on various legal tasks. This achievement is part of the paper’s contributions, which also introduce LegalBench-Instruct and model evaluation code &amp; licensing under the MIT License. LegalBench-Instruct, a refined iteration of LegalBench, aims to better assess and refine legal language model proficiency, incorporating tasks from the MMLU benchmark related to international law, professional law, and jurisprudence.</li>\n  <li>The paper details the data sources and preprocessing steps involved in constructing the training corpus, highlighting the combination of pre-existing datasets and new data scraped from the web. Rigorous data cleaning, deduplication, and the inclusion of “replay” sources to mitigate catastrophic forgetting during continued pretraining form the foundation of a robust 30 billion token corpus. Instruction fine-tuning mixes further refine the model’s ability to understand and follow legal instructions.</li>\n  <li>Evaluation of SaulLM-7B involves comparing its performance against other open-source models using benchmarks like LegalBench-Instruct and Legal-MMLU. The results demonstrate SaulLM-7B’s superior proficiency in legal document processing and task performance. The perplexity analysis across different types of legal documents further confirms the model’s effectiveness in the legal domain.</li>\n  <li>SaulLM-7B signifies a novel approach in the AI-driven assistance for legal professionals, aiming for widespread adoption and innovation across commercial and research endeavors in law. The release of SaulLM-7B under an open license encourages collaborative development and application in various legal contexts, setting a precedent for future advancements in AI-powered legal tools.</li>\n  <li>The model is open-sourced and allows commercial use, inviting the legal sector and AI engineers to further tinker with legal LLMs.</li>\n  <li><a href=\"https://huggingface.co/Equall/Saul-Instruct-v1\">Model</a></li>\n</ul>\n<h4 id=\"dbrx\"><a href=\"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\">DBRX</a></h4>\n<ul>\n  <li><a href=\"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\">DBRX</a> is a new state-of-the-art Open LLM by Databricks Mosaic Research Team that achieves state-of-the-art performance across multiple benchmarks, surpassing existing open models like GPT-3.5 and showing competitive results against Gemini 1.0 Pro. DBRX is notable for its exceptional capabilities in coding tasks, even outperforming specialized models such as CodeLLaMA-70B.</li>\n  <li>Key features and advancements of DBRX include:\n    <ul>\n      <li><strong>Efficiency and Performance</strong>: DBRX introduces significant efficiency improvements in training and inference through its fine-grained mixture-of-experts (MoE) architecture. It boasts inference speeds up to 2x faster than LLaMA2-70B and has about 40% fewer parameters compared to Grok-1, without compromising on model quality. The model demonstrates a quadruple reduction in computational requirements compared to its predecessors while maintaining similar performance levels.</li>\n      <li><strong>Architecture Innovations</strong>: The model utilizes a transformer-based decoder-only structure with 132B total parameters, incorporating advanced techniques such as rotary position encodings, gated linear units, and grouped query attention. It’s pretrained on a diverse mix of 12T tokens from text and code, allowing for a maximum context length of 32k tokens. This architectural choice enables DBRX to efficiently handle a wide range of tasks with fewer parameters activated per input.</li>\n      <li><strong>Benchmark Performance</strong>: DBRX sets new records on various benchmarks, including language understanding, programming, and mathematics, significantly outperforming other leading open models. It is also competitive with or surpasses leading closed models in nearly all considered benchmarks, particularly excelling in programming and mathematical reasoning. The plot below from the paper shows that DBRX outperforms established open source models on language understanding (MMLU), Programming (HumanEval), and Math (GSM8K).</li>\n    </ul>\n\n    <p><img src=\"assets/LLM/DBRX.jpg\" alt=\"\"></p>\n\n    <ul>\n      <li><strong>Open Access and Integration</strong>: DBRX weights for both base and fine-tuned versions are openly available on Hugging Face, facilitating easy access for further experimentation and development. Databricks customers can utilize DBRX via APIs and have the option to pretrain or continue training DBRX-class models using Databricks tools and infrastructure.</li>\n      <li><strong>Training and Inference Efficiency</strong>: The paper highlights DBRX’s training and inference efficiency, illustrating that MoE models like DBRX offer significant improvements in compute efficiency. It provides detailed comparisons in training efficiency, showing that models within the DBRX family require fewer FLOPs to reach similar or better performance scores compared to denser models. In terms of inference, DBRX achieves higher throughput than comparable non-MoE models, benefiting from the model’s efficient parameter usage.</li>\n      <li><strong>Development and Deployment</strong>: The development of DBRX leveraged a robust suite of tools provided by Databricks, including data management, experiment tracking, and large-scale model training and finetuning services. This comprehensive toolset facilitated the efficient creation of DBRX and its integration into GenAI-powered products, demonstrating the practical application and scalability of Databricks’ infrastructure.</li>\n    </ul>\n  </li>\n  <li>DBRX represents a significant milestone in the development of open-source LLMs, offering a highly efficient, powerful, and accessible model for a wide range of applications. Its release underscores the potential of open models to drive innovation and democratize access to cutting-edge AI technologies.</li>\n</ul>\n<ul>\n      <li><strong>Efficiency and Performance</strong>: DBRX introduces significant efficiency improvements in training and inference through its fine-grained mixture-of-experts (MoE) architecture. It boasts inference speeds up to 2x faster than LLaMA2-70B and has about 40% fewer parameters compared to Grok-1, without compromising on model quality. The model demonstrates a quadruple reduction in computational requirements compared to its predecessors while maintaining similar performance levels.</li>\n      <li><strong>Architecture Innovations</strong>: The model utilizes a transformer-based decoder-only structure with 132B total parameters, incorporating advanced techniques such as rotary position encodings, gated linear units, and grouped query attention. It’s pretrained on a diverse mix of 12T tokens from text and code, allowing for a maximum context length of 32k tokens. This architectural choice enables DBRX to efficiently handle a wide range of tasks with fewer parameters activated per input.</li>\n      <li><strong>Benchmark Performance</strong>: DBRX sets new records on various benchmarks, including language understanding, programming, and mathematics, significantly outperforming other leading open models. It is also competitive with or surpasses leading closed models in nearly all considered benchmarks, particularly excelling in programming and mathematical reasoning. The plot below from the paper shows that DBRX outperforms established open source models on language understanding (MMLU), Programming (HumanEval), and Math (GSM8K).</li>\n    </ul>\n<p><img src=\"assets/LLM/DBRX.jpg\" alt=\"\"></p>\n<ul>\n      <li><strong>Open Access and Integration</strong>: DBRX weights for both base and fine-tuned versions are openly available on Hugging Face, facilitating easy access for further experimentation and development. Databricks customers can utilize DBRX via APIs and have the option to pretrain or continue training DBRX-class models using Databricks tools and infrastructure.</li>\n      <li><strong>Training and Inference Efficiency</strong>: The paper highlights DBRX’s training and inference efficiency, illustrating that MoE models like DBRX offer significant improvements in compute efficiency. It provides detailed comparisons in training efficiency, showing that models within the DBRX family require fewer FLOPs to reach similar or better performance scores compared to denser models. In terms of inference, DBRX achieves higher throughput than comparable non-MoE models, benefiting from the model’s efficient parameter usage.</li>\n      <li><strong>Development and Deployment</strong>: The development of DBRX leveraged a robust suite of tools provided by Databricks, including data management, experiment tracking, and large-scale model training and finetuning services. This comprehensive toolset facilitated the efficient creation of DBRX and its integration into GenAI-powered products, demonstrating the practical application and scalability of Databricks’ infrastructure.</li>\n    </ul>\n<h4 id=\"jamba\"><a href=\"https://www.ai21.com/blog/announcing-jamba\">Jamba</a></h4>\n<ul>\n  <li>Jamba is AI21’s Groundbreaking SSM-Transformer Model, which represents a novel leap in language model architecture by integrating Mamba Structured State Space (SSM) technology with the traditional Transformer model, creating the world’s first production-grade Mamba based model. This hybrid approach notably addresses the scalability and performance limitations of pure SSM or Transformer models, providing a substantial increase in efficiency and throughput. Key advancements include a 256K context window and the capacity to fit up to 140K context on a single GPU, marking it as a leader in its class.</li>\n  <li>To capture the best that both Mamba and Transformer architectures have to offer, we developed the corresponding Joint Attention and Mamba (Jamba) architecture. Composed of Transformer, Mamba, and mixture-of-experts (MoE) layers, Jamba optimizes for memory, throughput, and performance – all at once – as depicted in the table below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/Jamba1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The architecture of Jamba combines Transformer layers, Mamba layers, and mixture-of-experts (MoE) layers to optimize memory usage, computational throughput, and overall performance. One of the critical innovations is the use of MoE layers, allowing Jamba to selectively utilize just 12B out of its available 52B parameters during inference, making it significantly more efficient than a Transformer model of equivalent size.</li>\n  <li>As depicted in the diagram below, AI21’s Jamba architecture features a blocks-and-layers approach that allows Jamba to successfully integrate the two architectures. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/Jamba2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Jamba has been scaled to a production-grade level, a feat previously unachieved by Mamba models beyond 3B parameters. Its architecture employs a blocks-and-layers design that alternates between attention or Mamba layers and multi-layer perceptrons (MLP), with a Transformer layer included for every eight total layers. This design is instrumental in optimizing the model for high-quality output and throughput on common hardware, such as a single 80GB GPU.</li>\n  <li>Significant results have been observed in Jamba’s performance, with a 3x improvement in throughput on long contexts compared to similar models like Mixtral 8x7B, without compromising on efficiency. These achievements have been made possible by innovative engineering choices, including the strategic use of MoE layers to manage computational demands and the integration of Mamba with Transformer architectures for superior model capacity and efficiency.</li>\n  <li>Jamba is released with open weights under Apache 2.0, encouraging further exploration and development within the AI community. Additionally, it’s made accessible via Hugging Face and is slated for inclusion in the NVIDIA API catalog, facilitating its adoption in enterprise applications through the NVIDIA AI Enterprise software platform.</li>\n  <li><a href=\"https://huggingface.co/ai21labs/Jamba-v0.1\">Model</a></li>\n</ul>\n<h5 id=\"jamba-15\"><a href=\"https://www.ai21.com/blog/announcing-jamba-model-family\">Jamba 1.5</a></h5>\n<ul>\n  <li>The Jamba 1.5 family, introduced by AI21, includes Mini and Large models. They leverage the innovative SSM-Transformer architecture for superior long context handling, speed, and quality.</li>\n  <li>Jamba 1.5 models feature a 256K effective context window, the longest in the market, enhancing applications like document summarization, analysis, and RAG workflows.</li>\n  <li>The models are up to 2.5 times faster on long contexts compared to competitors, making them ideal for high-demand enterprise use cases.</li>\n  <li>Jamba 1.5 models, available under an open license, support multiple languages and can be integrated across various cloud platforms and on-premise setups.</li>\n  <li>Utilizing the novel ExpertsInt8 quantization technique, these models offer efficiency in memory usage, allowing Jamba 1.5 Large to function with high performance on a single 8 GPU node.</li>\n  <li>Benchmarks indicate that Jamba 1.5 Mini and Large outperform competitors in their respective size classes in both speed and quality, demonstrating outstanding value for enterprise applications.</li>\n</ul>\n<h4 id=\"wizardlm-2\"><a href=\"https://huggingface.co/posts/WizardLM/329547800484476\">WizardLM-2</a></h4>\n<ul>\n  <li>WizardLM-2 is a series of three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B, which offer improved performance on complex chat, multilingual, reasoning and agent.</li>\n  <li>WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.</li>\n  <li>WizardLM 2 capabilities:\n    <ol>\n      <li>MT-Bench (Figure 1)\n  The WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary works such as GPT-4-Trubo and Glaude-3. Meanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.</li>\n      <li>Human Preferences Evaluation (Figure 2)\n  Through this human preferences evaluation, WizardLM-2’s capabilities are very close to the cutting-edge proprietary models such as GPT-4-1106-preview, and significantly ahead of all the other open source models.</li>\n    </ol>\n  </li>\n  <li>WizardLM-2 uses a fully AI powered synthetic training system as shown below:</li>\n</ul>\n<ol>\n      <li>MT-Bench (Figure 1)\n  The WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary works such as GPT-4-Trubo and Glaude-3. Meanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.</li>\n      <li>Human Preferences Evaluation (Figure 2)\n  Through this human preferences evaluation, WizardLM-2’s capabilities are very close to the cutting-edge proprietary models such as GPT-4-1106-preview, and significantly ahead of all the other open source models.</li>\n    </ol>\n<p><img src=\"/primers/ai/assets/LLM/WizardLM-2.png\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://wizardlm.github.io/WizardLM2\">Blog</a></li>\n</ul>\n<h4 id=\"gemini\"><a href=\"https://arxiv.org/abs/2312.11805\">Gemini</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2312.11805\">Gemini: A Family of Highly Capable Multimodal Models</a>.</li>\n  <li>Google’s Gemini series represents a milestone in AI development, featuring three models: Ultra, Pro, and Nano, each tailored for specific tasks ranging from complex problem-solving to on-device operations. Gemini Ultra, the flagship model, excels in demanding tasks and sets new benchmarks in AI performance. Gemini Pro is optimized for a wide range of tasks, while Nano is designed for efficiency in on-device applications. This suite of models, part of Google DeepMind’s vision, marks a significant scientific and engineering endeavor for the company.</li>\n  <li>Gemini models are built with a transformative architecture that allows for a “deep fusion” of modalities, surpassing the capabilities of typical modular AI designs. This integration enables seamless concept transfer across various domains, such as vision and language. The models, trained on TPUs, support a 32k context length and are capable of handling diverse inputs and outputs, including text, vision, and audio. The visual encoder, inspired by Flamingo, and the comprehensive training data, comprising web documents, books, code, and multimedia, contribute to the models’ versatility.</li>\n  <li>The figure below from the paper illustrates that Gemini supports interleaved sequences of text, image, audio, and video as inputs (illustrated by tokens of different colors in the input sequence). It can output responses with interleaved image and text.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemini1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The training infrastructure for Gemini utilizes Google’s latest TPU v4 and v5e accelerators, ensuring efficient scaling and reliable performance at an unprecedented scale. This advanced setup is integral to handling hardware failures and silent data corruption, ensuring high-quality training outcomes.</li>\n  <li>The training dataset is multimodal and multilingual, with quality and safety filters to enhance model performance. The dataset mix is adjusted during training to emphasize domain-relevant data, contributing to the models’ high performance.</li>\n  <li>Gemini Ultra showcases extraordinary capabilities across various benchmarks, surpassing GPT-4 in areas like coding and reasoning. Its performance in benchmarks like HumanEval and Natural2Code, as well as its superior reasoning capabilities in complex subjects like math and physics, demonstrate its state-of-the-art capabilities. For instance, the figure below from the paper shows solving a geometrical reasoning task. Gemini shows good understanding of the task and is able to provide meaningful reasoning steps despite slightly unclear instructions.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemini2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Furthermore, in another instance, the figure below from the paper shows Gemini verifying a student’s solution to a physics problem. The model is able to correctly recognize all of the handwritten content and verify the reasoning. On top of understanding the text in the image, it needs to understand the problem setup and correctly follow instructions to generate LaTeX.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemini3.jpg\" alt=\"\"></p>\n<ul>\n  <li>Gemini outperforms OpenAI’s GPT-4 in 30 out of 32 benchmarks. Furthermore, it’s worth noting is that Gemini Ultra is the first model to outperform human experts on MMLU (massive multitask language understanding). The following table from Google’s <a href=\"https://blog.google/technology/ai/google-gemini-ai/\">blog</a> Gemini surpasses state-of-the-art performance on a range of benchmarks including text and coding.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemini4.jpg\" alt=\"\"></p>\n<ul>\n  <li>For image understanding, Gemini Ultra sets new standards by outperforming existing models in zero-shot evaluations for OCR-related tasks. Its native multimodality and complex reasoning abilities enable it to excel in interpreting and reasoning with visual information. The following table from Google’s <a href=\"https://blog.google/technology/ai/google-gemini-ai/\">blog</a> Gemini surpasses state-of-the-art performance on a range of multimodal benchmarks.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemini5.jpg\" alt=\"\"></p>\n<ul>\n  <li>Gemini’s training involves Reinforcement Learning from Human Feedback (RLHF), enhancing its performance and capabilities. This advanced training, combined with its innovative architecture and diverse dataset, contributes to its exceptional performance across various tasks.</li>\n  <li>Despite its remarkable capabilities, specific details about Gemini’s architecture, training data, and the size of the Ultra and Pro models remain undisclosed. However, the models represent a significant leap in AI development, driven by the promise of AI to benefit humanity in diverse ways.</li>\n  <li>Safety and responsibility are central to Gemini’s development, with comprehensive safety evaluations for bias and toxicity. Google is collaborating with external experts and partners to stress-test the models and ensure they adhere to robust safety policies, aligning with Google’s AI Principles.</li>\n  <li>Gemini’s capabilities and its development approach reflect Google’s commitment to advancing AI responsibly and ethically, emphasizing safety and collaboration with the industry and broader ecosystem to define best practices and safety benchmarks.</li>\n  <li><a href=\"https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\">Report</a>; <a href=\"https://blog.google/technology/ai/google-gemini-ai/\">Blog</a>.</li>\n</ul>\n<h4 id=\"gemma\"><a href=\"https://arxiv.org/abs/2403.08295\">Gemma</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2403.08295\">Gemma: Open Models Based on Gemini Research and Technology</a>.</li>\n  <li>This paper by Gemma Team from Google DeepMind introduces Gemma, a family of open models based on the Gemini model architecture. It comprises two versions: a 7 billion parameter model for GPU and TPU applications, and a 2 billion parameter model suited for CPU and on-device implementations. Both models are trained using up to 6 trillion tokens from primarily English sources, focusing on web documents, mathematics, and code, with a tokenizer that supports a large vocabulary size of 256k entries.</li>\n  <li>The models utilize advanced techniques including Multi-Query Attention, RoPE Embeddings, GeGLU Activations, and RMSNorm. These improvements aim to enhance the model’s performance and efficiency, particularly in processing long sequences up to 8192 tokens.</li>\n  <li>Training infrastructure involves extensive use of TPUv5e across multiple pods, with specific configurations for different model scales. The training also incorporates techniques from Google’s earlier projects like Pathways and Jax to manage data efficiently across distributed systems.</li>\n  <li>A substantial focus of the Gemma project is on responsible and safe deployment. This includes rigorous filtering of the training data to avoid sensitive or harmful content, and a detailed evaluation of the models against various safety and performance benchmarks.</li>\n  <li>The figure below from the paper illustrates the language understanding and generation performance of Gemma 7B across different capabilities compared to similarly sized open models. They group together standard academic benchmark evaluations by capability and average the respective scores.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gemma.jpg\" alt=\"\"></p>\n<ul>\n  <li>Gemma models have shown superior performance on a range of tasks, outperforming other models in benchmarks for question answering, reasoning, math/science, and coding. They also display robust safety features, evaluated through automated benchmarks and human preference studies, ensuring that they behave predictably and safely in diverse applications.</li>\n  <li>The models are also equipped with capabilities for supervised fine-tuning and reinforcement learning from human feedback, enabling them to improve over time based on specific user interactions and feedback. This adaptability makes them suitable for a wide array of applications, from automated customer support to sophisticated data analysis tasks.</li>\n  <li>Despite their capabilities, the models come with an acknowledgment of their limitations, particularly in terms of their potential use in generating sensitive or misleading information. DeepMind emphasizes the importance of continuous monitoring and evaluation to mitigate any unintended consequences of their use.</li>\n</ul>\n<h5 id=\"gemma-2-improving-open-language-models-at-a-practical-size\"><a href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf\">Gemma 2: Improving Open Language Models at a Practical Size</a></h5>\n<ul>\n  <li>This paper by Gemma Team at Google DeepMind introduces Gemma 2, a family of lightweight, state-of-the-art open language models ranging from 2B to 27B parameters (that can fit on a single GPU). The models, particularly the 9 billion and 27 billion parameter versions, incorporate several architectural advancements and are trained with knowledge distillation, which enhances their performance and efficiency.</li>\n  <li>The Gemma 2 models are built on a decoder-only transformer architecture with parameters summarized as follows:\n    <ul>\n      <li>2.6B parameters: 26 layers, d_model 2304, GeGLU non-linearity, 18432 feedforward dimension, 8 attention heads.</li>\n      <li>9B parameters: 42 layers, d_model 3584, GeGLU non-linearity, 28672 feedforward dimension, 16 attention heads.</li>\n      <li>27B parameters: 46 layers, d_model 4608, GeGLU non-linearity, 73728 feedforward dimension, 32 attention heads.</li>\n    </ul>\n  </li>\n  <li>The core architectural innovations include the interleaving of local and global attention mechanisms and the use of Grouped-Query Attention (GQA). Specifically, local sliding window attention handles sequences of 4096 tokens, while global attention spans 8192 tokens. Logit soft-capping is employed to stabilize the training, ensuring the logits stay within defined bounds. Both pre-norm and post-norm with RMSNorm are used to normalize inputs and outputs of transformer sub-layers, improving training stability.</li>\n  <li>The models are trained on extensive datasets with the 27B model trained on 13 trillion primarily-English tokens, the 9B model on 8 trillion tokens, and the 2.6B model on 2 trillion tokens. The training infrastructure involves TPUv4 and TPUv5 configurations, leveraging significant parallelization and sharding techniques to handle the computational load efficiently.</li>\n  <li>Knowledge distillation is a pivotal part of the training process for Gemma 2 models. Smaller models are trained using the probability distributions provided by a larger model, enhancing their learning efficiency and enabling them to simulate training on a much larger token corpus than what is physically available. This method not only reduces training time but also allows smaller models to achieve performance levels competitive with significantly larger models.</li>\n  <li>In post-training, the models undergo supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). The SFT phase uses a mix of synthetic and real prompt-response pairs, predominantly generated by a larger teacher model. The RLHF phase involves training a reward model based on human-labeled preference data to fine-tune the policy further, enhancing the models’ conversational capabilities and safety.</li>\n  <li>Empirical evaluations show that the Gemma 2 models outperform previous versions and competitive models in various benchmarks, including MMLU, GSM8K, and ARC-C. The models are tested for their robustness to formatting variations, safety, and alignment, showcasing significant improvements in these areas.</li>\n  <li>Safety and responsible deployment are critical components of the Gemma 2 project. The models are rigorously tested and filtered to prevent harmful content generation, and extensive safety mechanisms are integrated into the training process. This approach ensures that the models align with Google’s safety policies, mitigating risks associated with malicious use.</li>\n  <li>The performance of the Gemma 2 models is on par with models twice or more their size! Model weights are open-source, thanks to Google DeepMind.</li>\n  <li><strong>Key architectural components</strong>:\n    <ol>\n      <li><strong>Grouped Query Attention (GQA)</strong>: The key difference between GQA and the standard Multi-headed attention is the reduction in the number of key and value heads, effectively grouping heads together. This approach balances between MHA and MQA, maintaining efficiency and performance.:</li>\n      <li><strong>Sliding Window Attention (SWA)</strong>: The authors interleaved local and global attentions in alternating layers. This technique reduces the number of parameters while maintaining performance. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.:</li>\n      <li><strong>Rotary Position Embeddings (RoPE)</strong>: RoPE encodes absolute position with a rotation matrix and incorporates relative position dependency in self-attention formulation, enabling sequence length flexibility and improved model performance. This is standard with prevalent LLMs.</li>\n      <li><strong>Logit soft-capping</strong>: Logit soft-capping stabilizes training by ensuring logits stay within defined bounds. This technique is implemented using a scaled tanh function, capping attention logits at 50.0 and final logits at 30.0.:</li>\n      <li><strong>Model merging</strong>: Model merging involves averaging models from experiments run with different hyperparameters, improving overall performance. Techniques like Exponential Moving Average (EMA), Spherical Linear Interpolation (SLERP), and Linear Interpolation Towards Initialization (LITI) are employed during the merging process.</li>\n      <li><strong>Knowledge distillation for training 2B and 9B models (instead of next token prediction)</strong>: The 2B and 9B models in Gemma 2 are trained using knowledge distillation, where a larger, pre-trained model (the teacher) provides probability distributions over the vocabulary for each token. Instead of predicting the next token using one-hot vectors, the smaller models (students) learn from these richer distributions, using the Kullback-Leibler (KL) divergence as the loss function. The gradients derived from the richer, softer probability distributions help the student model to learn more effectively than from hard one-hot vectors due to the nuanced signal provided by the teacher model. This method improves performance and efficiency, allowing smaller models to achieve results comparable to much larger models.</li>\n    </ol>\n  </li>\n  <li>In conclusion, Gemma 2 represents a significant advancement in the development of open language models, offering competitive performance at a practical size. The use of knowledge distillation and advanced attention mechanisms provides a pathway for smaller models to achieve high performance, making state-of-the-art language modeling more accessible to the community.</li>\n  <li><a href=\"https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315\">Hugging Face</a>; <a href=\"https://amaarora.github.io/posts/2024-07-07%20Gemma.html\">Aman Arora’s Blog</a></li>\n</ul>\n<ul>\n      <li>2.6B parameters: 26 layers, d_model 2304, GeGLU non-linearity, 18432 feedforward dimension, 8 attention heads.</li>\n      <li>9B parameters: 42 layers, d_model 3584, GeGLU non-linearity, 28672 feedforward dimension, 16 attention heads.</li>\n      <li>27B parameters: 46 layers, d_model 4608, GeGLU non-linearity, 73728 feedforward dimension, 32 attention heads.</li>\n    </ul>\n<ol>\n      <li><strong>Grouped Query Attention (GQA)</strong>: The key difference between GQA and the standard Multi-headed attention is the reduction in the number of key and value heads, effectively grouping heads together. This approach balances between MHA and MQA, maintaining efficiency and performance.:</li>\n      <li><strong>Sliding Window Attention (SWA)</strong>: The authors interleaved local and global attentions in alternating layers. This technique reduces the number of parameters while maintaining performance. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.:</li>\n      <li><strong>Rotary Position Embeddings (RoPE)</strong>: RoPE encodes absolute position with a rotation matrix and incorporates relative position dependency in self-attention formulation, enabling sequence length flexibility and improved model performance. This is standard with prevalent LLMs.</li>\n      <li><strong>Logit soft-capping</strong>: Logit soft-capping stabilizes training by ensuring logits stay within defined bounds. This technique is implemented using a scaled tanh function, capping attention logits at 50.0 and final logits at 30.0.:</li>\n      <li><strong>Model merging</strong>: Model merging involves averaging models from experiments run with different hyperparameters, improving overall performance. Techniques like Exponential Moving Average (EMA), Spherical Linear Interpolation (SLERP), and Linear Interpolation Towards Initialization (LITI) are employed during the merging process.</li>\n      <li><strong>Knowledge distillation for training 2B and 9B models (instead of next token prediction)</strong>: The 2B and 9B models in Gemma 2 are trained using knowledge distillation, where a larger, pre-trained model (the teacher) provides probability distributions over the vocabulary for each token. Instead of predicting the next token using one-hot vectors, the smaller models (students) learn from these richer distributions, using the Kullback-Leibler (KL) divergence as the loss function. The gradients derived from the richer, softer probability distributions help the student model to learn more effectively than from hard one-hot vectors due to the nuanced signal provided by the teacher model. This method improves performance and efficiency, allowing smaller models to achieve results comparable to much larger models.</li>\n    </ol>\n<h5 id=\"gemma-3\"><a href=\"https://blog.google/technology/developers/gemma-3/\">Gemma 3</a></h5>\n<ul>\n  <li>Gemma 3 is a family of open-source AI models, designed for high performance and portability. It builds on the success of the original Gemma, which saw over 100 million downloads and a thriving developer community. With sizes ranging from 1B to 27B, Gemma 3 models are optimized to run efficiently across devices—from phones to workstations—while supporting advanced capabilities like text and visual reasoning, function calling, and multilingual applications.</li>\n  <li>Gemma 3 includes quantized versions for lower compute requirements and offers a massive 128k-token context window. It outperforms models like Llama3-405B and DeepSeek-V3 in human preference benchmarks. ShieldGemma 2, a 4B image safety model, is also released to ensure responsible AI use in visual applications.</li>\n  <li><a href=\"https://huggingface.co/google/gemma-3-12b-it\">Hugging Face</a></li>\n</ul>\n<h4 id=\"jetmoe\"><a href=\"https://arxiv.org/abs/2404.07413\">JetMoE</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2404.07413\">JetMoE: Reaching Llama2 Performance with 0.1M Dollars</a> by Shen et al. from MIT-IBM Watson AI Lab, MIT EECS, Princeton University, and MyShell.ai &amp; MIT, JetMoE-8B is a cost-effective large language model, outperforming established models like Llama2-7B and Llama2-13B-Chat. JetMoE-8B extends the concept of sparse activation to both the attention and feed-forward layers. Despite being trained on a tight budget of under $100,000, JetMoE-8B employs 8 billion parameters, leveraging a Sparsely-gated Mixture-of-Experts (SMoE) architecture that activates only 2 billion parameters per input token. This architecture reduces inference computation by approximately 70% compared to Llama2-7B.</li>\n  <li>JetMoE-8B is trained using the Megatron framework with Megablock enhancements, using pipeline parallelism to optimize computational costs and load balance during training. Notably, it incorporates innovations like shared KV projection in attention layers and a frequency-based auxiliary loss for training efficiency.</li>\n  <li>The figure below from the paper illustrates the JetMoE architecture.</li>\n</ul>\n<p><img src=\"../../../images/papers/JetMoE.jpg\" alt=\"\"></p>\n<ul>\n  <li>For pretraining, JetMoE-8B utilized a mixture of real-world and synthetic datasets, totaling 1.25 trillion tokens. Datasets include RefinedWeb, StarCoder, and various components from The Pile, combined with synthetic datasets like OpenHermes 2.5 for diverse training inputs.</li>\n  <li>Utilized a two-phase training approach, incorporating a mix of real and synthetic datasets with adjustments in data weighting during the learning rate decay phase to enhance model performance.</li>\n  <li>The model underwent Distilled Supervised Fine-Tuning (dSFT) and Distilled Direct Preference Optimization (dDPO), refining model responses based on preferences from a teacher model to improve alignment with human-like conversational abilities.</li>\n  <li>JetMoE-8B’s performance was benchmarked against other models in tasks like ARC-challenge, Hellaswag, and MMLU, showing superior performance in many areas, particularly in code-related benchmarks like MBPP and HumanEval.</li>\n  <li>The training parameters, model configurations, and data mixtures are fully documented and made open-source to foster further academic and practical advancements in efficient LLM training methodologies.</li>\n  <li><a href=\"https://github.com/myshell-ai/JetMoE\">Code</a></li>\n</ul>\n<h4 id=\"minimax-text\"><a href=\"https://www.minimaxi.com/en/news/minimax-01-series-2\">Minimax-Text</a></h4>\n<ul>\n  <li>Minimax-Text from Hailuo has perfect needle in haystack recall on 4M token context!\n  — 10x cheaper than GPT4o ($0.2/M input and $1.1/M output tokens)\n  — Benchmarks on par with SOTA models\n  — 456B param MoE with ~46B active</li>\n  <li><a href=\"https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf\">Tech report</a>, <a href=\"https://www.hailuo.ai/\">Demo</a></li>\n</ul>",
    "contentMarkdown": "*   The following descriptions of models are from their respective project pages.\n\n#### Llama\n\n##### [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n\n*   Read our LLaMA primer [here](../llama).\n\n##### [Llama 2](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n\n*   [Llama 2](https://arxiv.org/abs/2307.09288) is a collection of pretrained and fine-tuned LLMs from Meta AI ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Their models outperform open-source chat models on most benchmarks we tested, and based on their human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n*   Llama 2 is powered by Ghost Attention (GAtt), introduced in the paper, which improves multi-turn memory. From section 3.3 in the [technical report](https://arxiv.org/abs/2307.09288):\n    \n    *   “In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in the below figure (left) which shows that issues with multi-turn memory (left) can be improved with GAtt (right).\n    \n    ![](../../../images/papers/GAtt.jpg)\n    \n    *   To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation [(Bai et al., 2022)](#constitutional-ai-harmlessness-from-ai-feedback) that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in the figure above (right).\n    *   GAtt Method: Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages \\[u1,a1,…,un,an\\]\\[u1,a1,…,un,an\\]\\\\left\\[u\\_1, a\\_1, \\\\ldots, u\\_n, a\\_n\\\\right\\], where ununu\\_n and anana\\_n correspond to the user and assistant messages for turn nnn, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be “act as.” We can then synthetically concatenate this instruction to all the user messages of the conversation.\n    *   Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.\n    *   For the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy e.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists of hobbies and public figures, we asked Llama 2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also modify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”-> “Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.\n    *   GAtt Evaluation: We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5 in the paper). We tried to set constraints not present in the training of GAtt at inference time, for instance “Always answer with Haiku,” for which the model was found to remain consistent.\n    *   To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message (“Act as Oscar Wilde”). From the figure above, we can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left).\n    *   Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.”\n*   Another important aspect that is highlighted in the report is the effect of RLHF on Llama 2, and this graph from Meta’s paper shows how high-quality human preferences data (obtained from [Surge AI](https://www.surgehq.ai/)) keeps on improving Llama 2 – without saturation.\n\n*   “In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in the below figure (left) which shows that issues with multi-turn memory (left) can be improved with GAtt (right).\n\n![](../../../images/papers/GAtt.jpg)\n\n*   To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation [(Bai et al., 2022)](#constitutional-ai-harmlessness-from-ai-feedback) that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in the figure above (right).\n*   GAtt Method: Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages \\[u1,a1,…,un,an\\]\\[u1,a1,…,un,an\\]\\\\left\\[u\\_1, a\\_1, \\\\ldots, u\\_n, a\\_n\\\\right\\], where ununu\\_n and anana\\_n correspond to the user and assistant messages for turn nnn, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be “act as.” We can then synthetically concatenate this instruction to all the user messages of the conversation.\n*   Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.\n*   For the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy e.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists of hobbies and public figures, we asked Llama 2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also modify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”-> “Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.\n*   GAtt Evaluation: We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5 in the paper). We tried to set constraints not present in the training of GAtt at inference time, for instance “Always answer with Haiku,” for which the model was found to remain consistent.\n*   To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message (“Act as Oscar Wilde”). From the figure above, we can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left).\n*   Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.”\n\n![](../../../images/papers/Llama2RLHF.jpeg)\n\n*   They also call out the importance of supervised fine-tuning (SFT) data quality (in the “quality is all you need” section) – it’s not about volume, but diversity and quality.\n*   From [Linxi Fan](https://www.linkedin.com/in/drjimfan/)’s notes:\n    *   Llama-2 likely costed $20M+ to train. Meta has done an incredible service to the community by releasing the model with a commercially-friendly license. AI researchers from big companies were wary of Llama-1 due to licensing issues, but now many of them will jump on the ship and contribute their firepower.\n    *   Meta’s team did a human study on 4K prompts to evaluate Llama-2’s helpfulness. They use “win rate” as a metric to compare models, in similar spirit as the Vicuna benchmark. 70B model roughly ties with GPT-3.5-0301, and performs noticeably stronger than Falcon, MPT, and Vicuna. These real human ratings should be trusted more than academic benchmarks, because they typically capture the “in-the-wild vibe” better.\n    *   Llama-2 is not yet at GPT-3.5 level, mainly because of its weak coding abilities. On “HumanEval” (standard coding benchmark), it isn’t nearly as good as StarCoder or many other models specifically designed for coding. That being said, I have little doubt that Llama-2 will improve significantly thanks to its open weights.\n    *   Meta’s team goes above and beyond on AI safety issues. In fact, almost half of the paper is talking about safety guardrails, red-teaming, and evaluations. A round of applause for such responsible efforts!\n    *   In prior works, there’s a thorny trade-ff between helpfulness and safety. Meta mitigates this by training 2 separate reward models. They aren’t open-source yet, but would be extremely valuable to the community.\n    *   Llama-2 will dramatically boost multimodal AI and robotics research. These fields need more than just blackbox access to an API.\n    *   So far, we have to convert the complex sensory signals (video, audio, 3D perception) to text description and then feed to an LLM, which is awkward and leads to huge information loss. It’d be much more effective to graft sensory modules directly on a strong LLM backbone.\n    *   The [whitepaper](https://arxiv.org/abs/2307.09288) itself is a masterpiece. Unlike GPT-4’s paper that shared very little info, Llama-2 spelled out the entire recipe, including model details, training stages, hardware, data pipeline, and annotation process. For example, there’s a systematic analysis on the effect of RLHF with nice visualizations. Quote sec 5.1: “We posit that the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF.”\n*   The following figure from the paper shows the training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly available online sources. Following this, they create an initial version of Llama 2-Chat through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.\n\n*   Llama-2 likely costed $20M+ to train. Meta has done an incredible service to the community by releasing the model with a commercially-friendly license. AI researchers from big companies were wary of Llama-1 due to licensing issues, but now many of them will jump on the ship and contribute their firepower.\n*   Meta’s team did a human study on 4K prompts to evaluate Llama-2’s helpfulness. They use “win rate” as a metric to compare models, in similar spirit as the Vicuna benchmark. 70B model roughly ties with GPT-3.5-0301, and performs noticeably stronger than Falcon, MPT, and Vicuna. These real human ratings should be trusted more than academic benchmarks, because they typically capture the “in-the-wild vibe” better.\n*   Llama-2 is not yet at GPT-3.5 level, mainly because of its weak coding abilities. On “HumanEval” (standard coding benchmark), it isn’t nearly as good as StarCoder or many other models specifically designed for coding. That being said, I have little doubt that Llama-2 will improve significantly thanks to its open weights.\n*   Meta’s team goes above and beyond on AI safety issues. In fact, almost half of the paper is talking about safety guardrails, red-teaming, and evaluations. A round of applause for such responsible efforts!\n*   In prior works, there’s a thorny trade-ff between helpfulness and safety. Meta mitigates this by training 2 separate reward models. They aren’t open-source yet, but would be extremely valuable to the community.\n*   Llama-2 will dramatically boost multimodal AI and robotics research. These fields need more than just blackbox access to an API.\n*   So far, we have to convert the complex sensory signals (video, audio, 3D perception) to text description and then feed to an LLM, which is awkward and leads to huge information loss. It’d be much more effective to graft sensory modules directly on a strong LLM backbone.\n*   The [whitepaper](https://arxiv.org/abs/2307.09288) itself is a masterpiece. Unlike GPT-4’s paper that shared very little info, Llama-2 spelled out the entire recipe, including model details, training stages, hardware, data pipeline, and annotation process. For example, there’s a systematic analysis on the effect of RLHF with nice visualizations. Quote sec 5.1: “We posit that the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF.”\n\n![](../../../images/papers/Llama2.jpg)\n\n*   Summary:\n    *   Llama 2 is available for free (including commercial license).\n    *   Llama 2 can be accessed via managed services in Azure and AWS.\n    *   Llama is trained on 2B tokens, with 4 variants, ranging from 7-70B parameters.\n    *   Llama is intended to be used in English, with almost 90% of the pre-training data being in English.\n    *   The commercial license specifies a number of harmful use cases that violate the license, including spam!\n    *   Llama 2 is very comparable to ChatGPT 3.5 in most benchmarks (particularly, it beats ChatGPT in human evaluation on helpfulness: Win 36%; Tie 32%; Loss 32%) other than coding, looking at the data mix coding data is still quite small (classified under the - unknown language category)\n    *   Llama 2 outperforms all other open-source models including Falcon and MPT, and has three variants including 7B, 13B, and 70B; the 70B variant achieves top performance across the board.\n    *   Benchmarks were done both on standardized ones (like MMLU) and head to head competition against other models, including PaLM-2 Bison and ChatGPT 3.5.\n    *   A large portion of the paper focuses on RLHF improvements and objectives which is super neat.\n    *   Model toxicity and evaluation is another large focus, including evaluations like red-teaming which were found in the Claude 2 model card. Generally Llama 2 performed very well with fewer safety violations than ChatGPT in human evaluations.\n    *   The tokenizer is the same as Llama 1 which is interesting, but the context length is now 4k, double the original 2k!\n    *   There’s both a regular and chat variation, as has been the trend in recent papers.\n    *   Llama 2 (with fine tuning) offers better domain-specificity via fine-tuning at lower cost, and better guardrails.\n    *   Llama 2 is trained on 40% more data than Llama 1 and performs well against benchmarks.\n    *   In short: companies can create their own enterprise “ChatGPT” (without sharing any data with OpenAI).\n*   Quantized Llama 2 weights are available for local inference [here](https://huggingface.co/TheBloke).\n    \n*   The following diagram presents summarizes the key graphs/tables of the Llama 2 paper:\n\n*   Llama 2 is available for free (including commercial license).\n*   Llama 2 can be accessed via managed services in Azure and AWS.\n*   Llama is trained on 2B tokens, with 4 variants, ranging from 7-70B parameters.\n*   Llama is intended to be used in English, with almost 90% of the pre-training data being in English.\n*   The commercial license specifies a number of harmful use cases that violate the license, including spam!\n*   Llama 2 is very comparable to ChatGPT 3.5 in most benchmarks (particularly, it beats ChatGPT in human evaluation on helpfulness: Win 36%; Tie 32%; Loss 32%) other than coding, looking at the data mix coding data is still quite small (classified under the - unknown language category)\n*   Llama 2 outperforms all other open-source models including Falcon and MPT, and has three variants including 7B, 13B, and 70B; the 70B variant achieves top performance across the board.\n*   Benchmarks were done both on standardized ones (like MMLU) and head to head competition against other models, including PaLM-2 Bison and ChatGPT 3.5.\n*   A large portion of the paper focuses on RLHF improvements and objectives which is super neat.\n*   Model toxicity and evaluation is another large focus, including evaluations like red-teaming which were found in the Claude 2 model card. Generally Llama 2 performed very well with fewer safety violations than ChatGPT in human evaluations.\n*   The tokenizer is the same as Llama 1 which is interesting, but the context length is now 4k, double the original 2k!\n*   There’s both a regular and chat variation, as has been the trend in recent papers.\n*   Llama 2 (with fine tuning) offers better domain-specificity via fine-tuning at lower cost, and better guardrails.\n*   Llama 2 is trained on 40% more data than Llama 1 and performs well against benchmarks.\n*   In short: companies can create their own enterprise “ChatGPT” (without sharing any data with OpenAI).\n\nQuantized Llama 2 weights are available for local inference [here](https://huggingface.co/TheBloke).\n\n![](../../../images/papers/Llama_overv.jpeg)\n\n*   The following infographic [(source)](https://www.linkedin.com/in/sebastianraschka/) presents an overview of Llama 2:\n\n![](../../../images/papers/LLama2Summ.jpeg)\n\n*   [Demo](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI); HuggingFace [repo](https://huggingface.co/meta-llama); [Project page](https://ai.meta.com/resources/models-and-libraries/llama/).\n    \n*   Related: [llama2.c](https://github.com/karpathy/llama2.c)\n    *   The quest for running LLMs on a single computer landed [Andrej Karpathy](https://www.linkedin.com/in/andrej-karpathy-9a650716/) to embark on a weekend project to create a simplified version of the Llama 2 model, informally called TinyLlama or BabyLlama.\n    *   Based on [llama.cpp](https://github.com/ggerganov/llama.cpp), this is a bare-bones project with the Llama 2 architecture hard-coded, FP32 precision, one inference file of pure C with no dependencies.\n    *   “With the code in this repo you can train the Llama 2 LLM architecture from scratch in PyTorch, then export the weights to a binary file, and load that into one ~simple 500-line C file ([run.c](https://github.com/karpathy/llama2.c/blob/master/run.c)) that inferences the model. You might think that you need many billion parameter LLMs to do anything useful, but in fact very small LLMs can have surprisingly strong performance if you make the domain narrow enough.” [(source)](https://github.com/karpathy/llama2.c)\n    *   Basically, it is nanoGPT, tuned to implement the Llama 2 architecture instead of GPT-2, and the meat of it was writing the C inference engine in run.c,” explained Karpathy in Llama2.c GitHub repository. His objective was to implement nanoGPT into Llama 2 architecture, instead of GPT within C programming language. The repository has already got 2.2K stars.\n    *   The success of Karpathy’s approach lies in its ability to achieve highly interactive rates, even with reasonably sized models containing a few million parameters and trained on a 15 million parameter model of the TinyStories dataset.\n    *   On a M1 MacBook Air, the Llama 2 model with ~15 million parameters can infer at around 100 tokens per second in fp32, all through the C code he developed.\n    *   This surprising result demonstrates the feasibility of running complex models on resource-constrained devices with a straightforward implementation.\n*   [Llama 2 is about as factually accurate as GPT-4 for summaries and is 30X cheaper](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper) evaluates Llama 2 for summarization and obtains stellar results compared to GPT-4, with investigations around the issues of LLMs not following instructions and ordering bias.\n\n[Demo](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI); HuggingFace [repo](https://huggingface.co/meta-llama); [Project page](https://ai.meta.com/resources/models-and-libraries/llama/).\n\n*   The quest for running LLMs on a single computer landed [Andrej Karpathy](https://www.linkedin.com/in/andrej-karpathy-9a650716/) to embark on a weekend project to create a simplified version of the Llama 2 model, informally called TinyLlama or BabyLlama.\n*   Based on [llama.cpp](https://github.com/ggerganov/llama.cpp), this is a bare-bones project with the Llama 2 architecture hard-coded, FP32 precision, one inference file of pure C with no dependencies.\n*   “With the code in this repo you can train the Llama 2 LLM architecture from scratch in PyTorch, then export the weights to a binary file, and load that into one ~simple 500-line C file ([run.c](https://github.com/karpathy/llama2.c/blob/master/run.c)) that inferences the model. You might think that you need many billion parameter LLMs to do anything useful, but in fact very small LLMs can have surprisingly strong performance if you make the domain narrow enough.” [(source)](https://github.com/karpathy/llama2.c)\n*   Basically, it is nanoGPT, tuned to implement the Llama 2 architecture instead of GPT-2, and the meat of it was writing the C inference engine in run.c,” explained Karpathy in Llama2.c GitHub repository. His objective was to implement nanoGPT into Llama 2 architecture, instead of GPT within C programming language. The repository has already got 2.2K stars.\n*   The success of Karpathy’s approach lies in its ability to achieve highly interactive rates, even with reasonably sized models containing a few million parameters and trained on a 15 million parameter model of the TinyStories dataset.\n*   On a M1 MacBook Air, the Llama 2 model with ~15 million parameters can infer at around 100 tokens per second in fp32, all through the C code he developed.\n*   This surprising result demonstrates the feasibility of running complex models on resource-constrained devices with a straightforward implementation.\n\n##### [Llama 3](https://ai.meta.com/blog/meta-llama-3/)\n\n*   [Llama 3](https://ai.meta.com/blog/meta-llama-3/) by Meta offers substantial enhancements and novelties in the capabilities of the model. An analysis of its development illustrates a significant advance over its predecessors in multiple aspects, reflecting a sustained effort towards refining language model technology.\n*   **Tokenizer Enhancements**: Llama 3 has seen a notable expansion in its tokenizer capacity, increasing from 32,000 tokens in Llama 2 to 128,000 tokens. This enlargement allows for more efficient sequence compression, with a reduction in sequence length by approximately 15%, thus potentially enhancing downstream task performance due to a denser information representation.\n*   **Architectural Developments**: Despite no radical changes in the overall architecture from Llama 2, all variants of Llama 3 now incorporate Grouped Query Attention (GQA), a scheme previously reserved for larger models. GQA facilitates a more compact representation of the keys/values in the Attention mechanism, significantly reducing the footprint of the Key-Value (KV) cache during inference, thus optimizing computational efficiency.\n*   **Sequence Length Capacity**: The context window for Llama 3 has been increased to 8,192 tokens, up from 4,096 in Llama 2 and 2,048 in Llama 1. While this expansion is modest compared to the capabilities of models like GPT-4, which supports up to 128,000 tokens, it marks a progressive improvement, with potential future enhancements in subsequent versions.\n*   **Training Data Scope**: The training dataset size for Llama 3 has escalated dramatically to 15 trillion tokens, a substantial increment from the 2 trillion tokens used for Llama 2. This dataset not only focuses on English but also includes a 5% representation from over 30 different languages, incorporating a richer diversity in data, albeit still predominantly English-centric.\n*   **Scaling Laws and Efficiency**: The utilization of a 15 trillion token dataset to train a model with 8 billion parameters represents an unconventional approach by current standards, where such large datasets are typically reserved for much larger models. Meta’s approach indicates a shift towards maximizing model capability and efficiency beyond traditional compute-to-performance ratios, as indicated by scaling laws such as those outlined in the Chinchilla study.\n*   **Systems and Infrastructure**: Llama 3’s training was executed on a system of 16,000 GPUs, achieving an observed throughput of 400 TFLOPS. This figure suggests approximately 40% utilization of the peak theoretical output based on NVIDIA’s stated capabilities for the H100 GPUs at fp16 precision, acknowledging the adjustments required for realistic sparsity conditions.\n*   **Model “Strength”**: Incorporating insights from the model card for Llama 3, the performance comparison between the 8 billion parameter version (Llama 3 8B) and the larger 70 billion parameter version (Llama 2 70B) reveals intriguing nuances. Notably, Llama 3 8B, which was trained with a staggering 15 trillion tokens, exhibits comparable performance to Llama 2 70B, which was trained with just 2 trillion tokens. This discrepancy in training data volume underscores the significant impact of extensive training on model performance.\n    *   **Performance Metrics Based on Computational Training**: The metrics defining the strength of Llama 3 8B highlight its computational intensity. The model accrued approximately 1.8×10241.8×10241.8 \\\\times 10^{24} floating point operations (FLOPs) over 1.3 million GPU hours, assuming a throughput of 400 TFLOPS. In contrast, an alternative calculation method estimating FLOPs as 6ND6ND6ND (where NNN is the number of parameters and DDD is the number of tokens) yields approximately 7.2×10237.2×10237.2 \\\\times 10^{23} FLOPs, suggesting some variability in these estimates. Prioritizing the more comprehensive GPU hours calculation, Llama 3 8B’s total computational input stands around 2×10242×10242 \\\\times 10^{24} FLOPs.\n    *   **Comparative Analysis with Llama 3 70B and 400B Models**: For Llama 3 70B, the computational input is substantially higher, reaching approximately 9.2×10249.2×10249.2 \\\\times 10^{24} FLOPs calculated over 6.4 million GPU hours, which aligns closely with the alternative method’s estimate of 6.3×10246.3×10246.3 \\\\times 10^{24} FLOPs. Should the 400 billion parameter model train on the same dataset, the expected computational investment would scale up to approximately 4×10254×10254 \\\\times 10^{25} FLOPs. This projection places it just below the threshold outlined in regulatory frameworks such as the recent Biden Executive Order, which sets a reporting requirement at 1×10261×10261 \\\\times 10^{26} FLOPs.\n    *   **The Significance of Data Quality and Comprehensive Model Evaluation**: Beyond raw computational power, the quality of training data plays a critical role in shaping a model’s effectiveness. The integration of diverse and high-quality data can significantly enhance model performance, emphasizing the importance of not reducing the model’s capability to merely its computational input. However, when simplifying the comparison across models, total FLOPs provide a useful measure, amalgamating the scale of the model and the extent of its training into a singular metric indicative of its overall ‘strength.’\n*   In conclusion, Llama 3’s architecture and training regimen illustrate Meta’s strategic emphasis on maximizing model efficiency and performance through both scaled parameter counts and extensive training, setting new benchmarks in the landscape of language models. This approach not only boosts performance but also extends the model’s applicability and utility across a wider range of tasks and scenarios.\n*   **Conclusion**: The advancements in Llama 3 underscore Meta’s commitment to pushing the boundaries of what small yet powerfully trained models can achieve. This strategy not only enhances the capabilities of such models but also broadens their applicability in real-world scenarios, paving the way for future innovations in machine learning landscapes. Moreover, the anticipation surrounding the potential release of a 400 billion parameter model highlights the community’s eagerness for more robust, accessible AI tools, reflecting a growing trend towards democratizing high-performance computational models.\n*   [Blog](https://ai.meta.com/blog/meta-llama-3/); [Model Demo](https://meta.ai); [Model Card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md); [TorchTune](https://github.com/pytorch/torchtune)\n\n*   **Performance Metrics Based on Computational Training**: The metrics defining the strength of Llama 3 8B highlight its computational intensity. The model accrued approximately 1.8×10241.8×10241.8 \\\\times 10^{24} floating point operations (FLOPs) over 1.3 million GPU hours, assuming a throughput of 400 TFLOPS. In contrast, an alternative calculation method estimating FLOPs as 6ND6ND6ND (where NNN is the number of parameters and DDD is the number of tokens) yields approximately 7.2×10237.2×10237.2 \\\\times 10^{23} FLOPs, suggesting some variability in these estimates. Prioritizing the more comprehensive GPU hours calculation, Llama 3 8B’s total computational input stands around 2×10242×10242 \\\\times 10^{24} FLOPs.\n*   **Comparative Analysis with Llama 3 70B and 400B Models**: For Llama 3 70B, the computational input is substantially higher, reaching approximately 9.2×10249.2×10249.2 \\\\times 10^{24} FLOPs calculated over 6.4 million GPU hours, which aligns closely with the alternative method’s estimate of 6.3×10246.3×10246.3 \\\\times 10^{24} FLOPs. Should the 400 billion parameter model train on the same dataset, the expected computational investment would scale up to approximately 4×10254×10254 \\\\times 10^{25} FLOPs. This projection places it just below the threshold outlined in regulatory frameworks such as the recent Biden Executive Order, which sets a reporting requirement at 1×10261×10261 \\\\times 10^{26} FLOPs.\n*   **The Significance of Data Quality and Comprehensive Model Evaluation**: Beyond raw computational power, the quality of training data plays a critical role in shaping a model’s effectiveness. The integration of diverse and high-quality data can significantly enhance model performance, emphasizing the importance of not reducing the model’s capability to merely its computational input. However, when simplifying the comparison across models, total FLOPs provide a useful measure, amalgamating the scale of the model and the extent of its training into a singular metric indicative of its overall ‘strength.’\n\n##### [Llama 3.1](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)\n\n*   [Llama 3.1](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) by the Llama Team at Meta introduces the Llama 3 foundation models, which are dense Transformers designed to support multilinguality, coding, reasoning, and tool usage. The largest model contains 405B parameters and can process up to 128K tokens. Llama 3 models are publicly released, including pre-trained and post-trained versions and a Llama Guard model for input and output safety. The models integrate image, video, and speech capabilities using a compositional approach.\n*   Llama 3 development optimizes data quality and diversity, scaling, and managing complexity. The model is trained on a corpus of approximately 15T multilingual tokens, which is a significant increase from Llama 2’s 1.8T tokens. The flagship model is pre-trained using 3.8×10^25 FLOPs, nearly 50 times more than the largest Llama 2 model. The architecture uses a dense Transformer model with grouped query attention (GQA) for improved inference speed and reduced key-value cache size during decoding.\n*   **Pre-Training**:\n    *   **Data Curation**: Pre-training data is curated from various sources, filtered for quality, and de-duplicated at multiple levels. PII and unsafe content are removed, and a custom parser extracts high-quality text from web data. Multilingual data processing involves fasttext-based language identification and document de-duplication. The data mix includes 50% general knowledge, 25% mathematical and reasoning data, 17% code, and 8% multilingual data, optimized through scaling law experiments.\n    *   **Model Architecture**: Llama 3 uses a standard dense Transformer architecture with enhancements like GQA and a vocabulary size of 128K tokens. The RoPE (Rotary Position Embedding) base frequency is increased to 500,000 to support longer contexts, enabling the model to better handle sequences of up to 128K tokens.\n    *   **Preprocessing and Filtering**: The data preprocessing pipeline is highly sophisticated and involves several key steps to ensure high-quality input for the model. Roberta and DistilRoberta models are employed to classify data quality, providing a robust initial screening. The system also utilizes fasttext for language identification, categorizing documents into 176 languages, which aids in the precise removal of non-relevant content. Extensive de-duplication processes are applied at URL, document, and line levels to eliminate redundant information. This includes:\n        *   **URL-level de-duplication**: Removing duplicate URLs to keep the most recent versions.\n        *   **Document-level de-duplication**: Utilizing MinHash techniques to remove near-duplicate documents.\n        *   **Line-level de-duplication**: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.\n        *   **Heuristic Filtering**: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.\n        *   **Model-Based Quality Filtering**: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.\n        *   **Quality Scoring**: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.\n        *   **Difficulty Scoring**: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.\n*   **Training Details**: The model training employs a combination of pipeline parallelism and Fully Sharded Data Parallelism (FSDP). Pipeline parallelism partitions the model into stages across GPUs, while FSDP shards the model weights and optimizer states, allowing efficient training across multiple GPUs. The training process also includes a model averaging technique across the Reward Model (RM), Supervised Finetuning (SFT), and Direct Preference Optimization (DPO) stages, ensuring consistency and performance optimization.\n*   **Post-Training**:\n    *   **Supervised Finetuning and Direct Preference Optimization (DPO)**: The model undergoes multiple rounds of post-training involving supervised finetuning and DPO. A reward model is trained on human-annotated preference data, and the language model is finetuned on a mixture of human-curated and synthetic data.\n    *   **Data Quality Control**: Extensive data cleaning, pruning, and quality control measures ensure high-quality training samples. Data is categorized and scored for quality using both reward model and Llama-based signals.\n    *   **Multimodal Capabilities**: Separate encoders for images and speech are trained and integrated into the language model using adapters. This approach enables the model to handle image, video, and speech inputs effectively.\n    *   **Float8 Quantization**: The Llama 3 models utilize Float8 (fp8) quantization, where both weights and inputs are quantized to fp8. This quantization is followed by multiplication with scaling factors, resulting in outputs in bf16 format. This approach reduces VRAM usage and speeds up inference, making the models more efficient for deployment.\n*   The experimental results show that Llama 3 models perform competitively with state-of-the-art models like GPT-4 across various tasks, including language understanding, coding, and reasoning. The models also demonstrate robustness and scalability, with improvements in safety and alignment with human preferences.\n*   [Blog](https://ai.meta.com/blog/meta-llama-3-1/); [Model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md); [Hugging Face](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)\n\n*   **Data Curation**: Pre-training data is curated from various sources, filtered for quality, and de-duplicated at multiple levels. PII and unsafe content are removed, and a custom parser extracts high-quality text from web data. Multilingual data processing involves fasttext-based language identification and document de-duplication. The data mix includes 50% general knowledge, 25% mathematical and reasoning data, 17% code, and 8% multilingual data, optimized through scaling law experiments.\n*   **Model Architecture**: Llama 3 uses a standard dense Transformer architecture with enhancements like GQA and a vocabulary size of 128K tokens. The RoPE (Rotary Position Embedding) base frequency is increased to 500,000 to support longer contexts, enabling the model to better handle sequences of up to 128K tokens.\n*   **Preprocessing and Filtering**: The data preprocessing pipeline is highly sophisticated and involves several key steps to ensure high-quality input for the model. Roberta and DistilRoberta models are employed to classify data quality, providing a robust initial screening. The system also utilizes fasttext for language identification, categorizing documents into 176 languages, which aids in the precise removal of non-relevant content. Extensive de-duplication processes are applied at URL, document, and line levels to eliminate redundant information. This includes:\n    *   **URL-level de-duplication**: Removing duplicate URLs to keep the most recent versions.\n    *   **Document-level de-duplication**: Utilizing MinHash techniques to remove near-duplicate documents.\n    *   **Line-level de-duplication**: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.\n    *   **Heuristic Filtering**: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.\n    *   **Model-Based Quality Filtering**: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.\n    *   **Quality Scoring**: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.\n    *   **Difficulty Scoring**: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.\n\n*   **URL-level de-duplication**: Removing duplicate URLs to keep the most recent versions.\n*   **Document-level de-duplication**: Utilizing MinHash techniques to remove near-duplicate documents.\n*   **Line-level de-duplication**: Filtering out lines that appear excessively within document batches to remove boilerplate and repetitive content.\n*   **Heuristic Filtering**: Further filters are applied using techniques like duplicated n-gram coverage ratio to eliminate content like log messages, and dirty word counting to filter out adult content. Additionally, a token-distribution Kullback-Leibler divergence measure is used to remove documents with unusual token distributions.\n*   **Model-Based Quality Filtering**: Fast classifiers such as fasttext are used alongside more complex Roberta-based classifiers to select high-quality tokens. These classifiers are trained on a curated set of documents labeled for quality by previous Llama models.\n*   **Quality Scoring**: Quality scoring involves both reward model (RM) and Llama-based signals. For RM-based scoring, data in the top quartile of RM scores is considered high quality. For Llama-based scoring, the Llama 3 checkpoint rates samples on a three-point scale for general English data (Accuracy, Instruction Following, and Tone/Presentation) and a two-point scale for coding data (Bug Identification and User Intention). Samples marked as high quality by either the RM or Llama-based filters are selected, though the systems often disagree, suggesting complementary strengths.\n*   **Difficulty Scoring**: Difficulty scoring prioritizes more complex examples using Instag and Llama-based measures. Instag involves intention tagging of SFT prompts with Llama 3 70B, where more intentions indicate higher complexity. Additionally, Llama 3 rates dialog difficulty on a three-point scale, providing another layer of complexity assessment.\n\n*   **Supervised Finetuning and Direct Preference Optimization (DPO)**: The model undergoes multiple rounds of post-training involving supervised finetuning and DPO. A reward model is trained on human-annotated preference data, and the language model is finetuned on a mixture of human-curated and synthetic data.\n*   **Data Quality Control**: Extensive data cleaning, pruning, and quality control measures ensure high-quality training samples. Data is categorized and scored for quality using both reward model and Llama-based signals.\n*   **Multimodal Capabilities**: Separate encoders for images and speech are trained and integrated into the language model using adapters. This approach enables the model to handle image, video, and speech inputs effectively.\n*   **Float8 Quantization**: The Llama 3 models utilize Float8 (fp8) quantization, where both weights and inputs are quantized to fp8. This quantization is followed by multiplication with scaling factors, resulting in outputs in bf16 format. This approach reduces VRAM usage and speeds up inference, making the models more efficient for deployment.\n\n##### [Llama 3.2](https://huggingface.co/meta-llama)\n\n*   The Llama 3.2 heard of models were created from bigger Llama 3.1 models (8B and 70B) using pruning and distillation as described below.\n    \n*   **Pruning:**\n    *   Structured pruning was used in a single shot manner from the Llama 3.1 8B.\n    *   It involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network\n*   **Knowledge distillation:**\n    *   For the 1B and 3B in Llama 3.2, incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets.\n    *   It was used after pruning to recover performance.\n*   **Vision models:**\n    *   As the first Llama models to support vision tasks, Llama 3.2 11B and 90B required an entirely new model architecture that supports image reasoning.\n    *   To add image input support, a set of adapter weights were trained that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. The adapter was trained on text-image pairs to align the image representations with the language representations. During adapter training, the parameters of the image encoder were also updated, but intentionally did not update the language-model parameters. By doing that, all the text-only capabilities were kept intact, providing developers a drop-in replacement for Llama 3.1 models.\n*   **Post-training:**\n    *   Used similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model\n    *   Each round involved supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).\n    *   Scaled context length support to 128K tokens, while maintaining the same quality as the pre-trained model.\n    *   Filtered synthetic data was also used to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.\n\nThe Llama 3.2 heard of models were created from bigger Llama 3.1 models (8B and 70B) using pruning and distillation as described below.\n\n*   Structured pruning was used in a single shot manner from the Llama 3.1 8B.\n*   It involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network\n\n*   For the 1B and 3B in Llama 3.2, incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets.\n*   It was used after pruning to recover performance.\n\n*   As the first Llama models to support vision tasks, Llama 3.2 11B and 90B required an entirely new model architecture that supports image reasoning.\n*   To add image input support, a set of adapter weights were trained that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. The adapter was trained on text-image pairs to align the image representations with the language representations. During adapter training, the parameters of the image encoder were also updated, but intentionally did not update the language-model parameters. By doing that, all the text-only capabilities were kept intact, providing developers a drop-in replacement for Llama 3.1 models.\n\n*   Used similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model\n*   Each round involved supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).\n*   Scaled context length support to 128K tokens, while maintaining the same quality as the pre-trained model.\n*   Filtered synthetic data was also used to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.\n\n![](assets/LLM/Llama32.png)\n\n#### GPT\n\n##### [GPT-3.5 Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)\n\n*   OpenAI’s GPT-3.5 Turbo is based on the transformer architecture, with a context window of 16K tokens.\n*   While the exact parameters of GPT-3.5 Turbo haven’t been publicly detailed as specifically distinct from other GPT-3 models, it generally falls within the range of the larger GPT-3 models, which can have up to 175 billion parameters.\n\n##### [GPT-4](https://openai.com/research/gpt-4)\n\n*   Read our GPT-4 primer [here](../GPT-4).\n*   Per a [rumor](https://www.reddit.com/r/mlscaling/comments/14eowmw/gpt4_rumors_a_mixtureofexperts_w8_gpt3220bs/), GPT-4 might be an 8-way Mixture-of-Experts (MoE) model with 8 220B parameters (a total of 1.76T parameters).\n*   A Mixture of Experts (MoE) model essentially revolves around a router that directs questions to the appropriate expert. If GPT-4 does adopt the MoE approach, it would consist of eight specialist models each trained in a specific domain, like mathematics, history, storytelling, etc. When a question is posed, the router analyses it and seamlessly forwards it to the most suitable expert.\n*   The concept of MoE is quite prevalent (refer [Outrageously Large Neural Networks: the Sparsely-Gated Mixture-of-Experts Layer](../../../papers/#outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)), with Langchain’s high-level implementation of an [LLMRouterChain](https://python.langchain.com/docs/modules/chains/foundational/router), and notable low-level integrated examples like Google’s Switch Transformer (refer [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](../../../papers/#switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsity)).\n*   Per yet another [rumor](https://archive.is/2RQ8X#selection-449.1-1067.86), here are the specifics:\n    *   **Parameter count:** GPT-4 is more than 10x the size of GPT-3; with a total of ~1.8 trillion parameters across 120 layers.\n    *   **Architecture:** GPT-4 uses an MoE architecture; the main idea behind used an MoE model was to keep costs training/inference reasonable while ensuring great performance. In other words, it is not a dense transformer like, for instance, PaLM (or GPT-3). They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed per forward pass. There roughly ~55B shared parameters for attention.\n    *   **MoE routing:** While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.\n    *   **Inference:** Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model (vs. the MoE architecture that’s used).\n    *   **Dataset:** GPT-4 is trained on ~13T tokens. These are not unique tokens, but the total amount of tokens seen over all epochs. There are millions of instruction fine-tuning data samples from ScaleAI & internally (probably acquired through ChatGPT + their API before they changed the policy).\n    *   **Training epochs:** 2 epochs for text-based data and 4 for code-based data.\n    *   **Training paradigm:** For pre-training GPT-4 32K, they utilized an 8K context length. The 32K context version of GPT-4 was based on fine-tuning of the 8K after the pre-training. [Extending context is hard… but not impossible](https://kaiokendev.github.io/context) is a good reference on how to achieve this.\n    *   **Batch size:** The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens. For the real batch size:\\*\\* Divide this number by the context width to get the real batch size.\n    *   **Parallelism strategies:** To parallelize across all their A100s GPUs, they utilized 8-way tensor parallelism as that is the limit for NVLink. Beyond that, they used 15-way pipeline parallelism. Also apparently they used DeepSpeed ZeRo Stage 1 or block-level FSDP.\n    *   **Training cost**: OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU. Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from. If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million. Had H100s been used, pre-training could be done with ~8,192 H100s in ~55 days for $21.5 million at $2 per H100 hour.\n    *   **MoE tradeoffs**: There are multiple MoE tradeoffs taken; for example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation. This means some parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates. Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research. There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with. With such a large training run, OpenAI instead chose to be more conservative on the number of experts.\n    *   **GPT-4 inference cost**: GPT-4 costs 3x that of the 175B parameter DaVinci. This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved. An estimate of it’s costs is $0.0049 cents per 1K tokens for 128 A100s to inference GPT-4 8K context width and $0.0021 cents per 1K tokens for 128 H100s to inference GPT-4 8K context width. It should be noted that they assume decent high utilization and keep batch sizes large.\n    *   **Multi-Query Attention**: GPT-4 uses MQA instead of MHA (MQA is a classic choice at this point). Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32K context width GPT-4 definitely cannot run on 40GB A100s, and the 8K is capped on max batch size.\n    *   **Continuous batching**: OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.\n    *   **Vision multi-modal**: They have a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Google DeepMind’s [Flamingo](../../../papers/#flamingo-a-visual-language-model-for-few-shot-learning). This adds more parameters on top of the 1.8T text-only GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training. On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text. One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video. Some of the data they train on is joint data (rendered LaTeX/text), screenshots of web pages, YouTube videos: sampling frames, and run Whisper around it to get transcript.\n    *   **Speculative decoding**: OpenAI might be using speculative decoding on GPT-4’s inference. The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch. If the small model was right about its predictions (i.e., the larger model agrees), we can decode several tokens in a single batch. But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model. **The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.**\n        *   Per [Andrej Karpathy](https://twitter.com/karpathy/status/1697318534555336961), speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on KKK input tokens in a batch (for larger KKK than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n            *   At batch\\_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n            *   Let’s take a look:\n                *   A100: 1935 GB/s memory bandwidth, 1248 TOPS\n                *   MacBook M2: 100 GB/s, 7 TFLOPS\n            *   The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n            *   The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n            *   In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.\n        *   The reason we can’t naively use this fact to sample in chunks of KKK tokens at a time is that every NthNthN^{th} token depends on what token we sample at time at step N−1N−1N-1. There is a serial dependency, so the baseline implementation just goes one by one left to right.\n        *   Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of KKK tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).\n        *   The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.\n        *   In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.\n        *   More on this here: [Blockwise Parallel Decoding for Deep Autoregressive Models](https://arxiv.org/abs/1811.03115), [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318), and [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)\n    *   **Inference architecture**: The inference runs on a cluster of 128 GPUs. There are multiple of these clusters in multiple datacenters in different locations. It is done in 8-way tensor parallelism and 16-way pipeline parallelism. Each node of 8 GPUs has only ~130B parameters, or less than 30GB per GPU at FP16 and less than 15GB at FP8/int8. The model has 120 layers, so it fits in 15 different nodes. (Possibly the there are less layers on the first node since it needs to also compute the embeddings). According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by Chinchilla’s optimal. This goes to show that they are struggling to get high quality data.\n    *   **Why no Fully Sharded Data Parallel (FSDP)?** A possible reason for this could be that some of the hardware infra they secured is of an older generation. This is pretty common at local compute clusters as the organisation usually upgrade the infra in several “waves” to avoid a complete pause of operation. With such a high amount of pipeline parallelism it is very likely that they suffer from the “batch bubble”: slight idle time between batches.\n    *   **Dataset mixture**: They trained on 13T tokens. CommonCrawl & RefinedWeb are both 5T. Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of “unaccounted for” tokens: the “secret” data – parts of it probably came from Twitter, Reddit, and YouTube. Some speculations are: LibGen (4M+ books), Sci-Hub (80M+ papers), all of GitHub. Part of the missing dataset could also be custom dataset of college textbooks collected by hand for as much courses as possible. This is very easy to convert to text form and than use [Self-Instruct](../../../papers/#self-instruct-aligning-language-model-with-self-generated-instructions) to transform it into instruction form. This creates the “illusion” that GPT-4 “is smart” no matter who uses it: for computer scientists, it can help you with your questions about P!=NP; for a philosophy major, it can totally talk to you about epistemology. There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. There are some books it knows so well that it had seen them for sure. Moreover, it even knows the unique ids of project Euler problems.\n\n*   **Parameter count:** GPT-4 is more than 10x the size of GPT-3; with a total of ~1.8 trillion parameters across 120 layers.\n*   **Architecture:** GPT-4 uses an MoE architecture; the main idea behind used an MoE model was to keep costs training/inference reasonable while ensuring great performance. In other words, it is not a dense transformer like, for instance, PaLM (or GPT-3). They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed per forward pass. There roughly ~55B shared parameters for attention.\n*   **MoE routing:** While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.\n*   **Inference:** Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model (vs. the MoE architecture that’s used).\n*   **Dataset:** GPT-4 is trained on ~13T tokens. These are not unique tokens, but the total amount of tokens seen over all epochs. There are millions of instruction fine-tuning data samples from ScaleAI & internally (probably acquired through ChatGPT + their API before they changed the policy).\n*   **Training epochs:** 2 epochs for text-based data and 4 for code-based data.\n*   **Training paradigm:** For pre-training GPT-4 32K, they utilized an 8K context length. The 32K context version of GPT-4 was based on fine-tuning of the 8K after the pre-training. [Extending context is hard… but not impossible](https://kaiokendev.github.io/context) is a good reference on how to achieve this.\n*   **Batch size:** The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens. For the real batch size:\\*\\* Divide this number by the context width to get the real batch size.\n*   **Parallelism strategies:** To parallelize across all their A100s GPUs, they utilized 8-way tensor parallelism as that is the limit for NVLink. Beyond that, they used 15-way pipeline parallelism. Also apparently they used DeepSpeed ZeRo Stage 1 or block-level FSDP.\n*   **Training cost**: OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU. Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from. If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million. Had H100s been used, pre-training could be done with ~8,192 H100s in ~55 days for $21.5 million at $2 per H100 hour.\n*   **MoE tradeoffs**: There are multiple MoE tradeoffs taken; for example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation. This means some parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates. Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research. There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with. With such a large training run, OpenAI instead chose to be more conservative on the number of experts.\n*   **GPT-4 inference cost**: GPT-4 costs 3x that of the 175B parameter DaVinci. This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved. An estimate of it’s costs is $0.0049 cents per 1K tokens for 128 A100s to inference GPT-4 8K context width and $0.0021 cents per 1K tokens for 128 H100s to inference GPT-4 8K context width. It should be noted that they assume decent high utilization and keep batch sizes large.\n*   **Multi-Query Attention**: GPT-4 uses MQA instead of MHA (MQA is a classic choice at this point). Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32K context width GPT-4 definitely cannot run on 40GB A100s, and the 8K is capped on max batch size.\n*   **Continuous batching**: OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.\n*   **Vision multi-modal**: They have a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Google DeepMind’s [Flamingo](../../../papers/#flamingo-a-visual-language-model-for-few-shot-learning). This adds more parameters on top of the 1.8T text-only GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training. On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text. One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video. Some of the data they train on is joint data (rendered LaTeX/text), screenshots of web pages, YouTube videos: sampling frames, and run Whisper around it to get transcript.\n*   **Speculative decoding**: OpenAI might be using speculative decoding on GPT-4’s inference. The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch. If the small model was right about its predictions (i.e., the larger model agrees), we can decode several tokens in a single batch. But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model. **The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.**\n    *   Per [Andrej Karpathy](https://twitter.com/karpathy/status/1697318534555336961), speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on KKK input tokens in a batch (for larger KKK than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n        *   At batch\\_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n        *   Let’s take a look:\n            *   A100: 1935 GB/s memory bandwidth, 1248 TOPS\n            *   MacBook M2: 100 GB/s, 7 TFLOPS\n        *   The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n        *   The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n        *   In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.\n    *   The reason we can’t naively use this fact to sample in chunks of KKK tokens at a time is that every NthNthN^{th} token depends on what token we sample at time at step N−1N−1N-1. There is a serial dependency, so the baseline implementation just goes one by one left to right.\n    *   Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of KKK tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).\n    *   The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.\n    *   In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.\n    *   More on this here: [Blockwise Parallel Decoding for Deep Autoregressive Models](https://arxiv.org/abs/1811.03115), [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318), and [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)\n*   **Inference architecture**: The inference runs on a cluster of 128 GPUs. There are multiple of these clusters in multiple datacenters in different locations. It is done in 8-way tensor parallelism and 16-way pipeline parallelism. Each node of 8 GPUs has only ~130B parameters, or less than 30GB per GPU at FP16 and less than 15GB at FP8/int8. The model has 120 layers, so it fits in 15 different nodes. (Possibly the there are less layers on the first node since it needs to also compute the embeddings). According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by Chinchilla’s optimal. This goes to show that they are struggling to get high quality data.\n*   **Why no Fully Sharded Data Parallel (FSDP)?** A possible reason for this could be that some of the hardware infra they secured is of an older generation. This is pretty common at local compute clusters as the organisation usually upgrade the infra in several “waves” to avoid a complete pause of operation. With such a high amount of pipeline parallelism it is very likely that they suffer from the “batch bubble”: slight idle time between batches.\n*   **Dataset mixture**: They trained on 13T tokens. CommonCrawl & RefinedWeb are both 5T. Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of “unaccounted for” tokens: the “secret” data – parts of it probably came from Twitter, Reddit, and YouTube. Some speculations are: LibGen (4M+ books), Sci-Hub (80M+ papers), all of GitHub. Part of the missing dataset could also be custom dataset of college textbooks collected by hand for as much courses as possible. This is very easy to convert to text form and than use [Self-Instruct](../../../papers/#self-instruct-aligning-language-model-with-self-generated-instructions) to transform it into instruction form. This creates the “illusion” that GPT-4 “is smart” no matter who uses it: for computer scientists, it can help you with your questions about P!=NP; for a philosophy major, it can totally talk to you about epistemology. There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. There are some books it knows so well that it had seen them for sure. Moreover, it even knows the unique ids of project Euler problems.\n\n*   Per [Andrej Karpathy](https://twitter.com/karpathy/status/1697318534555336961), speculative sampling/decoding/execution for LLMs is an excellent inference-time optimization. It hinges on the following unintuitive observation: forwarding an LLM on a single input token takes about as much time as forwarding an LLM on KKK input tokens in a batch (for larger KKK than what might be obvious). This unintuitive fact is because sampling is heavily memory bound: most of the “work” is not doing compute, it is reading in the weights of the transformer from VRAM into on-chip cache for processing. So if you’re going to do all that work of reading in all those weights, you might as well apply them to a whole batch of input vectors.\n    *   At batch\\_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n    *   Let’s take a look:\n        *   A100: 1935 GB/s memory bandwidth, 1248 TOPS\n        *   MacBook M2: 100 GB/s, 7 TFLOPS\n    *   The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n    *   The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n    *   In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.\n*   The reason we can’t naively use this fact to sample in chunks of KKK tokens at a time is that every NthNthN^{th} token depends on what token we sample at time at step N−1N−1N-1. There is a serial dependency, so the baseline implementation just goes one by one left to right.\n*   Now the clever idea is to use a small and cheap draft model to first generate a candidate sequence of KKK tokens – a “draft”. Then we feed all of these together through the big model in a batch. This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).\n*   The reason this works in practice is that most of the time the draft tokens get accepted, because they are easy, so even a much smaller draft model gets them. As these easy tokens get accepted, we skip through those parts in leaps. The hard tokens where the big model disagrees “fall back” to original speed, but actually a bit slower because of all the extra work.\n*   In summary, this one weird trick works because LLMs are memory bound at inference time, in the “batch size 1” setting of sampling a single sequence of interest, that a large fraction of “local LLM” use cases fall into. And because most tokens are “easy”.\n*   More on this here: [Blockwise Parallel Decoding for Deep Autoregressive Models](https://arxiv.org/abs/1811.03115), [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318), and [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)\n\n*   At batch\\_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n*   Let’s take a look:\n    *   A100: 1935 GB/s memory bandwidth, 1248 TOPS\n    *   MacBook M2: 100 GB/s, 7 TFLOPS\n*   The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n*   The situation becomes a lot more different when you inference at a very high batch size (e.g. ~160+), such as when you’re hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren’t forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n*   In summary, why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single “stream” of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops.\n\n*   A100: 1935 GB/s memory bandwidth, 1248 TOPS\n*   MacBook M2: 100 GB/s, 7 TFLOPS\n\n##### [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)\n\n*   GPT-4 Turbo utilizes the transformer architecture, with a context window of 128K tokens. It features improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.\n\n##### [GPT-4o](https://openai.com/index/hello-gpt-4o/)\n\n*   GPT-4o is a multimodal model just like GPT-4, which accepts text, audio, image, and video inputs, and generating text, audio, and image outputs. It boasts response times similar to human conversation, matches GPT-4 Turbo’s performance in text and coding, excels in non-English languages, and is significantly faster and cheaper.\n*   Unlike previous models, GPT-4o processes inputs and outputs through a single unified model, improving vision and audio understanding.\n*   [System Card](https://arxiv.org/abs/2410.21276)\n\n##### [GPT-4o Mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)\n\n*   OpenAI’s GPT-4o mini is a highly cost-efficient small model designed to broaden AI application access by offering intelligence at a significantly reduced cost. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it surpasses GPT-3.5 Turbo and other small models in performance on several academic benchmarks, including reasoning, math, and coding.\n*   Supporting text and vision, with future expansions to other media, it boasts a 128K token context window and superior non-English text handling. Enhanced safety measures, informed by expert evaluations and new techniques, ensure reliable and secure usage.\n\n#### [Bard API](https://github.com/dsdanielpark/Bard-API)\n\n*   Bard is a conversational generative artificial intelligence chatbot developed by Google, based initially on the LaMDA family of LLMs(Large Language Models) and later the PaLM LLM.\n*   Bard API is a python package that returns response of Google Bard through value of cookie.\n\n#### [Claude](https://claude.ai/)\n\n*   Claude 2 is Anthropic’s second-gen AI chatbot that’s cheaper, stronger, faster, can handle multiple PDFs, and supports longer conversations. It’s basically Anthropic’s answer to OpenAI’s GPT-4:\n    *   Claude 2 has 100K context. GPT-4 has 32K. So 3x more context.\n    *   Claude 2 is 4-5x cheaper than GPT-4-32k.\n    *   Claude 2’s knowledge cutoff is early 2023, while GPT-4 is late 2021. So more fresher knowledge.\n*   You can easily upload large files (say, an 80-page 2022 Apple annual report) and ask for a summary, key takeaways, provide financial projections (not 100% there yet), and more.\n*   Furthermore, you can import several documents into Claude 2 and perform conceptual blending by asking about the relationship between the concepts found in each document.\n\n*   Claude 2 has 100K context. GPT-4 has 32K. So 3x more context.\n*   Claude 2 is 4-5x cheaper than GPT-4-32k.\n*   Claude 2’s knowledge cutoff is early 2023, while GPT-4 is late 2021. So more fresher knowledge.\n\n##### [Claude 2.1](https://www.anthropic.com/index/claude-2-1)\n\n*   Claude 2.1 boasts significant enhancements including a 200K token context window, reduced hallucination rates, system prompts, and a beta feature for tool use. Concurrently, pricing updates aim to enhance cost efficiency for customers. Unique features include:\n    *   **200K Token Context Window**: Allows processing of up to 200,000 tokens, equivalent to about 150,000 words or over 500 pages, suitable for handling extensive documents like full codebases, financial statements, or lengthy literary works.\n    *   **Reduced Hallucination Rates**: Achieves a 50% reduction in false statements, enhancing reliability and honesty in outputs, essential for enterprise AI applications.\n    *   **Improved Comprehension and Summarization**: Demonstrates a 30% reduction in incorrect answers and significantly lower rates of mistakenly supporting false claims in complex documents like legal texts and financial reports.\n    *   **Tool Use (Beta Feature)**: Integrates with users’ existing processes, products, and APIs, enabling functionalities like using calculators, translating requests into API calls, searching databases or the web, and interacting with software or product datasets.\n    *   **Enhanced Developer Experience**: Includes a new Workbench product for easier testing and iteration of prompts, with capabilities to create, navigate, and save multiple prompts for different projects and generate code snippets for SDK integration.\n    *   **System Prompts**: Allows custom instructions for Claude to adopt specific personalities, roles, or structured responses, enhancing performance and user experience.\n    *   **Availability**: Offered for both free and Pro tiers on claude.ai, with exclusive features like the 200K token context window reserved for Pro users.\n\n*   **200K Token Context Window**: Allows processing of up to 200,000 tokens, equivalent to about 150,000 words or over 500 pages, suitable for handling extensive documents like full codebases, financial statements, or lengthy literary works.\n*   **Reduced Hallucination Rates**: Achieves a 50% reduction in false statements, enhancing reliability and honesty in outputs, essential for enterprise AI applications.\n*   **Improved Comprehension and Summarization**: Demonstrates a 30% reduction in incorrect answers and significantly lower rates of mistakenly supporting false claims in complex documents like legal texts and financial reports.\n*   **Tool Use (Beta Feature)**: Integrates with users’ existing processes, products, and APIs, enabling functionalities like using calculators, translating requests into API calls, searching databases or the web, and interacting with software or product datasets.\n*   **Enhanced Developer Experience**: Includes a new Workbench product for easier testing and iteration of prompts, with capabilities to create, navigate, and save multiple prompts for different projects and generate code snippets for SDK integration.\n*   **System Prompts**: Allows custom instructions for Claude to adopt specific personalities, roles, or structured responses, enhancing performance and user experience.\n*   **Availability**: Offered for both free and Pro tiers on claude.ai, with exclusive features like the 200K token context window reserved for Pro users.\n\n##### [Claude 3](https://www.anthropic.com/news/claude-3-family)\n\n*   Anthropic announced Claude 3 family of LLMs that are competitive compared to OpenAI’s GPT4.\n*   Based on the comparison table below, Claude 3 Haiku is a GPT3.5 alternative as it performs better and it is cheaper (I: $0.25/M tok; O:$1.25/M tok) in comparison to GPT3.5 (I:$0.50/M tok; O:$1.50/M tok). On the other hand, while Claude 3 Opus is much better than GPT4 in performance, GPT4 is cheaper (I: $30/M tok; O: $60/M tok) for long output generation tasks while Opus is better for long context tasks. (I: $15/M tok; O: $75/M tok).\n\n![](/primers/ai/assets/LLM/claude3.webp)\n\n##### [Claude 3.5](https://www.anthropic.com/news/claude-3-5-sonnet)\n\n*   At launch, Claude 3.5 Sonnet set new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.\n*   Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.\n*   [Model card](https://cdn.sanity.io/files/4zrzovbb/website/fed9cc193a14b84131812372d8d5857f8f304c52.pdf)\n\n![](/primers/ai/assets/LLM/Claude-3.5.webp)\n\n#### [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n\n*   Stanford’s Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On their preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce.\n\n#### [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n\n*   Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90% quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90% of cases. The cost of training Vicuna-13B was around $300.\n*   Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.\n*   Their training recipe builds on top of Stanford’s [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.\n    *   **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).\n    *   **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.\n    *   **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\n\n*   **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).\n*   **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.\n*   **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\n\n#### [StableVicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)\n\n*   StableVicuna is the first large-scale open source chatbot trained via reinforced learning from human feedback (RLHF). StableVicuna is a further instruction fine tuned and RLHF trained version of Vicuna v0 13b, which is an instruction fine tuned [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) 13B model.\n\n#### [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\n\n*   Dolly 2.0 is the world’s first open source, instruction-tuned LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.\n*   Dolly 2.0 is a 12B parameter language model based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees.\n*   They have also released the `databricks-dolly-15k` dataset:\n    *   `databricks-dolly-15k` contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning large language models. Under the licensing terms for `databricks-dolly-15k` ([Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/)), anyone can use, modify, or extend this dataset for any purpose, including commercial applications.\n    *   This dataset is the first open source, human-generated instruction dataset specifically designed to make large language models exhibit the magical interactivity of ChatGPT. `databricks-dolly-15k` was authored by more than 5,000 Databricks employees during March and April of 2023. These training records are natural, expressive and designed to represent a wide range of the behaviors, from brainstorming and content generation to information extraction and summarization.\n\n*   `databricks-dolly-15k` contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning large language models. Under the licensing terms for `databricks-dolly-15k` ([Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/)), anyone can use, modify, or extend this dataset for any purpose, including commercial applications.\n*   This dataset is the first open source, human-generated instruction dataset specifically designed to make large language models exhibit the magical interactivity of ChatGPT. `databricks-dolly-15k` was authored by more than 5,000 Databricks employees during March and April of 2023. These training records are natural, expressive and designed to represent a wide range of the behaviors, from brainstorming and content generation to information extraction and summarization.\n\n#### [StableLM](https://github.com/Stability-AI/StableLM)\n\n*   Stability AI’s StableLM series of language models. StableLM comes in two variants: StableVicuna and StableLM-Alpha.\n*   StableVicuna is an RLHF fine-tune of [Vicuna-13B v0](https://huggingface.co/lmsys/vicuna-13b-delta-v0), which itself is a fine-tune of [LLaMA-13B](https://github.com/facebookresearch/llama). It is our attempt at creating an open-source RLHF LLM Chatbot.\n*   StableLM-Alpha models are trained on the new dataset that build on [The Pile](https://pile.eleuther.ai/), which contains 1.5 trillion tokens, roughly 3x the size of The Pile. These models will be trained on up to 1.5 trillion tokens. The context length for these models is 4096 tokens. The models range from 3B to 175B parameters. As a proof-of-concept, we also fine-tuned the model with [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)’s procedure using a combination of five recent datasets for conversational agents: Stanford’s [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), Nomic-AI’s [gpt4all](https://github.com/nomic-ai/gpt4all), RyokoAI’s [ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K) datasets, Databricks labs’ [Dolly](https://github.com/databrickslabs/dolly), and Anthropic’s [HH](https://github.com/anthropics/hh-rlhf). We will be releasing these models as StableLM-Tuned-Alpha.\n*   [Code](https://github.com/Stability-AI/StableLM)\n\n#### [OpenLLaMA](https://github.com/openlm-research/open_llama)\n\n*   OpenLLaMA is a permissively licensed open source reproduction of Meta AI’s [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) large language model. They have released a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. They provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models.\n\n#### [MPT](https://huggingface.co/mosaicml/mpt-7b)\n\n*   MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by [MosaicML](https://www.mosaicml.com/).\n*   MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\n*   These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing positional embeddings with Attention with Linear Biases (ALiBi). Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA’s FasterTransformer.\n*   MPT-7B is licensed for the possibility of commercial use (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n*   MPT-7B is trained on a large amount of data (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n*   MPT-7B is prepared to handle extremely long inputs, due to [ALiBi](https://arxiv.org/abs/2108.12409) (they finetuned [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter) on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\n*   MPT-7B is capable of fast training and inference (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n*   MPT-7B is equipped with highly efficient open-source training code via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry).\n\n#### [Falcon](https://huggingface.co/tiiuae)\n\n*   Falcon is a causal decoder-only model built by [TII](https://www.tii.ae/) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the [TII Falcon LLM License](https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt).\n*   It was the best open-source model available at the time of release. Falcon-40B outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n*   It features an architecture optimized for inference, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery attention ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)).\n*   Unlike LLaMA, it is made available under a license allowing commercial use, see the details of the [TII Falcon LLM License](https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt).\n*   There are two variants: [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), and [Falcon-180B](https://huggingface.co/tiiuae/falcon-180b).\n\n##### [Falcon 2](https://falconllm.tii.ae/falcon-2.html)\n\n*   Falcon 2 includes two powerful versions: Falcon 2 11B, an efficient LLM with 11 billion parameters, and Falcon 2 11B VLM, which adds vision-to-language (VLM) capabilities.\n*   Falcon 2 11B VLM is TII’s first multimodal model, enabling seamless image-to-text conversion, a groundbreaking feature in the AI market.\n*   Both models are open-source under the TII Falcon License 2.0, encouraging responsible and unrestricted AI usage globally.\n*   Falcon 2 11B outperforms Meta’s Llama 3 (8B) and closely matches Google’s Gemma 7B in performance evaluations.\n*   The models support multiple languages, including English, French, Spanish, and more, making them versatile across diverse applications.\n*   Falcon 2 models are scalable, capable of running efficiently on a single GPU, and applicable across various industries, from healthcare to education, with potential uses in document management, visual impairment support, and more.\n*   [Blog](https://falconllm.tii.ae/falcon-2.html)\n\n##### [Falcon Mamba](https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html)\n\n*   Falcon Mamba 7B release is a groundbreaking leap in Open-Source AI, setting new standards and redefining the capabilities of language models.\n*   Falcon Mamba 7B outperforms leading Transformer-based LLMs of its size, Meta’s Llama 3.1 8B and Mistral’s 7B, on the newly introduced benchmarks by Hugging Face and it is officially the most capable open-source SSLM out there.\n*   **Background**:\n    *   Transformers have worked great leveraging “Self-Attention” to determine the relationships and dependencies that give language its meaning. But the elegance of self-attention comes at a cost, it requires extensive computations, particularly as the sequence length increases. Hence, there has been a growing interest in models that use a latent state that does not depend on the sequence length, referred to as “State Space Models” SSMs. This architecture can handle long sequences achieving linear complexity scaling with sequence length, removing the quadratic bottleneck in the Attention mechanism.\n    *   Falcon Mamba is an advanced State Space Language Model (SSLM), outperforming both Transformer and SSM-based LLMs of its size, as independently verified by Hugging Face. Falcon Mamba has a low memory cost and can generate long sequences without any increase in memory.\n    *   Falcon Mamba was trained with ~6 Trillion tokens, mostly from Refined-Web\n    *   Falcon Mamba-7B was trained on 256 H100 80GB GPUs, using a 3D parallelism strategy combined with ZeRO\n    *   Mamba was trained leveraging a multi-stage training strategy to increase context-length from 2k to 8k\n    *   Model training took roughly two months\n*   [Blog](https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html)\n\n*   Transformers have worked great leveraging “Self-Attention” to determine the relationships and dependencies that give language its meaning. But the elegance of self-attention comes at a cost, it requires extensive computations, particularly as the sequence length increases. Hence, there has been a growing interest in models that use a latent state that does not depend on the sequence length, referred to as “State Space Models” SSMs. This architecture can handle long sequences achieving linear complexity scaling with sequence length, removing the quadratic bottleneck in the Attention mechanism.\n*   Falcon Mamba is an advanced State Space Language Model (SSLM), outperforming both Transformer and SSM-based LLMs of its size, as independently verified by Hugging Face. Falcon Mamba has a low memory cost and can generate long sequences without any increase in memory.\n*   Falcon Mamba was trained with ~6 Trillion tokens, mostly from Refined-Web\n*   Falcon Mamba-7B was trained on 256 H100 80GB GPUs, using a 3D parallelism strategy combined with ZeRO\n*   Mamba was trained leveraging a multi-stage training strategy to increase context-length from 2k to 8k\n*   Model training took roughly two months\n\n##### [The RefinedWeb Dataset for Falcon LLM](https://arxiv.org/abs/2306.01116)\n\n*   Proposed in The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\\](https://arxiv.org/abs/2306.01116).\n*   Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon.\n*   This paper by Penedo et al. from the Falcon LLM team shows that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.\n*   The following figure from the paper shows subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall. Rates measured in % of documents in the document preparation phase, then in tokens.\n\n![](/primers/ai/assets/LLM/RefinedWeb.jpg)\n\n*   The following figure from the paper shows models trained on REFINEDWEB alone outperform models trained on curated corpora. Zero-shot performance on our `main-agg` task aggregate. At equivalent compute budgets, our models significantly outperform publicly available models trained on The Pile, and match the performance of the GPT-3 models when tested within our evaluation setup.\n\n![](/primers/ai/assets/LLM/RefinedWeb2.jpg)\n\n*   Related: Falcon LLM details –\n    1.  Data is key! As noted in the abstract and benchmarks, Falcon performs very well due to the data refinement techniques used. A key theme through the paper\n    2.  Falcon follows a very close scaling law to GPT-3, with the authors of the paper testing Babbage and Currie (two smaller variants). They measure it using the Eleuther AI evaluation harness.\n    3.  Data Filtering and deduplication is key. Starting with Common Crawl, they apply a 7 part pipeline including URL deduplication, website specific actions, document deduplication and line by line deduplication.\n    4.  The final dataset is only 19th19th\\\\frac{1}{9}^{th} of the Common Crawl original!\n    5.  The team conducts several tests on 1B and 3B param models to validate their data cleaning hypothesis. C4 is still an excellent dataset, but Refined web outperforms The Pile and Oscar, which have duplications.\n    6.  After blocking NSFW urls, the dataset/model toxicity matches that of The Pile, indicating that more work can be done to further decrease it. Minimal work was done on investigating social and other biases.\n    7.  The team open sourced a 600B subset from their 5000B Token dataset.\n\n1.  Data is key! As noted in the abstract and benchmarks, Falcon performs very well due to the data refinement techniques used. A key theme through the paper\n2.  Falcon follows a very close scaling law to GPT-3, with the authors of the paper testing Babbage and Currie (two smaller variants). They measure it using the Eleuther AI evaluation harness.\n3.  Data Filtering and deduplication is key. Starting with Common Crawl, they apply a 7 part pipeline including URL deduplication, website specific actions, document deduplication and line by line deduplication.\n4.  The final dataset is only 19th19th\\\\frac{1}{9}^{th} of the Common Crawl original!\n5.  The team conducts several tests on 1B and 3B param models to validate their data cleaning hypothesis. C4 is still an excellent dataset, but Refined web outperforms The Pile and Oscar, which have duplications.\n6.  After blocking NSFW urls, the dataset/model toxicity matches that of The Pile, indicating that more work can be done to further decrease it. Minimal work was done on investigating social and other biases.\n7.  The team open sourced a 600B subset from their 5000B Token dataset.\n\n![](/primers/ai/assets/LLM/falcon.jpeg)\n\n*   [Project page](https://falconllm.tii.ae/); [download](https://huggingface.co/datasets/tiiuae/falcon-refinedweb).\n\n#### [RedPajama](https://www.together.xyz/blog/redpajama-7b)\n\n*   The RedPajama project aims to create a set of leading open-source models and to rigorously understand the ingredients that yield good performance. In April 2023, they released the RedPajama base dataset based on the LLaMA paper, which has worked to kindle rapid innovation in open-source AI.\n*   The 5 terabyte dataset has been downloaded thousands of times and used to train over 100 models!\n*   They’ve trained 3B and 7B models on the Summit supercomputer, in collaboration with AAI CERC lab at Université de Montréal, EleutherAI & LAION for compute time on Summit within the INCITE program award “Scalable Foundation Models for Transferable Generalist AI”.\n*   They have released the v1 versions of the RedPajama-INCITE family of models, including instruct-tuned and chat versions under the Apache 2.0 license.\n    *   **RedPajama-INCITE-7B-Instruct** is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.\n    *   **RedPajama-INCITE-7B-Chat** is available in OpenChatKit, including a training script for easily fine-tuning the model and is available to try now! The chat model is built on fully open-source data and does not use distilled data from closed models like OpenAI’s – ensuring it is clean for use in open or commercial applications.\n    *   **RedPajama-INCITE-7B-Base** was trained on 1T tokens of the RedPajama-1T dataset and releases with 10 checkpoints from training and open data generation scripts allowing full reproducibility of the model. This model is 4 points behind LLaMA-7B, and 1.3 points behind Falcon-7B/MPT-7B on HELM.\n*   [Project page](https://www.together.xyz/blog/redpajama-7b).\n\n*   **RedPajama-INCITE-7B-Instruct** is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.\n*   **RedPajama-INCITE-7B-Chat** is available in OpenChatKit, including a training script for easily fine-tuning the model and is available to try now! The chat model is built on fully open-source data and does not use distilled data from closed models like OpenAI’s – ensuring it is clean for use in open or commercial applications.\n*   **RedPajama-INCITE-7B-Base** was trained on 1T tokens of the RedPajama-1T dataset and releases with 10 checkpoints from training and open data generation scripts allowing full reproducibility of the model. This model is 4 points behind LLaMA-7B, and 1.3 points behind Falcon-7B/MPT-7B on HELM.\n\n#### [Pythia](https://arxiv.org/abs/2304.01373)\n\n*   Proposed in [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373) by Biderman et al. from EleutherAI, Booz Allen Hamilton, Yale University, IIIT Delhi, Stability AI, Datasaur.ai, and University of Amsterdam. This paper introduces Pythia, a suite of 16 LLMs, ranging from 70M to 12B parameters, all trained on public data in the same order, aimed at understanding the development and evolution of LLMs across training and scaling.\n*   The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two models: one trained on the Pile, and one trained on the Pile after the dataset has been globally deduplicated. All 8 model sizes are trained on the exact same data, in the exact same order.\n*   Pythia allows public access to 154 checkpoints for each model, with tools to download and reconstruct their exact training data, offering insights into memorization, term frequency effects on few-shot performance, and reducing gender bias.\n*   The Pythia model suite was deliberately designed to promote scientific research on large language models, especially interpretability research. Despite not centering downstream performance as a design goal, they find the models [match or exceed](https://huggingface.co/EleutherAI/pythia-1.4b#evaluations) the performance of similar and same-sized models, such as those in the OPT and GPT-Neo suites.\n*   The following table from the paper shows commonly used model suites and how they rate according to theirmerg requirements.\n\n![](../../../images/papers/Pythia.jpg)\n\n*   The suite’s consistent setup across models is used to analyze gender bias mitigation by modifying training data’s gendered terms, demonstrating reduced bias measures in larger models.\n*   Another focus is memorization dynamics, where memorization is modeled as a Poisson point process, indicating that memorization occurrences are uniformly distributed throughout training, contrary to the theory that later training data is memorized more.\n*   The study also explores the impact of term frequency in pretraining data on model performance, finding a correlation between term frequency and task accuracy in larger models, an emergent property not observed in smaller models.\n*   The paper, presented at the International Conference on Machine Learning (ICML) 2023, emphasizes the utility of Pythia for detailed analysis and research on LLM behaviors, offering a new perspective on how pretraining data affects model development.\n*   [Hugging Face](https://huggingface.co/EleutherAI/pythia-1.4b).\n\n#### [Orca](https://arxiv.org/pdf/2306.02707)\n\n*   Orca 13B, a small yet mighty AI model developed by Microsoft that’s making waves in the AI community. Despite its size, Orca 13B is proving that it can stand toe-to-toe with the giants, demonstrating capabilities that rival even the larger foundation models (LFMs) like ChatGPT and GPT-4.\n*   Orca 13B’s progressive learning approach is a cornerstone of its success. By learning from rich signals from GPT-4, including explanation traces, step-by-step thought processes, and other complex instructions, Orca is able to develop a deeper understanding of the reasoning process. This is a significant departure from traditional AI models, which often focus on imitating the style of LFMs but fail to capture their reasoning process.\n*   The use of explanation traces, for instance, allows Orca to understand the underlying logic behind the responses generated by GPT-4. This not only enhances Orca’s ability to generate accurate responses, but also enables it to understand the context and nuances of different scenarios, thereby improving its overall performance.\n*   Furthermore, the role of ChatGPT as a teacher assistant is crucial in providing a supportive learning environment for Orca. By providing guidance and feedback, ChatGPT helps Orca refine its learning process and improve its understanding of complex instructions. This teacher-student dynamic is a key factor in Orca’s ability to imitate the reasoning process of LFMs.\n*   Orca’s performance in various benchmarks is a testament to its capabilities. In complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and AGIEval, Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% and 42% respectively. This is a significant achievement, considering that these benchmarks are designed to test the model’s ability to reason and make decisions in complex scenarios.\n*   One of the most remarkable aspects of Orca is its size. Despite being a smaller AI model compared to giants like ChatGPT, Orca manages to perform at the same level. This is a significant breakthrough in technology as it demonstrates that powerful AI models can be built by smaller teams, making AI development more accessible.\n*   The size of Orca also has implications for its efficiency and scalability. Being a smaller model, Orca requires less computational resources to train and operate, making it a more sustainable and cost-effective solution for AI development. Furthermore, its smaller size makes it easier to scale and adapt to different applications, thereby increasing its versatility and utility.\n\n#### [XGen](https://blog.salesforceairesearch.com/xgen/)\n\n*   A series of 7B LLMs named XGen-7B from Salesforce with standard dense attention on up to 8K sequence length for up to 1.5T tokens. We also fine tune the models on public-domain instructional data. The main take-aways are:\n    *   On standard NLP benchmarks, XGen achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size.\n    *   Our targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models.\n    *   XGen-7B archives equally strong results both in text (e.g., MMLU, QA) and code (HumanEval) tasks.\n    *   Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.\n*   [Project page](https://blog.salesforceairesearch.com/xgen/); [Code](https://github.com/salesforce/xGen?ref=blog.salesforceairesearch.com); Model [checkpoint](https://huggingface.co/Salesforce/xgen-7b-8k-base?ref=blog.salesforceairesearch.com).\n\n*   On standard NLP benchmarks, XGen achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size.\n*   Our targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models.\n*   XGen-7B archives equally strong results both in text (e.g., MMLU, QA) and code (HumanEval) tasks.\n*   Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.\n\n#### [OpenLLMs](https://github.com/imoneoi/openchat)\n\n*   OpenLLMs is a series of open-source language models fine-tuned on a small, yet diverse and high-quality dataset of multi-round conversations. Specifically, we utilize only ~6K GPT-4 conversations directly filtered from the ~90K ShareGPT conversations. Despite the small size of the dataset, OpenLLMs has demonstrated remarkable performance.\n*   **Generic Models:**\n    *   [OpenChat](https://huggingface.co/openchat/openchat): based on LLaMA-13B with a context length of 2048.\n        *   Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.\n        *   Achieves 80.9% win-rate on AlpacaEval.\n    *   [OpenChat-8192](https://huggingface.co/openchat/openchat_8192): based on LLaMA-13B, with an extended context length of 8192.\n        *   Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.\n        *   Achieves 79.5% win-rate on AlpacaEval.\n*   **Code Models:**\n    *   [OpenCoderPlus](https://huggingface.co/openchat/opencoderplus): based on StarCoderPlus with a native context length of 8192.\n        *   Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.\n        *   Achieves a 78.7% win-rate on AlpacaEval.\n*   **Dataset:**\n    *   [openchat\\_sharegpt4\\_dataset](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset): ~6k cleaned and filtered GPT-4 data from ShareGPT.\n\n*   [OpenChat](https://huggingface.co/openchat/openchat): based on LLaMA-13B with a context length of 2048.\n    *   Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.\n    *   Achieves 80.9% win-rate on AlpacaEval.\n*   [OpenChat-8192](https://huggingface.co/openchat/openchat_8192): based on LLaMA-13B, with an extended context length of 8192.\n    *   Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.\n    *   Achieves 79.5% win-rate on AlpacaEval.\n\n*   Achieves 105.7% of ChatGPT score on the Vicuna GPT-4 evaluation.\n*   Achieves 80.9% win-rate on AlpacaEval.\n\n*   Achieves 106.6% of ChatGPT score on the Vicuna GPT-4 evaluation.\n*   Achieves 79.5% win-rate on AlpacaEval.\n\n*   [OpenCoderPlus](https://huggingface.co/openchat/opencoderplus): based on StarCoderPlus with a native context length of 8192.\n    *   Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.\n    *   Achieves a 78.7% win-rate on AlpacaEval.\n\n*   Achieves 102.5% of ChatGPT score on the Vicuna GPT-4 evaluation.\n*   Achieves a 78.7% win-rate on AlpacaEval.\n\n*   [openchat\\_sharegpt4\\_dataset](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset): ~6k cleaned and filtered GPT-4 data from ShareGPT.\n\n#### [LlongMA-2](https://huggingface.co/conceptofmind/LLongMA-2-13b)\n\n*   LlongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with @theemozilla of NousResearch and Kaiokendev1, by extending the context length of the Llama-2 7b model through fine-tuning. The models maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.\n*   The model has similar performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with `trust_remote_code` for <= 4.30.\n\n#### [Qwen](https://github.com/QwenLM/Qwen-7B/)\n\n*   Qwen is a Transformer-based LLM by Alibaba, opensourced as two variants: [Qwen-7B](https://huggingface.co/Qwen/Qwen-7B) and [Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat). Qwen-7B is pre-trained on self-constructed large-scale high-quality dataset of over 2.2 trillion tokens created using web texts, books, codes, etc.\n*   In comparison with similar-sized models, Qwen outperforms the competitors on a series of benchmarks that evaluates natural language understanding, mathematics, coding, etc. Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts. Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent!\n*   The features of the Qwen-7B series include:\n    *   **Trained with high-quality pretraining data.** Qwen-7B is pretrained on a self-constructed large-scale high-quality dataset of over 2.2 trillion tokens. The dataset includes plain texts and codes, and it covers a wide range of domains, including general domain data and professional domain data.\n    *   **Strong performance.** In comparison with the models of the similar model size, we outperform the competitors on a series of benchmark datasets, which evaluates natural language understanding, mathematics, coding, etc.\n    *   **Better support of languages.** Qwen’s tokenizer, based on a large vocabulary of over 150K tokens, is a more efficient one compared with other tokenizers. It is friendly to many languages, and it is helpful for users to further finetune Qwen-7B for the extension of understanding a certain language.\n    *   **Support of 8K Context Length.** Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.\n    *   **Support of Plugins.** Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent.\n\n*   **Trained with high-quality pretraining data.** Qwen-7B is pretrained on a self-constructed large-scale high-quality dataset of over 2.2 trillion tokens. The dataset includes plain texts and codes, and it covers a wide range of domains, including general domain data and professional domain data.\n*   **Strong performance.** In comparison with the models of the similar model size, we outperform the competitors on a series of benchmark datasets, which evaluates natural language understanding, mathematics, coding, etc.\n*   **Better support of languages.** Qwen’s tokenizer, based on a large vocabulary of over 150K tokens, is a more efficient one compared with other tokenizers. It is friendly to many languages, and it is helpful for users to further finetune Qwen-7B for the extension of understanding a certain language.\n*   **Support of 8K Context Length.** Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.\n*   **Support of Plugins.** Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent.\n\n##### [Qwen 2](https://qwenlm.github.io/blog/qwen2/)\n\n*   Slightly behind Llama-3-70b-Instruct (#11:1208) on LMSYS Chatbot Arena Leaderboard, Qwen2-72B-Instruct ranks at #15 with a score of 1187.\n*   **Model size:** 0.5B, 1.5B, 7B, 57B-14B (MoE), 72B as Base & Instruct versions\n*   **Context length:**\n    *   32k for 0.5B & 1.5B\n    *   64k for 57B MoE\n    *   128k for 7B and 72B\n*   **Formats available at launch:** AWQ, GPTQ & GGUFs\n*   **Compatibility:** Works with Ollama and vLLM\n*   **Supported languages:** Arabic, Bengali, Burmese, Cebuano, Czech, Dutch, English, French, German, Hebrew, Hindi, Indonesian, Italian, Japanese, Khmer, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Spanish, Tagalog, Thai, Turkish, Urdu, Vietnamese\n*   **Post-training improvements:** Utilizes SFT & DPO with merging for alignment.\n*   **Token vocabulary:** 151k tokens with ChatML format.\n*   **Performance:**\n    *   The 7B model outperforms Llama3 8B and GLM 4.\n    *   The 72B model surpasses Llama 3 70B and Mixtral.\n*   **Licensing:**\n    *   Qianwen license for the 72B model (free for usage below 100M monthly users)\n    *   Apache 2.0 for all other models\n*   [Hugging Face](https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f)\n\n*   32k for 0.5B & 1.5B\n*   64k for 57B MoE\n*   128k for 7B and 72B\n\n*   The 7B model outperforms Llama3 8B and GLM 4.\n*   The 72B model surpasses Llama 3 70B and Mixtral.\n\n*   Qianwen license for the 72B model (free for usage below 100M monthly users)\n*   Apache 2.0 for all other models\n\n##### [Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)\n\n*   Qwen2.5-Omni is a multimodal model from the Qwen series, capable of processing and generating text, audio, image, and video in real time. It introduces the Thinker-Talker architecture, where the Thinker handles input understanding and the Talker produces natural speech output. The model supports real-time video and voice chat with chunked input and streaming response capabilities.\n*   It uses a novel TMRoPE positional embedding for synchronized audio-video input and delivers high-quality, robust speech synthesis. Qwen2.5-Omni outperforms or matches specialized models like Qwen2-Audio and Qwen2.5-VL-7B in benchmarks. It excels in tasks across multiple modalities such as speech recognition, translation, image reasoning, and video understanding. Future updates will enhance voice command handling and multimodal collaboration.\n*   [Code](https://github.com/QwenLM/Qwen2.5-Omni); [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)\n\n#### [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)\n\n*   Mistral 7B is a 7.3B parameter model that:\n    *   Outperforms Llama 2 13B on all benchmarks.\n    *   Outperforms Llama 1 34B on many benchmarks.\n    *   Approaches CodeLlama 7B performance on code, while remaining good at English tasks.\n    *   Uses Grouped-query attention (GQA) for faster inference.\n    *   Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost.\n*   Mistral 7B uses a sliding window attention (SWA) mechanism ([Child et al.](https://arxiv.org/pdf/1904.10509.pdf), [Beltagy et al.](https://arxiv.org/pdf/2004.05150v2.pdf)), in which each layer attends to the previous 4,096 hidden states. The main improvement, and reason for which this was initially investigated, is a linear compute cost of `O(sliding_window.seq_len)`. In practice, changes made to [FlashAttention](https://github.com/Dao-AILab/flash-attention) and [xFormers](https://facebookresearch.github.io/xformers) yield a 2x speed improvement for sequence length of 16k with a window of 4k.\n*   Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token `i` at layer `k` attends to tokens `[i-sliding_window, i]` at layer `k-1`. These tokens attended to tokens `[i-2*sliding_window, i]`. Higher layers have access to information further in the past than what the attention patterns seems to entail.\n\n*   Outperforms Llama 2 13B on all benchmarks.\n*   Outperforms Llama 1 34B on many benchmarks.\n*   Approaches CodeLlama 7B performance on code, while remaining good at English tasks.\n*   Uses Grouped-query attention (GQA) for faster inference.\n*   Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost.\n\n![](../assets/LLM/attention_local.webp)\n\n*   The following image from the paper illustrates sliding window attention. The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most WWW tokens from the previous layer (here, WWW = 3). Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after kkk attention layers, information can move forward by up to k×Wk×Wk \\\\times W tokens.\n\n![](../../../images/papers/SlidingWindowAttention.jpg)\n\n*   Finally, a fixed attention span means we can limit our cache to a size of sliding\\_window tokens, using rotating buffers (read more in their [reference implementation](https://github.com/mistralai/mistral-src) repo). This saves half of the cache memory for inference on sequence length of 8192, without impacting model quality.\n*   Mistral 7B has been released under the Apache 2.0 license, it can be used without restrictions.\n*   [Download it](https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar) and use it anywhere (including locally) with their [reference implementation](https://github.com/mistralai/mistral-src).\n*   Deploy it on any cloud (AWS/GCP/Azure), using vLLM [inference server and skypilot](https://docs.mistral.ai/cloud-deployment/skypilot).\n*   Use it on [HuggingFace](https://huggingface.co/mistralai).\n*   Mistral 7B is easy to fine-tune on any task. As a demonstration, they’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.\n\n##### Mixtral 8x7B MoE\n\n*   Proposed in [Mixtral of Experts](https://arxiv.org/pdf/2401.04088) by Jiang et al. from Mistral.AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) language model that notably enhances the architecture of its predecessor, Mistral 7B, by incorporating 8 feedforward blocks per layer, termed “experts”. Unlike conventional models where each token is processed by the entire network, Mixtral uses a router network to select two experts per token per layer, allowing for a dynamic engagement of 47B parameters while maintaining 13B active parameters during inference.\n*   The architecture employs a novel gating mechanism where a softmax function is applied over the top-K logits of a linear layer to dynamically allocate computational resources per token, ensuring efficient processing without engaging all available parameters. This approach significantly reduces computational costs while maintaining or enhancing performance metrics compared to larger models like Llama 2 70B and GPT-3.5.\n*   Mixtral is pretrained using multilingual data and demonstrates superior performance in mathematics, code generation, and multilingual tasks. The model’s unique capability to handle large context sizes (up to 32k tokens) allows it to effectively manage long-range dependencies and complex queries, showcasing its robustness in retrieving contextually relevant information across varied sequence lengths and information densities.\n*   They perform a routing analysis (i.e., a study on expert specialization) which indicated showed no significant patterns in expert assignment across different topics such as biology, philosophy, or mathematics within The Pile validation dataset, suggesting a mostly syntactic rather than semantic specialization. However, a notable syntactic specialization was observed, where specific tokens in different domains consistently mapped to the same experts, indicating structured syntactic behavior that impacts the model’s training and inference efficiency. Proportion of tokens assigned to each expert on different domains from The Pile dataset for layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform sampling. Here, they consider experts that are either selected as a first or second choice by the router.\n\n![](../../../images/papers/Mixtral.jpg)\n\n*   The paper also discusses Mixtral 8x7B – Instruct, a variant fine-tuned to follow instructions more precisely, using techniques such as supervised fine-tuning and Direct Preference Optimization. This version surpasses other leading models on human evaluation benchmarks and exhibits reduced biases and a balanced sentiment profile across diverse datasets.\n*   Despite its expansive parameter space, Mixtral is optimized for efficiency, using only a fraction of its parameters per inference, which allows for faster computation speeds and lower operational costs. Both the base and instruct models are released under the Apache 2.0 license, promoting widespread use and adaptation in both academic and commercial settings.\n*   The model’s integration with the open-source vLLM project and its compatibility with Megablocks CUDA kernels for enhanced execution speeds illustrate a commitment to community-driven improvements and accessibility. The provided modifications ensure that Mixtral can be deployed efficiently across different computing environments, including cloud-based platforms via Skypilot.\n*   Extensive benchmarks reveal that Mixtral matches or outperforms Llama 2 70B across a spectrum of tasks, with particular strengths in code synthesis and mathematical reasoning. Detailed results highlight its efficacy in multilingual settings and its capability to handle extensive context lengths without performance degradation.\n*   The paper positions Mixtral 8x7B as a state-of-the-art model in the landscape of sparse mixture of experts architectures, providing substantial improvements over existing models in terms of scalability, efficiency, and performance, while maintaining lower computational and memory costs.\n\n###### Summary\n\n*   Mixtral 8x7B (56B params) from Mistral follows a Mixture of Experts (MoE) architecture, consisting of 8x 7B experts. With 8 experts and a router network that selects two of them at every layer for the inference of each token, it looks directly inspired from rumors about GPT-4’s architecture. This information can be derived from the model metadata:\n\n![](https://aman.ai/images/copy.png)\n\n`{\"dim\": 4096, \"n_layers\": 32, \"head_dim\": 128, \"hidden_dim\": 14336, \"n_heads\": 32, \"n_kv_heads\": 8, \"norm_eps\": 1e-05, \"vocab_size\": 32000, \"moe\": {\"num_experts_per_tok\": 2, \"num_experts\": 8}}`\n\n![](https://aman.ai/images/copy.png)\n\n`{\"dim\": 4096, \"n_layers\": 32, \"head_dim\": 128, \"hidden_dim\": 14336, \"n_heads\": 32, \"n_kv_heads\": 8, \"norm_eps\": 1e-05, \"vocab_size\": 32000, \"moe\": {\"num_experts_per_tok\": 2, \"num_experts\": 8}}`\n\n*   Based on leaks associated with GPT-4, we can speculate that GPT-4 is a MoE model with 8 experts, each with 111B parameters of their own and 55B shared attention parameters (166B parameters per model). For the inference of each token, also only 2 experts are used.\n*   Since the model size (87GB) is smaller than 8x Mistral 7B (8\\*15GB=120GB), we could assume that the new model uses the same architecture as Mistral 7B but the attention parameters are shared, reducing the naïve 8x7B model size estimation.\n*   The conclusion is that Mistral 8x7B uses a very similar architecture to that of GPT-4, but scaled down:\n    *   8 total experts instead of 16 (2x reduction).\n    *   7B parameters per expert instead of 166B (24x reduction).\n    *   42B total parameters (estimated) instead of 1.8T (42x reduction).\n    *   Free to use under Apache 2.0 license\n    *   Outperforms Llama 2 70B with 6x faster inference.\n    *   Matches or outperforms GPT-3.5\n    *   Multilingual: vastly outperforms LLaMA 2 70B on French, Italian, German and Spanish\n    *   Same 32K context as the original GPT-4.\n*   Each layer in a 8x MoE model has its FFN split into 8 chunks and a router picks 2 of them, while the attention weights are always used in full for each token. This means that if the new mistral model uses 5B parameters for the attention, you will use 5+(42-5)/4 = 14.25B params per forward pass.\n*   They’ve also released Mixtral 8x7B Instruct v0.1 trained using supervised fine-tuning and direct preference optimization (DPO). It scores 8.3 on MT-Bench making it the best open-source model, with performance comparable to GPT3.5.\n*   They offer three chat endpoints with competitive pricing via Mistral AI La Plateforme:\n    *   Mistral-tiny: Mistral 7B Instruct v0.2, upgraded base model with higher context length 8K →→\\\\rightarrow 32K and better fine-tuning, 6.84 →→\\\\rightarrow 7.61 on MT Bench.\n    *   Mistral-small: Mistral 8x7B Instruct v0.1, matches or exceeds GPT-3.5 performance, multilingual.\n    *   Mistral-medium: Outperforms GPT-3.5 on all metrics, multilingual.\n*   They’ve also announced Mistral-embed, an embedding model with a 1024 embedding dimension, which achieves 55.26 on MTEB.\n*   Refer [MoE Explanation](https://youtu.be/ccBMRryxGog?si=QPmlkNMIDFnRTJGR&t=1038).\n*   [Blog](https://mistral.ai/news/mixtral-of-experts/); [La Plateforme](https://mistral.ai/news/la-plateforme/); [Mixtral-8x7B-v0.1 Base model](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1); [Mixtral-8x7B-v0.1 Instruct model](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).\n\n*   8 total experts instead of 16 (2x reduction).\n*   7B parameters per expert instead of 166B (24x reduction).\n*   42B total parameters (estimated) instead of 1.8T (42x reduction).\n*   Free to use under Apache 2.0 license\n*   Outperforms Llama 2 70B with 6x faster inference.\n*   Matches or outperforms GPT-3.5\n*   Multilingual: vastly outperforms LLaMA 2 70B on French, Italian, German and Spanish\n*   Same 32K context as the original GPT-4.\n\n*   Mistral-tiny: Mistral 7B Instruct v0.2, upgraded base model with higher context length 8K →→\\\\rightarrow 32K and better fine-tuning, 6.84 →→\\\\rightarrow 7.61 on MT Bench.\n*   Mistral-small: Mistral 8x7B Instruct v0.1, matches or exceeds GPT-3.5 performance, multilingual.\n*   Mistral-medium: Outperforms GPT-3.5 on all metrics, multilingual.\n\n##### Mixtral 8x22B MoE\n\n*   A [262 GB open 8x22B MoE](https://twitter.com/MistralAI/status/1777869263778291896) from Mistral with 65k token context and 2 active experts (overall 44B active params).\n\n##### [Mistral NeMo](https://mistral.ai/news/mistral-nemo/)\n\n*   Mistral NeMo is a 12B model built by Mistal in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\n*   Mistral NeMo is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n*   Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. In particular, it is ~30% more efficient at compressing source code, Chinese, Italian, French, German, Spanish, and Russian. It is also 2x and 3x more efficient at compressing Korean and Arabic, respectively. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages.\n*   Hugging Face: [Base](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407); [Instruct](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)\n*   [Unsloth.AI Blog](https://unsloth.ai/blog/mistral-nemo)\n\n##### [Mistral Large](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)\n\n*   Mistral-Large-Instruct-2407 is a dense LLM of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.\n*   **Key features:**\n    *   **Multi-lingual by design:** Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\n    *   **Proficient in coding:** Trained on 80+ coding languages such as Python, Java, C, C++, Javacsript, and Bash. Also trained on more specific languages such as Swift and Fortran.\n    *   **Agentic-centric:** Best-in-class agentic capabilities with native function calling and JSON outputting.\n    *   **Advanced Reasoning:** State-of-the-art mathematical and reasoning capabilities.\n    *   **Mistral Research License:** Allows usage and modification for research and non-commercial usages.\n    *   **Large Context:** A large 128k context window.\n*   [Blog](https://mistral.ai/news/mistral-large-2407/)\n\n*   **Multi-lingual by design:** Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\n*   **Proficient in coding:** Trained on 80+ coding languages such as Python, Java, C, C++, Javacsript, and Bash. Also trained on more specific languages such as Swift and Fortran.\n*   **Agentic-centric:** Best-in-class agentic capabilities with native function calling and JSON outputting.\n*   **Advanced Reasoning:** State-of-the-art mathematical and reasoning capabilities.\n*   **Mistral Research License:** Allows usage and modification for research and non-commercial usages.\n*   **Large Context:** A large 128k context window.\n\n#### [Zephyr](https://arxiv.org/abs/2310.16944)\n\n*   Introduced by Tunstall et al. from Huggingface in [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944). Tunstall et al. introduce a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n    2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n    3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n*   They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.\n*   Results:\n    *   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n    *   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n    *   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n*   The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.\n*   The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.\n*   Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.\n*   The image below [(source)](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.\n\n1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n\n*   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n*   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n*   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n\n![](/primers/ai/assets/LLM/zephyr.png)\n\n##### HuggingFace’s Alignment Handbook\n\n*   [The Alignment Handbook](https://github.com/huggingface/alignment-handbook) contains robust recipes to align language models with human and AI preferences. It also contains code to train your very own Zephyr models:\n    *   Full fine-tuning with Microsoft’s DeepSpeed ZeRO-3 on A100s\n    *   LoRA or QLoRA fine-tuning on consumer GPUs\n\n*   Full fine-tuning with Microsoft’s DeepSpeed ZeRO-3 on A100s\n*   LoRA or QLoRA fine-tuning on consumer GPUs\n\n![](/primers/ai/assets/LLM/Alignment.jpg)\n\n*   Dataset from HuggingFace called [No Robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) of 10k instructions and demonstrations to train instruct models. This is based on the SFT dataset from OpenAI’s InstructGPT paper. 100% organic and written entirely by skilled human annotators.\n\n#### [Yi](https://01.ai/)\n\n*   [01.AI](https://01.ai) offers two new opensource LLMs Yi-34B and Yi-6B trained on 3 trillion tokens with an extraordinarily long 200K context window.\n*   It outperforms Llama-2 70B and Falcon-180B in most of the benchmarks and comes with a free commercial license, based on global open-source LLM English/Chinese benchmarks:\n\n![](/primers/ai/assets/LLM/YI.png)\n\n##### [Yi 1.5](https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8)\n\n*   Yi 1.5 from [01.AI](https://01.ai) has been significantly improved compared to Yi 1.0 for coding, math and reasoning, closely matching or exceeding Llama 3 and Mixtral 8x22B.\n*   Features:\n    *   Further pre-trained Yi 1.0 on 500B tokens\n    *   Additionally fine-tuned on “3M diverse samples”\n    *   Available as 6B, 9B, and 34B checkpoints\n    *   Base + Chat models\n    *   Context = 4K\n    *   9B beats Llama 3 8B on MMLU, GSM8K, HumanEval and AlpacaEval2.0\n    *   34B closely matches or beats Mixtral 8x22B on benchmarks\n    *   Apache 2.0\n\n*   Further pre-trained Yi 1.0 on 500B tokens\n*   Additionally fine-tuned on “3M diverse samples”\n*   Available as 6B, 9B, and 34B checkpoints\n*   Base + Chat models\n*   Context = 4K\n*   9B beats Llama 3 8B on MMLU, GSM8K, HumanEval and AlpacaEval2.0\n*   34B closely matches or beats Mixtral 8x22B on benchmarks\n*   Apache 2.0\n\n#### [effi](https://huggingface.co/aiplanet/effi-13b)\n\n*   effi-13B parameters is a causal decoder-only model built by AI Planet based on Llama-2-13b-chat-hf and fine tuned using the 1.8 Million coversations from CoT dataset available in huggingface datasets. The model is made available under the Apache 2.0 license.\n*   **Why use effi-13B-Instruct?**\n    *   This is a ready to use chat/instruct model based on Llama-2-13b-chat-hf, which provides a rationale for the context provided.\n    *   Llama-2 is the best open-source model available. This is an instruct model, which may not be ideal for further fine-tuning. If you are interested in building your own instruct/chat model, we recommend starting from Llama-2-13b-chat-hf.\n    *   You will need at least 85-100GB of memory to swiftly run inference with effi-13b.\n*   **Model Description:**\n    *   This model has been fine-tuned on Chain of Thought datasets, which has context from mixed sources with corresponding rationale. The final finetuned Large Language Model(LLM) have shown enhanced capabilities of solving novel tasks by providing a reasoning.\n\n*   This is a ready to use chat/instruct model based on Llama-2-13b-chat-hf, which provides a rationale for the context provided.\n*   Llama-2 is the best open-source model available. This is an instruct model, which may not be ideal for further fine-tuning. If you are interested in building your own instruct/chat model, we recommend starting from Llama-2-13b-chat-hf.\n*   You will need at least 85-100GB of memory to swiftly run inference with effi-13b.\n\n*   This model has been fine-tuned on Chain of Thought datasets, which has context from mixed sources with corresponding rationale. The final finetuned Large Language Model(LLM) have shown enhanced capabilities of solving novel tasks by providing a reasoning.\n\n#### [Starling](https://starling.cs.berkeley.edu/)\n\n*   Proposed in [Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/), this report by Zhu et al. from UC Berkeley introduces Starling-7B, a large language model enhanced by Reinforcement Learning from AI Feedback (RLAIF). It utilizes a new GPT-4 labeled ranking dataset, [Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), and a novel reward training and policy tuning pipeline.\n*   Starling-7B-alpha achieves a score of 8.09 on MT Bench, evaluated by GPT-4, surpassing most models except GPT-4 and GPT-4 Turbo. The model and its components, including the ranking dataset Nectar and the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha), are available on HuggingFace and as an online demo in LMSYS Chatbot Arena.\n\n![](../../../images/papers/Starling.jpg)\n\n*   The report discusses the effectiveness of Supervised Fine-Tuning (SFT) in chatbot systems, contrasting it with Reinforcement Learning from Human Feedback (RLHF) and AI feedback (RLAIF). It emphasizes the need for high-quality ranking datasets for chat, leading to the creation of Nectar, which includes 183K chat prompts and 3.8M pairwise comparisons.\n*   Starling-7B is fine-tuned using the Starling-RM-7B-alpha reward model, improving its MT-Bench and AlpacaEval scores, reflecting increased helpfulness.\n*   The model’s evaluation involves MT-Bench and AlpacaEval, with results indicating improvements in helpfulness and safety but minor regressions in areas like QA, math, and coding.\n*   The report details the dataset creation process, particularly the efforts to mitigate positional bias in GPT-4-based rankings, resulting in the Nectar dataset.\n*   Training of the reward model involves the K-wise maximum likelihood estimator under the Plackett-Luce Model. Policy fine-tuning experiments are conducted using different RL methods, with APA being the most effective.\n*   The report highlights challenges in RLHF evaluation and discusses limitations, including Starling-7B’s struggles with reasoning tasks and susceptibility to jailbreaking prompts.\n*   The research is subject to licenses and terms from various sources, including LLaMA, OpenAI, and ShareGPT, and acknowledges contributions from the broader research community.\n*   [Project page](https://starling.cs.berkeley.edu/).\n\n#### [NexusRaven-V2](https://nexusflow.ai/blogs/ravenv2)\n\n*   Proposed in [NexusRaven-V2: Surpassing GPT-4 for Zero-shot Function Calling](https://nexusflow.ai/blogs/ravenv2), this blog by Nexusflow introduces the open-source NexusRaven-V2, a 13B LLM that excels in zero-shot function calling, surpassing GPT-4’s capabilities. This model is pivotal in converting natural language instructions into executable code, integral to the OpenAI Assistants API. It’s a major stride in enhancing copilots and agents for using software tools, emphasizing open-source models’ role in technology and society.\n*   NexusRaven-V2 achieves up to 7% higher function calling success rates than GPT-4, particularly in complex cases involving nested and composite functions. This is notable considering NexusRaven-V2 was never trained on these functions.\n*   The model is instruction-tuned on Meta’s CodeLlama-13B-instruct, utilizing data solely from open-code corpora. Its open-source nature and commercial permissiveness cater to both community developers and enterprises.\n*   NexusRaven-V2 is designed for easy integration into existing software workflows, replacing mainstream proprietary function calling APIs. It includes open-source utility artifacts, online demos, and Colab notebooks for seamless onboarding.\n*   The following figure from the blog shows that NexusRaven-V2 provides the function calling capability to enable copilots and agents to use software tools. Given human instruction prompts and software documentations, the function calling capability generates executable code to run the functions/APIs.\n\n![](../../../images/papers/NexusRaven1.jpg)\n\n*   The team introduces the Nexus-Function-Calling benchmark and a Hugging Face [leaderboard](https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard), featuring a wide array of real-life, human-curated function-calling examples. This benchmark, with 8 out of 9 tasks open-sourced, aims to standardize evaluations in function calling.\n*   The following figure from the blog shows NexusRaven-V2 evaluation with their human-curated Benchmark.\n\n![](../../../images/papers/NexusRaven2.jpg)\n\n*   The model’s robustness is evident in its handling of various descriptions of functions by developers, indicating its potential to match or surpass proprietary LLM APIs in accuracy and robustness.\n*   [Project page](https://nexusflow.ai/blogs/ravenv2).\n\n#### [Llama Guard](https://scontent-sin6-4.xx.fbcdn.net/v/t39.2365-6/408725049_3688557441468029_8103913771964668529_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_gWcFD1K96gAX9rRWN9&_nc_ht=scontent-sin6-4.xx&oh=00_AfD8Cg11tI6b7yw1YXftHQpKdigclsUWye_Cxn51IhAkog&oe=65773519)\n\n*   Proposed in [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://scontent-sin6-4.xx.fbcdn.net/v/t39.2365-6/408725049_3688557441468029_8103913771964668529_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_gWcFD1K96gAX9rRWN9&_nc_ht=scontent-sin6-4.xx&oh=00_AfD8Cg11tI6b7yw1YXftHQpKdigclsUWye_Cxn51IhAkog&oe=65773519).\n*   The Purple Llama initiative by Meta, aimed at promoting responsible and safe development in generative AI, encompasses a variety of tools and evaluations, including the notable Llama-Guard and the Llama 7B model for content moderation.\n*   Llama Guard, a component of Meta’s Purple Llama Initiative, is a 7B parameter model based on Llama2, designed to classify content in LLM prompts and responses, enhancing trust and safety in AI applications.\n*   It uses a safety risk taxonomy for content moderation, detecting policy violations and indicating the safety level of text, with detailed subcategory violations when necessary.\n*   The model is instruction-tuned on a dataset comprising about 13,000 examples, including prompts and responses annotated for safety, with training inputs from the Anthropic [dataset](https://github.com/anthropics/hh-rlhf) and in-house redteaming examples.\n*   Llama Guard outperforms existing moderation tools in benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, and is adept at detecting harmful content across various categories.\n*   Its functionality includes evaluating probabilities for classifying text as safe or unsafe, and it can generate outputs indicating safety status and policy violations.\n*   The following figure from the blog shows example task instructions for the Llama Guard prompt and response classification tasks. A task consists of four main components. Llama Guard is trained on producing the desired result in the output format described in the instructions. It acts as an LLM: it generates text in its output that indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories. Here is an example:\n\n![](../../../images/papers/Llama-Guard.jpg)\n\n*   Customizable for different use cases, Llama Guard is adaptable for chatbots and digital assistants, offering flexibility without compromising safety.\n*   Part of the broader Purple Llama ecosystem, which includes industry collaborations and is available on Hugging Face, Llama Guard’s model weights are released for public use, licensed permissively for research and commercial applications.\n*   In summary, Meta’s Purple Llama initiative represents a major advancement in ensuring safe and responsible development in generative AI. By providing a suite of tools, including the Llama-Guard and the Llama 7B model for content moderation, the initiative addresses the need for comprehensive safety measures in AI applications, fostering an open environment for ongoing innovation in the field.\n*   [Blog](https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/); [Model](https://huggingface.co/meta-llama/LlamaGuard-7b); [Notebook](https://colab.research.google.com/drive/16s0tlCSEDtczjPzdIK3jq0Le5LlnSYGf?usp=sharing); [Benchmark](https://huggingface.co/spaces/facebook/CyberSecEval).\n\n#### [Notus](https://huggingface.co/argilla/notus-7b-v1)\n\n*   Argilla’s Notus-7B-v1 is an open-source LLM developed using Direct Preference Optimization (DPO) and related Reinforcement Learning from Human Feedback (RLHF) techniques. This model represents the first version, fine-tuned with DPO over [alignment-handbook/zephyr-7b-sft-full](https://huggingface.co/alignment-handbook/zephyr-7b-sft-full), which is the SFT model used to create zephyr-7b-beta.\n*   Notus-7B-v1 differentiates itself from Zephyr-7B-beta primarily in the preference dataset used for fine-tuning. Argilla identified and rectified issues in the original UltraFeedback dataset by OpenBMB, which were leading to high scores for bad responses. They proposed a modified version of the UltraFeedback dataset by binarizing the dataset using preference ratings instead of the original overall critique score. This was accomplished by taking the mean of preference ratings for each of the different preference aspects, namely: helpfulness, honesty, instruction-following, and truthfulness. This change led to a significant difference in chosen responses, approximately 50% different from the original dataset. The new dataset version can be found at [Argilla’s UltraFeedback Binarized Preferences](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences).\n*   Notus-7B-v1, with its refined dataset, surpasses both Zephyr-7B-beta and Claude 2 in the AlpacaEval benchmark. It’s evaluated using both Chat (MT-Bench, AlpacaEval) and Academic (Open LLM Leaderboard) benchmarks, providing a direct comparison with the original Zephyr dDPO model and other 7B models.\n*   This model’s success owes much to resources like the Alignment Handbook and OpenBMB’s release of the UltraFeedback dataset. Discussions with the HuggingFace H4 team and the utilization of zephyr-7b-beta’s recipe were instrumental in its development.\n*   Intended for use in chat-like applications as an assistant, Notus models are a testament to high-quality data focus. An open question remains regarding the efficacy of using critique scores versus preference ratings post dataset correction, a comparison Argilla is excited to explore in the future.\n*   [Hugging Face](https://huggingface.co/argilla/notus-7b-v1).\n\n#### [OpenChat](https://arxiv.org/abs/2309.11235)\n\n*   Proposed in [OpenChat: Advancing Open-Source Language Models with Mixed-Quality Data](https://arxiv.org/abs/2309.11235) by Wang et al. from Tsinghua University and Shanghai Artificial Intelligence Laboratory, OpenChat is a novel framework designed to advance open-source language models using mixed-quality data. It explores the integration of supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to enhance language model performance with diverse data qualities.\n*   OpenChat introduces a new approach, Conditioned-RLFT (C-RLFT), which utilizes coarse-grained reward labels to distinguish between different data sources, like GPT-4 or GPT-3.5, within the general SFT training data. This method effectively leverages the mixed-quality training data, comprising a small amount of expert data and a larger proportion of sub-optimal data, without requiring preference labels.\n*   An intuitive approach to align LLMs is through RL via RLHF, which models rewards according to human preference feedback and fine-tune LLMs to maximize the reward. The reward either modeled explicitly (by training a separate model) or implicitly (via the LLM itself, as in DPO), assigns high values on desirable responses and low values on bad ones to guide the alignment of the finetuned LLM. A popular RL framework for fine-tuning LLMs is the KL-regularized RL as in DPO, which adds an additional KL penalty to constrain the fine-tuned LLM πθ(y∣x)πθ(y∣x)\\\\pi\\_\\\\theta(y \\\\mid x) to stay close to the base pre-trained LLM π0(y∣x)π0(y∣x)\\\\pi\\_0(y \\\\mid x). This has been shown beneficial to avoid distribution collapse as compared to naïvely maximize reward using RL.\n*   C-RLFT, based on the KL-regularized RL framework, focuses on maximizing reward while minimizing the difference between the fine-tuned policy and a reference policy. Uniquely, it employs single-stage supervised learning (which implies that reinforcement learning step, e.g., based on PPO, is not required similar to DPO), which is both lightweight and free from the need for costly human preference labeling. C-RLFT learns to distinguish expert and sub-optimal data. To this end, mixed-quality data from different sources is sufficient. The reward label can be as simple as a relative value differentiating different classes of data sources, i.e., GPT-4 or GPT-3.5.\n*   Note that regular SFT treats all training data uniformly but that’s not the case for C-RLFT. The paper used a dataset of a small amount of expert data and a large proportion of easily accessible sub-optimal data using coarse-grained reward labels.\n*   The figure below from the paper shows the proposed framework OpenChat with Conditioned-RLFT to advance the open-source language model fine-tuning with mixed-quality data, comparing to previous supervised fine-tuning (SFT) method and reinforcement learning fine-tuning (RLFT) method. MLE and RL denote maximum likelihood estimates and reinforcement learning, respectively.\n\n![](../../../images/papers/OpenChat.jpg)\n\n*   **Implementation details:**\n    1.  Collect mixed-quality data from different sources (e.g. GPT-4 and GPT-3.5 conversations) and assign coarse-grained rewards based on data source quality, e.g., GPT-4=1.0 GPT-3.5=0.1.\n    2.  Construct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}<\n        \n        end\\_of\\_turn\n        \n        \\>GPT4 Assistant: {RESPONSE}”\n        \n    3.  Train LLM using C-RLFT regularizing the class-conditioned references for the optimal the policy.\n*   The paper demonstrates that OpenChat, particularly when fine-tuned with C-RLFT on the ShareGPT dataset, significantly outperforms other 13b open-source language models in terms of average performance across standard benchmarks. Notably, OpenChat-13b excels in AGIEval, surpassing the base model in terms of generalization performance.\n*   The implementation details include using the llama-2-13b as the base model and fine-tuning it for 5 epochs on the ShareGPT dataset with AdamW optimizer, a sequence length of 4096 tokens, and an effective batch size of 200k tokens. The training involves class-conditioning and assigning weights to different data types, with a cosine learning rate schedule.\n*   Extensive ablation studies and analyses were conducted to validate the contributions of different components, like coarse-grained rewards and class-conditioned policy, and performance consistency. These studies shed light on the effectiveness and robustness of OpenChat.\n*   The paper suggests future research directions, including refining the coarse-grained rewards to better reflect the actual quality of data points and exploring applications of OpenChat to enhance reasoning abilities in language models.\n*   [Code](https://github.com/imoneoi/openchat); [HuggingFace](https://huggingface.co/openchat/openchat-3.5-1210)\n\n1.  Collect mixed-quality data from different sources (e.g. GPT-4 and GPT-3.5 conversations) and assign coarse-grained rewards based on data source quality, e.g., GPT-4=1.0 GPT-3.5=0.1.\n2.  Construct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}<\n    \n    end\\_of\\_turn\n    \n    \\>GPT4 Assistant: {RESPONSE}”\n    \n3.  Train LLM using C-RLFT regularizing the class-conditioned references for the optimal the policy.\n\nConstruct class-conditioned dataset by augmenting data with source class labels, e.g., “User: {QUESTION}<\n\nend\\_of\\_turn\n\n\\>GPT4 Assistant: {RESPONSE}”\n\n#### [DeciLM](https://huggingface.co/Deci/DeciLM-7B)\n\n*   [Deci AI](https://deci.ai/) has released DeciLM-7B, a new state-of-the-art 7B pretrained LLM with a permissive Apache-2.0 license, outperforming the excellent Mistral-7B.\n*   DeciLM-7B is Apache 2.0-licensed, and as of this writing, it’s officially #1 on the Open LLM Leaderboard for the 7B text generation category.\n*   DeciLM-7B’s throughput is also 1.83x and 2.39x faster than Mistral 7B and Llama 2 7B, respectively.\n*   [Base](https://huggingface.co/Deci/DeciLM-7B); [Instruct](https://huggingface.co/Deci/DeciLM-7B-instruct); [Demo](https://console.deci.ai/infery-llm-demo?utm_source=organic-social&utm_medium=post&utm_campaign=decilm-7b); [Technical blog](https://deci.ai/blog/introducing-DeciLM-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date?utm_campaign=DeciLM%207B%20Launch&utm_source=twitter&utm_medium=social); \\[Notebooks: [DeciLM-7B-Instruct](https://colab.research.google.com/drive/1VU98ezHJr1-ry6LDkhayu5-6SZKTsnsX); [Fine-tune DeciLM-7B for Translation](https://colab.research.google.com/drive/1kEV6i96AQ94xTCvSd11TxkEaksTb5o3U?usp=sharing)\n\n#### [LLM360](https://arxiv.org/abs/2312.06550)\n\n*   Proposed in [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550) by Liu et al. from Petuum, MBZUAI, and CMU, LLM360 is a framework aimed at enhancing the transparency and reproducibility of LLMs. It emphasizes the importance of fully open-sourcing LLMs, including training code, data, model checkpoints, and intermediate results.\n*   The paper introduces two 7B parameter LLMs, AMBER and CRYSTALCODER. AMBER is an English LLM pre-trained on 1.3 trillion tokens, while CRYSTALCODER is an English and code LLM pre-trained on 1.4 trillion tokens. Both models are notable for their transparency, with the release of all training components.\n*   For AMBER, data preparation entailed combining RefinedWeb, StarCoder, and RedPajama-v1 datasets, with no further cleaning or sub-sampling, resulting in 1.26 trillion tokens. CRYSTALCODER’s pre-training dataset blended SlimPajama and StarCoder data, totaling around 1382 billion tokens. The training for CRYSTALCODER was divided into three stages, with a unique mix of English and coding data.\n*   The following figure from the paper shows a summary of notable open-source LLMs. We note a trend of progressively less disclosure of important pretraining details over time: (1) availability of pretraining code, (2) disclosure of training configurations and hyperparameters, (3) intermediate checkpoints of model weights, (4) intermediate checkpoints of optimizer states, (5) disclosure of data mixture and sources, (6) reproducibility of pretraining data sequence, and (7) availability (or reconstruction scripts) of the pretraining data.\n\n![](../../../images/papers/LLM360.jpg)\n\n*   In terms of infrastructure, AMBER was trained on an in-house GPU cluster, utilizing 56 DGX A100 nodes, with a throughput of approximately 582.4k tokens per second. CRYSTALCODER was trained on the Cerebras Condor Galaxy 1, a 4 exaFLOPS supercomputer.\n*   The paper discusses challenges encountered during pre-training, such as NaN loss on specific data chunks and missing optimizer states. It also highlights the importance of data cleaning and mixing ratios in LLM pre-training.\n*   One of the key contributions of LLM360 is the release of training code, hyperparameters, configurations, model checkpoints, and evaluation metrics, all aimed at fostering a collaborative and transparent research environment.\n*   The paper concludes with insights into future work, including a more detailed analysis of AMBER and CRYSTALCODER, exploration of optimal data mixing ratios, and the pre-training of a larger LLM.\n*   [Project page](https://www.llm360.ai/)\n\n#### [OLMo](https://arxiv.org/pdf/2402.00838)\n\n*   Proposed in [OLMo: Accelerating the Science of Language Models](https://arxiv.org/pdf/2402.00838) by Groeneveld et al. from the Allen AI, UW, Yale, NYU, and CMU.\n*   OLMo is a state-of-the-art open language model and framework. It aims to advance the science of language modeling by providing open access to model weights, training, and evaluation code, in response to the proprietary nature of recent powerful models. This initiative seeks to foster innovation and scientific study of language models, including their biases and potential risks.\n*   Unlike previous efforts that provided limited access to model components, OLMo releases a comprehensive framework encompassing the training data Dolma, hundreds of intermediate checkpoints, training logs, and evaluation tools like Catwalk for downstream evaluation and Paloma for perplexity-based evaluation. This release supports the examination of the impact of training data, design choices, and optimization methods on model performance.\n*   OLMo adopts a decoder-only transformer architecture with several enhancements for stability and efficiency, such as the exclusion of bias terms and the use of SwiGLU activation functions. It is available in 1B and 7B variants, with a 65B version in progress, all trained on a diverse, multi-source corpus of 3T tokens across 5B documents.\n*   The pretraining dataset, Dolma, is a significant contribution to open research, comprising 3T tokens from various sources with detailed documentation. It underwent extensive curation, including language and quality filtering, deduplication, and mixing from multiple sources, to support the study of training data’s impact on model capabilities.\n*   OLMo’s evaluation showcases competitive performance across various metrics compared to other models, underscoring its potential as a base model for further research and application. Additionally, the adaptation of OLMo using instruction tuning and Direct Preference Optimization demonstrates its versatility for creating safer and more effective language models.\n*   The release also emphasizes the environmental impact of training large models, providing detailed estimates of power consumption and carbon footprint to highlight the cost of developing state-of-the-art models and encourage the use of open models to reduce redundant efforts and emissions.\n*   OLMo is part of a broader effort to push the boundaries of open language models, with plans for future releases that include larger models, more modalities, and enhanced safety measures. This endeavor aims to empower the research community and drive forward innovation in language modeling.\n*   [Code](https://github.com/allenai/OLMo); [Data](https://huggingface.co/datasets/allenai/dolma); [Weights](https://huggingface.co/allenai/OLMo-7B)\n\n##### [OLMOE](https://arxiv.org/abs/2409.02060)\n\n*   Proposed in OLMOE: Open Mixture-of-Experts Language Models\\](https://arxiv.org/abs/2409.02060) by Muennighoff et al. from Allen AI, Contextual AI, UW, and Princeton, OLMOE is an open, state-of-the-art language model that leverages sparse Mixture-of-Experts (MoE) architecture. OLMOE-1B-7B has a total of 7 billion parameters, but activates only 1 billion for each input token, thus making it more efficient compared to dense models of similar size. The model is pre-trained on 5.1 trillion tokens and further adapted to create OLMOE-1B-7B-INSTRUCT, which outperforms models with similar active parameters, even surpassing larger models such as Llama2-13B-Chat and DeepSeekMoE-16B.\n*   OLMOE is a decoder-only language model consisting of NL transformer layers, and the dense feedforward network (FFN) modules typically found in such architectures are replaced with smaller, MoE-based FFN modules. Only a subset of these experts is activated per token, reducing computational requirements. Specifically, OLMOE-1B-7B employs 64 experts per layer, with 8 activated per input token. The routing of tokens is handled by a dropless token-based routing mechanism, which consistently chooses experts for each token.\n*   Key aspects of the implementation include:\n    *   **Pretraining:** OLMOE-1B-7B is trained from scratch using a combination of load balancing loss and router z-loss. The load balancing loss penalizes the model if too many tokens are routed to a few experts, encouraging more equal distribution across experts. The router z-loss helps stabilize the routing mechanism by limiting the size of the logits used in routing.\n    *   **Dataset:** The model was pretrained on a mixture of datasets, including Common Crawl, arXiv, Wikipedia, and StarCoder, referred to as OLMOE-MIX. This dataset was filtered for quality, including the removal of low-quality GitHub repositories and documents with high-frequency repetitive n-grams.\n    *   **Sparse Upcycling:** Although some prior works recommend upcycling dense models into sparse MoEs, the authors found that training from scratch outperforms sparse upcycling when using larger compute budgets and more training tokens. Thus, OLMOE-1B-7B was not upcycled from a dense model.\n    *   **Adaptation:** The model is fine-tuned for instruction and preference tuning. This adaptation improves performance on math and code-related tasks due to the inclusion of math and code datasets. Fine-tuning with preference data (DPO) particularly enhances tasks like HumanEval and AlpacaEval.\n*   The following figure from the paper shows a comparison of the architecture of dense LMs and MoE models like OLMOE. The figure excludes some details, e.g., OLMOE-1B-7B also uses QK-Norm.\n\n*   **Pretraining:** OLMOE-1B-7B is trained from scratch using a combination of load balancing loss and router z-loss. The load balancing loss penalizes the model if too many tokens are routed to a few experts, encouraging more equal distribution across experts. The router z-loss helps stabilize the routing mechanism by limiting the size of the logits used in routing.\n*   **Dataset:** The model was pretrained on a mixture of datasets, including Common Crawl, arXiv, Wikipedia, and StarCoder, referred to as OLMOE-MIX. This dataset was filtered for quality, including the removal of low-quality GitHub repositories and documents with high-frequency repetitive n-grams.\n*   **Sparse Upcycling:** Although some prior works recommend upcycling dense models into sparse MoEs, the authors found that training from scratch outperforms sparse upcycling when using larger compute budgets and more training tokens. Thus, OLMOE-1B-7B was not upcycled from a dense model.\n*   **Adaptation:** The model is fine-tuned for instruction and preference tuning. This adaptation improves performance on math and code-related tasks due to the inclusion of math and code datasets. Fine-tuning with preference data (DPO) particularly enhances tasks like HumanEval and AlpacaEval.\n\n![](../../../images/papers/OLMOE.jpg)\n\n*   The results demonstrate that OLMOE-1B-7B achieves higher performance while being more efficient in terms of compute compared to dense models like Llama2-7B, and even performs on par with some larger models. It achieves this by optimizing the trade-offs between active and total parameters, routing methods, and expert specialization, all within a fully open framework that shares training data, code, and logs.\n*   The authors also share insights from various design experiments, highlighting:\n    *   **Expert Granularity:** Using more granular experts (smaller but more numerous experts) yields better performance, though there are diminishing returns after a certain threshold.\n    *   **Routing Mechanism:** Token choice routing, where tokens select their experts, performs better than expert choice routing in this setup.\n    *   **Loss Functions:** The combination of load balancing loss and router z-loss is crucial to maintaining stability and ensuring expert specialization.\n*   OLMOE is an important step in making advanced MoE-based language models accessible to the open-source community by releasing all aspects of the model development, from code and data to intermediate training checkpoints.\n*   [Hugging Face](https://hf.co/allenai/OLMoE-1B-7B-0924)\n\n*   **Expert Granularity:** Using more granular experts (smaller but more numerous experts) yields better performance, though there are diminishing returns after a certain threshold.\n*   **Routing Mechanism:** Token choice routing, where tokens select their experts, performs better than expert choice routing in this setup.\n*   **Loss Functions:** The combination of load balancing loss and router z-loss is crucial to maintaining stability and ensuring expert specialization.\n\n#### [DeepSeek](https://arxiv.org/abs/2401.02954v1)\n\n*   Proposed in [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954v1) by Bi et al. from the DeepSeek-AI team, DeepSeek LLM is a project aimed at advancing the capabilities of open-source language models with a long-term perspective. Building upon the foundations laid by previous literature which presented varied conclusions on scaling LLMs, this paper presents novel findings that facilitate the scaling of large-scale models in two commonly used open-source configurations: 7B and 67B parameters.\n*   At the core of their approach is the development of a dataset comprising 2 trillion tokens, which supports the pre-training phase. Additionally, supervised fine-tuning (SFT) and direct preference optimization (DPO) are conducted on the base models to create the DeepSeek Chat models. Through rigorous evaluation, DeepSeek LLM 67B demonstrates its superiority over LLaMA-2 70B across various benchmarks, especially in code, mathematics, and reasoning, and exhibits enhanced performance in open-ended evaluations against GPT-3.5.\n*   The architecture of DeepSeek LLM adheres closely to the LLaMA design, utilizing a Pre-Norm structure with RMSNorm, SwiGLU for the Feed-Forward Network (FFN), and incorporating Rotary Embedding for positional encoding. Modifications include a 30-layer network for the 7B model and a 95-layer network for the 67B model, differing in layer adjustments to optimize training and inference efficiency.\n*   A critical contribution of this study is the exploration of scaling laws for hyperparameters, where optimal values for batch size and learning rate are identified based on extensive experimentation. This leads to a significant revelation: the quality of training data critically impacts the optimal scaling strategy between model size and data volume. The higher the data quality, the more a scaling budget should lean towards model scaling.\n*   The paper also delves into alignment strategies through SFT and DPO, employing a dataset with 1.5 million instances to enhance the model’s helpfulness and harmlessness. The evaluation framework spans across a wide array of public benchmarks in both English and Chinese, addressing various domains such as language understanding, reasoning, and coding.\n*   Safety evaluation forms a pivotal part of the study, ensuring that the models adhere to ethical guidelines and are devoid of harmful outputs. The results across multiple safety evaluation metrics underscore the model’s reliability and safe interaction capabilities.\n*   The DeepSeek LLM initiative not only pushes the envelope in the open-source landscape of LLMs but also sets a new benchmark for future research in scaling, safety, and alignment of language models, driving forward the quest towards Artificial General Intelligence (AGI).\n\n##### [DeepSeek-V3](https://arxiv.org/abs/2412.19437)\n\n*   **Overview**:\n    *   Proposed in [DeepSeek-V3](https://arxiv.org/abs/2412.19437) by the DeepSeek team, DeepSeek-V3 is an open-source Mixture-of-Experts (MoE) language model with a massive 671B total parameters, activating 37B parameters per token. The model prioritizes efficient inference and training cost-effectiveness through key innovations in architecture and training strategies.\n*   **Key Contributions**:\n    1.  **Architecture Enhancements**:\n        \n        *   **Multi-Head Latent Attention (MLA)** and **DeepSeekMoE**:\n            *   Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.\n            *   DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.\n        *   **Multi-Token Prediction (MTP)**:\n            *   Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.\n        *   The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.\n        \n        ![](../../../images/papers/DeepSeekV3.jpg)\n        \n    2.  **Pre-Training Efficiency**:\n        *   Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.\n        *   Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.\n        *   Training completed within 2.788M H800 GPU hours, costing only $5.576M.\n    3.  **Post-Training Innovations**:\n        *   Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.\n        *   Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.\n    4.  **Evaluation and Performance**:\n        *   Benchmarked as the strongest open-source base model, excelling in:\n            *   Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).\n            *   Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.\n            *   Multilingual capabilities, especially strong in Chinese factual knowledge.\n    5.  **Inference and Deployment**:\n        *   Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.\n        *   Implements redundancy and dynamic load balancing among experts to optimize inference throughput.\n*   **Technical Implementation**:\n    1.  **Architectural Details**:\n        *   MLA compresses key-value and query tensors for efficient attention.\n        *   DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.\n    2.  **Training Framework**:\n        *   Uses **DualPipe** for overlapping computation and communication during pipeline parallelism.\n        *   Efficient memory management via recomputation, shared embeddings, and low-precision caching.\n    3.  **FP8 Training**:\n        *   Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.\n        *   Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.\n*   **Limitations and Future Directions**:\n    *   While performing well, challenges remain in fine-grained task specialization and scaling inference strategies for broader applications.\n    *   Future research could explore advanced dynamic expert routing and further reducing training costs with emergent hardware.\n*   [Code](https://github.com/deepseek-ai/DeepSeek-V3)\n\n*   Proposed in [DeepSeek-V3](https://arxiv.org/abs/2412.19437) by the DeepSeek team, DeepSeek-V3 is an open-source Mixture-of-Experts (MoE) language model with a massive 671B total parameters, activating 37B parameters per token. The model prioritizes efficient inference and training cost-effectiveness through key innovations in architecture and training strategies.\n\n1.  **Architecture Enhancements**:\n    \n    *   **Multi-Head Latent Attention (MLA)** and **DeepSeekMoE**:\n        *   Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.\n        *   DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.\n    *   **Multi-Token Prediction (MTP)**:\n        *   Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.\n    *   The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.\n    \n    ![](../../../images/papers/DeepSeekV3.jpg)\n    \n2.  **Pre-Training Efficiency**:\n    *   Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.\n    *   Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.\n    *   Training completed within 2.788M H800 GPU hours, costing only $5.576M.\n3.  **Post-Training Innovations**:\n    *   Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.\n    *   Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.\n4.  **Evaluation and Performance**:\n    *   Benchmarked as the strongest open-source base model, excelling in:\n        *   Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).\n        *   Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.\n        *   Multilingual capabilities, especially strong in Chinese factual knowledge.\n5.  **Inference and Deployment**:\n    *   Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.\n    *   Implements redundancy and dynamic load balancing among experts to optimize inference throughput.\n\n*   **Multi-Head Latent Attention (MLA)** and **DeepSeekMoE**:\n    *   Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.\n    *   DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.\n*   **Multi-Token Prediction (MTP)**:\n    *   Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.\n*   The following figure from the paper illustrates the basic architecture of DeepSeek-V3. Following DeepSeek-V2, they adopt MLA and DeepSeekMoE for efficient inference and economical training.\n\n*   Refined from DeepSeek-V2, MLA reduces memory usage for attention by low-rank compression of key and value tensors.\n*   DeepSeekMoE optimizes Feed-Forward Networks using fine-grained experts with an auxiliary-loss-free load balancing mechanism.\n\n*   Extends prediction scope to multiple future tokens to improve data efficiency and model planning. Implemented sequentially while preserving causal chains for robust training.\n\n![](../../../images/papers/DeepSeekV3.jpg)\n\n*   Trained on 14.8 trillion diverse tokens using FP8 mixed-precision for computational efficiency.\n*   Novel optimizations in the DualPipe pipeline parallelism strategy and cross-node communication kernels minimize communication overhead.\n*   Training completed within 2.788M H800 GPU hours, costing only $5.576M.\n\n*   Combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages, including distillation from DeepSeek-R1 series models to improve reasoning capabilities.\n*   Incorporates a reward model and Group Relative Policy Optimization (GRPO) for aligning outputs with human preferences.\n\n*   Benchmarked as the strongest open-source base model, excelling in:\n    *   Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).\n    *   Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.\n    *   Multilingual capabilities, especially strong in Chinese factual knowledge.\n\n*   Educational benchmarks (e.g., MMLU: 88.5%, MMLU-Pro: 75.9%).\n*   Math and coding tasks, surpassing many closed-source models like GPT-4o on specific datasets.\n*   Multilingual capabilities, especially strong in Chinese factual knowledge.\n\n*   Deployed with distinct strategies for prefilling and decoding stages, leveraging Tensor Parallelism and Expert Parallelism across H800 clusters.\n*   Implements redundancy and dynamic load balancing among experts to optimize inference throughput.\n\n1.  **Architectural Details**:\n    *   MLA compresses key-value and query tensors for efficient attention.\n    *   DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.\n2.  **Training Framework**:\n    *   Uses **DualPipe** for overlapping computation and communication during pipeline parallelism.\n    *   Efficient memory management via recomputation, shared embeddings, and low-precision caching.\n3.  **FP8 Training**:\n    *   Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.\n    *   Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.\n\n*   MLA compresses key-value and query tensors for efficient attention.\n*   DeepSeekMoE combines shared and routed experts, dynamically adjusting load using bias terms to maintain performance.\n\n*   Uses **DualPipe** for overlapping computation and communication during pipeline parallelism.\n*   Efficient memory management via recomputation, shared embeddings, and low-precision caching.\n\n*   Fine-grained quantization and improved matrix multiplication precision enhance low-precision stability.\n*   Stored activations, optimizer states, and weights in BF16/FP8 to minimize memory usage.\n\n*   While performing well, challenges remain in fine-grained task specialization and scaling inference strategies for broader applications.\n*   Future research could explore advanced dynamic expert routing and further reducing training costs with emergent hardware.\n\n#### [Liberated-Qwen1.5](https://huggingface.co/abacusai/Liberated-Qwen1.5-72B)\n\n*   [Abacus.AI](https://abacus.ai/) has released an intriguing new open-source large language model called Liberated-Qwen1.5-72B. What makes this LLM unique is its strict adherence to system prompts, even when those prompts conflict with the user’s instructions.\n*   As enterprises adopt LLMs for uses like customer service chatbots, getting models to reliably follow set guidelines is crucial. Too often, existing LLMs can veer off in unexpected directions when conversing with users over multiple turns. Liberated-Qwen aims to solve this with its system prompt compliance.\n*   The team at Abacus fine-tuned the Alibaba Qwen1.5-72B model on a new 7K SystemChat dataset specifically designed to train obedience to system messages, even when contradicting user input. The result is an uncensored model that will execute directives like answering only in capitals.\n*   Initial tests show Liberated-Qwen performing slightly better than the original Qwen model on areas like coding (HumanEval) while maintaining strong general capabilities (MMLU scores). However, it lacks safety guardrails, so caution is advised before deployment.\n*   The team plans to further improve the model’s coding performance and release enhanced versions blending the SystemChat data with the training used for their previous Smaug release.\n*   This innovative approach could make Liberated-Qwen and its successors compelling options for businesses needing LLMs that prioritize obedience to rules and guidelines, but getting safe deployment right will be key.\n\n#### [Dolphin Llama](https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b)\n\n*   Dolphin is a model with 8B and 70B sizes by Eric Hartford based on Llama that has a variety of instruction, conversational, and coding skills.\n*   Dolphin has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\n*   Dolphin is uncensored. The dataset has been filtered to remove alignment and bias. This makes the model more compliant.\n\n#### [Command-R](https://txt.cohere.com/command-r/)\n\n*   Cohere’s Command-R is optimized for long context tasks such as retrieval augmented generation (RAG), using external APIs and tools. It boasts low latency, high accuracy, supports 10 languages, and has 128k context length.\n*   The model excels at 10 major languages of global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.\n*   [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r-v01)\n\n##### [Command R+](https://huggingface.co/CohereForAI/c4ai-command-r-plus)\n\n*   C4AI Command R+ is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks.\n*   C4AI Command R+ is a multilingual model evaluated in 10 languages for performance: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Arabic, and Simplified Chinese. Command R+ is optimized for a variety of use cases including reasoning, summarization, and question answering.\n*   [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r-plus); [Demo](https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus);\n\n#### [EagleX](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)\n\n*   [EagleX 1.7T](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b) is a early research release of a 7.52B parameter model training that is\n    *   Built on the [RWKV-v5 architecture](https://wiki.rwkv.com/), a linear transformer with 10-100x+ lower inference cost.\n    *   Is continuation based on the original [Eagle 7B](https://substack.recursal.ai/cp/141146731) model\n    *   Ranks as the world’s greenest 7B model (per token)\n    *   Trained on 1.7 Trillion tokens across 100+ languages\n    *   Outperforms all 7B class models in multi-lingual benchmarks\n    *   Passes LLaMA2 (2T) in multiple English evals, approaches Mistral (>2T?)\n    *   All while being an “Attention-Free Transformer”\n*   [Hugging Face](https://huggingface.co/recursal/EagleX_1-7T); [Hugging Face Demo](https://huggingface.co/spaces/recursal/EagleX-7B-1.7T-Gradio-Demo)\n\n*   Built on the [RWKV-v5 architecture](https://wiki.rwkv.com/), a linear transformer with 10-100x+ lower inference cost.\n*   Is continuation based on the original [Eagle 7B](https://substack.recursal.ai/cp/141146731) model\n*   Ranks as the world’s greenest 7B model (per token)\n*   Trained on 1.7 Trillion tokens across 100+ languages\n*   Outperforms all 7B class models in multi-lingual benchmarks\n*   Passes LLaMA2 (2T) in multiple English evals, approaches Mistral (>2T?)\n*   All while being an “Attention-Free Transformer”\n\n#### [Grok](https://x.ai/blog/grok-os)\n\n##### [Grok-1](https://x.ai/blog/grok-os)\n\n*   [xAI](https://x.ai) has released the base model weights and network architecture of Grok-1, a 314 billion parameter Mixture-of-Experts model trained from scratch by xAI using a custom training stack on top of JAX and Rust in October 2023.\n*   This is the raw base model checkpoint from the Grok-1 pre-training phase, which concluded in October 2023. This means that the model is not fine-tuned for any specific application, such as dialogue.\n*   [Code](https://github.com/xai-org/grok)\n\n##### [Grok-1.5](https://x.ai/blog/grok-1.5)\n\n*   Grok-1.5 is introduced as the latest advancement in long context understanding and advanced reasoning, promising availability to early testers and existing users on the X platform shortly. Following the release of Grok-1’s model weights and network architecture, xAI’s Grok-1.5 showcases enhanced reasoning and problem-solving capabilities, particularly highlighted in coding and math-related tasks.\n*   In performance benchmarks, Grok-1.5 demonstrated significant improvements by achieving a 50.6% score on the MATH benchmark, a 90% score on the GSM8K benchmark, and a 74.1% score on the HumanEval benchmark, showcasing its prowess in a wide range of mathematical and coding challenges.\n*   A notable feature of Grok-1.5 is its ability to process contexts up to 128,000 tokens, expanding its memory capacity significantly to handle information from longer documents. This is visualized through a graph indicating a 100% recall rate across varying context lengths, emphasizing the model’s robust information retrieval capacity even with extensive contexts.\n*   Grok-1.5’s infrastructure is based on a custom distributed training framework integrating JAX, Rust, and Kubernetes, designed for the demanding requirements of LLM research. This framework addresses challenges in training LLMs on large compute clusters by optimizing reliability, uptime, and efficient resource management through a custom training orchestrator and improvements in checkpointing, data loading, and training job restarts.\n\n#### [SaulLM](https://arxiv.org/abs/2403.03883)\n\n*   Proposed in [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883), SaulLM-7B, introduced by Colombo et al., is the first LLM with 7 billion parameters, designed specifically for the legal domain, based on the Mistral 7B architecture. It is trained on over 30 billion tokens from an English legal corpus, showing state-of-the-art proficiency in legal document comprehension and generation. Additionally, the paper introduces an instructional fine-tuning method using legal datasets to enhance SaulLM-7B’s performance on legal tasks, released under the MIT License.\n*   The creation of SaulLM-7B addresses the gap in specialized LLM applications within the legal field, marked by complex document volumes and unique linguistic challenges. The model’s pretraining incorporates extensive legal corpora from English-speaking jurisdictions, including the USA, Canada, the UK, and Europe, aiming to comprehend and adapt to the evolving legal discourse. This focus targets the needs of legal practitioners, representing a significant step towards integrating artificial intelligence within legal applications.\n*   SaulLM-7B’s family includes SaulLM-7B-Instruct, an instruction-tuned variant that outperforms models like Mistral and Llama on various legal tasks. This achievement is part of the paper’s contributions, which also introduce LegalBench-Instruct and model evaluation code & licensing under the MIT License. LegalBench-Instruct, a refined iteration of LegalBench, aims to better assess and refine legal language model proficiency, incorporating tasks from the MMLU benchmark related to international law, professional law, and jurisprudence.\n*   The paper details the data sources and preprocessing steps involved in constructing the training corpus, highlighting the combination of pre-existing datasets and new data scraped from the web. Rigorous data cleaning, deduplication, and the inclusion of “replay” sources to mitigate catastrophic forgetting during continued pretraining form the foundation of a robust 30 billion token corpus. Instruction fine-tuning mixes further refine the model’s ability to understand and follow legal instructions.\n*   Evaluation of SaulLM-7B involves comparing its performance against other open-source models using benchmarks like LegalBench-Instruct and Legal-MMLU. The results demonstrate SaulLM-7B’s superior proficiency in legal document processing and task performance. The perplexity analysis across different types of legal documents further confirms the model’s effectiveness in the legal domain.\n*   SaulLM-7B signifies a novel approach in the AI-driven assistance for legal professionals, aiming for widespread adoption and innovation across commercial and research endeavors in law. The release of SaulLM-7B under an open license encourages collaborative development and application in various legal contexts, setting a precedent for future advancements in AI-powered legal tools.\n*   The model is open-sourced and allows commercial use, inviting the legal sector and AI engineers to further tinker with legal LLMs.\n*   [Model](https://huggingface.co/Equall/Saul-Instruct-v1)\n\n#### [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n\n*   [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) is a new state-of-the-art Open LLM by Databricks Mosaic Research Team that achieves state-of-the-art performance across multiple benchmarks, surpassing existing open models like GPT-3.5 and showing competitive results against Gemini 1.0 Pro. DBRX is notable for its exceptional capabilities in coding tasks, even outperforming specialized models such as CodeLLaMA-70B.\n*   Key features and advancements of DBRX include:\n    \n    *   **Efficiency and Performance**: DBRX introduces significant efficiency improvements in training and inference through its fine-grained mixture-of-experts (MoE) architecture. It boasts inference speeds up to 2x faster than LLaMA2-70B and has about 40% fewer parameters compared to Grok-1, without compromising on model quality. The model demonstrates a quadruple reduction in computational requirements compared to its predecessors while maintaining similar performance levels.\n    *   **Architecture Innovations**: The model utilizes a transformer-based decoder-only structure with 132B total parameters, incorporating advanced techniques such as rotary position encodings, gated linear units, and grouped query attention. It’s pretrained on a diverse mix of 12T tokens from text and code, allowing for a maximum context length of 32k tokens. This architectural choice enables DBRX to efficiently handle a wide range of tasks with fewer parameters activated per input.\n    *   **Benchmark Performance**: DBRX sets new records on various benchmarks, including language understanding, programming, and mathematics, significantly outperforming other leading open models. It is also competitive with or surpasses leading closed models in nearly all considered benchmarks, particularly excelling in programming and mathematical reasoning. The plot below from the paper shows that DBRX outperforms established open source models on language understanding (MMLU), Programming (HumanEval), and Math (GSM8K).\n    \n    ![](assets/LLM/DBRX.jpg)\n    \n    *   **Open Access and Integration**: DBRX weights for both base and fine-tuned versions are openly available on Hugging Face, facilitating easy access for further experimentation and development. Databricks customers can utilize DBRX via APIs and have the option to pretrain or continue training DBRX-class models using Databricks tools and infrastructure.\n    *   **Training and Inference Efficiency**: The paper highlights DBRX’s training and inference efficiency, illustrating that MoE models like DBRX offer significant improvements in compute efficiency. It provides detailed comparisons in training efficiency, showing that models within the DBRX family require fewer FLOPs to reach similar or better performance scores compared to denser models. In terms of inference, DBRX achieves higher throughput than comparable non-MoE models, benefiting from the model’s efficient parameter usage.\n    *   **Development and Deployment**: The development of DBRX leveraged a robust suite of tools provided by Databricks, including data management, experiment tracking, and large-scale model training and finetuning services. This comprehensive toolset facilitated the efficient creation of DBRX and its integration into GenAI-powered products, demonstrating the practical application and scalability of Databricks’ infrastructure.\n*   DBRX represents a significant milestone in the development of open-source LLMs, offering a highly efficient, powerful, and accessible model for a wide range of applications. Its release underscores the potential of open models to drive innovation and democratize access to cutting-edge AI technologies.\n\n*   **Efficiency and Performance**: DBRX introduces significant efficiency improvements in training and inference through its fine-grained mixture-of-experts (MoE) architecture. It boasts inference speeds up to 2x faster than LLaMA2-70B and has about 40% fewer parameters compared to Grok-1, without compromising on model quality. The model demonstrates a quadruple reduction in computational requirements compared to its predecessors while maintaining similar performance levels.\n*   **Architecture Innovations**: The model utilizes a transformer-based decoder-only structure with 132B total parameters, incorporating advanced techniques such as rotary position encodings, gated linear units, and grouped query attention. It’s pretrained on a diverse mix of 12T tokens from text and code, allowing for a maximum context length of 32k tokens. This architectural choice enables DBRX to efficiently handle a wide range of tasks with fewer parameters activated per input.\n*   **Benchmark Performance**: DBRX sets new records on various benchmarks, including language understanding, programming, and mathematics, significantly outperforming other leading open models. It is also competitive with or surpasses leading closed models in nearly all considered benchmarks, particularly excelling in programming and mathematical reasoning. The plot below from the paper shows that DBRX outperforms established open source models on language understanding (MMLU), Programming (HumanEval), and Math (GSM8K).\n\n![](assets/LLM/DBRX.jpg)\n\n*   **Open Access and Integration**: DBRX weights for both base and fine-tuned versions are openly available on Hugging Face, facilitating easy access for further experimentation and development. Databricks customers can utilize DBRX via APIs and have the option to pretrain or continue training DBRX-class models using Databricks tools and infrastructure.\n*   **Training and Inference Efficiency**: The paper highlights DBRX’s training and inference efficiency, illustrating that MoE models like DBRX offer significant improvements in compute efficiency. It provides detailed comparisons in training efficiency, showing that models within the DBRX family require fewer FLOPs to reach similar or better performance scores compared to denser models. In terms of inference, DBRX achieves higher throughput than comparable non-MoE models, benefiting from the model’s efficient parameter usage.\n*   **Development and Deployment**: The development of DBRX leveraged a robust suite of tools provided by Databricks, including data management, experiment tracking, and large-scale model training and finetuning services. This comprehensive toolset facilitated the efficient creation of DBRX and its integration into GenAI-powered products, demonstrating the practical application and scalability of Databricks’ infrastructure.\n\n#### [Jamba](https://www.ai21.com/blog/announcing-jamba)\n\n*   Jamba is AI21’s Groundbreaking SSM-Transformer Model, which represents a novel leap in language model architecture by integrating Mamba Structured State Space (SSM) technology with the traditional Transformer model, creating the world’s first production-grade Mamba based model. This hybrid approach notably addresses the scalability and performance limitations of pure SSM or Transformer models, providing a substantial increase in efficiency and throughput. Key advancements include a 256K context window and the capacity to fit up to 140K context on a single GPU, marking it as a leader in its class.\n*   To capture the best that both Mamba and Transformer architectures have to offer, we developed the corresponding Joint Attention and Mamba (Jamba) architecture. Composed of Transformer, Mamba, and mixture-of-experts (MoE) layers, Jamba optimizes for memory, throughput, and performance – all at once – as depicted in the table below.\n\n![](/primers/ai/assets/LLM/Jamba1.jpg)\n\n*   The architecture of Jamba combines Transformer layers, Mamba layers, and mixture-of-experts (MoE) layers to optimize memory usage, computational throughput, and overall performance. One of the critical innovations is the use of MoE layers, allowing Jamba to selectively utilize just 12B out of its available 52B parameters during inference, making it significantly more efficient than a Transformer model of equivalent size.\n*   As depicted in the diagram below, AI21’s Jamba architecture features a blocks-and-layers approach that allows Jamba to successfully integrate the two architectures. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.\n\n![](/primers/ai/assets/LLM/Jamba2.jpg)\n\n*   Jamba has been scaled to a production-grade level, a feat previously unachieved by Mamba models beyond 3B parameters. Its architecture employs a blocks-and-layers design that alternates between attention or Mamba layers and multi-layer perceptrons (MLP), with a Transformer layer included for every eight total layers. This design is instrumental in optimizing the model for high-quality output and throughput on common hardware, such as a single 80GB GPU.\n*   Significant results have been observed in Jamba’s performance, with a 3x improvement in throughput on long contexts compared to similar models like Mixtral 8x7B, without compromising on efficiency. These achievements have been made possible by innovative engineering choices, including the strategic use of MoE layers to manage computational demands and the integration of Mamba with Transformer architectures for superior model capacity and efficiency.\n*   Jamba is released with open weights under Apache 2.0, encouraging further exploration and development within the AI community. Additionally, it’s made accessible via Hugging Face and is slated for inclusion in the NVIDIA API catalog, facilitating its adoption in enterprise applications through the NVIDIA AI Enterprise software platform.\n*   [Model](https://huggingface.co/ai21labs/Jamba-v0.1)\n\n##### [Jamba 1.5](https://www.ai21.com/blog/announcing-jamba-model-family)\n\n*   The Jamba 1.5 family, introduced by AI21, includes Mini and Large models. They leverage the innovative SSM-Transformer architecture for superior long context handling, speed, and quality.\n*   Jamba 1.5 models feature a 256K effective context window, the longest in the market, enhancing applications like document summarization, analysis, and RAG workflows.\n*   The models are up to 2.5 times faster on long contexts compared to competitors, making them ideal for high-demand enterprise use cases.\n*   Jamba 1.5 models, available under an open license, support multiple languages and can be integrated across various cloud platforms and on-premise setups.\n*   Utilizing the novel ExpertsInt8 quantization technique, these models offer efficiency in memory usage, allowing Jamba 1.5 Large to function with high performance on a single 8 GPU node.\n*   Benchmarks indicate that Jamba 1.5 Mini and Large outperform competitors in their respective size classes in both speed and quality, demonstrating outstanding value for enterprise applications.\n\n#### [WizardLM-2](https://huggingface.co/posts/WizardLM/329547800484476)\n\n*   WizardLM-2 is a series of three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B, which offer improved performance on complex chat, multilingual, reasoning and agent.\n*   WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.\n*   WizardLM 2 capabilities:\n    1.  MT-Bench (Figure 1) The WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary works such as GPT-4-Trubo and Glaude-3. Meanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.\n    2.  Human Preferences Evaluation (Figure 2) Through this human preferences evaluation, WizardLM-2’s capabilities are very close to the cutting-edge proprietary models such as GPT-4-1106-preview, and significantly ahead of all the other open source models.\n*   WizardLM-2 uses a fully AI powered synthetic training system as shown below:\n\n1.  MT-Bench (Figure 1) The WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary works such as GPT-4-Trubo and Glaude-3. Meanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.\n2.  Human Preferences Evaluation (Figure 2) Through this human preferences evaluation, WizardLM-2’s capabilities are very close to the cutting-edge proprietary models such as GPT-4-1106-preview, and significantly ahead of all the other open source models.\n\n![](/primers/ai/assets/LLM/WizardLM-2.png)\n\n*   [Blog](https://wizardlm.github.io/WizardLM2)\n\n#### [Gemini](https://arxiv.org/abs/2312.11805)\n\n*   Proposed in [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805).\n*   Google’s Gemini series represents a milestone in AI development, featuring three models: Ultra, Pro, and Nano, each tailored for specific tasks ranging from complex problem-solving to on-device operations. Gemini Ultra, the flagship model, excels in demanding tasks and sets new benchmarks in AI performance. Gemini Pro is optimized for a wide range of tasks, while Nano is designed for efficiency in on-device applications. This suite of models, part of Google DeepMind’s vision, marks a significant scientific and engineering endeavor for the company.\n*   Gemini models are built with a transformative architecture that allows for a “deep fusion” of modalities, surpassing the capabilities of typical modular AI designs. This integration enables seamless concept transfer across various domains, such as vision and language. The models, trained on TPUs, support a 32k context length and are capable of handling diverse inputs and outputs, including text, vision, and audio. The visual encoder, inspired by Flamingo, and the comprehensive training data, comprising web documents, books, code, and multimedia, contribute to the models’ versatility.\n*   The figure below from the paper illustrates that Gemini supports interleaved sequences of text, image, audio, and video as inputs (illustrated by tokens of different colors in the input sequence). It can output responses with interleaved image and text.\n\n![](../../../images/papers/Gemini1.jpg)\n\n*   The training infrastructure for Gemini utilizes Google’s latest TPU v4 and v5e accelerators, ensuring efficient scaling and reliable performance at an unprecedented scale. This advanced setup is integral to handling hardware failures and silent data corruption, ensuring high-quality training outcomes.\n*   The training dataset is multimodal and multilingual, with quality and safety filters to enhance model performance. The dataset mix is adjusted during training to emphasize domain-relevant data, contributing to the models’ high performance.\n*   Gemini Ultra showcases extraordinary capabilities across various benchmarks, surpassing GPT-4 in areas like coding and reasoning. Its performance in benchmarks like HumanEval and Natural2Code, as well as its superior reasoning capabilities in complex subjects like math and physics, demonstrate its state-of-the-art capabilities. For instance, the figure below from the paper shows solving a geometrical reasoning task. Gemini shows good understanding of the task and is able to provide meaningful reasoning steps despite slightly unclear instructions.\n\n![](../../../images/papers/Gemini2.jpg)\n\n*   Furthermore, in another instance, the figure below from the paper shows Gemini verifying a student’s solution to a physics problem. The model is able to correctly recognize all of the handwritten content and verify the reasoning. On top of understanding the text in the image, it needs to understand the problem setup and correctly follow instructions to generate LaTeX.\n\n![](../../../images/papers/Gemini3.jpg)\n\n*   Gemini outperforms OpenAI’s GPT-4 in 30 out of 32 benchmarks. Furthermore, it’s worth noting is that Gemini Ultra is the first model to outperform human experts on MMLU (massive multitask language understanding). The following table from Google’s [blog](https://blog.google/technology/ai/google-gemini-ai/) Gemini surpasses state-of-the-art performance on a range of benchmarks including text and coding.\n\n![](../../../images/papers/Gemini4.jpg)\n\n*   For image understanding, Gemini Ultra sets new standards by outperforming existing models in zero-shot evaluations for OCR-related tasks. Its native multimodality and complex reasoning abilities enable it to excel in interpreting and reasoning with visual information. The following table from Google’s [blog](https://blog.google/technology/ai/google-gemini-ai/) Gemini surpasses state-of-the-art performance on a range of multimodal benchmarks.\n\n![](../../../images/papers/Gemini5.jpg)\n\n*   Gemini’s training involves Reinforcement Learning from Human Feedback (RLHF), enhancing its performance and capabilities. This advanced training, combined with its innovative architecture and diverse dataset, contributes to its exceptional performance across various tasks.\n*   Despite its remarkable capabilities, specific details about Gemini’s architecture, training data, and the size of the Ultra and Pro models remain undisclosed. However, the models represent a significant leap in AI development, driven by the promise of AI to benefit humanity in diverse ways.\n*   Safety and responsibility are central to Gemini’s development, with comprehensive safety evaluations for bias and toxicity. Google is collaborating with external experts and partners to stress-test the models and ensure they adhere to robust safety policies, aligning with Google’s AI Principles.\n*   Gemini’s capabilities and its development approach reflect Google’s commitment to advancing AI responsibly and ethically, emphasizing safety and collaboration with the industry and broader ecosystem to define best practices and safety benchmarks.\n*   [Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf); [Blog](https://blog.google/technology/ai/google-gemini-ai/).\n\n#### [Gemma](https://arxiv.org/abs/2403.08295)\n\n*   Proposed in [Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/abs/2403.08295).\n*   This paper by Gemma Team from Google DeepMind introduces Gemma, a family of open models based on the Gemini model architecture. It comprises two versions: a 7 billion parameter model for GPU and TPU applications, and a 2 billion parameter model suited for CPU and on-device implementations. Both models are trained using up to 6 trillion tokens from primarily English sources, focusing on web documents, mathematics, and code, with a tokenizer that supports a large vocabulary size of 256k entries.\n*   The models utilize advanced techniques including Multi-Query Attention, RoPE Embeddings, GeGLU Activations, and RMSNorm. These improvements aim to enhance the model’s performance and efficiency, particularly in processing long sequences up to 8192 tokens.\n*   Training infrastructure involves extensive use of TPUv5e across multiple pods, with specific configurations for different model scales. The training also incorporates techniques from Google’s earlier projects like Pathways and Jax to manage data efficiently across distributed systems.\n*   A substantial focus of the Gemma project is on responsible and safe deployment. This includes rigorous filtering of the training data to avoid sensitive or harmful content, and a detailed evaluation of the models against various safety and performance benchmarks.\n*   The figure below from the paper illustrates the language understanding and generation performance of Gemma 7B across different capabilities compared to similarly sized open models. They group together standard academic benchmark evaluations by capability and average the respective scores.\n\n![](../../../images/papers/Gemma.jpg)\n\n*   Gemma models have shown superior performance on a range of tasks, outperforming other models in benchmarks for question answering, reasoning, math/science, and coding. They also display robust safety features, evaluated through automated benchmarks and human preference studies, ensuring that they behave predictably and safely in diverse applications.\n*   The models are also equipped with capabilities for supervised fine-tuning and reinforcement learning from human feedback, enabling them to improve over time based on specific user interactions and feedback. This adaptability makes them suitable for a wide array of applications, from automated customer support to sophisticated data analysis tasks.\n*   Despite their capabilities, the models come with an acknowledgment of their limitations, particularly in terms of their potential use in generating sensitive or misleading information. DeepMind emphasizes the importance of continuous monitoring and evaluation to mitigate any unintended consequences of their use.\n\n##### [Gemma 2: Improving Open Language Models at a Practical Size](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)\n\n*   This paper by Gemma Team at Google DeepMind introduces Gemma 2, a family of lightweight, state-of-the-art open language models ranging from 2B to 27B parameters (that can fit on a single GPU). The models, particularly the 9 billion and 27 billion parameter versions, incorporate several architectural advancements and are trained with knowledge distillation, which enhances their performance and efficiency.\n*   The Gemma 2 models are built on a decoder-only transformer architecture with parameters summarized as follows:\n    *   2.6B parameters: 26 layers, d\\_model 2304, GeGLU non-linearity, 18432 feedforward dimension, 8 attention heads.\n    *   9B parameters: 42 layers, d\\_model 3584, GeGLU non-linearity, 28672 feedforward dimension, 16 attention heads.\n    *   27B parameters: 46 layers, d\\_model 4608, GeGLU non-linearity, 73728 feedforward dimension, 32 attention heads.\n*   The core architectural innovations include the interleaving of local and global attention mechanisms and the use of Grouped-Query Attention (GQA). Specifically, local sliding window attention handles sequences of 4096 tokens, while global attention spans 8192 tokens. Logit soft-capping is employed to stabilize the training, ensuring the logits stay within defined bounds. Both pre-norm and post-norm with RMSNorm are used to normalize inputs and outputs of transformer sub-layers, improving training stability.\n*   The models are trained on extensive datasets with the 27B model trained on 13 trillion primarily-English tokens, the 9B model on 8 trillion tokens, and the 2.6B model on 2 trillion tokens. The training infrastructure involves TPUv4 and TPUv5 configurations, leveraging significant parallelization and sharding techniques to handle the computational load efficiently.\n*   Knowledge distillation is a pivotal part of the training process for Gemma 2 models. Smaller models are trained using the probability distributions provided by a larger model, enhancing their learning efficiency and enabling them to simulate training on a much larger token corpus than what is physically available. This method not only reduces training time but also allows smaller models to achieve performance levels competitive with significantly larger models.\n*   In post-training, the models undergo supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). The SFT phase uses a mix of synthetic and real prompt-response pairs, predominantly generated by a larger teacher model. The RLHF phase involves training a reward model based on human-labeled preference data to fine-tune the policy further, enhancing the models’ conversational capabilities and safety.\n*   Empirical evaluations show that the Gemma 2 models outperform previous versions and competitive models in various benchmarks, including MMLU, GSM8K, and ARC-C. The models are tested for their robustness to formatting variations, safety, and alignment, showcasing significant improvements in these areas.\n*   Safety and responsible deployment are critical components of the Gemma 2 project. The models are rigorously tested and filtered to prevent harmful content generation, and extensive safety mechanisms are integrated into the training process. This approach ensures that the models align with Google’s safety policies, mitigating risks associated with malicious use.\n*   The performance of the Gemma 2 models is on par with models twice or more their size! Model weights are open-source, thanks to Google DeepMind.\n*   **Key architectural components**:\n    1.  **Grouped Query Attention (GQA)**: The key difference between GQA and the standard Multi-headed attention is the reduction in the number of key and value heads, effectively grouping heads together. This approach balances between MHA and MQA, maintaining efficiency and performance.:\n    2.  **Sliding Window Attention (SWA)**: The authors interleaved local and global attentions in alternating layers. This technique reduces the number of parameters while maintaining performance. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.:\n    3.  **Rotary Position Embeddings (RoPE)**: RoPE encodes absolute position with a rotation matrix and incorporates relative position dependency in self-attention formulation, enabling sequence length flexibility and improved model performance. This is standard with prevalent LLMs.\n    4.  **Logit soft-capping**: Logit soft-capping stabilizes training by ensuring logits stay within defined bounds. This technique is implemented using a scaled tanh function, capping attention logits at 50.0 and final logits at 30.0.:\n    5.  **Model merging**: Model merging involves averaging models from experiments run with different hyperparameters, improving overall performance. Techniques like Exponential Moving Average (EMA), Spherical Linear Interpolation (SLERP), and Linear Interpolation Towards Initialization (LITI) are employed during the merging process.\n    6.  **Knowledge distillation for training 2B and 9B models (instead of next token prediction)**: The 2B and 9B models in Gemma 2 are trained using knowledge distillation, where a larger, pre-trained model (the teacher) provides probability distributions over the vocabulary for each token. Instead of predicting the next token using one-hot vectors, the smaller models (students) learn from these richer distributions, using the Kullback-Leibler (KL) divergence as the loss function. The gradients derived from the richer, softer probability distributions help the student model to learn more effectively than from hard one-hot vectors due to the nuanced signal provided by the teacher model. This method improves performance and efficiency, allowing smaller models to achieve results comparable to much larger models.\n*   In conclusion, Gemma 2 represents a significant advancement in the development of open language models, offering competitive performance at a practical size. The use of knowledge distillation and advanced attention mechanisms provides a pathway for smaller models to achieve high performance, making state-of-the-art language modeling more accessible to the community.\n*   [Hugging Face](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315); [Aman Arora’s Blog](https://amaarora.github.io/posts/2024-07-07%20Gemma.html)\n\n*   2.6B parameters: 26 layers, d\\_model 2304, GeGLU non-linearity, 18432 feedforward dimension, 8 attention heads.\n*   9B parameters: 42 layers, d\\_model 3584, GeGLU non-linearity, 28672 feedforward dimension, 16 attention heads.\n*   27B parameters: 46 layers, d\\_model 4608, GeGLU non-linearity, 73728 feedforward dimension, 32 attention heads.\n\n1.  **Grouped Query Attention (GQA)**: The key difference between GQA and the standard Multi-headed attention is the reduction in the number of key and value heads, effectively grouping heads together. This approach balances between MHA and MQA, maintaining efficiency and performance.:\n2.  **Sliding Window Attention (SWA)**: The authors interleaved local and global attentions in alternating layers. This technique reduces the number of parameters while maintaining performance. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.:\n3.  **Rotary Position Embeddings (RoPE)**: RoPE encodes absolute position with a rotation matrix and incorporates relative position dependency in self-attention formulation, enabling sequence length flexibility and improved model performance. This is standard with prevalent LLMs.\n4.  **Logit soft-capping**: Logit soft-capping stabilizes training by ensuring logits stay within defined bounds. This technique is implemented using a scaled tanh function, capping attention logits at 50.0 and final logits at 30.0.:\n5.  **Model merging**: Model merging involves averaging models from experiments run with different hyperparameters, improving overall performance. Techniques like Exponential Moving Average (EMA), Spherical Linear Interpolation (SLERP), and Linear Interpolation Towards Initialization (LITI) are employed during the merging process.\n6.  **Knowledge distillation for training 2B and 9B models (instead of next token prediction)**: The 2B and 9B models in Gemma 2 are trained using knowledge distillation, where a larger, pre-trained model (the teacher) provides probability distributions over the vocabulary for each token. Instead of predicting the next token using one-hot vectors, the smaller models (students) learn from these richer distributions, using the Kullback-Leibler (KL) divergence as the loss function. The gradients derived from the richer, softer probability distributions help the student model to learn more effectively than from hard one-hot vectors due to the nuanced signal provided by the teacher model. This method improves performance and efficiency, allowing smaller models to achieve results comparable to much larger models.\n\n##### [Gemma 3](https://blog.google/technology/developers/gemma-3/)\n\n*   Gemma 3 is a family of open-source AI models, designed for high performance and portability. It builds on the success of the original Gemma, which saw over 100 million downloads and a thriving developer community. With sizes ranging from 1B to 27B, Gemma 3 models are optimized to run efficiently across devices—from phones to workstations—while supporting advanced capabilities like text and visual reasoning, function calling, and multilingual applications.\n*   Gemma 3 includes quantized versions for lower compute requirements and offers a massive 128k-token context window. It outperforms models like Llama3-405B and DeepSeek-V3 in human preference benchmarks. ShieldGemma 2, a 4B image safety model, is also released to ensure responsible AI use in visual applications.\n*   [Hugging Face](https://huggingface.co/google/gemma-3-12b-it)\n\n#### [JetMoE](https://arxiv.org/abs/2404.07413)\n\n*   Proposed in [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](https://arxiv.org/abs/2404.07413) by Shen et al. from MIT-IBM Watson AI Lab, MIT EECS, Princeton University, and MyShell.ai & MIT, JetMoE-8B is a cost-effective large language model, outperforming established models like Llama2-7B and Llama2-13B-Chat. JetMoE-8B extends the concept of sparse activation to both the attention and feed-forward layers. Despite being trained on a tight budget of under $100,000, JetMoE-8B employs 8 billion parameters, leveraging a Sparsely-gated Mixture-of-Experts (SMoE) architecture that activates only 2 billion parameters per input token. This architecture reduces inference computation by approximately 70% compared to Llama2-7B.\n*   JetMoE-8B is trained using the Megatron framework with Megablock enhancements, using pipeline parallelism to optimize computational costs and load balance during training. Notably, it incorporates innovations like shared KV projection in attention layers and a frequency-based auxiliary loss for training efficiency.\n*   The figure below from the paper illustrates the JetMoE architecture.\n\n![](../../../images/papers/JetMoE.jpg)\n\n*   For pretraining, JetMoE-8B utilized a mixture of real-world and synthetic datasets, totaling 1.25 trillion tokens. Datasets include RefinedWeb, StarCoder, and various components from The Pile, combined with synthetic datasets like OpenHermes 2.5 for diverse training inputs.\n*   Utilized a two-phase training approach, incorporating a mix of real and synthetic datasets with adjustments in data weighting during the learning rate decay phase to enhance model performance.\n*   The model underwent Distilled Supervised Fine-Tuning (dSFT) and Distilled Direct Preference Optimization (dDPO), refining model responses based on preferences from a teacher model to improve alignment with human-like conversational abilities.\n*   JetMoE-8B’s performance was benchmarked against other models in tasks like ARC-challenge, Hellaswag, and MMLU, showing superior performance in many areas, particularly in code-related benchmarks like MBPP and HumanEval.\n*   The training parameters, model configurations, and data mixtures are fully documented and made open-source to foster further academic and practical advancements in efficient LLM training methodologies.\n*   [Code](https://github.com/myshell-ai/JetMoE)\n\n#### [Minimax-Text](https://www.minimaxi.com/en/news/minimax-01-series-2)\n\n*   Minimax-Text from Hailuo has perfect needle in haystack recall on 4M token context! — 10x cheaper than GPT4o ($0.2/M input and $1.1/M output tokens) — Benchmarks on par with SOTA models — 456B param MoE with ~46B active\n*   [Tech report](https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf), [Demo](https://www.hailuo.ai/)",
    "order": 41,
    "orderInChapter": 1,
    "difficulty": 5,
    "estimatedMinutes": 167,
    "tags": [
      "nlpllms",
      "neural network",
      "machine learning",
      "transformer",
      "attention",
      "embedding",
      "bert",
      "gpt"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 33356,
      "contentLength": 377316
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#popular-foundation-llms",
    "scrapedAt": "2025-12-28T11:53:26.176Z"
  },
  {
    "id": "ai-LLM-popular-slms-42",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Popular LLMs",
    "title": "Popular SLMs",
    "subtitle": "Popular LLMs",
    "contentHtml": "<h4 id=\"phi\">Phi</h4>\n<h5 id=\"phi-1\"><a href=\"https://arxiv.org/abs/2306.11644\">Phi-1</a></h5>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2306.11644\">Textbooks Are All You Need</a> by Gunasekar from Microsoft Research, phi-1 is a large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality” data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens).</li>\n  <li>Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, the model before their finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.</li>\n  <li>They demonstrate that increasing layer count and sacrificing computational cost is not the only approach to increase LLM accuracy, but instead focusing on data quality can also result in a significant accuracy boost – reinforcing the fact that a data-centric approach also helps in making your model better.</li>\n</ul>\n<p><img src=\"../../../images/papers/Textbooks.jpg\" alt=\"\"></p>\n<h5 id=\"phi-15\"><a href=\"https://arxiv.org/abs/2309.05463\">Phi-1.5</a></h5>\n<ul>\n  <li>Proposed in Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Li et al. from Microsoft Research, phi-1.5 extends previous work on Transformer-based language models. It continues the exploration of the power of smaller models, following up on the phi-1, a 1.3 billion parameter model. The phi-1.5 model, also with 1.3 billion parameters, emphasizes “textbook quality” data for training, diverging from traditional web data usage. This approach aims to enhance common sense reasoning in natural language and achieves performance comparable to models five times its size. It excels in complex reasoning tasks like grade-school mathematics and basic coding, while showing characteristics of larger models, including the ability to think step by step and some in-context learning capabilities. However, it also shares some negative traits like hallucinations and potential for biased outputs, though improvements are noted in the absence of web data.</li>\n  <li>Phi-1.5 shares the same architecture as phi-1: a Transformer with 24 layers, 32 heads (each head with dimension 64), rotary embedding, context length of 2048, and flash-attention for training speedup. The training data comprises 7 billion tokens from phi-1’s data and 20 billion tokens of new synthetic data focusing on common sense reasoning. Additionally, two variants, phi-1.5-web-only and phi-1.5-web, were created to investigate the value of traditional web data. Phi-1.5-web-only was trained purely on 95 billion tokens of filtered web data, while phi-1.5-web used a mix of filtered web data, phi-1’s code data, and synthetic NLP data.</li>\n  <li>Phi-1.5 was evaluated on standard natural language benchmarks including common sense reasoning, language understanding, mathematics, and coding. It achieved results comparable to models like Llama2-7B, Falcon-7B, and Vicuna-13B in these benchmarks. In reasoning tasks, including elementary school math and entry-level Python coding, phi-1.5 outperformed all existing models, including Llama 65B. The addition of web data in phi-1.5-web showed significant improvements on these reasoning tasks.</li>\n  <li>The plot below from the paper illustrates benchmark results comparing phi-1.5, its version enhanced with filtered web data phi-1.5-web, and other state-of-the-art open-source LLMs. Sizes range from phi-1.5’s 1.3 billion parameters (Falcon-RW-1.3B) to 10x larger models like Vicuna-13B, a fine-tuned version of Llama-13B). Benchmarks are broadly classified into three categories: common sense reasoning, language skills, and multi-step reasoning. The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning, it arguably relies more on “memorized knowledge”. One can see that phi-1.5 models perform comparable in common sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning. Note that the numbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ slightly from numbers reported elsewhere.</li>\n</ul>\n<p><img src=\"../../../images/papers/phi-1.5.jpg\" alt=\"\"></p>\n<ul>\n  <li>The paper acknowledges the challenge of toxic and biased content generation in language models. To assess this, they used the ToxiGen dataset and crafted 86 prompts to test the model’s responses. Phi-1.5 showed an improvement over models like Llama2-7B and Falcon-7B, passing more prompts and failing fewer compared to these models.</li>\n  <li>Phi-1.5 and phi-1.5-web, despite not undergoing instruction-based finetuning, demonstrated the ability to comprehend and execute basic human instructions and chat capabilities. This ability is attributed to the synthetic textbook data used in training, which included exercises and answers. The paper describes standard prompting techniques and showcases the model’s flexibility in natural language processing and code generation.</li>\n</ul>\n<h5 id=\"phi-2\"><a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\">Phi-2</a></h5>\n<ul>\n  <li>Microsoft’s Research team has been addressing inefficiencies in LLMs, specifically the trade-off between size and performance. Smaller models traditionally underperform in tasks like coding, common-sense reasoning, and language understanding compared to their larger counterparts. By advancing a suite of Small Language Models (SLMs), named “Phi”, Microsoft aims to bridge this performance gap, ensuring that more compact models can still deliver high levels of accuracy and utility in various applications.</li>\n  <li>Introduced in <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\">Phi-2: The surprising power of small language models</a> by Javaheripi and Bubeck. The article details Microsoft Research’s release of Phi-2, a 2.7 billion-parameter language model. This model is part of the “Phi” series of SLMs, including Phi-1 (1.3 billion parameters) and Phi-1.5 (also 1.3 billion parameters). Phi-2 stands out for its exceptional capabilities, achieving equivalent language understanding capabilities to models 5x larger and matching reasoning capabilities of models up to 25x larger.</li>\n  <li>The Phi series of models scale down the number of parameters without a proportional loss in performance. Phi-1 showcased this in coding benchmarks, performing on par with larger models. With Phi-1.5 and the latest Phi-2, Microsoft has implemented novel model scaling techniques and refined training data curation to achieve results comparable to models many times their size. The success of Phi-2, a 2.7 billion-parameter language model, signifies a leap in optimization that allows it to demonstrate state-of-the-art reasoning and language understanding, matching or exceeding models with up to 25 times more parameters.</li>\n  <li>Phi-2’s success relies on two core strategies: firstly, Phi-2’s prowess stems from a relentless focus on high-quality “textbook-quality” data, integrating synthetic datasets designed to impart common sense reasoning and general knowledge. Thus, highly selected/curated/generated data used in Phi-2’s training to educate the model on some specific foundational capabilities (e.g., common sense reasoning, problem solving, math, etc) is central to Phi-2’s exceptional performance. Secondly, it utilizes innovative scaling techniques by building upon the knowledge embedded in the 1.3 billion parameter Phi-1.5, employing scaled knowledge transfer for enhanced performance and faster training convergence. By valuing textbook-caliber content and embedding knowledge from its predecessor Phi-1.5, Phi-2 emerges as a powerhouse in reasoning and comprehension. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores, as shown in the graphs from the blog below.</li>\n</ul>\n<p><img src=\"../../../images/papers/Phi-2.jpg\" alt=\"\"></p>\n<ul>\n  <li>The model, which is a Transformer-based model, was trained on 1.4 trillion tokens from a mix of Synthetic and Web datasets, over 14 days using 96 A100 GPUs. Notably, Phi-2 has not undergone alignment through reinforcement learning from human feedback (RLHF) or been instruct fine-tuned, yet demonstrates improved behavior regarding toxicity and bias.</li>\n  <li>Phi-2 is so small that it can run on a device, thus opening the door to a bunch of very interesting edge scenarios where latency or data sensitivity (e.g., for personalization) is paramount.</li>\n  <li>Phi-2’s performance is highlighted in several benchmarks, including Big Bench Hard (BBH), commonsense reasoning, language understanding, math, and coding tasks, often surpassing or matching other models like Mistral, Llama-2, and Gemini Nano 2 despite its smaller size.</li>\n  <li>Additionally, the article presents Phi-2’s proficiency in practical applications, such as solving physics problems and correcting student errors, showcasing its potential beyond benchmark tasks.</li>\n  <li>The research underlines the significance of quality training data and strategic model scaling in achieving high performance with smaller models, challenging conventional beliefs about language model scaling laws.</li>\n  <li>Smaller yet high-performing models like Phi-2 present an ideal testbed for experiments in mechanistic interpretability and safety, reducing the computational resources required for fine-tuning and exploring new tasks. Their more manageable size also makes them suitable for applications where deploying larger models is impractical. The ongoing work from Microsoft Research signals continuous improvements in SLMs, which could redefine industry benchmarks and open new avenues for widespread adoption of sophisticated AI tools in diverse fields.</li>\n  <li><a href=\"https://huggingface.co/microsoft/phi-2\">Hugging Face</a>; <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\">Blog</a></li>\n</ul>\n<h5 id=\"phi-35\"><a href=\"https://aka.ms/phi3.5-techblog\">Phi-3.5</a></h5>\n<ul>\n  <li>Phi-3.5-Mini-Instruct (3.8B parameters) and MoE-Instruct (42B parameters) are MIT-licensed.</li>\n  <li><a href=\"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\">Phi-3.5-MoE-Instruct</a>:\n    <ul>\n      <li>Trained on 4.9T tokens of high-quality data.</li>\n      <li>Outperforms Llama 3.1 8B across benchmarks; competitive with GPT4o mini.</li>\n      <li>Phi-3.5-MoE has 42B parameters, with 6.6B activated during generation.</li>\n      <li>Outperforms several larger models in reasoning capability, only behind GPT-4o-mini.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://huggingface.co/microsoft/Phi-3.5-mini-instruct\">Phi-3.5-Mini-Instruct</a>:\n    <ul>\n      <li>Trained on 3.4T tokens of high-quality data.</li>\n      <li>Outperforms Llama3.1 8B and Mistral 7B; competitive with Mistral NeMo 12B.</li>\n    </ul>\n  </li>\n  <li>Only instruct models were released; no base models.</li>\n  <li>Long-context support up to 128k.</li>\n  <li><a href=\"https://huggingface.co/microsoft/Phi-3.5-mini-instruct\">Hugging Face: Phi-3.5-Mini-Instruct</a>; <a href=\"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\">Hugging Face: Phi-3.5-MoE-Instruct</a></li>\n</ul>\n<ul>\n      <li>Trained on 4.9T tokens of high-quality data.</li>\n      <li>Outperforms Llama 3.1 8B across benchmarks; competitive with GPT4o mini.</li>\n      <li>Phi-3.5-MoE has 42B parameters, with 6.6B activated during generation.</li>\n      <li>Outperforms several larger models in reasoning capability, only behind GPT-4o-mini.</li>\n    </ul>\n<ul>\n      <li>Trained on 3.4T tokens of high-quality data.</li>\n      <li>Outperforms Llama3.1 8B and Mistral 7B; competitive with Mistral NeMo 12B.</li>\n    </ul>\n<h5 id=\"phi-4\"><a href=\"https://arxiv.org/abs/2412.08905\">Phi-4</a></h5>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2412.08905\">Phi-4 Technical Report</a> by Abdin et al., phi-4 is a 14-billion parameter LLM, which excels at tasks requiring complex reasoning, with significant improvements over earlier models. Its integration of synthetic data is a core innovation that enhances performance, particularly in academic and STEM-related benchmarks.</li>\n  <li><strong>Overview</strong>:\n    <ul>\n      <li><strong>phi-4</strong> focuses on data quality, particularly incorporating synthetic data into the pre-training and mid-training processes, as opposed to relying solely on organic data sources (like web content or code).</li>\n      <li>The model outperforms its teacher model, GPT-4, especially in reasoning-focused tasks (like STEM-based QA) and demonstrates strong performance on academic benchmarks.</li>\n      <li>Despite using minimal changes to the phi-3 architecture, phi-4 achieves better results thanks to improved training data, synthetic data generation techniques, and post-training innovations.</li>\n    </ul>\n  </li>\n  <li><strong>Training Process</strong>:\n    <ol>\n      <li><strong>Synthetic Data Integration:</strong>\n        <ul>\n          <li>phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.</li>\n          <li>This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.</li>\n        </ul>\n      </li>\n      <li><strong>Training Phases:</strong>\n        <ul>\n          <li><strong>Pretraining:</strong> The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.</li>\n          <li><strong>Midtraining:</strong> Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.</li>\n          <li><strong>Post-Training:</strong> Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.</li>\n        </ul>\n      </li>\n      <li><strong>Data Composition and Mixtures:</strong>\n        <ul>\n          <li>The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).</li>\n          <li>The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Model Architecture</strong>:\n    <ul>\n      <li>phi-4 is based on a decoder-only transformer architecture, with a context length extended during midtraining. The architecture largely mirrors phi-3-medium, with updates in the tokenizer and a shift to a full attention mechanism over a 4K context.</li>\n    </ul>\n  </li>\n  <li><strong>Performance</strong>:\n    <ul>\n      <li>phi-4 demonstrates remarkable performance on reasoning tasks, surpassing even much larger models in domains like STEM Q&amp;A (e.g., GPQA) and math competitions (e.g., MATH). It also outperforms GPT-4 on various benchmarks.</li>\n      <li>The model is especially strong in areas like coding (HumanEval) and mathematical reasoning, even when compared to models like Llama-3 and Qwen.</li>\n      <li>SimpleQA, DROP, and IFEval are areas where phi-4 shows weaknesses, notably in instruction-following and strict adherence to format.</li>\n      <li>The plot below from the paper illustrates\tthe average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. t = 0.5 was chosen to follow simple-evals. Error bars are 2σ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to non–open-weight models.</li>\n    </ul>\n\n    <p><img src=\"../../../images/papers/phi-4.jpg\" alt=\"\"></p>\n  </li>\n  <li><strong>Safety and Ethics</strong>:\n    <ul>\n      <li>phi-4 was developed in line with Microsoft’s Responsible AI principles. The team performed red-teaming exercises to evaluate and improve safety measures, addressing potential risks in harmful behaviors, bias, and hallucinations.</li>\n      <li>Techniques like Pivotal Token Search (PTS) were used to mitigate errors in reasoning-heavy tasks, ensuring the model produces better outputs by targeting crucial decision-making tokens.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>phi-4</strong> focuses on data quality, particularly incorporating synthetic data into the pre-training and mid-training processes, as opposed to relying solely on organic data sources (like web content or code).</li>\n      <li>The model outperforms its teacher model, GPT-4, especially in reasoning-focused tasks (like STEM-based QA) and demonstrates strong performance on academic benchmarks.</li>\n      <li>Despite using minimal changes to the phi-3 architecture, phi-4 achieves better results thanks to improved training data, synthetic data generation techniques, and post-training innovations.</li>\n    </ul>\n<ol>\n      <li><strong>Synthetic Data Integration:</strong>\n        <ul>\n          <li>phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.</li>\n          <li>This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.</li>\n        </ul>\n      </li>\n      <li><strong>Training Phases:</strong>\n        <ul>\n          <li><strong>Pretraining:</strong> The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.</li>\n          <li><strong>Midtraining:</strong> Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.</li>\n          <li><strong>Post-Training:</strong> Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.</li>\n        </ul>\n      </li>\n      <li><strong>Data Composition and Mixtures:</strong>\n        <ul>\n          <li>The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).</li>\n          <li>The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.</li>\n          <li>This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.</li>\n        </ul>\n<ul>\n          <li><strong>Pretraining:</strong> The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.</li>\n          <li><strong>Midtraining:</strong> Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.</li>\n          <li><strong>Post-Training:</strong> Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.</li>\n        </ul>\n<ul>\n          <li>The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).</li>\n          <li>The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.</li>\n        </ul>\n<ul>\n      <li>phi-4 is based on a decoder-only transformer architecture, with a context length extended during midtraining. The architecture largely mirrors phi-3-medium, with updates in the tokenizer and a shift to a full attention mechanism over a 4K context.</li>\n    </ul>\n<ul>\n      <li>phi-4 demonstrates remarkable performance on reasoning tasks, surpassing even much larger models in domains like STEM Q&amp;A (e.g., GPQA) and math competitions (e.g., MATH). It also outperforms GPT-4 on various benchmarks.</li>\n      <li>The model is especially strong in areas like coding (HumanEval) and mathematical reasoning, even when compared to models like Llama-3 and Qwen.</li>\n      <li>SimpleQA, DROP, and IFEval are areas where phi-4 shows weaknesses, notably in instruction-following and strict adherence to format.</li>\n      <li>The plot below from the paper illustrates\tthe average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. t = 0.5 was chosen to follow simple-evals. Error bars are 2σ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to non–open-weight models.</li>\n    </ul>\n<p><img src=\"../../../images/papers/phi-4.jpg\" alt=\"\"></p>\n<ul>\n      <li>phi-4 was developed in line with Microsoft’s Responsible AI principles. The team performed red-teaming exercises to evaluate and improve safety measures, addressing potential risks in harmful behaviors, bias, and hallucinations.</li>\n      <li>Techniques like Pivotal Token Search (PTS) were used to mitigate errors in reasoning-heavy tasks, ensuring the model produces better outputs by targeting crucial decision-making tokens.</li>\n    </ul>\n<h4 id=\"mobilellm\"><a href=\"https://arxiv.org/abs/2402.14905\">MobileLLM</a></h4>\n<ul>\n  <li>MobileLLM is a family of sub-billion parameter language models optimized for on-device use, focusing on efficiency and reducing computational costs, latency, and energy consumption.</li>\n  <li>The models utilize deep-and-thin architectures, embedding sharing, and grouped query attention to maximize weight utilization and improve performance, challenging the traditional emphasis on model size.</li>\n  <li>An innovative immediate block-wise weight-sharing technique is introduced, allowing for deeper networks without additional memory overhead, crucial for memory-constrained devices.</li>\n  <li>The MobileLLM models, including 125M and 350M variants, are trained on large datasets using advanced training setups, achieving superior performance on zero-shot reasoning tasks compared to previous sub-billion models.</li>\n  <li>The figure below from the paper illustrates the design roadmap of sub-billion sized transformer models. The foreground and background bars represent the averaged accuracy on zero-shot common sense reasoning tasks for 125M and 350M models, respectively. The 125M model, initially a 12-layer 768-dimension structure, is enhanced via improving feed-forward network design, network depth adjustments, and weight-sharing strategies. The detailed accuracy of each modification can be found in the appendix.</li>\n</ul>\n<p><img src=\"../../../images/papers/MobileLLM.jpg\" alt=\"\"></p>\n<ul>\n  <li>The models are tested in practical applications like chat and API calling, demonstrating competitive performance, even achieving results comparable to larger models like Llama 2 7B in specific tasks.</li>\n  <li>This research suggests that with the right architectural choices and optimizations, sub-billion parameter models can effectively serve in resource-limited environments, providing high-quality language model capabilities.</li>\n  <li><a href=\"https://github.com/facebookresearch/MobileLLM\">Code</a></li>\n</ul>\n<h4 id=\"smollm\"><a href=\"https://huggingface.co/blog/smollm\">SmolLM</a></h4>\n<ul>\n  <li>SmolLM is family of small language models with sizes of 135M, 360M, and 1.7B parameters. These models are built on a meticulously curated high-quality training corpus, released as <a href=\"https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus\">SmolLM-Corpus</a>.</li>\n  <li>The training corpus includes high-quality datasets such as Cosmopedia v2, Python-Edu, and FineWeb-Edu, curated to optimize model performance.</li>\n  <li>SmolLM models outperform other small language models in their respective parameter categories on benchmarks testing reasoning and knowledge.</li>\n  <li>The models were trained using a trapezoidal learning rate scheduler, with special attention to optimizing data quality and training efficiency.</li>\n  <li>SmolLM models were instruction-tuned using permissive datasets, demonstrating balanced performance for their size.</li>\n  <li>The models are designed to run efficiently on local devices, including smartphones, with available WebGPU demos and planned GGUF versions for broader compatibility.</li>\n</ul>\n<h5 id=\"smollm2\"><a href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\">SmolLM2</a></h5>\n<ul>\n  <li>SmolLM2 is a family of compact language models available in three similar sizes as SmolLM: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.</li>\n  <li>The 1.7B variant demonstrates significant advances over its predecessor SmolLM1-1.7B, particularly in instruction following, knowledge, reasoning, and mathematics. It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using <a href=\"https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized\">UltraFeedback</a>.</li>\n  <li>The instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.</li>\n</ul>",
    "contentMarkdown": "#### Phi\n\n##### [Phi-1](https://arxiv.org/abs/2306.11644)\n\n*   Proposed in [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Gunasekar from Microsoft Research, phi-1 is a large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality” data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens).\n*   Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, the model before their finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.\n*   They demonstrate that increasing layer count and sacrificing computational cost is not the only approach to increase LLM accuracy, but instead focusing on data quality can also result in a significant accuracy boost – reinforcing the fact that a data-centric approach also helps in making your model better.\n\n![](../../../images/papers/Textbooks.jpg)\n\n##### [Phi-1.5](https://arxiv.org/abs/2309.05463)\n\n*   Proposed in Textbooks Are All You Need II: phi-1.5 technical report\\](https://arxiv.org/abs/2309.05463) by Li et al. from Microsoft Research, phi-1.5 extends previous work on Transformer-based language models. It continues the exploration of the power of smaller models, following up on the phi-1, a 1.3 billion parameter model. The phi-1.5 model, also with 1.3 billion parameters, emphasizes “textbook quality” data for training, diverging from traditional web data usage. This approach aims to enhance common sense reasoning in natural language and achieves performance comparable to models five times its size. It excels in complex reasoning tasks like grade-school mathematics and basic coding, while showing characteristics of larger models, including the ability to think step by step and some in-context learning capabilities. However, it also shares some negative traits like hallucinations and potential for biased outputs, though improvements are noted in the absence of web data.\n*   Phi-1.5 shares the same architecture as phi-1: a Transformer with 24 layers, 32 heads (each head with dimension 64), rotary embedding, context length of 2048, and flash-attention for training speedup. The training data comprises 7 billion tokens from phi-1’s data and 20 billion tokens of new synthetic data focusing on common sense reasoning. Additionally, two variants, phi-1.5-web-only and phi-1.5-web, were created to investigate the value of traditional web data. Phi-1.5-web-only was trained purely on 95 billion tokens of filtered web data, while phi-1.5-web used a mix of filtered web data, phi-1’s code data, and synthetic NLP data.\n*   Phi-1.5 was evaluated on standard natural language benchmarks including common sense reasoning, language understanding, mathematics, and coding. It achieved results comparable to models like Llama2-7B, Falcon-7B, and Vicuna-13B in these benchmarks. In reasoning tasks, including elementary school math and entry-level Python coding, phi-1.5 outperformed all existing models, including Llama 65B. The addition of web data in phi-1.5-web showed significant improvements on these reasoning tasks.\n*   The plot below from the paper illustrates benchmark results comparing phi-1.5, its version enhanced with filtered web data phi-1.5-web, and other state-of-the-art open-source LLMs. Sizes range from phi-1.5’s 1.3 billion parameters (Falcon-RW-1.3B) to 10x larger models like Vicuna-13B, a fine-tuned version of Llama-13B). Benchmarks are broadly classified into three categories: common sense reasoning, language skills, and multi-step reasoning. The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning, it arguably relies more on “memorized knowledge”. One can see that phi-1.5 models perform comparable in common sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning. Note that the numbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ slightly from numbers reported elsewhere.\n\n![](../../../images/papers/phi-1.5.jpg)\n\n*   The paper acknowledges the challenge of toxic and biased content generation in language models. To assess this, they used the ToxiGen dataset and crafted 86 prompts to test the model’s responses. Phi-1.5 showed an improvement over models like Llama2-7B and Falcon-7B, passing more prompts and failing fewer compared to these models.\n*   Phi-1.5 and phi-1.5-web, despite not undergoing instruction-based finetuning, demonstrated the ability to comprehend and execute basic human instructions and chat capabilities. This ability is attributed to the synthetic textbook data used in training, which included exercises and answers. The paper describes standard prompting techniques and showcases the model’s flexibility in natural language processing and code generation.\n\n##### [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)\n\n*   Microsoft’s Research team has been addressing inefficiencies in LLMs, specifically the trade-off between size and performance. Smaller models traditionally underperform in tasks like coding, common-sense reasoning, and language understanding compared to their larger counterparts. By advancing a suite of Small Language Models (SLMs), named “Phi”, Microsoft aims to bridge this performance gap, ensuring that more compact models can still deliver high levels of accuracy and utility in various applications.\n*   Introduced in [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) by Javaheripi and Bubeck. The article details Microsoft Research’s release of Phi-2, a 2.7 billion-parameter language model. This model is part of the “Phi” series of SLMs, including Phi-1 (1.3 billion parameters) and Phi-1.5 (also 1.3 billion parameters). Phi-2 stands out for its exceptional capabilities, achieving equivalent language understanding capabilities to models 5x larger and matching reasoning capabilities of models up to 25x larger.\n*   The Phi series of models scale down the number of parameters without a proportional loss in performance. Phi-1 showcased this in coding benchmarks, performing on par with larger models. With Phi-1.5 and the latest Phi-2, Microsoft has implemented novel model scaling techniques and refined training data curation to achieve results comparable to models many times their size. The success of Phi-2, a 2.7 billion-parameter language model, signifies a leap in optimization that allows it to demonstrate state-of-the-art reasoning and language understanding, matching or exceeding models with up to 25 times more parameters.\n*   Phi-2’s success relies on two core strategies: firstly, Phi-2’s prowess stems from a relentless focus on high-quality “textbook-quality” data, integrating synthetic datasets designed to impart common sense reasoning and general knowledge. Thus, highly selected/curated/generated data used in Phi-2’s training to educate the model on some specific foundational capabilities (e.g., common sense reasoning, problem solving, math, etc) is central to Phi-2’s exceptional performance. Secondly, it utilizes innovative scaling techniques by building upon the knowledge embedded in the 1.3 billion parameter Phi-1.5, employing scaled knowledge transfer for enhanced performance and faster training convergence. By valuing textbook-caliber content and embedding knowledge from its predecessor Phi-1.5, Phi-2 emerges as a powerhouse in reasoning and comprehension. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores, as shown in the graphs from the blog below.\n\n![](../../../images/papers/Phi-2.jpg)\n\n*   The model, which is a Transformer-based model, was trained on 1.4 trillion tokens from a mix of Synthetic and Web datasets, over 14 days using 96 A100 GPUs. Notably, Phi-2 has not undergone alignment through reinforcement learning from human feedback (RLHF) or been instruct fine-tuned, yet demonstrates improved behavior regarding toxicity and bias.\n*   Phi-2 is so small that it can run on a device, thus opening the door to a bunch of very interesting edge scenarios where latency or data sensitivity (e.g., for personalization) is paramount.\n*   Phi-2’s performance is highlighted in several benchmarks, including Big Bench Hard (BBH), commonsense reasoning, language understanding, math, and coding tasks, often surpassing or matching other models like Mistral, Llama-2, and Gemini Nano 2 despite its smaller size.\n*   Additionally, the article presents Phi-2’s proficiency in practical applications, such as solving physics problems and correcting student errors, showcasing its potential beyond benchmark tasks.\n*   The research underlines the significance of quality training data and strategic model scaling in achieving high performance with smaller models, challenging conventional beliefs about language model scaling laws.\n*   Smaller yet high-performing models like Phi-2 present an ideal testbed for experiments in mechanistic interpretability and safety, reducing the computational resources required for fine-tuning and exploring new tasks. Their more manageable size also makes them suitable for applications where deploying larger models is impractical. The ongoing work from Microsoft Research signals continuous improvements in SLMs, which could redefine industry benchmarks and open new avenues for widespread adoption of sophisticated AI tools in diverse fields.\n*   [Hugging Face](https://huggingface.co/microsoft/phi-2); [Blog](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)\n\n##### [Phi-3.5](https://aka.ms/phi3.5-techblog)\n\n*   Phi-3.5-Mini-Instruct (3.8B parameters) and MoE-Instruct (42B parameters) are MIT-licensed.\n*   [Phi-3.5-MoE-Instruct](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct):\n    *   Trained on 4.9T tokens of high-quality data.\n    *   Outperforms Llama 3.1 8B across benchmarks; competitive with GPT4o mini.\n    *   Phi-3.5-MoE has 42B parameters, with 6.6B activated during generation.\n    *   Outperforms several larger models in reasoning capability, only behind GPT-4o-mini.\n*   [Phi-3.5-Mini-Instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct):\n    *   Trained on 3.4T tokens of high-quality data.\n    *   Outperforms Llama3.1 8B and Mistral 7B; competitive with Mistral NeMo 12B.\n*   Only instruct models were released; no base models.\n*   Long-context support up to 128k.\n*   [Hugging Face: Phi-3.5-Mini-Instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [Hugging Face: Phi-3.5-MoE-Instruct](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct)\n\n*   Trained on 4.9T tokens of high-quality data.\n*   Outperforms Llama 3.1 8B across benchmarks; competitive with GPT4o mini.\n*   Phi-3.5-MoE has 42B parameters, with 6.6B activated during generation.\n*   Outperforms several larger models in reasoning capability, only behind GPT-4o-mini.\n\n*   Trained on 3.4T tokens of high-quality data.\n*   Outperforms Llama3.1 8B and Mistral 7B; competitive with Mistral NeMo 12B.\n\n##### [Phi-4](https://arxiv.org/abs/2412.08905)\n\n*   Proposed in [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905) by Abdin et al., phi-4 is a 14-billion parameter LLM, which excels at tasks requiring complex reasoning, with significant improvements over earlier models. Its integration of synthetic data is a core innovation that enhances performance, particularly in academic and STEM-related benchmarks.\n*   **Overview**:\n    *   **phi-4** focuses on data quality, particularly incorporating synthetic data into the pre-training and mid-training processes, as opposed to relying solely on organic data sources (like web content or code).\n    *   The model outperforms its teacher model, GPT-4, especially in reasoning-focused tasks (like STEM-based QA) and demonstrates strong performance on academic benchmarks.\n    *   Despite using minimal changes to the phi-3 architecture, phi-4 achieves better results thanks to improved training data, synthetic data generation techniques, and post-training innovations.\n*   **Training Process**:\n    1.  **Synthetic Data Integration:**\n        *   phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.\n        *   This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.\n    2.  **Training Phases:**\n        *   **Pretraining:** The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.\n        *   **Midtraining:** Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.\n        *   **Post-Training:** Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.\n    3.  **Data Composition and Mixtures:**\n        *   The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).\n        *   The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.\n*   **Model Architecture**:\n    *   phi-4 is based on a decoder-only transformer architecture, with a context length extended during midtraining. The architecture largely mirrors phi-3-medium, with updates in the tokenizer and a shift to a full attention mechanism over a 4K context.\n*   **Performance**:\n    \n    *   phi-4 demonstrates remarkable performance on reasoning tasks, surpassing even much larger models in domains like STEM Q&A (e.g., GPQA) and math competitions (e.g., MATH). It also outperforms GPT-4 on various benchmarks.\n    *   The model is especially strong in areas like coding (HumanEval) and mathematical reasoning, even when compared to models like Llama-3 and Qwen.\n    *   SimpleQA, DROP, and IFEval are areas where phi-4 shows weaknesses, notably in instruction-following and strict adherence to format.\n    *   The plot below from the paper illustrates the average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. t = 0.5 was chosen to follow simple-evals. Error bars are 2σ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to non–open-weight models.\n    \n    ![](../../../images/papers/phi-4.jpg)\n    \n*   **Safety and Ethics**:\n    *   phi-4 was developed in line with Microsoft’s Responsible AI principles. The team performed red-teaming exercises to evaluate and improve safety measures, addressing potential risks in harmful behaviors, bias, and hallucinations.\n    *   Techniques like Pivotal Token Search (PTS) were used to mitigate errors in reasoning-heavy tasks, ensuring the model produces better outputs by targeting crucial decision-making tokens.\n\n*   **phi-4** focuses on data quality, particularly incorporating synthetic data into the pre-training and mid-training processes, as opposed to relying solely on organic data sources (like web content or code).\n*   The model outperforms its teacher model, GPT-4, especially in reasoning-focused tasks (like STEM-based QA) and demonstrates strong performance on academic benchmarks.\n*   Despite using minimal changes to the phi-3 architecture, phi-4 achieves better results thanks to improved training data, synthetic data generation techniques, and post-training innovations.\n\n1.  **Synthetic Data Integration:**\n    *   phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.\n    *   This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.\n2.  **Training Phases:**\n    *   **Pretraining:** The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.\n    *   **Midtraining:** Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.\n    *   **Post-Training:** Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.\n3.  **Data Composition and Mixtures:**\n    *   The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).\n    *   The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.\n\n*   phi-4 uses synthetic data as the bulk of its training data, generated via diverse techniques like multi-agent prompting, self-revision workflows, and instruction reversal.\n*   This synthetic data helps improve the model’s reasoning and problem-solving abilities, overcoming the limitations of traditional unsupervised data sources.\n\n*   **Pretraining:** The model was trained on around 10 trillion tokens using a mixture of synthetic, filtered organic, and reasoning-heavy web data.\n*   **Midtraining:** Focused on extending the context length from 4k to 16k to enable better performance on long-context tasks.\n*   **Post-Training:** Used Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align the model with human preferences and further refine its outputs. The DPO technique includes a novel method for generating preference pairs based on pivotal tokens, crucial for guiding the model toward more robust problem-solving paths.\n\n*   The pretraining dataset is composed of 40% synthetic data, 15% web data, 15% web rewrites, 20% code data, and 10% targeted acquisitions (e.g., academic books, research papers).\n*   The synthetic datasets aim for diversity, complexity, and alignment with inference contexts, contributing to the model’s reasoning capabilities.\n\n*   phi-4 is based on a decoder-only transformer architecture, with a context length extended during midtraining. The architecture largely mirrors phi-3-medium, with updates in the tokenizer and a shift to a full attention mechanism over a 4K context.\n\n*   phi-4 demonstrates remarkable performance on reasoning tasks, surpassing even much larger models in domains like STEM Q&A (e.g., GPQA) and math competitions (e.g., MATH). It also outperforms GPT-4 on various benchmarks.\n*   The model is especially strong in areas like coding (HumanEval) and mathematical reasoning, even when compared to models like Llama-3 and Qwen.\n*   SimpleQA, DROP, and IFEval are areas where phi-4 shows weaknesses, notably in instruction-following and strict adherence to format.\n*   The plot below from the paper illustrates the average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. t = 0.5 was chosen to follow simple-evals. Error bars are 2σ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to non–open-weight models.\n\n![](../../../images/papers/phi-4.jpg)\n\n*   phi-4 was developed in line with Microsoft’s Responsible AI principles. The team performed red-teaming exercises to evaluate and improve safety measures, addressing potential risks in harmful behaviors, bias, and hallucinations.\n*   Techniques like Pivotal Token Search (PTS) were used to mitigate errors in reasoning-heavy tasks, ensuring the model produces better outputs by targeting crucial decision-making tokens.\n\n#### [MobileLLM](https://arxiv.org/abs/2402.14905)\n\n*   MobileLLM is a family of sub-billion parameter language models optimized for on-device use, focusing on efficiency and reducing computational costs, latency, and energy consumption.\n*   The models utilize deep-and-thin architectures, embedding sharing, and grouped query attention to maximize weight utilization and improve performance, challenging the traditional emphasis on model size.\n*   An innovative immediate block-wise weight-sharing technique is introduced, allowing for deeper networks without additional memory overhead, crucial for memory-constrained devices.\n*   The MobileLLM models, including 125M and 350M variants, are trained on large datasets using advanced training setups, achieving superior performance on zero-shot reasoning tasks compared to previous sub-billion models.\n*   The figure below from the paper illustrates the design roadmap of sub-billion sized transformer models. The foreground and background bars represent the averaged accuracy on zero-shot common sense reasoning tasks for 125M and 350M models, respectively. The 125M model, initially a 12-layer 768-dimension structure, is enhanced via improving feed-forward network design, network depth adjustments, and weight-sharing strategies. The detailed accuracy of each modification can be found in the appendix.\n\n![](../../../images/papers/MobileLLM.jpg)\n\n*   The models are tested in practical applications like chat and API calling, demonstrating competitive performance, even achieving results comparable to larger models like Llama 2 7B in specific tasks.\n*   This research suggests that with the right architectural choices and optimizations, sub-billion parameter models can effectively serve in resource-limited environments, providing high-quality language model capabilities.\n*   [Code](https://github.com/facebookresearch/MobileLLM)\n\n#### [SmolLM](https://huggingface.co/blog/smollm)\n\n*   SmolLM is family of small language models with sizes of 135M, 360M, and 1.7B parameters. These models are built on a meticulously curated high-quality training corpus, released as [SmolLM-Corpus](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus).\n*   The training corpus includes high-quality datasets such as Cosmopedia v2, Python-Edu, and FineWeb-Edu, curated to optimize model performance.\n*   SmolLM models outperform other small language models in their respective parameter categories on benchmarks testing reasoning and knowledge.\n*   The models were trained using a trapezoidal learning rate scheduler, with special attention to optimizing data quality and training efficiency.\n*   SmolLM models were instruction-tuned using permissive datasets, demonstrating balanced performance for their size.\n*   The models are designed to run efficiently on local devices, including smartphones, with available WebGPU demos and planned GGUF versions for broader compatibility.\n\n##### [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)\n\n*   SmolLM2 is a family of compact language models available in three similar sizes as SmolLM: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.\n*   The 1.7B variant demonstrates significant advances over its predecessor SmolLM1-1.7B, particularly in instruction following, knowledge, reasoning, and mathematics. It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using [UltraFeedback](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n*   The instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.",
    "order": 42,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 17,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "gpt",
      "llm",
      "nlp",
      "reinforcement learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 3290,
      "contentLength": 27083
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#popular-slms",
    "scrapedAt": "2025-12-28T11:53:26.177Z"
  },
  {
    "id": "ai-LLM-popular-medical-llms-43",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Popular LLMs",
    "title": "Popular Medical LLMs",
    "subtitle": "Popular LLMs",
    "contentHtml": "<h4 id=\"med-palm\">Med-PaLM</h4>\n<h5 id=\"med-palm-1\"><a href=\"https://arxiv.org/abs/2212.13138\">Med-PaLM 1</a></h5>\n<ul>\n  <li>Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models’ clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks.</li>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2212.13138\">Large Language Models Encode Clinical Knowledge</a> by Singal et al. from Google Research and DeepMind, the authors seek to address this aforementioned gap by presenting MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. They propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias.</li>\n  <li>In addition, they evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this they introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. As the name suggests, instruction prompt tuning uses prompt tuning instead of full-model finetuning given compute and clinician data generation costs. Their approach effectively extends Flan-PaLM’s principle of “learning to follow instructions” to the prompt tuning stage. Specifically, rather than using the soft prompt learned by prompt tuning as a replacement for a task-specific human-engineered prompt, they instead use the soft prompt as an initial prefix that is shared across multiple medical datasets, and which is followed by the relevant task-specific human-engineered prompt (consisting of instructions and/or few-shot exemplars, which may be chain-of-thought examples) along with the actual question and/or context. Instruction prompt tuning can thus be seen as a lightweight way (data-efficient, parameter-efficient, compute-efficient during both training and inference) of training a model to follow instructions in one or more domains. In their setting, instruction prompt tuning adapted LLMs to better follow the specific type of instructions used in the family of medical datasets that they target. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians.</li>\n  <li>They show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.</li>\n  <li>Given the combination of soft prompt with hard prompt, instruction prompt tuning can be considered a type of “hard-soft hybrid prompt tuning”, alongside existing techniques that insert hard anchor tokens into a soft prompt, insert learned soft tokens into a hard prompt [28], or use a learned soft prompt as a prefix for a short zero-shot hard prompt. To the best of their knowledge, ours is the first published example of learning a soft prompt that is prefixed in front of a full hard prompt containing a mixture of instructions and few-shot exemplars.</li>\n</ul>\n<h5 id=\"med-palm-2\"><a href=\"https://arxiv.org/abs/2305.09617v1\">Med-PaLM 2</a></h5>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2305.09617v1\">Towards Expert-Level Medical Question Answering with Large Language Models</a> by Singhal et al. from Google Research and DeepMind, Med-PaLM 2 is a large language model (LLM) significantly advancing the field of medical question answering. The model builds on the previous Med-PaLM, incorporating improvements from the base model PaLM 2, specialized medical domain fine-tuning, and novel prompting strategies, including ensemble refinement.</li>\n  <li>Med-PaLM 2 notably scored up to 86.5% on the MedQA dataset, surpassing the previous model by over 19%, and demonstrated competitive performance on MedMCQA, PubMedQA, and MMLU clinical topics, often reaching or exceeding state-of-the-art results.</li>\n  <li>A novel component of Med-PaLM 2’s development is the ensemble refinement prompting strategy, which involves generating multiple reasoning paths from the model, then refining these into a single, more accurate response. This method leveraged chain-of-thought and self-consistency approaches to enhance reasoning capabilities.</li>\n  <li>Illustration of Ensemble Refinement (ER) with Med-PaLM 2. In this approach, an LLM is conditioned on\nmultiple possible reasoning paths that it generates to enable it to refine and improves its answer</li>\n</ul>\n<p><img src=\"../../../images/papers/Med-PaLM2.jpg\" alt=\"\"></p>\n<ul>\n  <li>The model’s efficacy was extensively tested through various benchmarks and human evaluations, comparing its performance to that of practicing physicians across multiple axes, such as factual accuracy, medical knowledge recall, and reasoning. In tests involving 1066 consumer medical questions, Med-PaLM 2’s responses were preferred over those from human physicians in the majority of cases, especially in terms of reflecting medical consensus and reducing the likelihood of harm.</li>\n  <li>Despite its successes, the paper notes the need for ongoing validation in real-world settings, stressing that while Med-PaLM 2 represents a significant advance in medical LLMs, further research is essential to optimize its practical application and ensure safety in clinical environments.</li>\n</ul>\n<h4 id=\"meditron-70b\"><a href=\"https://arxiv.org/abs/2311.16079\">MediTron-70B</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2311.16079\">MediTron-70B: Scaling Medical Pretraining for Large Language Models</a> by EPFL, Idiap Research Institute, Open Assistant, and Yale.</li>\n  <li>MediTron-7B and 70B are language models focused on medical reasoning, adapted from Llama-2 and pretrained on a curated medical corpus including PubMed articles, abstracts, and medical guidelines.</li>\n  <li>Engineering challenges were addressed using Nvidia’s Megatron-LM for distributed training, incorporating various forms of parallelism and optimization techniques for handling large-scale models.</li>\n  <li>The following figure from the paper shows the complete pipeline for continued pretraining, supervised fine-tuning, and evaluation of MediTron-7B and MediTron-70B.</li>\n</ul>\n<p><img src=\"../../../images/papers/MediTron.jpg\" alt=\"\"></p>\n<ul>\n  <li>Performance was evaluated using four medical benchmarks, showing significant gains over several baselines, both before and after task-specific fine-tuning. MediTron outperformed GPT-3.5 and Med-PaLM and closely approached the performance of GPT-4 and Med-PaLM-2.</li>\n  <li>The study emphasizes the use of chain-of-thought and self-consistency methods for improving inference. MediTron models demonstrated strong medical reasoning capabilities even before task-specific fine-tuning.</li>\n</ul>\n<h4 id=\"biomistral\"><a href=\"https://arxiv.org/abs/2402.10373\">BioMistral</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2402.10373\">BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</a> by Labrak et al. from Avignon University, Zenidoc, and Nantes University, BioMistral is a family of LLMs that leverage the open-source Mistral model and perform continued pretrainong on it on PubMed Central, thereby tailoring it for the biomedical domain. The model offers advancements in handling medical question-answering tasks across multiple languages and implements novel model compression techniques for efficient deployment.</li>\n  <li>BioMistral 7B is evaluated against a set of 10 medical question-answering tasks in English, demonstrating superior performance over other open-source models and maintaining competitive results against proprietary models. To test its multilingual capabilities, these tasks were translated into seven additional languages, marking the first large-scale multilingual evaluation in this domain.</li>\n  <li>The team introduced quantization strategies to develop lightweight models, notably Activation-aware Weight Quantization (AWQ) and BitsandBytes (BnB), enabling the model’s deployment on consumer-grade devices without significant loss in performance.</li>\n  <li>They also explored model merging techniques combining BioMistral with the original Mistral model to leverage both domain-specific medical knowledge and general linguistic understanding. Techniques like Spherical Linear Interpolation (SLERP) and Task-Induced Ensemble Strategy (TIES) were applied to merge models effectively.</li>\n  <li>All resources, including datasets, multilingual evaluation benchmarks, scripts, and models, are made freely available, promoting transparency and facilitating further research in the community.</li>\n  <li><a href=\"https://huggingface.co/BioMistral/BioMistral-7B\">Models</a></li>\n</ul>\n<h4 id=\"openbiollm\"><a href=\"https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B\">OpenBioLLM</a></h4>\n<ul>\n  <li>Saama’s release of OpenBioLLM outperforms OpenAI’s GPT-4, Google’s Gemini, Meditron-70B, Google’s Med-PaLM-1, and Med-PaLM-2 in the biomedical domain, setting a new state-of-the-art for the most capable openly available Medical-domain LLMs to date.</li>\n  <li><a href=\"https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B\">OpenBioLLM-70B</a> delivers SoTA performance, while the OpenBioLLM-8B model even surpasses GPT-3.5 and Meditron-70B.</li>\n  <li>The models underwent a rigorous two-phase fine-tuning process using the LLama-3 70B &amp; 8B models as the base and leveraging Direct Preference Optimization (DPO) for optimal performance.</li>\n  <li>Results are available on the <a href=\"https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard\">Open Medical-LLM Leaderboard</a>.</li>\n  <li><a href=\"https://twitter.com/aadityaura/status/1783662626901528803\">Evaluation reports</a></li>\n</ul>",
    "contentMarkdown": "#### Med-PaLM\n\n##### [Med-PaLM 1](https://arxiv.org/abs/2212.13138)\n\n*   Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models’ clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks.\n*   Proposed in [Large Language Models Encode Clinical Knowledge](https://arxiv.org/abs/2212.13138) by Singal et al. from Google Research and DeepMind, the authors seek to address this aforementioned gap by presenting MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. They propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias.\n*   In addition, they evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this they introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. As the name suggests, instruction prompt tuning uses prompt tuning instead of full-model finetuning given compute and clinician data generation costs. Their approach effectively extends Flan-PaLM’s principle of “learning to follow instructions” to the prompt tuning stage. Specifically, rather than using the soft prompt learned by prompt tuning as a replacement for a task-specific human-engineered prompt, they instead use the soft prompt as an initial prefix that is shared across multiple medical datasets, and which is followed by the relevant task-specific human-engineered prompt (consisting of instructions and/or few-shot exemplars, which may be chain-of-thought examples) along with the actual question and/or context. Instruction prompt tuning can thus be seen as a lightweight way (data-efficient, parameter-efficient, compute-efficient during both training and inference) of training a model to follow instructions in one or more domains. In their setting, instruction prompt tuning adapted LLMs to better follow the specific type of instructions used in the family of medical datasets that they target. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians.\n*   They show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.\n*   Given the combination of soft prompt with hard prompt, instruction prompt tuning can be considered a type of “hard-soft hybrid prompt tuning”, alongside existing techniques that insert hard anchor tokens into a soft prompt, insert learned soft tokens into a hard prompt \\[28\\], or use a learned soft prompt as a prefix for a short zero-shot hard prompt. To the best of their knowledge, ours is the first published example of learning a soft prompt that is prefixed in front of a full hard prompt containing a mixture of instructions and few-shot exemplars.\n\n##### [Med-PaLM 2](https://arxiv.org/abs/2305.09617v1)\n\n*   Proposed in [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617v1) by Singhal et al. from Google Research and DeepMind, Med-PaLM 2 is a large language model (LLM) significantly advancing the field of medical question answering. The model builds on the previous Med-PaLM, incorporating improvements from the base model PaLM 2, specialized medical domain fine-tuning, and novel prompting strategies, including ensemble refinement.\n*   Med-PaLM 2 notably scored up to 86.5% on the MedQA dataset, surpassing the previous model by over 19%, and demonstrated competitive performance on MedMCQA, PubMedQA, and MMLU clinical topics, often reaching or exceeding state-of-the-art results.\n*   A novel component of Med-PaLM 2’s development is the ensemble refinement prompting strategy, which involves generating multiple reasoning paths from the model, then refining these into a single, more accurate response. This method leveraged chain-of-thought and self-consistency approaches to enhance reasoning capabilities.\n*   Illustration of Ensemble Refinement (ER) with Med-PaLM 2. In this approach, an LLM is conditioned on multiple possible reasoning paths that it generates to enable it to refine and improves its answer\n\n![](../../../images/papers/Med-PaLM2.jpg)\n\n*   The model’s efficacy was extensively tested through various benchmarks and human evaluations, comparing its performance to that of practicing physicians across multiple axes, such as factual accuracy, medical knowledge recall, and reasoning. In tests involving 1066 consumer medical questions, Med-PaLM 2’s responses were preferred over those from human physicians in the majority of cases, especially in terms of reflecting medical consensus and reducing the likelihood of harm.\n*   Despite its successes, the paper notes the need for ongoing validation in real-world settings, stressing that while Med-PaLM 2 represents a significant advance in medical LLMs, further research is essential to optimize its practical application and ensure safety in clinical environments.\n\n#### [MediTron-70B](https://arxiv.org/abs/2311.16079)\n\n*   Proposed in [MediTron-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079) by EPFL, Idiap Research Institute, Open Assistant, and Yale.\n*   MediTron-7B and 70B are language models focused on medical reasoning, adapted from Llama-2 and pretrained on a curated medical corpus including PubMed articles, abstracts, and medical guidelines.\n*   Engineering challenges were addressed using Nvidia’s Megatron-LM for distributed training, incorporating various forms of parallelism and optimization techniques for handling large-scale models.\n*   The following figure from the paper shows the complete pipeline for continued pretraining, supervised fine-tuning, and evaluation of MediTron-7B and MediTron-70B.\n\n![](../../../images/papers/MediTron.jpg)\n\n*   Performance was evaluated using four medical benchmarks, showing significant gains over several baselines, both before and after task-specific fine-tuning. MediTron outperformed GPT-3.5 and Med-PaLM and closely approached the performance of GPT-4 and Med-PaLM-2.\n*   The study emphasizes the use of chain-of-thought and self-consistency methods for improving inference. MediTron models demonstrated strong medical reasoning capabilities even before task-specific fine-tuning.\n\n#### [BioMistral](https://arxiv.org/abs/2402.10373)\n\n*   Proposed in [BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains](https://arxiv.org/abs/2402.10373) by Labrak et al. from Avignon University, Zenidoc, and Nantes University, BioMistral is a family of LLMs that leverage the open-source Mistral model and perform continued pretrainong on it on PubMed Central, thereby tailoring it for the biomedical domain. The model offers advancements in handling medical question-answering tasks across multiple languages and implements novel model compression techniques for efficient deployment.\n*   BioMistral 7B is evaluated against a set of 10 medical question-answering tasks in English, demonstrating superior performance over other open-source models and maintaining competitive results against proprietary models. To test its multilingual capabilities, these tasks were translated into seven additional languages, marking the first large-scale multilingual evaluation in this domain.\n*   The team introduced quantization strategies to develop lightweight models, notably Activation-aware Weight Quantization (AWQ) and BitsandBytes (BnB), enabling the model’s deployment on consumer-grade devices without significant loss in performance.\n*   They also explored model merging techniques combining BioMistral with the original Mistral model to leverage both domain-specific medical knowledge and general linguistic understanding. Techniques like Spherical Linear Interpolation (SLERP) and Task-Induced Ensemble Strategy (TIES) were applied to merge models effectively.\n*   All resources, including datasets, multilingual evaluation benchmarks, scripts, and models, are made freely available, promoting transparency and facilitating further research in the community.\n*   [Models](https://huggingface.co/BioMistral/BioMistral-7B)\n\n#### [OpenBioLLM](https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B)\n\n*   Saama’s release of OpenBioLLM outperforms OpenAI’s GPT-4, Google’s Gemini, Meditron-70B, Google’s Med-PaLM-1, and Med-PaLM-2 in the biomedical domain, setting a new state-of-the-art for the most capable openly available Medical-domain LLMs to date.\n*   [OpenBioLLM-70B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B) delivers SoTA performance, while the OpenBioLLM-8B model even surpasses GPT-3.5 and Meditron-70B.\n*   The models underwent a rigorous two-phase fine-tuning process using the LLama-3 70B & 8B models as the base and leveraging Direct Preference Optimization (DPO) for optimal performance.\n*   Results are available on the [Open Medical-LLM Leaderboard](https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard).\n*   [Evaluation reports](https://twitter.com/aadityaura/status/1783662626901528803)",
    "order": 43,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 7,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "optimization",
      "activation",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1298,
      "contentLength": 10700
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#popular-medical-llms",
    "scrapedAt": "2025-12-28T11:53:26.177Z"
  },
  {
    "id": "ai-LLM-popular-indic-llms-44",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Popular LLMs",
    "title": "Popular Indic LLMs",
    "subtitle": "Popular LLMs",
    "contentHtml": "<h4 id=\"openhathi\"><a href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\">OpenHathi</a></h4>\n<ul>\n  <li>Indian AI startup <a href=\"https://www.sarvam.ai/\">Sarvam AI</a> has released <a href=\"https://www.sarvam.ai/blogs/openhathi-series\">OpenHathi-Hi-v0.1</a>, the first open-source LLM foundation model for Hindi. Developed on a budget-friendly platform, the model, a fine-tuned version of Llama2-7B, boasts GPT-3.5-like performance for Indic languages.</li>\n  <li>OpenHathi, featuring a new sentence-piece tokenizer with a vocabulary size of 16K, trained on Hindi text, is merged with Llama2-7B’s tokenizer with a 32K original vocabulary (i.e., overall, a tokenizer with a 48K vocbulary), undergoes a two-phase training process. The initial phase focuses on embedding alignment, aligning randomly initialized Hindi embeddings, followed by bilingual language modeling, teaching the model cross-lingual attention across tokens.</li>\n  <li>The model demonstrates robust performance across various Hindi tasks, comparable to, if not surpassing, GPT-3.5, while maintaining English proficiency. Sarvam AI’s evaluation includes non-academic, real-world tasks alongside standard Natural Language Generation (NLG) tasks. Evaluations against GPT-3.5 generation with GPT-4 as the judge revealed superior performance in Hindi, both in native and Romanised scripts.</li>\n  <li>Developed in collaboration with academic partners at AI4Bhārat, who contributed language resources and benchmarks, and fine-tuned in partnership with KissanAI, the model leverages conversational data from a bot interacting with farmers in multiple languages.</li>\n  <li><a href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\">Hugging Face</a></li>\n</ul>\n<h4 id=\"bharatgpt\"><a href=\"https://corover.ai/bharatgpt/\">BharatGPT</a></h4>\n<ul>\n  <li>BharatGPT is an Indic LLM by <a href=\"https://corover.ai/\">CoRover.ai</a>. It supports 14 Indian languages, with generative text, voice, and video.</li>\n</ul>\n<h4 id=\"kannada-llama\"><a href=\"https://www.tensoic.com/blog/kannada-llama/\">Kannada Llama</a></h4>\n<ul>\n  <li><a href=\"https://www.tensoic.com/blog/kannada-llama/\">Kannada Llama</a> is aimed at enhancing the capabilities of the Llama 2 model for Kannada, a language predominantly spoken in Southwestern India.</li>\n  <li>The project involves adapting the model to better handle Kannada text, notably through an expanded tokenizer. The tokenizer increases Llama 2’s vocabulary from 32K to 48K tokens, incorporating a SentencePiece tokenizer trained on Kannada text. This change significantly improves the model’s ability to process Kannada text, as demonstrated with examples.</li>\n  <li>Kannada Llama also utilizes Low-Rank Adaptation (LoRA) for efficient pre-training and fine-tuning of the Kannada Base Llama for conversational abilities. Kannada Llama models and datasets have been released under open licenses for community development.</li>\n</ul>\n<h4 id=\"tamil-llama\"><a href=\"https://arxiv.org/abs/2311.05845\">Tamil-LLaMA</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2311.05845\">Tamil-LLaMA: A New Tamil Language Model Based on LLaMA 2</a> by Abhinand Balachandran, Tamil-LLaMA is an enhancement of the open-source LLaMA model, tailored for Tamil language processing.</li>\n  <li>The tokenization process is a crucial aspect of enhancing the model’s proficiency in handling the Tamil language. The integration of an additional 16,000 Tamil tokens into the LLaMA model’s vocabulary is a key step in this process. This expansion of the vocabulary allows for a more accurate and nuanced representation of the Tamil language, improving the model’s ability to understand and generate Tamil text. The tokenization specifically aims to address the unique linguistic features of Tamil, making the model more effective in tasks involving this language.</li>\n  <li>The approach uses the Low-Rank Adaptation (LoRA) methodology for efficient model training, focusing on a comprehensive Tamil corpus. This ensures computational feasibility while enhancing the model’s robustness in text generation.</li>\n  <li>Tamil-LLaMA utilizes datasets like CulturaX for pre-training and a Tamil-translated version of the Alpaca dataset, along with a subset of the OpenOrca dataset, for instruction fine-tuning.</li>\n  <li>Key contributions include the expansion of LLaMA’s vocabulary with 16,000 Tamil tokens, training on a comprehensive Tamil dataset, and presenting Tamil-translated versions of Alpaca and OpenOrca datasets for instruction fine-tuning.</li>\n  <li>Tamil LLaMA outperforms its predecessors and other open-source models in tasks specific to the Tamil language, demonstrating significant advancements in performance. The paper presents results from instruction tasks, showing Tamil-LLaMA’s superior performance in areas like reasoning, translation, code generation, and open question answering. It surpasses GPT-3.5-turbo in many tasks, according to evaluations using GPT-4.</li>\n  <li>Performance comparison on the IndicSentiment-7B dataset (left) and the IndicGLUE Text Classification (right).</li>\n</ul>\n<p><img src=\"../../../images/papers/Tamil-LLaMA.jpg\" alt=\"\"></p>\n<ul>\n  <li>The paper emphasizes the importance of language diversity in LLMs and contributes to advancing language models for Indian languages, with public access to models, datasets, and code to foster further research.</li>\n  <li>The table below shows a list of available models:</li>\n</ul>\n<p><img src=\"../../../images/papers/Tamil-LLaMA2.jpg\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://github.com/abhinand5/tamil-llama\">Code</a></li>\n</ul>\n<h4 id=\"ambari\"><a href=\"https://www.cognitivelab.in/blog/introducing-ambari\">Ambari</a></h4>\n<ul>\n  <li>CognitiveLab’s <a href=\"https://www.cognitivelab.in/blog/introducing-ambari\">Ambari</a> is a series of open-source Bilingual Kannada-English LLMs.</li>\n  <li>Ambari’s goal is to adapt language models to bridge the gap between Kannada and English, training on a modest 1 billion-token dataset, and identifying efficient training methods.</li>\n  <li>A key focus was on specialized tokenization, where the team developed a custom tokenization model for Kannada using SentencePiece. This model was integrated with the base Llama tokenizer, resulting in an enhanced and comprehensive vocabulary of 49,600, expanded by 17,600 tokens. The approach involved training on three different dataset sizes, leading to optimal results with a 100,000 token dataset. The project’s main aim was to bridge linguistic gaps in LLMs and optimize training with a modest 1B-token dataset.</li>\n  <li>The model’s journey encompasses stages like pre-training, bilingual next token prediction, and fine-tuning with a focus on language adaptability, efficient tokenization, and bilingual instruction.</li>\n  <li>Ambari represents a step forward in the LLM landscape, addressing challenges like limited world knowledge and translation nuances, with future plans to incorporate Romanized Kannada and further refine the model.</li>\n  <li>Its inaugural models, Ambari-7B-base-v0.1 and Ambari-7B-Instruct-v0.1, achieve impressive results on a compact 1 billion-token training dataset, trained across multiple stages.</li>\n</ul>\n<h4 id=\"krutrim\"><a href=\"https://blog.olakrutrim.com/press-release/\">Krutrim</a></h4>\n<ul>\n  <li><a href=\"https://blog.olakrutrim.com/press-release/\">Krutrim</a> by Ola converses in 22 Indian languages and creates text in 10, trained on a staggering 2 trillion tokens using a specially crafted homegrown tokeniser tailored for Indic languages.</li>\n</ul>\n<h4 id=\"bengaligpt\"><a href=\"https://medium.com/@shantipriya.parida/odiagenai-released-an-instruction-following-llama-model-for-bengali-5804897ac4bf\">BengaliGPT</a></h4>\n<ul>\n  <li>OdiaGenAI’s <a href=\"https://huggingface.co/OdiaGenAI/odiagenAI-bengali-lora-model-v1\">BengaliGPT</a> model is based on Llama-7b and finetuned with a 252k Bengali instruction set. It also comes with an integrated Text-To-Speech (TTS) feature.</li>\n  <li>The instruction set is available at Hugging Face for research and non-commercial purposes.</li>\n  <li>The code (translation, training, and inference) is available on <a href=\"https://github.com/OdiaGenAI\">GitHub</a>.</li>\n</ul>\n<h4 id=\"gajendra\"><a href=\"https://huggingface.co/BhabhaAI/Gajendra-v0.1\">Gajendra</a></h4>\n<ul>\n  <li>Gajendra is a 7B Hindi-Hinglish-English instruction finetuned model on top of <a href=\"#OpenHathi\">OpenHathi</a> using the <a href=\"https://huggingface.co/datasets/BhabhaAI/translation-classify\">translation-classify</a> dataset.</li>\n</ul>\n<h4 id=\"airavata\"><a href=\"https://ai4bharat.github.io/airavata/\">Airavata</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2401.15006\">Airavata: Introducing Hindi Instruction-Tuned LLM</a> by researchers from Nilekani Centre at AI4Bharat, IIT Madras, IIIT D&amp;M Kancheepuram, Flipkart, University of Surrey, STAR, NICT, IBM Research, Microsoft, <a href=\"https://ai4bharat.github.io/airavata/\">Airavata</a> is an instruction-tuned Hindi LLM. This addresses the limited support for Indian languages in current LLMs.</li>\n  <li>The model has been built by fine-tuning Sarvam AI’s OpenHathi, with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks.</li>\n  <li>The paper describes creating high-quality Hindi datasets by translating English-supervised instruction-tuning datasets. The process involved using the IndicTrans2 model for translation, ensuring balanced task representation while retaining high-quality examples. Two native Hindi instruction datasets, wikiHow and Anudesh, were also created for training.</li>\n  <li>The process of Instruction Tuning dataset creation can be summarized as follows:\n    <ul>\n      <li>Sampled instances from diverse English datasets for balance.</li>\n      <li>Translated sampled instances (instructions, input, and outputs) in English to Hindi using IndicTrans2. For translation, the authors chose IndicTrans2 over OpenAI models as IndicTrans2 is the state-of-the-art open-source MT model for Indian languages.</li>\n      <li>Filtered translated instances using chrF++ score to ensure quality.</li>\n      <li>Final instruction tuning dataset includes 385k instances.</li>\n    </ul>\n  </li>\n  <li>Airavata underwent supervised fine-tuning with these datasets using LoRA (Low-Rank Adaptation) technique, optimizing for specific hyperparameters. Model selection was based on evaluating checkpoints from different epochs on NLU and NLG tasks, leading to a blend of checkpoints providing balanced performance.</li>\n  <li>The model was evaluated on various NLP benchmarks, including native Hindi test sets and translated English benchmarks. It showed significant improvement over the base model (OpenHathi) in most tasks, especially in aligning with the instruction-tuned dataset. The plot below from the paper shows an ablation experiment to understand the performance gaps between Full fine-tuning and LoRA fine-tuning across a mix of English and Hindi NLU tasks.</li>\n</ul>\n<ul>\n      <li>Sampled instances from diverse English datasets for balance.</li>\n      <li>Translated sampled instances (instructions, input, and outputs) in English to Hindi using IndicTrans2. For translation, the authors chose IndicTrans2 over OpenAI models as IndicTrans2 is the state-of-the-art open-source MT model for Indian languages.</li>\n      <li>Filtered translated instances using chrF++ score to ensure quality.</li>\n      <li>Final instruction tuning dataset includes 385k instances.</li>\n    </ul>\n<p><img src=\"../../../images/papers/Airavata.jpg\" alt=\"\"></p>\n<ul>\n  <li>Human evaluation of Airavata focused on its ability to generate long-form text, provide factual opinions, make content accessible, demonstrate creativity in language, and answer culturally relevant questions. It was compared with other models like ChatGPT, GPT-4, and BactrianX-llama-7B.</li>\n  <li>Despite showing promise, the paper acknowledges limitations such as potential generation of biased or objectionable content, and challenges in cultural nuances and mixed-language contexts. The model’s performance is closely tied to the quality and scope of its training data.</li>\n  <li>Airavata and its associated resources have been released to facilitate further research in instruction-tuning for Indian language LLMs.</li>\n  <li><a href=\"https://huggingface.co/ai4bharat/Airavata\">HuggingFace Repo</a>; <a href=\"https://huggingface.co/datasets/ai4bharat/indic-instruct-data-v0.1\">Instruction tuning dataset</a>; <a href=\"https://arxiv.org/pdf/2401.15006.pdf\">Airavata paper</a></li>\n</ul>\n<h4 id=\"malayallm\"><a href=\"https://github.com/VishnuPJ/MalayaLLM\">MalayaLLM</a></h4>\n<ul>\n  <li>MalayaLLM is a pioneering 7B LLaMA-2 Indic model, specifically pre-trained and fine-tuned for the Malayalam language using LoRA techniques. It aims to enhance generative AI capabilities in Malayalam by focusing on tasks like content generation and question answering.</li>\n  <li>The MalayaLLM models have been improved and customized to incorporate a comprehensive Malayalam vocabulary comprising approximately 18,000 tokens, expanding upon the groundwork laid by the original LLaMA-2.</li>\n  <li>While the model, in its early development stages, shows promise for Malayalam-specific applications, it requires further training and updates to reach optimal performance. The training leverages datasets from AI4Bharat and CulturaX, with fine-tuning on Alpaca_Instruct_Malayalam.</li>\n  <li><a href=\"https://huggingface.co/collections/VishnuPJ/malayallm-65b0ddb50e37d746c6d07370\">HuggingFace Repo</a>; <a href=\"https://huggingface.co/datasets/VishnuPJ/Alpaca_Instruct_Malayalam\">Alpaca_Instruct_Malayalam</a></li>\n</ul>\n<h4 id=\"hanooman\"><a href=\"https://analyticsindiamag.com/bharatgpt-unveils-hanooman-a-newsuite-of-indic-generative-ai-models/\">Hanooman</a></h4>\n<ul>\n  <li>The BharatGPT team, led by IIT Bombay and seven other top Indian engineering institutes, has announced the launch of the <a href=\"https://analyticsindiamag.com/bharatgpt-unveils-hanooman-a-newsuite-of-indic-generative-ai-models/\">Hanooman</a> series of language models for Indian languages with the help of Seetha Mahalaxmi Healthcare.</li>\n  <li>Hanooman can respond in 11 prominent Indian languages, such as Hindi, Tamil, and Marathi, showcasing its versatility and inclusive linguistic proficiency.</li>\n  <li>It can create text, speech, videos, and much more in various Indian languages, making communication more dynamic and accessible than ever before.</li>\n</ul>\n<h4 id=\"navarasa-20\"><a href=\"https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750\">Navarasa 2.0</a></h4>\n<ul>\n  <li><a href=\"https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750\">Navarasa 2.0</a> are a series of instruction-tuned models based on Gemma 7B and 2B on 15 Indian languages, created by <a href=\"https://www.linkedin.com/in/ravidesetty/\">Ravi Theja Desetty</a> and <a href=\"https://www.linkedin.com/in/ramsrig/\">Ramsri Golla</a>.</li>\n  <li>Navarasa 2.0 supports Hindi, Telugu, Tamil, Kannada, Malayalam, Marathi, Gujarati, Bengali, Punjabi, Odia, Urdu, Konkani, Assamese, Nepali, Sindhi, and English.</li>\n  <li>In addition, they have releasing models in GGUF format alongside a collection of Alpaca-cleaned datasets for all 15 languages.</li>\n  <li>The models were fine-tuned on a single A100 machine which took approx. 44 hours for the 7B model and 18 hours for the 2B model.</li>\n  <li>To demonstrate real-world information capabilities, they have provided examples of a Retrieval Augmented Generation (RAG) model using LlamaIndex in 15 Indian languages, along with datasets, and examples of reasoning, translation, and question answering with context in their <a href=\"https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750\">blog post</a>.</li>\n  <li><a href=\"https://huggingface.co/collections/Telugu-LLM-Labs/navarasa-20-models-65f7c72addf0619cb0991309\">Models</a>; <a href=\"https://huggingface.co/collections/Telugu-LLM-Labs/indic-alpaca-datasets-65f2a3687d5cdbce8880c581\">Datasets</a></li>\n</ul>\n<h4 id=\"gujju-llama-10\"><a href=\"https://huggingface.co/sampoorna42/Gujju-Llama-Instruct-v0.1\">Gujju Llama 1.0</a></h4>\n<ul>\n  <li>At the core of Gujju Llama lies the powerful Llama 2 7B base model, which was expanded by incorporating a staggering 17,000 Gujarati tokens derived from the ‘Cultura X’ Gujarati subset. This strategic move extended the original 32,000 token vocabulary to an impressive 49,000, significantly enhancing the model’s ability to comprehend and generate Gujarati text.</li>\n  <li>To further refine Gujju Llama understanding of the language, they conducted a meticulous fine-tuning process. They leveraged multiple high-quality datasets from Hugging Face 🤗, including Dolly, Alpaca, and Open-Orca. Additionally, they incorporated synthetic data generated by the Google’s Gemini API, culminating in a total of approximately 200,000 data points, meticulously translated into Gujarati using the Google Cloud Translation API.</li>\n  <li>Powered by the advanced A100 80GB GPUs from RunPod’s serverless cloud platform, they dedicated 36 hours to pretraining and 52 hours to fine-tuning, ensuring a smooth and efficient training process.</li>\n  <li><a href=\"https://huggingface.co/datasets/sampoorna42/gujarati-alpaca-orca-dolly\">Dataset</a>; <a href=\"https://huggingface.co/sampoorna42/gujju-llama-base-v1.0\">Base-model</a>; <a href=\"https://huggingface.co/sampoorna42/Gujju-Llama-Instruct-v0.1\">Instruction-tuned model</a></li>\n</ul>\n<h4 id=\"pragna\"><a href=\"https://soket.ai/blogs/pragna_1b\">Pragna</a></h4>\n<ul>\n  <li>Pragna-1B from Soket AI Labs is India’s inaugural open-source multilingual model, Pragna-1B, meticulously crafted for Indian languages including Hindi, Gujarati, Bangla, and English.</li>\n  <li>Pragna-1B is a decoder-only transformer model, inspired by TinyLlama and featuring the following specifications:\n    <ul>\n      <li>Layers: 22</li>\n      <li>Attention Heads: 32</li>\n      <li>Context Length: 2048</li>\n      <li>Hidden Dimension: 2048</li>\n      <li>Expansion Dimension: 5632</li>\n      <li>Vocabulary Size: 69632</li>\n      <li>Parameter Count: 1.25B</li>\n    </ul>\n  </li>\n  <li>This model incorporates Rotary Positional Encoding to infuse positional information into the embeddings, utilising a base of 10,000. It employs RMSNorm with an epsilon value of 1e-5 and the Sigmoid Activation Unit (SiLU) as the activation function. Additionally, Pragna-1B adopts Grouped Query Attention, an alternative to Multi-Head Attention, which enhances training and inference speed while reducing memory bandwidth. This also supports the use of lower-compute devices for inference tasks.</li>\n  <li>Pragna’s BPE tokenizer has been specifically fine-tuned for Indian languages to enhance tokenization efficiency—significantly reducing the number of tokens per word and increasing throughput.</li>\n  <li>Despite its compact size, Pragna-1B delivers performance on par with models nearly six times its size, making it a formidable tool for processing Indic languages.</li>\n  <li>Pragna-1B leverages ‘Bhasha’ and other open datasets, training on a total of 150 billion tokens (overall 3.15 trillion), which mirrors the rich linguistic diversity of India.</li>\n  <li>Designed for edge deployment, Pragna-1B offers powerful AI capabilities at the source of data generation, ensuring efficiency and effectiveness.</li>\n  <li><a href=\"https://huggingface.co/soketlabs/pragna-1b\">Hugging Face</a>; <a href=\"https://soket.ai/blogs/pragna_1b\">Blog</a></li>\n</ul>\n<ul>\n      <li>Layers: 22</li>\n      <li>Attention Heads: 32</li>\n      <li>Context Length: 2048</li>\n      <li>Hidden Dimension: 2048</li>\n      <li>Expansion Dimension: 5632</li>\n      <li>Vocabulary Size: 69632</li>\n      <li>Parameter Count: 1.25B</li>\n    </ul>\n<h4 id=\"sarvam-1\"><a href=\"https://www.sarvam.ai/blogs/sarvam-1\">Sarvam-1</a></h4>\n<ul>\n  <li>Sarvam-1 is a 2B parameter language model specifically optimized for Indian languages. It provides best in-class performance in 10 Indic languages (bn, gu, hi, kn, ml, mr, or, pa, ta, te) when compared with popular models like Gemma-2-2B and Llama-3.2-3B. It is also competitive against the much larger models like Llama-3.1-8B in these languages.</li>\n  <li>The model was trained with <a href=\"https://github.com/NVIDIA/NeMo\">NVIDIA NeMo Framework</a> on the Yotta Shakti Cloud using HGX H100 systems.</li>\n  <li><strong>Key Features</strong>:\n    <ul>\n      <li><strong>Optimized for 10 Indian Languages:</strong> Built from the ground up to support major Indian languages alongside English</li>\n      <li><strong>Superior Token Efficiency:</strong> Achieves fertility rates of 1.4-2.1 across all supported languages, 2-4x more efficient than existing multilingual models</li>\n      <li><strong>High-Quality Training Data:</strong> Trained on a curated corpus of ~4 trillion tokens with 2 trillion high-quality Indic tokens</li>\n      <li><strong>Efficient Inference:</strong> 4-6x faster inference compared to larger models while matching or exceeding their performance on Indic language tasks</li>\n    </ul>\n  </li>\n  <li><strong>Model Architecture</strong>:\n    <ul>\n      <li>Hidden size: 2048</li>\n      <li>Intermediate size: 11,008</li>\n      <li>Number of attention heads: 16</li>\n      <li>Number of hidden layers: 28</li>\n      <li>Number of key-value heads: 8</li>\n      <li>Maximum position embeddings: 8,192</li>\n      <li>Activation function: SwiGLU</li>\n      <li>Positional embeddings: Rotary (RoPE) with theta=10,000</li>\n      <li>Training: Grouped-query attention and bfloat16 mixed-precision</li>\n    </ul>\n  </li>\n  <li>Note: This is a text-completion model. It is meant to be finetuned on downstream tasks, and cannot be used directly as a chat or an instruction-following model.\nKey Features.</li>\n  <li><a href=\"https://huggingface.co/sarvamai/sarvam-1\">Hugging Face</a>; <a href=\"https://www.sarvam.ai/blogs/sarvam-1\">Release blog</a></li>\n</ul>\n<ul>\n      <li><strong>Optimized for 10 Indian Languages:</strong> Built from the ground up to support major Indian languages alongside English</li>\n      <li><strong>Superior Token Efficiency:</strong> Achieves fertility rates of 1.4-2.1 across all supported languages, 2-4x more efficient than existing multilingual models</li>\n      <li><strong>High-Quality Training Data:</strong> Trained on a curated corpus of ~4 trillion tokens with 2 trillion high-quality Indic tokens</li>\n      <li><strong>Efficient Inference:</strong> 4-6x faster inference compared to larger models while matching or exceeding their performance on Indic language tasks</li>\n    </ul>\n<ul>\n      <li>Hidden size: 2048</li>\n      <li>Intermediate size: 11,008</li>\n      <li>Number of attention heads: 16</li>\n      <li>Number of hidden layers: 28</li>\n      <li>Number of key-value heads: 8</li>\n      <li>Maximum position embeddings: 8,192</li>\n      <li>Activation function: SwiGLU</li>\n      <li>Positional embeddings: Rotary (RoPE) with theta=10,000</li>\n      <li>Training: Grouped-query attention and bfloat16 mixed-precision</li>\n    </ul>\n<h4 id=\"nanda\"><a href=\"https://huggingface.co/MBZUAI/Llama-3-Nanda-10B-Chat\">Nanda</a></h4>\n<ul>\n  <li>Llama-3-Nanda-10B-Chat (or Nanda for short) is a 10 billion parameter pre-trained and instruction-tuned bilingual large language model for both Hindi and English, trained on a dataset containing 65 billion Hindi tokens. The model is based on transformer-based decoder-only (LLaMA-3) architecture. It implements Rotary Position Embeddings (RoPE), enabling the model to extrapolate to long sequence lengths, providing improved context handling and model precision.</li>\n  <li><a href=\"https://huggingface.co/MBZUAI/Llama-3-Nanda-10B-Chat\">Hugging Face</a>; <a href=\"https://github.com/mbzuai-nlp/Llama-3-Nanda-10B-Chat/blob/main/Llama-3-Nanda-10B-Chat-Paper.pdf\">Paper</a></li>\n</ul>\n<h4 id=\"shivaay\"><a href=\"https://shivaay.futurixai.com/\">Shivaay</a></h4>\n<ul>\n  <li>Shivaay is a 4B foundation model from FuturixAI with exceptional benchmark performance.</li>\n  <li>ARC-Challenge: Ranked #3 globally with 91.04% accuracy, outperforming Claude 2, GPT-3.5, and Llama 3 8B—just 2 points behind GPT-4! View Leaderboard\n    <ul>\n      <li><a href=\"https://github.com/FuturixAI-and-Quantum-Works/Shivaay_ARC-C\">ARC-C benchmark results</a></li>\n    </ul>\n  </li>\n  <li>GSM8K: Ranked #11 among models not using extra training data, achieving 87.41% accuracy, surpassing GPT-4 and the 70B-parameter Gemma 70B! View Leaderboard\n    <ul>\n      <li><a href=\"https://github.com/FuturixAI-and-Quantum-Works/Shivaay_GSM8K\">GSM8K benchmark results</a></li>\n    </ul>\n  </li>\n  <li><a href=\"https://shivaay.futurixai.com/\">Playground</a></li>\n</ul>\n<ul>\n      <li><a href=\"https://github.com/FuturixAI-and-Quantum-Works/Shivaay_ARC-C\">ARC-C benchmark results</a></li>\n    </ul>\n<ul>\n      <li><a href=\"https://github.com/FuturixAI-and-Quantum-Works/Shivaay_GSM8K\">GSM8K benchmark results</a></li>\n    </ul>",
    "contentMarkdown": "#### [OpenHathi](https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base)\n\n*   Indian AI startup [Sarvam AI](https://www.sarvam.ai/) has released [OpenHathi-Hi-v0.1](https://www.sarvam.ai/blogs/openhathi-series), the first open-source LLM foundation model for Hindi. Developed on a budget-friendly platform, the model, a fine-tuned version of Llama2-7B, boasts GPT-3.5-like performance for Indic languages.\n*   OpenHathi, featuring a new sentence-piece tokenizer with a vocabulary size of 16K, trained on Hindi text, is merged with Llama2-7B’s tokenizer with a 32K original vocabulary (i.e., overall, a tokenizer with a 48K vocbulary), undergoes a two-phase training process. The initial phase focuses on embedding alignment, aligning randomly initialized Hindi embeddings, followed by bilingual language modeling, teaching the model cross-lingual attention across tokens.\n*   The model demonstrates robust performance across various Hindi tasks, comparable to, if not surpassing, GPT-3.5, while maintaining English proficiency. Sarvam AI’s evaluation includes non-academic, real-world tasks alongside standard Natural Language Generation (NLG) tasks. Evaluations against GPT-3.5 generation with GPT-4 as the judge revealed superior performance in Hindi, both in native and Romanised scripts.\n*   Developed in collaboration with academic partners at AI4Bhārat, who contributed language resources and benchmarks, and fine-tuned in partnership with KissanAI, the model leverages conversational data from a bot interacting with farmers in multiple languages.\n*   [Hugging Face](https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base)\n\n#### [BharatGPT](https://corover.ai/bharatgpt/)\n\n*   BharatGPT is an Indic LLM by [CoRover.ai](https://corover.ai/). It supports 14 Indian languages, with generative text, voice, and video.\n\n#### [Kannada Llama](https://www.tensoic.com/blog/kannada-llama/)\n\n*   [Kannada Llama](https://www.tensoic.com/blog/kannada-llama/) is aimed at enhancing the capabilities of the Llama 2 model for Kannada, a language predominantly spoken in Southwestern India.\n*   The project involves adapting the model to better handle Kannada text, notably through an expanded tokenizer. The tokenizer increases Llama 2’s vocabulary from 32K to 48K tokens, incorporating a SentencePiece tokenizer trained on Kannada text. This change significantly improves the model’s ability to process Kannada text, as demonstrated with examples.\n*   Kannada Llama also utilizes Low-Rank Adaptation (LoRA) for efficient pre-training and fine-tuning of the Kannada Base Llama for conversational abilities. Kannada Llama models and datasets have been released under open licenses for community development.\n\n#### [Tamil-LLaMA](https://arxiv.org/abs/2311.05845)\n\n*   Proposed in [Tamil-LLaMA: A New Tamil Language Model Based on LLaMA 2](https://arxiv.org/abs/2311.05845) by Abhinand Balachandran, Tamil-LLaMA is an enhancement of the open-source LLaMA model, tailored for Tamil language processing.\n*   The tokenization process is a crucial aspect of enhancing the model’s proficiency in handling the Tamil language. The integration of an additional 16,000 Tamil tokens into the LLaMA model’s vocabulary is a key step in this process. This expansion of the vocabulary allows for a more accurate and nuanced representation of the Tamil language, improving the model’s ability to understand and generate Tamil text. The tokenization specifically aims to address the unique linguistic features of Tamil, making the model more effective in tasks involving this language.\n*   The approach uses the Low-Rank Adaptation (LoRA) methodology for efficient model training, focusing on a comprehensive Tamil corpus. This ensures computational feasibility while enhancing the model’s robustness in text generation.\n*   Tamil-LLaMA utilizes datasets like CulturaX for pre-training and a Tamil-translated version of the Alpaca dataset, along with a subset of the OpenOrca dataset, for instruction fine-tuning.\n*   Key contributions include the expansion of LLaMA’s vocabulary with 16,000 Tamil tokens, training on a comprehensive Tamil dataset, and presenting Tamil-translated versions of Alpaca and OpenOrca datasets for instruction fine-tuning.\n*   Tamil LLaMA outperforms its predecessors and other open-source models in tasks specific to the Tamil language, demonstrating significant advancements in performance. The paper presents results from instruction tasks, showing Tamil-LLaMA’s superior performance in areas like reasoning, translation, code generation, and open question answering. It surpasses GPT-3.5-turbo in many tasks, according to evaluations using GPT-4.\n*   Performance comparison on the IndicSentiment-7B dataset (left) and the IndicGLUE Text Classification (right).\n\n![](../../../images/papers/Tamil-LLaMA.jpg)\n\n*   The paper emphasizes the importance of language diversity in LLMs and contributes to advancing language models for Indian languages, with public access to models, datasets, and code to foster further research.\n*   The table below shows a list of available models:\n\n![](../../../images/papers/Tamil-LLaMA2.jpg)\n\n*   [Code](https://github.com/abhinand5/tamil-llama)\n\n#### [Ambari](https://www.cognitivelab.in/blog/introducing-ambari)\n\n*   CognitiveLab’s [Ambari](https://www.cognitivelab.in/blog/introducing-ambari) is a series of open-source Bilingual Kannada-English LLMs.\n*   Ambari’s goal is to adapt language models to bridge the gap between Kannada and English, training on a modest 1 billion-token dataset, and identifying efficient training methods.\n*   A key focus was on specialized tokenization, where the team developed a custom tokenization model for Kannada using SentencePiece. This model was integrated with the base Llama tokenizer, resulting in an enhanced and comprehensive vocabulary of 49,600, expanded by 17,600 tokens. The approach involved training on three different dataset sizes, leading to optimal results with a 100,000 token dataset. The project’s main aim was to bridge linguistic gaps in LLMs and optimize training with a modest 1B-token dataset.\n*   The model’s journey encompasses stages like pre-training, bilingual next token prediction, and fine-tuning with a focus on language adaptability, efficient tokenization, and bilingual instruction.\n*   Ambari represents a step forward in the LLM landscape, addressing challenges like limited world knowledge and translation nuances, with future plans to incorporate Romanized Kannada and further refine the model.\n*   Its inaugural models, Ambari-7B-base-v0.1 and Ambari-7B-Instruct-v0.1, achieve impressive results on a compact 1 billion-token training dataset, trained across multiple stages.\n\n#### [Krutrim](https://blog.olakrutrim.com/press-release/)\n\n*   [Krutrim](https://blog.olakrutrim.com/press-release/) by Ola converses in 22 Indian languages and creates text in 10, trained on a staggering 2 trillion tokens using a specially crafted homegrown tokeniser tailored for Indic languages.\n\n#### [BengaliGPT](https://medium.com/@shantipriya.parida/odiagenai-released-an-instruction-following-llama-model-for-bengali-5804897ac4bf)\n\n*   OdiaGenAI’s [BengaliGPT](https://huggingface.co/OdiaGenAI/odiagenAI-bengali-lora-model-v1) model is based on Llama-7b and finetuned with a 252k Bengali instruction set. It also comes with an integrated Text-To-Speech (TTS) feature.\n*   The instruction set is available at Hugging Face for research and non-commercial purposes.\n*   The code (translation, training, and inference) is available on [GitHub](https://github.com/OdiaGenAI).\n\n#### [Gajendra](https://huggingface.co/BhabhaAI/Gajendra-v0.1)\n\n*   Gajendra is a 7B Hindi-Hinglish-English instruction finetuned model on top of [OpenHathi](#OpenHathi) using the [translation-classify](https://huggingface.co/datasets/BhabhaAI/translation-classify) dataset.\n\n#### [Airavata](https://ai4bharat.github.io/airavata/)\n\n*   Proposed in [Airavata: Introducing Hindi Instruction-Tuned LLM](https://arxiv.org/abs/2401.15006) by researchers from Nilekani Centre at AI4Bharat, IIT Madras, IIIT D&M Kancheepuram, Flipkart, University of Surrey, STAR, NICT, IBM Research, Microsoft, [Airavata](https://ai4bharat.github.io/airavata/) is an instruction-tuned Hindi LLM. This addresses the limited support for Indian languages in current LLMs.\n*   The model has been built by fine-tuning Sarvam AI’s OpenHathi, with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks.\n*   The paper describes creating high-quality Hindi datasets by translating English-supervised instruction-tuning datasets. The process involved using the IndicTrans2 model for translation, ensuring balanced task representation while retaining high-quality examples. Two native Hindi instruction datasets, wikiHow and Anudesh, were also created for training.\n*   The process of Instruction Tuning dataset creation can be summarized as follows:\n    *   Sampled instances from diverse English datasets for balance.\n    *   Translated sampled instances (instructions, input, and outputs) in English to Hindi using IndicTrans2. For translation, the authors chose IndicTrans2 over OpenAI models as IndicTrans2 is the state-of-the-art open-source MT model for Indian languages.\n    *   Filtered translated instances using chrF++ score to ensure quality.\n    *   Final instruction tuning dataset includes 385k instances.\n*   Airavata underwent supervised fine-tuning with these datasets using LoRA (Low-Rank Adaptation) technique, optimizing for specific hyperparameters. Model selection was based on evaluating checkpoints from different epochs on NLU and NLG tasks, leading to a blend of checkpoints providing balanced performance.\n*   The model was evaluated on various NLP benchmarks, including native Hindi test sets and translated English benchmarks. It showed significant improvement over the base model (OpenHathi) in most tasks, especially in aligning with the instruction-tuned dataset. The plot below from the paper shows an ablation experiment to understand the performance gaps between Full fine-tuning and LoRA fine-tuning across a mix of English and Hindi NLU tasks.\n\n*   Sampled instances from diverse English datasets for balance.\n*   Translated sampled instances (instructions, input, and outputs) in English to Hindi using IndicTrans2. For translation, the authors chose IndicTrans2 over OpenAI models as IndicTrans2 is the state-of-the-art open-source MT model for Indian languages.\n*   Filtered translated instances using chrF++ score to ensure quality.\n*   Final instruction tuning dataset includes 385k instances.\n\n![](../../../images/papers/Airavata.jpg)\n\n*   Human evaluation of Airavata focused on its ability to generate long-form text, provide factual opinions, make content accessible, demonstrate creativity in language, and answer culturally relevant questions. It was compared with other models like ChatGPT, GPT-4, and BactrianX-llama-7B.\n*   Despite showing promise, the paper acknowledges limitations such as potential generation of biased or objectionable content, and challenges in cultural nuances and mixed-language contexts. The model’s performance is closely tied to the quality and scope of its training data.\n*   Airavata and its associated resources have been released to facilitate further research in instruction-tuning for Indian language LLMs.\n*   [HuggingFace Repo](https://huggingface.co/ai4bharat/Airavata); [Instruction tuning dataset](https://huggingface.co/datasets/ai4bharat/indic-instruct-data-v0.1); [Airavata paper](https://arxiv.org/pdf/2401.15006.pdf)\n\n#### [MalayaLLM](https://github.com/VishnuPJ/MalayaLLM)\n\n*   MalayaLLM is a pioneering 7B LLaMA-2 Indic model, specifically pre-trained and fine-tuned for the Malayalam language using LoRA techniques. It aims to enhance generative AI capabilities in Malayalam by focusing on tasks like content generation and question answering.\n*   The MalayaLLM models have been improved and customized to incorporate a comprehensive Malayalam vocabulary comprising approximately 18,000 tokens, expanding upon the groundwork laid by the original LLaMA-2.\n*   While the model, in its early development stages, shows promise for Malayalam-specific applications, it requires further training and updates to reach optimal performance. The training leverages datasets from AI4Bharat and CulturaX, with fine-tuning on Alpaca\\_Instruct\\_Malayalam.\n*   [HuggingFace Repo](https://huggingface.co/collections/VishnuPJ/malayallm-65b0ddb50e37d746c6d07370); [Alpaca\\_Instruct\\_Malayalam](https://huggingface.co/datasets/VishnuPJ/Alpaca_Instruct_Malayalam)\n\n#### [Hanooman](https://analyticsindiamag.com/bharatgpt-unveils-hanooman-a-newsuite-of-indic-generative-ai-models/)\n\n*   The BharatGPT team, led by IIT Bombay and seven other top Indian engineering institutes, has announced the launch of the [Hanooman](https://analyticsindiamag.com/bharatgpt-unveils-hanooman-a-newsuite-of-indic-generative-ai-models/) series of language models for Indian languages with the help of Seetha Mahalaxmi Healthcare.\n*   Hanooman can respond in 11 prominent Indian languages, such as Hindi, Tamil, and Marathi, showcasing its versatility and inclusive linguistic proficiency.\n*   It can create text, speech, videos, and much more in various Indian languages, making communication more dynamic and accessible than ever before.\n\n#### [Navarasa 2.0](https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750)\n\n*   [Navarasa 2.0](https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750) are a series of instruction-tuned models based on Gemma 7B and 2B on 15 Indian languages, created by [Ravi Theja Desetty](https://www.linkedin.com/in/ravidesetty/) and [Ramsri Golla](https://www.linkedin.com/in/ramsrig/).\n*   Navarasa 2.0 supports Hindi, Telugu, Tamil, Kannada, Malayalam, Marathi, Gujarati, Bengali, Punjabi, Odia, Urdu, Konkani, Assamese, Nepali, Sindhi, and English.\n*   In addition, they have releasing models in GGUF format alongside a collection of Alpaca-cleaned datasets for all 15 languages.\n*   The models were fine-tuned on a single A100 machine which took approx. 44 hours for the 7B model and 18 hours for the 2B model.\n*   To demonstrate real-world information capabilities, they have provided examples of a Retrieval Augmented Generation (RAG) model using LlamaIndex in 15 Indian languages, along with datasets, and examples of reasoning, translation, and question answering with context in their [blog post](https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750).\n*   [Models](https://huggingface.co/collections/Telugu-LLM-Labs/navarasa-20-models-65f7c72addf0619cb0991309); [Datasets](https://huggingface.co/collections/Telugu-LLM-Labs/indic-alpaca-datasets-65f2a3687d5cdbce8880c581)\n\n#### [Gujju Llama 1.0](https://huggingface.co/sampoorna42/Gujju-Llama-Instruct-v0.1)\n\n*   At the core of Gujju Llama lies the powerful Llama 2 7B base model, which was expanded by incorporating a staggering 17,000 Gujarati tokens derived from the ‘Cultura X’ Gujarati subset. This strategic move extended the original 32,000 token vocabulary to an impressive 49,000, significantly enhancing the model’s ability to comprehend and generate Gujarati text.\n*   To further refine Gujju Llama understanding of the language, they conducted a meticulous fine-tuning process. They leveraged multiple high-quality datasets from Hugging Face 🤗, including Dolly, Alpaca, and Open-Orca. Additionally, they incorporated synthetic data generated by the Google’s Gemini API, culminating in a total of approximately 200,000 data points, meticulously translated into Gujarati using the Google Cloud Translation API.\n*   Powered by the advanced A100 80GB GPUs from RunPod’s serverless cloud platform, they dedicated 36 hours to pretraining and 52 hours to fine-tuning, ensuring a smooth and efficient training process.\n*   [Dataset](https://huggingface.co/datasets/sampoorna42/gujarati-alpaca-orca-dolly); [Base-model](https://huggingface.co/sampoorna42/gujju-llama-base-v1.0); [Instruction-tuned model](https://huggingface.co/sampoorna42/Gujju-Llama-Instruct-v0.1)\n\n#### [Pragna](https://soket.ai/blogs/pragna_1b)\n\n*   Pragna-1B from Soket AI Labs is India’s inaugural open-source multilingual model, Pragna-1B, meticulously crafted for Indian languages including Hindi, Gujarati, Bangla, and English.\n*   Pragna-1B is a decoder-only transformer model, inspired by TinyLlama and featuring the following specifications:\n    *   Layers: 22\n    *   Attention Heads: 32\n    *   Context Length: 2048\n    *   Hidden Dimension: 2048\n    *   Expansion Dimension: 5632\n    *   Vocabulary Size: 69632\n    *   Parameter Count: 1.25B\n*   This model incorporates Rotary Positional Encoding to infuse positional information into the embeddings, utilising a base of 10,000. It employs RMSNorm with an epsilon value of 1e-5 and the Sigmoid Activation Unit (SiLU) as the activation function. Additionally, Pragna-1B adopts Grouped Query Attention, an alternative to Multi-Head Attention, which enhances training and inference speed while reducing memory bandwidth. This also supports the use of lower-compute devices for inference tasks.\n*   Pragna’s BPE tokenizer has been specifically fine-tuned for Indian languages to enhance tokenization efficiency—significantly reducing the number of tokens per word and increasing throughput.\n*   Despite its compact size, Pragna-1B delivers performance on par with models nearly six times its size, making it a formidable tool for processing Indic languages.\n*   Pragna-1B leverages ‘Bhasha’ and other open datasets, training on a total of 150 billion tokens (overall 3.15 trillion), which mirrors the rich linguistic diversity of India.\n*   Designed for edge deployment, Pragna-1B offers powerful AI capabilities at the source of data generation, ensuring efficiency and effectiveness.\n*   [Hugging Face](https://huggingface.co/soketlabs/pragna-1b); [Blog](https://soket.ai/blogs/pragna_1b)\n\n*   Layers: 22\n*   Attention Heads: 32\n*   Context Length: 2048\n*   Hidden Dimension: 2048\n*   Expansion Dimension: 5632\n*   Vocabulary Size: 69632\n*   Parameter Count: 1.25B\n\n#### [Sarvam-1](https://www.sarvam.ai/blogs/sarvam-1)\n\n*   Sarvam-1 is a 2B parameter language model specifically optimized for Indian languages. It provides best in-class performance in 10 Indic languages (bn, gu, hi, kn, ml, mr, or, pa, ta, te) when compared with popular models like Gemma-2-2B and Llama-3.2-3B. It is also competitive against the much larger models like Llama-3.1-8B in these languages.\n*   The model was trained with [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo) on the Yotta Shakti Cloud using HGX H100 systems.\n*   **Key Features**:\n    *   **Optimized for 10 Indian Languages:** Built from the ground up to support major Indian languages alongside English\n    *   **Superior Token Efficiency:** Achieves fertility rates of 1.4-2.1 across all supported languages, 2-4x more efficient than existing multilingual models\n    *   **High-Quality Training Data:** Trained on a curated corpus of ~4 trillion tokens with 2 trillion high-quality Indic tokens\n    *   **Efficient Inference:** 4-6x faster inference compared to larger models while matching or exceeding their performance on Indic language tasks\n*   **Model Architecture**:\n    *   Hidden size: 2048\n    *   Intermediate size: 11,008\n    *   Number of attention heads: 16\n    *   Number of hidden layers: 28\n    *   Number of key-value heads: 8\n    *   Maximum position embeddings: 8,192\n    *   Activation function: SwiGLU\n    *   Positional embeddings: Rotary (RoPE) with theta=10,000\n    *   Training: Grouped-query attention and bfloat16 mixed-precision\n*   Note: This is a text-completion model. It is meant to be finetuned on downstream tasks, and cannot be used directly as a chat or an instruction-following model. Key Features.\n*   [Hugging Face](https://huggingface.co/sarvamai/sarvam-1); [Release blog](https://www.sarvam.ai/blogs/sarvam-1)\n\n*   **Optimized for 10 Indian Languages:** Built from the ground up to support major Indian languages alongside English\n*   **Superior Token Efficiency:** Achieves fertility rates of 1.4-2.1 across all supported languages, 2-4x more efficient than existing multilingual models\n*   **High-Quality Training Data:** Trained on a curated corpus of ~4 trillion tokens with 2 trillion high-quality Indic tokens\n*   **Efficient Inference:** 4-6x faster inference compared to larger models while matching or exceeding their performance on Indic language tasks\n\n*   Hidden size: 2048\n*   Intermediate size: 11,008\n*   Number of attention heads: 16\n*   Number of hidden layers: 28\n*   Number of key-value heads: 8\n*   Maximum position embeddings: 8,192\n*   Activation function: SwiGLU\n*   Positional embeddings: Rotary (RoPE) with theta=10,000\n*   Training: Grouped-query attention and bfloat16 mixed-precision\n\n#### [Nanda](https://huggingface.co/MBZUAI/Llama-3-Nanda-10B-Chat)\n\n*   Llama-3-Nanda-10B-Chat (or Nanda for short) is a 10 billion parameter pre-trained and instruction-tuned bilingual large language model for both Hindi and English, trained on a dataset containing 65 billion Hindi tokens. The model is based on transformer-based decoder-only (LLaMA-3) architecture. It implements Rotary Position Embeddings (RoPE), enabling the model to extrapolate to long sequence lengths, providing improved context handling and model precision.\n*   [Hugging Face](https://huggingface.co/MBZUAI/Llama-3-Nanda-10B-Chat); [Paper](https://github.com/mbzuai-nlp/Llama-3-Nanda-10B-Chat/blob/main/Llama-3-Nanda-10B-Chat-Paper.pdf)\n\n#### [Shivaay](https://shivaay.futurixai.com/)\n\n*   Shivaay is a 4B foundation model from FuturixAI with exceptional benchmark performance.\n*   ARC-Challenge: Ranked #3 globally with 91.04% accuracy, outperforming Claude 2, GPT-3.5, and Llama 3 8B—just 2 points behind GPT-4! View Leaderboard\n    *   [ARC-C benchmark results](https://github.com/FuturixAI-and-Quantum-Works/Shivaay_ARC-C)\n*   GSM8K: Ranked #11 among models not using extra training data, achieving 87.41% accuracy, surpassing GPT-4 and the 70B-parameter Gemma 70B! View Leaderboard\n    *   [GSM8K benchmark results](https://github.com/FuturixAI-and-Quantum-Works/Shivaay_GSM8K)\n*   [Playground](https://shivaay.futurixai.com/)\n\n*   [ARC-C benchmark results](https://github.com/FuturixAI-and-Quantum-Works/Shivaay_ARC-C)\n\n*   [GSM8K benchmark results](https://github.com/FuturixAI-and-Quantum-Works/Shivaay_GSM8K)",
    "order": 44,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 14,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "gpt",
      "llm",
      "nlp",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 2685,
      "contentLength": 25264
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#popular-indic-llms",
    "scrapedAt": "2025-12-28T11:53:26.177Z"
  },
  {
    "id": "ai-LLM-popular-code-llms-45",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Popular LLMs",
    "title": "Popular Code LLMs",
    "subtitle": "Popular LLMs",
    "contentHtml": "<h4 id=\"sqlcoder\"><a href=\"https://huggingface.co/defog/sqlcoder-34b-alpha\">SQLCoder</a></h4>\n<ul>\n  <li>Defog’s SQLCoder is a state-of-the-art LLM for converting natural language questions to SQL queries.</li>\n  <li>SQLCoder-34B is a 34B parameter model that outperforms gpt-4 and gpt-4-turbo for natural language to SQL generation tasks on Defog’s <a href=\"https://github.com/defog-ai/sql-eval\">sql-eval</a> framework, and significantly outperforms all popular open-source models.</li>\n  <li>SQLCoder-34B is fine-tuned on a base CodeLlama model.</li>\n  <li>Defog classified each generated question into one of 5 categories. The table below displays the percentage of questions answered correctly by each model, broken down by category.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/SQLCoder.png\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://github.com/defog-ai/sqlcoder\">Code</a>; <a href=\"https://defog.ai/sqlcoder-demo/\">Demo</a>; <a href=\"https://colab.research.google.com/drive/1z4rmOEiFkxkMiecAWeTUlPl0OmKgfEu7?usp=sharing\">Colab</a></li>\n</ul>\n<h4 id=\"panda-coder\"><a href=\"https://huggingface.co/aiplanet/panda-coder-13B\">Panda-Coder</a></h4>\n<ul>\n  <li>Panda Coder is a state-of-the-art LLM capable of generating code on the NLP based Instructions</li>\n  <li><strong>Model description:</strong>\n    <ul>\n      <li>Panda-Coder is a state-of-the-art LLM, a fine-tuned model, specifically designed to generate code based on natural language instructions. It’s the result of relentless innovation and meticulous fine-tuning, all to make coding easier and more accessible for everyone.</li>\n    </ul>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>NLP-Based Coding:</strong> With Panda-Coder, you can transform your plain text instructions into functional code effortlessly. No need to grapple with syntax and semantics – it understands your language.</li>\n      <li><strong>Precision and Efficiency:</strong> The model is tailored for accuracy, ensuring your code is not just functional but also efficient.</li>\n      <li><strong>Unleash Creativity:</strong> Whether you’re a novice or an expert coder, Panda-Coder is here to support your coding journey, offering creative solutions to your programming challenges.</li>\n      <li><strong>Evol Instruct Code:</strong> It’s built on the robust Evol Instruct Code 80k-v1 dataset, guaranteeing top-notch code generation.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Panda-Coder is a state-of-the-art LLM, a fine-tuned model, specifically designed to generate code based on natural language instructions. It’s the result of relentless innovation and meticulous fine-tuning, all to make coding easier and more accessible for everyone.</li>\n    </ul>\n<ul>\n      <li><strong>NLP-Based Coding:</strong> With Panda-Coder, you can transform your plain text instructions into functional code effortlessly. No need to grapple with syntax and semantics – it understands your language.</li>\n      <li><strong>Precision and Efficiency:</strong> The model is tailored for accuracy, ensuring your code is not just functional but also efficient.</li>\n      <li><strong>Unleash Creativity:</strong> Whether you’re a novice or an expert coder, Panda-Coder is here to support your coding journey, offering creative solutions to your programming challenges.</li>\n      <li><strong>Evol Instruct Code:</strong> It’s built on the robust Evol Instruct Code 80k-v1 dataset, guaranteeing top-notch code generation.</li>\n    </ul>\n<h4 id=\"magicoder\"><a href=\"https://arxiv.org/abs/2312.02120v1\">Magicoder</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2312.02120v1\">Magicoder: Source Code Is All You Need</a>.</li>\n  <li>As AI emerges as a vital co-pilot for programmers, the need for high-quality and reliable machine-generated code is increasingly crucial.</li>\n  <li>This paper by Wei et al. from UIUC and Tsinghua introduces “Magicoder,” a fully open-source series of LLMs dedicated to code generation. Notably, Magicoder models, despite having no more than 7 billion parameters, significantly close the gap with top-tier code models.</li>\n  <li>The core innovation behind Magicoder is OSS-Instruct, a unique approach to code instruction tuning that incorporates real open-source code snippets into the training process as source references. OSS-Instruct functions by prompting an LLM, such as ChatGPT, to generate coding problems and solutions based on seed code snippets sourced from platforms like GitHub. This process not only enables the creation of diverse coding challenges but also mirrors real-world programming scenarios.This methodology enhances the diversity, realism, and controllability of the generated code. Magicoder is trained on 75,000 synthetic instruction data using OSS-Instruct, a novel approach leveraging open-source code snippets to generate high-quality instruction data for coding. The result is a model that not only surpasses its 7 billion parameter counterparts but also competes closely with the 34 billion parameter version of WizardCoder-SC.</li>\n  <li>The following figure from the paper shows an overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval+.</li>\n</ul>\n<p><img src=\"../../../images/papers/OSS-INSTRUCT1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The following figure from the paper shows the detailed prompt design for OSS-INSTRUCT.</li>\n</ul>\n<p><img src=\"../../../images/papers/OSS-INSTRUCT2.jpg\" alt=\"\"></p>\n<ul>\n  <li>The implementation details highlight the use of GPT-3.5-turbo-1106 as the foundation model for OSS-INSTRUCT, chosen for its cost-effectiveness. The process involves extracting 1–15 lines from each code document to generate coding problems and solutions, aiming for consistency. Data decontamination is applied to ensure quality and originality. The training process uses CodeLlama-Python-7B and DeepSeek-Coder-Base 6.7B as base LLMs, with fine-tuning on the OSS-Instruct dataset using PyTorch’s Distributed Data Parallel module, Adafactor optimizer, and specific learning rates and batch sizes. For MagicoderS, the models are further fine-tuned with an additional dataset from Evol-Instruct.</li>\n  <li>The evaluation of Magicoder involved benchmarks like HumanEval and MBPP, with enhanced versions (HumanEval+ and MBPP+) used for more rigorous testing. The evaluation included a range of baseline models for comparison. Magicoder-CL demonstrated clear improvements over its base model and surpassed most open-source models, including WizardCoder-SC-15B, in HumanEval and HumanEval+ benchmarks. The advanced MagicoderS-CL, trained with Evol-Instruct, outperformed all other models, including ChatGPT, especially in more challenging settings, suggesting its ability to generate more robust code</li>\n  <li>Despite having only 7 billion parameters, it challenges much larger models, rivaling even those 5 times its size. This impressive feat is achieved while ensuring both its weights and data are open, a critical aspect for fostering transparent and rapid advancements.</li>\n  <li>While GPT-4 remains the top performer in terms of raw output, the rapid advancements in models like Magicoder suggest that scaling these new approaches to 70 billion parameters and beyond is on the horizon. Such a development could signify a paradigm shift in the field, potentially surpassing OpenAI’s current offerings, unless GPT-5 enters the scene first. Magicoder, with its innovative training approach and impressive capabilities, is a strong contender in this evolving landscape, offering a glimpse into a future where AI-generated code becomes even more sophisticated and reliable.</li>\n  <li><a href=\"https://github.com/ise-uiuc/magicoder\">Code</a>.</li>\n</ul>\n<h4 id=\"alphacode-2\"><a href=\"https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf\">AlphaCode 2</a></h4>\n<ul>\n  <li>This technical report by the AlphaCode Team at Google DeepMind, published in 2023, introduces AlphaCode 2, an AI system that significantly improves competitive programming performance. It utilizes the Gemini model for various components like code generation and reranking.</li>\n  <li>The system’s approach includes fine-tuning Gemini Pro models using the GOLD training objective and datasets with numerous human code samples. This results in a diverse family of policy models.</li>\n  <li>AlphaCode 2 adopts an advanced search and reranking mechanism, involving sampling with diverse code samples, filtering out non-compliant samples, clustering similar samples, and employing a scoring model to select the best solutions.</li>\n  <li>Specific details are as follows:\n    <ul>\n      <li><strong>Policy and Fine-Tuning:</strong> AlphaCode 2 employs Gemini Pro models, fine-tuned using the <a href=\"https://arxiv.org/abs/2009.07839\">GOLD objective</a> on a large dataset of human code samples. First, they fine-tune on an updated version of the <a href=\"https://github.com/google-deepmind/code_contests\">CodeContests dataset</a> (containing more problems, more solutions and higher quality, manually-curated tests on the validation set). This dataset contains approximately 15 thousand problems and 30 million human code samples. They generate several fine-tuned models by varying hyperparameters, and end up with a family of fine-tuned models. Second, we conduct a few additional steps of fine-tuning on a different, higher-quality dataset. Relying on a family of policies instead of a single one allows them to maximize diversity, which remains key to tackling hard problems.</li>\n      <li><strong>Sampling:</strong> Their sampling approach is close to that of AlphaCode. They generate up to a million code samples per problem, using a randomized temperature parameter for each sample to encourage diversity. They also randomize targeted metadata included in the prompt, such as the problem difficulty rating and its categorical tags. They split their sampling budget evenly across our family of fine-tuned models. While they sampled in Python and C++ for AlphaCode, they only used C++ samples for AlphaCode 2 as they found them to be higher quality. Massive sampling allows us to search the model distribution thoroughly and generate a large diversity of code samples, maximizing the likelihood of generating at least some correct samples. Given the amount of samples, filtering and reranking are of paramount importance to the overall system’s performance, as they only submit a maximum of 10 code samples per problem.</li>\n      <li><strong>Filtering:</strong> Code samples are filtered based on compliance with problem descriptions, with approximately 95% being removed for non-conformity or compilation issues.</li>\n      <li><strong>Clustering:</strong> Remaining candidates are clustered based on runtime behavior, reducing redundancy. The 10 largest clusters are retained for further evaluation.</li>\n      <li><strong>Scoring Model:</strong> A second Gemini Pro model scores each candidate, selecting the best from each cluster for submission.</li>\n      <li><strong>Evaluation:</strong> Tested on Codeforces, AlphaCode 2 solved 43% of problems, a significant improvement over its predecessor, placing it in the top 15% of competitors.</li>\n    </ul>\n  </li>\n  <li>The following figure from the report shows a high-level overview of the AlphaCode 2 system.</li>\n</ul>\n<ul>\n      <li><strong>Policy and Fine-Tuning:</strong> AlphaCode 2 employs Gemini Pro models, fine-tuned using the <a href=\"https://arxiv.org/abs/2009.07839\">GOLD objective</a> on a large dataset of human code samples. First, they fine-tune on an updated version of the <a href=\"https://github.com/google-deepmind/code_contests\">CodeContests dataset</a> (containing more problems, more solutions and higher quality, manually-curated tests on the validation set). This dataset contains approximately 15 thousand problems and 30 million human code samples. They generate several fine-tuned models by varying hyperparameters, and end up with a family of fine-tuned models. Second, we conduct a few additional steps of fine-tuning on a different, higher-quality dataset. Relying on a family of policies instead of a single one allows them to maximize diversity, which remains key to tackling hard problems.</li>\n      <li><strong>Sampling:</strong> Their sampling approach is close to that of AlphaCode. They generate up to a million code samples per problem, using a randomized temperature parameter for each sample to encourage diversity. They also randomize targeted metadata included in the prompt, such as the problem difficulty rating and its categorical tags. They split their sampling budget evenly across our family of fine-tuned models. While they sampled in Python and C++ for AlphaCode, they only used C++ samples for AlphaCode 2 as they found them to be higher quality. Massive sampling allows us to search the model distribution thoroughly and generate a large diversity of code samples, maximizing the likelihood of generating at least some correct samples. Given the amount of samples, filtering and reranking are of paramount importance to the overall system’s performance, as they only submit a maximum of 10 code samples per problem.</li>\n      <li><strong>Filtering:</strong> Code samples are filtered based on compliance with problem descriptions, with approximately 95% being removed for non-conformity or compilation issues.</li>\n      <li><strong>Clustering:</strong> Remaining candidates are clustered based on runtime behavior, reducing redundancy. The 10 largest clusters are retained for further evaluation.</li>\n      <li><strong>Scoring Model:</strong> A second Gemini Pro model scores each candidate, selecting the best from each cluster for submission.</li>\n      <li><strong>Evaluation:</strong> Tested on Codeforces, AlphaCode 2 solved 43% of problems, a significant improvement over its predecessor, placing it in the top 15% of competitors.</li>\n    </ul>\n<p><img src=\"../../../images/papers/AlphaCode2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Evaluation on Codeforces shows AlphaCode 2 solves 43% of problems, a marked improvement over the original AlphaCode. It ranks around the 85th percentile among competitors, illustrating significant advances in AI’s reasoning and problem-solving in competitive programming.</li>\n  <li>The system’s sample efficiency and adaptability highlight its potential for interactive programming, aiding human coders in reasoning and code design.</li>\n</ul>\n<h4 id=\"phind-70b\"><a href=\"https://www.phind.com/blog/introducing-phind-70b\">Phind-70B</a></h4>\n<ul>\n  <li>Phind-70B closes the code generation quality gap with GPT-4 Turbo and is 4x faster. It can generate 80+ token/s (GPT-4 is reported to generate ~20 tokens/s).</li>\n  <li>Phind-70B is based on CodeLlama-70B and tuned on 50B tokens more, with a context window of 32K tokens.</li>\n  <li>Phind-70B is also less “lazy” than GPT-4 Turbo and doesn’t hesitate to generate detailed code examples.</li>\n</ul>\n<h4 id=\"granite\"><a href=\"https://research.ibm.com/blog/granite-code-models-open-source\">Granite</a></h4>\n<ul>\n  <li>IBM’s Granite code models are a family of high-performing code language models ranging from 3B to 34B parameters. They cover 116 programming languages and excel at code generation, fixing bugs, explaining code, and more. They are available in base and instruction-following variants for different use cases.</li>\n  <li>They outperforms existing open source models on major coding benchmarks and are optimized for enterprise software development workflows.</li>\n  <li>They are trained on high-quality data adhering to IBM’s AI ethics principles</li>\n  <li>With Granite models, developers can build innovative tools to streamline their workflow:\n    <ul>\n      <li>Automated code writing from plain English instructions</li>\n      <li>Code debugging and issue fixing assistants</li>\n      <li>Legacy application modernization and code translation</li>\n      <li>Automated documentation and unit test generation</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Automated code writing from plain English instructions</li>\n      <li>Code debugging and issue fixing assistants</li>\n      <li>Legacy application modernization and code translation</li>\n      <li>Automated documentation and unit test generation</li>\n    </ul>\n<h4 id=\"starcoder2\"><a href=\"https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1\">StarCoder2</a></h4>\n<ul>\n  <li>StarCoder2-15B-Instruct-v0.1 is the first entirely self-aligned code LLM trained with a fully permissive and transparent pipeline. Their open-source pipeline uses StarCoder2-15B to generate thousands of instruction-response pairs, which are then used to fine-tune StarCoder-15B itself without any human annotations or distilled data from huge and proprietary LLMs.</li>\n  <li>StarCoder2 Instruct achieves 72% HumanEval score using only self-generated content without any GPT-3.5/4 data.</li>\n  <li>This work demonstrates that self-instruct works already well at the 15B scale without data from proprietary models thus making StarCoder2 Instruct fully permissive.</li>\n  <li><a href=\"https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1\">Model</a>; <a href=\"https://github.com/bigcode-project/starcoder2-self-align\">Code</a>; <a href=\"https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k/\">Dataset</a></li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/StarCoder2.png\" alt=\"\"></p>\n<h4 id=\"codegemma\"><a href=\"https://huggingface.co/google/codegemma-7b-it\">CodeGemma</a></h4>\n<ul>\n  <li>CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models are text-to-text and text-to-code decoder-only models and are available as a 7 billion pretrained variant that specializes in code completion and code generation tasks, a 7 billion parameter instruction-tuned variant for code chat and instruction following and a 2 billion parameter pretrained variant for fast code completion.</li>\n</ul>\n<p><img src=\"assets/LLM/CodeGemma.png\" alt=\"\"></p>\n<h4 id=\"deepseek-coder-v2\"><a href=\"https://arxiv.org/abs/2406.11931v1\">DeepSeek-Coder-V2</a></h4>\n<ul>\n  <li>DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model developed to achieve performance comparable to GPT-4 Turbo in code-specific tasks. It extends the DeepSeek-V2 model, being further pre-trained with an additional 6 trillion tokens. This enhanced model significantly improves coding and mathematical reasoning capabilities while maintaining comparable performance in general language tasks. It supports 338 programming languages and extends the context length from 16K to 128K tokens.</li>\n  <li><strong>Implementation Details:</strong>\n    <ol>\n      <li><strong>Pre-training Dataset Composition:</strong>\n        <ul>\n          <li>60% source code</li>\n          <li>10% math corpus</li>\n          <li>30% natural language corpus</li>\n          <li>Source code tokens (1,170B) from GitHub and CommonCrawl</li>\n          <li>Math corpus (221B tokens) from CommonCrawl</li>\n          <li>Natural language corpus from the DeepSeek-V2 dataset</li>\n        </ul>\n      </li>\n      <li><strong>Model Training:</strong>\n        <ul>\n          <li>DeepSeek-Coder-V2 models with 16B and 236B parameters</li>\n          <li>Activation parameters of 2.4B and 21B, respectively</li>\n          <li>Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model</li>\n          <li>Training issues addressed by reverting from exponential to conventional normalization</li>\n        </ul>\n      </li>\n      <li><strong>Training Hyper-Parameters:</strong>\n        <ul>\n          <li>AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1</li>\n          <li>Cosine decay strategy for learning rate scheduling</li>\n          <li>Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens</li>\n        </ul>\n      </li>\n      <li><strong>Alignment:</strong>\n        <ul>\n          <li>Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data</li>\n          <li>Reinforcement learning with Group Relative Policy Optimization (GRPO)</li>\n          <li>Use of compiler feedback and test cases for coding domain preference data</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Evaluation:</strong>\n    <ul>\n      <li><strong>Code Generation Benchmarks:</strong>\n        <ul>\n          <li>Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models</li>\n          <li>Demonstrated superior performance on LiveCodeBench and USACO benchmarks</li>\n          <li>Achieved top-tier results in multilingual code generation</li>\n        </ul>\n      </li>\n      <li><strong>Code Completion:</strong>\n        <ul>\n          <li>Comparable performance to larger models in repository-level code completion (RepoBench v1.1)</li>\n          <li>High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%</li>\n        </ul>\n      </li>\n      <li><strong>Code Fixing:</strong>\n        <ul>\n          <li>Outperformed other open-source models on Defects4J and Aider datasets</li>\n          <li>Achieved the highest score on SWE-Bench</li>\n        </ul>\n      </li>\n      <li><strong>Code Understanding and Reasoning:</strong>\n        <ul>\n          <li>High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities</li>\n        </ul>\n      </li>\n      <li><strong>Mathematical Reasoning:</strong>\n        <ul>\n          <li>Achieved 75.7% on MATH and 53.7% on Math Odyssey</li>\n          <li>Solved more AIME 2024 problems compared to other models</li>\n        </ul>\n      </li>\n      <li><strong>General Natural Language:</strong>\n        <ul>\n          <li>Comparable general language performance to DeepSeek-V2</li>\n          <li>Strong results in reasoning-related benchmarks</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>In conclusion, DeepSeek-Coder-V2 significantly advances the capabilities of open-source code models, achieving performance levels comparable to state-of-the-art closed-source models. It excels in coding and mathematical reasoning while maintaining robust general language understanding. Future work will focus on enhancing the model’s instruction-following capabilities to better handle real-world complex programming scenarios.</li>\n  <li><a href=\"https://github.com/deepseek-ai/DeepSeek-Coder-V2\">Code</a></li>\n</ul>\n<ol>\n      <li><strong>Pre-training Dataset Composition:</strong>\n        <ul>\n          <li>60% source code</li>\n          <li>10% math corpus</li>\n          <li>30% natural language corpus</li>\n          <li>Source code tokens (1,170B) from GitHub and CommonCrawl</li>\n          <li>Math corpus (221B tokens) from CommonCrawl</li>\n          <li>Natural language corpus from the DeepSeek-V2 dataset</li>\n        </ul>\n      </li>\n      <li><strong>Model Training:</strong>\n        <ul>\n          <li>DeepSeek-Coder-V2 models with 16B and 236B parameters</li>\n          <li>Activation parameters of 2.4B and 21B, respectively</li>\n          <li>Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model</li>\n          <li>Training issues addressed by reverting from exponential to conventional normalization</li>\n        </ul>\n      </li>\n      <li><strong>Training Hyper-Parameters:</strong>\n        <ul>\n          <li>AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1</li>\n          <li>Cosine decay strategy for learning rate scheduling</li>\n          <li>Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens</li>\n        </ul>\n      </li>\n      <li><strong>Alignment:</strong>\n        <ul>\n          <li>Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data</li>\n          <li>Reinforcement learning with Group Relative Policy Optimization (GRPO)</li>\n          <li>Use of compiler feedback and test cases for coding domain preference data</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>60% source code</li>\n          <li>10% math corpus</li>\n          <li>30% natural language corpus</li>\n          <li>Source code tokens (1,170B) from GitHub and CommonCrawl</li>\n          <li>Math corpus (221B tokens) from CommonCrawl</li>\n          <li>Natural language corpus from the DeepSeek-V2 dataset</li>\n        </ul>\n<ul>\n          <li>DeepSeek-Coder-V2 models with 16B and 236B parameters</li>\n          <li>Activation parameters of 2.4B and 21B, respectively</li>\n          <li>Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model</li>\n          <li>Training issues addressed by reverting from exponential to conventional normalization</li>\n        </ul>\n<ul>\n          <li>AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1</li>\n          <li>Cosine decay strategy for learning rate scheduling</li>\n          <li>Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens</li>\n        </ul>\n<ul>\n          <li>Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data</li>\n          <li>Reinforcement learning with Group Relative Policy Optimization (GRPO)</li>\n          <li>Use of compiler feedback and test cases for coding domain preference data</li>\n        </ul>\n<ul>\n      <li><strong>Code Generation Benchmarks:</strong>\n        <ul>\n          <li>Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models</li>\n          <li>Demonstrated superior performance on LiveCodeBench and USACO benchmarks</li>\n          <li>Achieved top-tier results in multilingual code generation</li>\n        </ul>\n      </li>\n      <li><strong>Code Completion:</strong>\n        <ul>\n          <li>Comparable performance to larger models in repository-level code completion (RepoBench v1.1)</li>\n          <li>High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%</li>\n        </ul>\n      </li>\n      <li><strong>Code Fixing:</strong>\n        <ul>\n          <li>Outperformed other open-source models on Defects4J and Aider datasets</li>\n          <li>Achieved the highest score on SWE-Bench</li>\n        </ul>\n      </li>\n      <li><strong>Code Understanding and Reasoning:</strong>\n        <ul>\n          <li>High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities</li>\n        </ul>\n      </li>\n      <li><strong>Mathematical Reasoning:</strong>\n        <ul>\n          <li>Achieved 75.7% on MATH and 53.7% on Math Odyssey</li>\n          <li>Solved more AIME 2024 problems compared to other models</li>\n        </ul>\n      </li>\n      <li><strong>General Natural Language:</strong>\n        <ul>\n          <li>Comparable general language performance to DeepSeek-V2</li>\n          <li>Strong results in reasoning-related benchmarks</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models</li>\n          <li>Demonstrated superior performance on LiveCodeBench and USACO benchmarks</li>\n          <li>Achieved top-tier results in multilingual code generation</li>\n        </ul>\n<ul>\n          <li>Comparable performance to larger models in repository-level code completion (RepoBench v1.1)</li>\n          <li>High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%</li>\n        </ul>\n<ul>\n          <li>Outperformed other open-source models on Defects4J and Aider datasets</li>\n          <li>Achieved the highest score on SWE-Bench</li>\n        </ul>\n<ul>\n          <li>High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities</li>\n        </ul>\n<ul>\n          <li>Achieved 75.7% on MATH and 53.7% on Math Odyssey</li>\n          <li>Solved more AIME 2024 problems compared to other models</li>\n        </ul>\n<ul>\n          <li>Comparable general language performance to DeepSeek-V2</li>\n          <li>Strong results in reasoning-related benchmarks</li>\n        </ul>",
    "contentMarkdown": "#### [SQLCoder](https://huggingface.co/defog/sqlcoder-34b-alpha)\n\n*   Defog’s SQLCoder is a state-of-the-art LLM for converting natural language questions to SQL queries.\n*   SQLCoder-34B is a 34B parameter model that outperforms gpt-4 and gpt-4-turbo for natural language to SQL generation tasks on Defog’s [sql-eval](https://github.com/defog-ai/sql-eval) framework, and significantly outperforms all popular open-source models.\n*   SQLCoder-34B is fine-tuned on a base CodeLlama model.\n*   Defog classified each generated question into one of 5 categories. The table below displays the percentage of questions answered correctly by each model, broken down by category.\n\n![](/primers/ai/assets/LLM/SQLCoder.png)\n\n*   [Code](https://github.com/defog-ai/sqlcoder); [Demo](https://defog.ai/sqlcoder-demo/); [Colab](https://colab.research.google.com/drive/1z4rmOEiFkxkMiecAWeTUlPl0OmKgfEu7?usp=sharing)\n\n#### [Panda-Coder](https://huggingface.co/aiplanet/panda-coder-13B)\n\n*   Panda Coder is a state-of-the-art LLM capable of generating code on the NLP based Instructions\n*   **Model description:**\n    *   Panda-Coder is a state-of-the-art LLM, a fine-tuned model, specifically designed to generate code based on natural language instructions. It’s the result of relentless innovation and meticulous fine-tuning, all to make coding easier and more accessible for everyone.\n*   **Key Features:**\n    *   **NLP-Based Coding:** With Panda-Coder, you can transform your plain text instructions into functional code effortlessly. No need to grapple with syntax and semantics – it understands your language.\n    *   **Precision and Efficiency:** The model is tailored for accuracy, ensuring your code is not just functional but also efficient.\n    *   **Unleash Creativity:** Whether you’re a novice or an expert coder, Panda-Coder is here to support your coding journey, offering creative solutions to your programming challenges.\n    *   **Evol Instruct Code:** It’s built on the robust Evol Instruct Code 80k-v1 dataset, guaranteeing top-notch code generation.\n\n*   Panda-Coder is a state-of-the-art LLM, a fine-tuned model, specifically designed to generate code based on natural language instructions. It’s the result of relentless innovation and meticulous fine-tuning, all to make coding easier and more accessible for everyone.\n\n*   **NLP-Based Coding:** With Panda-Coder, you can transform your plain text instructions into functional code effortlessly. No need to grapple with syntax and semantics – it understands your language.\n*   **Precision and Efficiency:** The model is tailored for accuracy, ensuring your code is not just functional but also efficient.\n*   **Unleash Creativity:** Whether you’re a novice or an expert coder, Panda-Coder is here to support your coding journey, offering creative solutions to your programming challenges.\n*   **Evol Instruct Code:** It’s built on the robust Evol Instruct Code 80k-v1 dataset, guaranteeing top-notch code generation.\n\n#### [Magicoder](https://arxiv.org/abs/2312.02120v1)\n\n*   Proposed in [Magicoder: Source Code Is All You Need](https://arxiv.org/abs/2312.02120v1).\n*   As AI emerges as a vital co-pilot for programmers, the need for high-quality and reliable machine-generated code is increasingly crucial.\n*   This paper by Wei et al. from UIUC and Tsinghua introduces “Magicoder,” a fully open-source series of LLMs dedicated to code generation. Notably, Magicoder models, despite having no more than 7 billion parameters, significantly close the gap with top-tier code models.\n*   The core innovation behind Magicoder is OSS-Instruct, a unique approach to code instruction tuning that incorporates real open-source code snippets into the training process as source references. OSS-Instruct functions by prompting an LLM, such as ChatGPT, to generate coding problems and solutions based on seed code snippets sourced from platforms like GitHub. This process not only enables the creation of diverse coding challenges but also mirrors real-world programming scenarios.This methodology enhances the diversity, realism, and controllability of the generated code. Magicoder is trained on 75,000 synthetic instruction data using OSS-Instruct, a novel approach leveraging open-source code snippets to generate high-quality instruction data for coding. The result is a model that not only surpasses its 7 billion parameter counterparts but also competes closely with the 34 billion parameter version of WizardCoder-SC.\n*   The following figure from the paper shows an overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval+.\n\n![](../../../images/papers/OSS-INSTRUCT1.jpg)\n\n*   The following figure from the paper shows the detailed prompt design for OSS-INSTRUCT.\n\n![](../../../images/papers/OSS-INSTRUCT2.jpg)\n\n*   The implementation details highlight the use of GPT-3.5-turbo-1106 as the foundation model for OSS-INSTRUCT, chosen for its cost-effectiveness. The process involves extracting 1–15 lines from each code document to generate coding problems and solutions, aiming for consistency. Data decontamination is applied to ensure quality and originality. The training process uses CodeLlama-Python-7B and DeepSeek-Coder-Base 6.7B as base LLMs, with fine-tuning on the OSS-Instruct dataset using PyTorch’s Distributed Data Parallel module, Adafactor optimizer, and specific learning rates and batch sizes. For MagicoderS, the models are further fine-tuned with an additional dataset from Evol-Instruct.\n*   The evaluation of Magicoder involved benchmarks like HumanEval and MBPP, with enhanced versions (HumanEval+ and MBPP+) used for more rigorous testing. The evaluation included a range of baseline models for comparison. Magicoder-CL demonstrated clear improvements over its base model and surpassed most open-source models, including WizardCoder-SC-15B, in HumanEval and HumanEval+ benchmarks. The advanced MagicoderS-CL, trained with Evol-Instruct, outperformed all other models, including ChatGPT, especially in more challenging settings, suggesting its ability to generate more robust code\n*   Despite having only 7 billion parameters, it challenges much larger models, rivaling even those 5 times its size. This impressive feat is achieved while ensuring both its weights and data are open, a critical aspect for fostering transparent and rapid advancements.\n*   While GPT-4 remains the top performer in terms of raw output, the rapid advancements in models like Magicoder suggest that scaling these new approaches to 70 billion parameters and beyond is on the horizon. Such a development could signify a paradigm shift in the field, potentially surpassing OpenAI’s current offerings, unless GPT-5 enters the scene first. Magicoder, with its innovative training approach and impressive capabilities, is a strong contender in this evolving landscape, offering a glimpse into a future where AI-generated code becomes even more sophisticated and reliable.\n*   [Code](https://github.com/ise-uiuc/magicoder).\n\n#### [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)\n\n*   This technical report by the AlphaCode Team at Google DeepMind, published in 2023, introduces AlphaCode 2, an AI system that significantly improves competitive programming performance. It utilizes the Gemini model for various components like code generation and reranking.\n*   The system’s approach includes fine-tuning Gemini Pro models using the GOLD training objective and datasets with numerous human code samples. This results in a diverse family of policy models.\n*   AlphaCode 2 adopts an advanced search and reranking mechanism, involving sampling with diverse code samples, filtering out non-compliant samples, clustering similar samples, and employing a scoring model to select the best solutions.\n*   Specific details are as follows:\n    *   **Policy and Fine-Tuning:** AlphaCode 2 employs Gemini Pro models, fine-tuned using the [GOLD objective](https://arxiv.org/abs/2009.07839) on a large dataset of human code samples. First, they fine-tune on an updated version of the [CodeContests dataset](https://github.com/google-deepmind/code_contests) (containing more problems, more solutions and higher quality, manually-curated tests on the validation set). This dataset contains approximately 15 thousand problems and 30 million human code samples. They generate several fine-tuned models by varying hyperparameters, and end up with a family of fine-tuned models. Second, we conduct a few additional steps of fine-tuning on a different, higher-quality dataset. Relying on a family of policies instead of a single one allows them to maximize diversity, which remains key to tackling hard problems.\n    *   **Sampling:** Their sampling approach is close to that of AlphaCode. They generate up to a million code samples per problem, using a randomized temperature parameter for each sample to encourage diversity. They also randomize targeted metadata included in the prompt, such as the problem difficulty rating and its categorical tags. They split their sampling budget evenly across our family of fine-tuned models. While they sampled in Python and C++ for AlphaCode, they only used C++ samples for AlphaCode 2 as they found them to be higher quality. Massive sampling allows us to search the model distribution thoroughly and generate a large diversity of code samples, maximizing the likelihood of generating at least some correct samples. Given the amount of samples, filtering and reranking are of paramount importance to the overall system’s performance, as they only submit a maximum of 10 code samples per problem.\n    *   **Filtering:** Code samples are filtered based on compliance with problem descriptions, with approximately 95% being removed for non-conformity or compilation issues.\n    *   **Clustering:** Remaining candidates are clustered based on runtime behavior, reducing redundancy. The 10 largest clusters are retained for further evaluation.\n    *   **Scoring Model:** A second Gemini Pro model scores each candidate, selecting the best from each cluster for submission.\n    *   **Evaluation:** Tested on Codeforces, AlphaCode 2 solved 43% of problems, a significant improvement over its predecessor, placing it in the top 15% of competitors.\n*   The following figure from the report shows a high-level overview of the AlphaCode 2 system.\n\n*   **Policy and Fine-Tuning:** AlphaCode 2 employs Gemini Pro models, fine-tuned using the [GOLD objective](https://arxiv.org/abs/2009.07839) on a large dataset of human code samples. First, they fine-tune on an updated version of the [CodeContests dataset](https://github.com/google-deepmind/code_contests) (containing more problems, more solutions and higher quality, manually-curated tests on the validation set). This dataset contains approximately 15 thousand problems and 30 million human code samples. They generate several fine-tuned models by varying hyperparameters, and end up with a family of fine-tuned models. Second, we conduct a few additional steps of fine-tuning on a different, higher-quality dataset. Relying on a family of policies instead of a single one allows them to maximize diversity, which remains key to tackling hard problems.\n*   **Sampling:** Their sampling approach is close to that of AlphaCode. They generate up to a million code samples per problem, using a randomized temperature parameter for each sample to encourage diversity. They also randomize targeted metadata included in the prompt, such as the problem difficulty rating and its categorical tags. They split their sampling budget evenly across our family of fine-tuned models. While they sampled in Python and C++ for AlphaCode, they only used C++ samples for AlphaCode 2 as they found them to be higher quality. Massive sampling allows us to search the model distribution thoroughly and generate a large diversity of code samples, maximizing the likelihood of generating at least some correct samples. Given the amount of samples, filtering and reranking are of paramount importance to the overall system’s performance, as they only submit a maximum of 10 code samples per problem.\n*   **Filtering:** Code samples are filtered based on compliance with problem descriptions, with approximately 95% being removed for non-conformity or compilation issues.\n*   **Clustering:** Remaining candidates are clustered based on runtime behavior, reducing redundancy. The 10 largest clusters are retained for further evaluation.\n*   **Scoring Model:** A second Gemini Pro model scores each candidate, selecting the best from each cluster for submission.\n*   **Evaluation:** Tested on Codeforces, AlphaCode 2 solved 43% of problems, a significant improvement over its predecessor, placing it in the top 15% of competitors.\n\n![](../../../images/papers/AlphaCode2.jpg)\n\n*   Evaluation on Codeforces shows AlphaCode 2 solves 43% of problems, a marked improvement over the original AlphaCode. It ranks around the 85th percentile among competitors, illustrating significant advances in AI’s reasoning and problem-solving in competitive programming.\n*   The system’s sample efficiency and adaptability highlight its potential for interactive programming, aiding human coders in reasoning and code design.\n\n#### [Phind-70B](https://www.phind.com/blog/introducing-phind-70b)\n\n*   Phind-70B closes the code generation quality gap with GPT-4 Turbo and is 4x faster. It can generate 80+ token/s (GPT-4 is reported to generate ~20 tokens/s).\n*   Phind-70B is based on CodeLlama-70B and tuned on 50B tokens more, with a context window of 32K tokens.\n*   Phind-70B is also less “lazy” than GPT-4 Turbo and doesn’t hesitate to generate detailed code examples.\n\n#### [Granite](https://research.ibm.com/blog/granite-code-models-open-source)\n\n*   IBM’s Granite code models are a family of high-performing code language models ranging from 3B to 34B parameters. They cover 116 programming languages and excel at code generation, fixing bugs, explaining code, and more. They are available in base and instruction-following variants for different use cases.\n*   They outperforms existing open source models on major coding benchmarks and are optimized for enterprise software development workflows.\n*   They are trained on high-quality data adhering to IBM’s AI ethics principles\n*   With Granite models, developers can build innovative tools to streamline their workflow:\n    *   Automated code writing from plain English instructions\n    *   Code debugging and issue fixing assistants\n    *   Legacy application modernization and code translation\n    *   Automated documentation and unit test generation\n\n*   Automated code writing from plain English instructions\n*   Code debugging and issue fixing assistants\n*   Legacy application modernization and code translation\n*   Automated documentation and unit test generation\n\n#### [StarCoder2](https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1)\n\n*   StarCoder2-15B-Instruct-v0.1 is the first entirely self-aligned code LLM trained with a fully permissive and transparent pipeline. Their open-source pipeline uses StarCoder2-15B to generate thousands of instruction-response pairs, which are then used to fine-tune StarCoder-15B itself without any human annotations or distilled data from huge and proprietary LLMs.\n*   StarCoder2 Instruct achieves 72% HumanEval score using only self-generated content without any GPT-3.5/4 data.\n*   This work demonstrates that self-instruct works already well at the 15B scale without data from proprietary models thus making StarCoder2 Instruct fully permissive.\n*   [Model](https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1); [Code](https://github.com/bigcode-project/starcoder2-self-align); [Dataset](https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k/)\n\n![](/primers/ai/assets/LLM/StarCoder2.png)\n\n#### [CodeGemma](https://huggingface.co/google/codegemma-7b-it)\n\n*   CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models are text-to-text and text-to-code decoder-only models and are available as a 7 billion pretrained variant that specializes in code completion and code generation tasks, a 7 billion parameter instruction-tuned variant for code chat and instruction following and a 2 billion parameter pretrained variant for fast code completion.\n\n![](assets/LLM/CodeGemma.png)\n\n#### [DeepSeek-Coder-V2](https://arxiv.org/abs/2406.11931v1)\n\n*   DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model developed to achieve performance comparable to GPT-4 Turbo in code-specific tasks. It extends the DeepSeek-V2 model, being further pre-trained with an additional 6 trillion tokens. This enhanced model significantly improves coding and mathematical reasoning capabilities while maintaining comparable performance in general language tasks. It supports 338 programming languages and extends the context length from 16K to 128K tokens.\n*   **Implementation Details:**\n    1.  **Pre-training Dataset Composition:**\n        *   60% source code\n        *   10% math corpus\n        *   30% natural language corpus\n        *   Source code tokens (1,170B) from GitHub and CommonCrawl\n        *   Math corpus (221B tokens) from CommonCrawl\n        *   Natural language corpus from the DeepSeek-V2 dataset\n    2.  **Model Training:**\n        *   DeepSeek-Coder-V2 models with 16B and 236B parameters\n        *   Activation parameters of 2.4B and 21B, respectively\n        *   Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model\n        *   Training issues addressed by reverting from exponential to conventional normalization\n    3.  **Training Hyper-Parameters:**\n        *   AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1\n        *   Cosine decay strategy for learning rate scheduling\n        *   Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens\n    4.  **Alignment:**\n        *   Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data\n        *   Reinforcement learning with Group Relative Policy Optimization (GRPO)\n        *   Use of compiler feedback and test cases for coding domain preference data\n*   **Evaluation:**\n    *   **Code Generation Benchmarks:**\n        *   Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models\n        *   Demonstrated superior performance on LiveCodeBench and USACO benchmarks\n        *   Achieved top-tier results in multilingual code generation\n    *   **Code Completion:**\n        *   Comparable performance to larger models in repository-level code completion (RepoBench v1.1)\n        *   High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%\n    *   **Code Fixing:**\n        *   Outperformed other open-source models on Defects4J and Aider datasets\n        *   Achieved the highest score on SWE-Bench\n    *   **Code Understanding and Reasoning:**\n        *   High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities\n    *   **Mathematical Reasoning:**\n        *   Achieved 75.7% on MATH and 53.7% on Math Odyssey\n        *   Solved more AIME 2024 problems compared to other models\n    *   **General Natural Language:**\n        *   Comparable general language performance to DeepSeek-V2\n        *   Strong results in reasoning-related benchmarks\n*   In conclusion, DeepSeek-Coder-V2 significantly advances the capabilities of open-source code models, achieving performance levels comparable to state-of-the-art closed-source models. It excels in coding and mathematical reasoning while maintaining robust general language understanding. Future work will focus on enhancing the model’s instruction-following capabilities to better handle real-world complex programming scenarios.\n*   [Code](https://github.com/deepseek-ai/DeepSeek-Coder-V2)\n\n1.  **Pre-training Dataset Composition:**\n    *   60% source code\n    *   10% math corpus\n    *   30% natural language corpus\n    *   Source code tokens (1,170B) from GitHub and CommonCrawl\n    *   Math corpus (221B tokens) from CommonCrawl\n    *   Natural language corpus from the DeepSeek-V2 dataset\n2.  **Model Training:**\n    *   DeepSeek-Coder-V2 models with 16B and 236B parameters\n    *   Activation parameters of 2.4B and 21B, respectively\n    *   Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model\n    *   Training issues addressed by reverting from exponential to conventional normalization\n3.  **Training Hyper-Parameters:**\n    *   AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1\n    *   Cosine decay strategy for learning rate scheduling\n    *   Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens\n4.  **Alignment:**\n    *   Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data\n    *   Reinforcement learning with Group Relative Policy Optimization (GRPO)\n    *   Use of compiler feedback and test cases for coding domain preference data\n\n*   60% source code\n*   10% math corpus\n*   30% natural language corpus\n*   Source code tokens (1,170B) from GitHub and CommonCrawl\n*   Math corpus (221B tokens) from CommonCrawl\n*   Natural language corpus from the DeepSeek-V2 dataset\n\n*   DeepSeek-Coder-V2 models with 16B and 236B parameters\n*   Activation parameters of 2.4B and 21B, respectively\n*   Two training objectives: Next-Token-Prediction and Fill-In-Middle (FIM) for the 16B model\n*   Training issues addressed by reverting from exponential to conventional normalization\n\n*   AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.1\n*   Cosine decay strategy for learning rate scheduling\n*   Training stages for long context extension using Yarn, with sequences of 32K and 128K tokens\n\n*   Supervised fine-tuning with an instruction training dataset including code, math, and general instruction data\n*   Reinforcement learning with Group Relative Policy Optimization (GRPO)\n*   Use of compiler feedback and test cases for coding domain preference data\n\n*   **Code Generation Benchmarks:**\n    *   Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models\n    *   Demonstrated superior performance on LiveCodeBench and USACO benchmarks\n    *   Achieved top-tier results in multilingual code generation\n*   **Code Completion:**\n    *   Comparable performance to larger models in repository-level code completion (RepoBench v1.1)\n    *   High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%\n*   **Code Fixing:**\n    *   Outperformed other open-source models on Defects4J and Aider datasets\n    *   Achieved the highest score on SWE-Bench\n*   **Code Understanding and Reasoning:**\n    *   High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities\n*   **Mathematical Reasoning:**\n    *   Achieved 75.7% on MATH and 53.7% on Math Odyssey\n    *   Solved more AIME 2024 problems compared to other models\n*   **General Natural Language:**\n    *   Comparable general language performance to DeepSeek-V2\n    *   Strong results in reasoning-related benchmarks\n\n*   Achieved 90.2% on HumanEval and 76.2% on MBPP+, surpassing other open-source models\n*   Demonstrated superior performance on LiveCodeBench and USACO benchmarks\n*   Achieved top-tier results in multilingual code generation\n\n*   Comparable performance to larger models in repository-level code completion (RepoBench v1.1)\n*   High effectiveness in Fill-in-the-Middle tasks with a mean score of 86.4%\n\n*   Outperformed other open-source models on Defects4J and Aider datasets\n*   Achieved the highest score on SWE-Bench\n\n*   High scores on CruxEval benchmarks, demonstrating strong reasoning capabilities\n\n*   Achieved 75.7% on MATH and 53.7% on Math Odyssey\n*   Solved more AIME 2024 problems compared to other models\n\n*   Comparable general language performance to DeepSeek-V2\n*   Strong results in reasoning-related benchmarks",
    "order": 45,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 16,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "nlp",
      "reinforcement learning",
      "optimization",
      "activation",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 3171,
      "contentLength": 28084
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#popular-code-llms",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-langchain-build-apps-with-llms-46",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "LangChain: Build Apps with LLMs",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>LangChain is an open-source framework to build applications with LLMs. It enhances the capabilities of LLMs by providing a standard interface for prompt templates and integration of language models with different APIs and external databases. It can be used to build a chatbot, a Q&amp;A answering platform, and any intelligent applications that can understand natural language and respond to user requests in real-time.</li>\n  <li>Let’s consider an example of you wanting to build a chatbot for a particular company.</li>\n  <li>First, you need to make your text data discoverable by the LLM. The typical way to do that is to index your data into a vector database. You’ll need to partition your data into chunks and encode those chunks into embeddings using a LLM and index those chunks using those embeddings. When you have a question, you can encode the question into another embedding and search in that database for the corresponding piece of data with the highest cosine similarity metric. By feeding that piece of data within a prompt, the LLM can more confidently recover the right answer.</li>\n  <li>You might need to insert users’ questions within a prompt template with additional examples to provide the right context for the LLM, or you might need to augment it by asking the LLM to establish a plan of action to accurately respond to the question. Often, chaining multiple (prompt, answer) pairs will be necessary to arrive at the desired outcome.</li>\n  <li>There are multiple tools that a LLM can use: Google Search, Python REPL, GraphQL, Wikipedia, etc. We need to prompt the LLM with a set of tools it can use and how to utilize the result to answer the question if it decides to use one of them. Additionally, we may want to keep track of the history for the LLM to remember what was previously discussed. This needs to be coordinated with the data search in the vector database that may happen in parallel.</li>\n  <li>When you take into account the initial question, prompt templates, prompt augmentation, data search, tools, plan of action, and memory, we start to understand the difficulty in juggling all those items when we need to construct the right prompts for meaningful results. LangChain is most likely one of the most powerful LLMops tools today! It provides an abstracted interface to chain all those items together in a simple manner to build applications. It has an API connection to ~40 of the public LLMs, Chat and embedding models. It integrates with more than 30 different tools and 20 different vector databases.</li>\n  <li>The following flowchart offers a visual summary of the entire process <a href=\"https://theaiedge.io\">(source)</a>.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/LLM/langchain.jpeg\" alt=\"\"></p>\n<h4 id=\"cheatsheet\">Cheatsheet</h4>\n<ul>\n  <li>Credits for the following section go to <a href=\"https://www.linkedin.com/in/sonali-pattnaik/\">Sonali Pattnaik</a>.</li>\n  <li>Langchain is a popular AI framework for the fast prototyping of AI applications pipelines centered around LLMs. It is used for document summarization, question answering, chat over documents, and many more applications.</li>\n  <li>The six major components of LangChain are Prompt Templates, LLMs, Agents, Memory, Indexes, and Chains as described below:\n    <ol>\n      <li>Prompt templates: Prompt templates are a way to generate prompts consistently and reproducibly. The template is a ‘text string’ that the user can customize in multiple ways. The prompt templates can consist of instructions to LLMs, a few shot examples, etc.</li>\n      <li>LLMs: LangChains provide a standard interface to connect to a number of LLMs (OpenAI, Hugging Face, Cohere, Anthropic, and many more) out there. It can also connect to all major cloud providers like Azure, Amazon, and Google Cloud.</li>\n      <li>Agents: LangChain Agents use #LMs to decide an action sequence for task completion (e.g., in AutoGPT, BabyAGI, etc.).\n        <ul>\n          <li>What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.</li>\n          <li>Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.</li>\n        </ul>\n      </li>\n      <li>Memory: “Memory” refers to the ability of an agent (such as a chatbot or language model) to retain information about previous interactions with the user. By default, agents are stateless, which means that each new incoming query or message is processed independently, without any knowledge or recollection of past interactions.\n        <ul>\n          <li>Popular types:\n            <ul>\n              <li>ConversationBufferMemory</li>\n              <li>ConversationBufferWindowMemory</li>\n              <li>ConversationTokenBufferMemory</li>\n              <li>ConversationSummaryMemory</li>\n              <li>ConversationKnowledgeGraphMemory</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>Indexes: Indexes are used to structure documents for interaction with LLMs. Indexes are commonly used for “retrieval” to find the most relevant documents for a user’s query.</li>\n      <li>Chains: LLMs work great for building simple applications. However, for building more complex applications one can leverage LangChain. The ‘main’ idea of LangChain is the ability to chain different components together so that the user can build intricate and interconnected applications that leverage LLMs.</li>\n    </ol>\n\n    <ul>\n      <li>The following infographic <a href=\"https://www.linkedin.com/in/sonali-pattnaik/\">(source)</a> illustrates the aforementioned components:</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/LLM/langchaincomp.jpg\" alt=\"\"></p>\n  </li>\n</ul>\n<ol>\n      <li>Prompt templates: Prompt templates are a way to generate prompts consistently and reproducibly. The template is a ‘text string’ that the user can customize in multiple ways. The prompt templates can consist of instructions to LLMs, a few shot examples, etc.</li>\n      <li>LLMs: LangChains provide a standard interface to connect to a number of LLMs (OpenAI, Hugging Face, Cohere, Anthropic, and many more) out there. It can also connect to all major cloud providers like Azure, Amazon, and Google Cloud.</li>\n      <li>Agents: LangChain Agents use #LMs to decide an action sequence for task completion (e.g., in AutoGPT, BabyAGI, etc.).\n        <ul>\n          <li>What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.</li>\n          <li>Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.</li>\n        </ul>\n      </li>\n      <li>Memory: “Memory” refers to the ability of an agent (such as a chatbot or language model) to retain information about previous interactions with the user. By default, agents are stateless, which means that each new incoming query or message is processed independently, without any knowledge or recollection of past interactions.\n        <ul>\n          <li>Popular types:\n            <ul>\n              <li>ConversationBufferMemory</li>\n              <li>ConversationBufferWindowMemory</li>\n              <li>ConversationTokenBufferMemory</li>\n              <li>ConversationSummaryMemory</li>\n              <li>ConversationKnowledgeGraphMemory</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>Indexes: Indexes are used to structure documents for interaction with LLMs. Indexes are commonly used for “retrieval” to find the most relevant documents for a user’s query.</li>\n      <li>Chains: LLMs work great for building simple applications. However, for building more complex applications one can leverage LangChain. The ‘main’ idea of LangChain is the ability to chain different components together so that the user can build intricate and interconnected applications that leverage LLMs.</li>\n    </ol>\n<ul>\n          <li>What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.</li>\n          <li>Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.</li>\n        </ul>\n<ul>\n          <li>Popular types:\n            <ul>\n              <li>ConversationBufferMemory</li>\n              <li>ConversationBufferWindowMemory</li>\n              <li>ConversationTokenBufferMemory</li>\n              <li>ConversationSummaryMemory</li>\n              <li>ConversationKnowledgeGraphMemory</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li>ConversationBufferMemory</li>\n              <li>ConversationBufferWindowMemory</li>\n              <li>ConversationTokenBufferMemory</li>\n              <li>ConversationSummaryMemory</li>\n              <li>ConversationKnowledgeGraphMemory</li>\n            </ul>\n<ul>\n      <li>The following infographic <a href=\"https://www.linkedin.com/in/sonali-pattnaik/\">(source)</a> illustrates the aforementioned components:</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/LLM/langchaincomp.jpg\" alt=\"\"></p>\n<h4 id=\"resources\">Resources</h4>\n<ul>\n  <li>LangChain <a href=\"https://docs.langchain.com/docs/\">docs</a>:</li>\n</ul>\n<p><a href=\"https://docs.langchain.com/docs/\"><img src=\"/primers/ai/assets/LLM/langchain2.jpg\" alt=\"\"></a></p>\n<ul>\n  <li>LangChain <a href=\"https://github.com/gkamradt/langchain-tutorials\">tutorials and cookbooks</a></li>\n  <li><a href=\"https://github.com/hwchase17/langchain\">Code</a></li>\n  <li>Official LangChain <a href=\"https://blog.langchain.dev/\">Blog</a></li>\n  <li>Medium <a href=\"https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?gi=07a15e1c91bc\">post</a>: Getting started with LangChain</li>\n</ul>\n<h5 id=\"youtube-videos\">YouTube Videos</h5>\n<ul>\n  <li><a href=\"https://www.youtube.com/watch?v=RoR4XJw8wIc\">LangChain explained by AssemblyAI</a></li>\n  <li><a href=\"https://www.youtube.com/watch?v=_v_fgW2SkkQ&amp;list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5\">LangChain video series by Data Independent</a></li>\n  <li><a href=\"https://www.youtube.com/watch?v=LbT1yp6quS8&amp;t=19s\">LangChain Crash Course by Patrick Löber</a></li>\n  <li><a href=\"https://www.youtube.com/watch?v=nE2skSRWTTs&amp;list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F\">LangChain series by James Briggs</a></li>\n  <li><a href=\"https://www.youtube.com/playlist?list=PLfaIDFEXuae0gBSJ9T0w7cu7iJZbH3T31\">LangChain v0.1.0 Launch</a></li>\n</ul>\n<h5 id=\"deploying-langchain\">Deploying LangChain</h5>\n<ul>\n  <li><a href=\"https://github.com/hwchase17/langchain-streamlit-template\">LangChain-streamlit Template</a></li>\n  <li><a href=\"https://github.com/hwchase17/langchain-gradio-template\">LangChain Gradio Template</a></li>\n  <li><a href=\"https://langchain.readthedocs.io/en/latest/?\">More examples</a></li>\n</ul>\n<h5 id=\"langchain-with-mlflow\">LangChain with MLFlow</h5>\n<ul>\n  <li><a href=\"https://mlflow.org/docs/latest/python_api/mlflow.langchain.html\">API for logging and loading LangChain models</a></li>\n  <li><a href=\"https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html\">Example</a></li>\n</ul>\n<h5 id=\"langchain-with-nemo-guardrails-building-safe-and-secure-apps\">LangChain with NeMo Guardrails (building Safe and Secure Apps)</h5>\n<ul>\n  <li><a href=\"https://github.com/NVIDIA/NeMo-Guardrails\">Github</a></li>\n  <li><a href=\"https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/demo_chain_with_guardrails.py\">Example</a></li>\n</ul>\n<h5 id=\"langflow---gui-for-langchain\">LangFlow - GUI for LangChain</h5>\n<ul>\n  <li><a href=\"https://github.com/logspace-ai/langflow\">Github</a></li>\n  <li><a href=\"https://cobusgreyling.medium.com/langflow-for-langchain-58c143ba9218\">Medium</a></li>\n  <li><a href=\"https://huggingface.co/spaces/Logspace/LangFlow\">LangFlow via HuggingFace</a></li>\n</ul>\n<h5 id=\"popular-use-cases-examples\">Popular Use Cases Examples</h5>\n<ul>\n  <li><a href=\"https://python.langchain.com/en/latest/use_cases/chatbots.html\">Chatbots</a></li>\n  <li><a href=\"https://python.langchain.com/en/latest/use_cases/question_answering.html\">Question Answering over Docs</a></li>\n  <li><a href=\"https://github.com/kyrolabs/awesome-langchain\">More examples of tools and projects: Awesome LangChain</a></li>\n</ul>",
    "contentMarkdown": "*   LangChain is an open-source framework to build applications with LLMs. It enhances the capabilities of LLMs by providing a standard interface for prompt templates and integration of language models with different APIs and external databases. It can be used to build a chatbot, a Q&A answering platform, and any intelligent applications that can understand natural language and respond to user requests in real-time.\n*   Let’s consider an example of you wanting to build a chatbot for a particular company.\n*   First, you need to make your text data discoverable by the LLM. The typical way to do that is to index your data into a vector database. You’ll need to partition your data into chunks and encode those chunks into embeddings using a LLM and index those chunks using those embeddings. When you have a question, you can encode the question into another embedding and search in that database for the corresponding piece of data with the highest cosine similarity metric. By feeding that piece of data within a prompt, the LLM can more confidently recover the right answer.\n*   You might need to insert users’ questions within a prompt template with additional examples to provide the right context for the LLM, or you might need to augment it by asking the LLM to establish a plan of action to accurately respond to the question. Often, chaining multiple (prompt, answer) pairs will be necessary to arrive at the desired outcome.\n*   There are multiple tools that a LLM can use: Google Search, Python REPL, GraphQL, Wikipedia, etc. We need to prompt the LLM with a set of tools it can use and how to utilize the result to answer the question if it decides to use one of them. Additionally, we may want to keep track of the history for the LLM to remember what was previously discussed. This needs to be coordinated with the data search in the vector database that may happen in parallel.\n*   When you take into account the initial question, prompt templates, prompt augmentation, data search, tools, plan of action, and memory, we start to understand the difficulty in juggling all those items when we need to construct the right prompts for meaningful results. LangChain is most likely one of the most powerful LLMops tools today! It provides an abstracted interface to chain all those items together in a simple manner to build applications. It has an API connection to ~40 of the public LLMs, Chat and embedding models. It integrates with more than 30 different tools and 20 different vector databases.\n*   The following flowchart offers a visual summary of the entire process [(source)](https://theaiedge.io).\n\n![](/primers/ai/assets/LLM/langchain.jpeg)\n\n#### Cheatsheet\n\n*   Credits for the following section go to [Sonali Pattnaik](https://www.linkedin.com/in/sonali-pattnaik/).\n*   Langchain is a popular AI framework for the fast prototyping of AI applications pipelines centered around LLMs. It is used for document summarization, question answering, chat over documents, and many more applications.\n*   The six major components of LangChain are Prompt Templates, LLMs, Agents, Memory, Indexes, and Chains as described below:\n    \n    1.  Prompt templates: Prompt templates are a way to generate prompts consistently and reproducibly. The template is a ‘text string’ that the user can customize in multiple ways. The prompt templates can consist of instructions to LLMs, a few shot examples, etc.\n    2.  LLMs: LangChains provide a standard interface to connect to a number of LLMs (OpenAI, Hugging Face, Cohere, Anthropic, and many more) out there. It can also connect to all major cloud providers like Azure, Amazon, and Google Cloud.\n    3.  Agents: LangChain Agents use #LMs to decide an action sequence for task completion (e.g., in AutoGPT, BabyAGI, etc.).\n        *   What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.\n        *   Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.\n    4.  Memory: “Memory” refers to the ability of an agent (such as a chatbot or language model) to retain information about previous interactions with the user. By default, agents are stateless, which means that each new incoming query or message is processed independently, without any knowledge or recollection of past interactions.\n        *   Popular types:\n            *   ConversationBufferMemory\n            *   ConversationBufferWindowMemory\n            *   ConversationTokenBufferMemory\n            *   ConversationSummaryMemory\n            *   ConversationKnowledgeGraphMemory\n    5.  Indexes: Indexes are used to structure documents for interaction with LLMs. Indexes are commonly used for “retrieval” to find the most relevant documents for a user’s query.\n    6.  Chains: LLMs work great for building simple applications. However, for building more complex applications one can leverage LangChain. The ‘main’ idea of LangChain is the ability to chain different components together so that the user can build intricate and interconnected applications that leverage LLMs.\n    \n    *   The following infographic [(source)](https://www.linkedin.com/in/sonali-pattnaik/) illustrates the aforementioned components:\n    \n    ![](/primers/ai/assets/LLM/langchaincomp.jpg)\n    \n\n1.  Prompt templates: Prompt templates are a way to generate prompts consistently and reproducibly. The template is a ‘text string’ that the user can customize in multiple ways. The prompt templates can consist of instructions to LLMs, a few shot examples, etc.\n2.  LLMs: LangChains provide a standard interface to connect to a number of LLMs (OpenAI, Hugging Face, Cohere, Anthropic, and many more) out there. It can also connect to all major cloud providers like Azure, Amazon, and Google Cloud.\n3.  Agents: LangChain Agents use #LMs to decide an action sequence for task completion (e.g., in AutoGPT, BabyAGI, etc.).\n    *   What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.\n    *   Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.\n4.  Memory: “Memory” refers to the ability of an agent (such as a chatbot or language model) to retain information about previous interactions with the user. By default, agents are stateless, which means that each new incoming query or message is processed independently, without any knowledge or recollection of past interactions.\n    *   Popular types:\n        *   ConversationBufferMemory\n        *   ConversationBufferWindowMemory\n        *   ConversationTokenBufferMemory\n        *   ConversationSummaryMemory\n        *   ConversationKnowledgeGraphMemory\n5.  Indexes: Indexes are used to structure documents for interaction with LLMs. Indexes are commonly used for “retrieval” to find the most relevant documents for a user’s query.\n6.  Chains: LLMs work great for building simple applications. However, for building more complex applications one can leverage LangChain. The ‘main’ idea of LangChain is the ability to chain different components together so that the user can build intricate and interconnected applications that leverage LLMs.\n\n*   What are LangChain Agents? LangChain Agents employ an LLM as a reasoning mechanism to connect apps to the outside world based on user input. Instead of using a predetermined chain of calls to LLMs/other tools, it uses an LLM to decide a potentially new chain to use that depends on the user’s input.\n*   Why use Agents? Agents enable the integration of language models with external data sources and computation, such as search APIs and databases. Agents offer enhanced flexibility and capability compared to simple language model-tool connections, enabling better handling of edge cases and multi-hop tasks.\n\n*   Popular types:\n    *   ConversationBufferMemory\n    *   ConversationBufferWindowMemory\n    *   ConversationTokenBufferMemory\n    *   ConversationSummaryMemory\n    *   ConversationKnowledgeGraphMemory\n\n*   ConversationBufferMemory\n*   ConversationBufferWindowMemory\n*   ConversationTokenBufferMemory\n*   ConversationSummaryMemory\n*   ConversationKnowledgeGraphMemory\n\n*   The following infographic [(source)](https://www.linkedin.com/in/sonali-pattnaik/) illustrates the aforementioned components:\n\n![](/primers/ai/assets/LLM/langchaincomp.jpg)\n\n#### Resources\n\n*   LangChain [docs](https://docs.langchain.com/docs/):\n\n[![](/primers/ai/assets/LLM/langchain2.jpg)](https://docs.langchain.com/docs/)\n\n*   LangChain [tutorials and cookbooks](https://github.com/gkamradt/langchain-tutorials)\n*   [Code](https://github.com/hwchase17/langchain)\n*   Official LangChain [Blog](https://blog.langchain.dev/)\n*   Medium [post](https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?gi=07a15e1c91bc): Getting started with LangChain\n\n##### YouTube Videos\n\n*   [LangChain explained by AssemblyAI](https://www.youtube.com/watch?v=RoR4XJw8wIc)\n*   [LangChain video series by Data Independent](https://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n*   [LangChain Crash Course by Patrick Löber](https://www.youtube.com/watch?v=LbT1yp6quS8&t=19s)\n*   [LangChain series by James Briggs](https://www.youtube.com/watch?v=nE2skSRWTTs&list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)\n*   [LangChain v0.1.0 Launch](https://www.youtube.com/playlist?list=PLfaIDFEXuae0gBSJ9T0w7cu7iJZbH3T31)\n\n##### Deploying LangChain\n\n*   [LangChain-streamlit Template](https://github.com/hwchase17/langchain-streamlit-template)\n*   [LangChain Gradio Template](https://github.com/hwchase17/langchain-gradio-template)\n*   [More examples](https://langchain.readthedocs.io/en/latest/?)\n\n##### LangChain with MLFlow\n\n*   [API for logging and loading LangChain models](https://mlflow.org/docs/latest/python_api/mlflow.langchain.html)\n*   [Example](https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html)\n\n##### LangChain with NeMo Guardrails (building Safe and Secure Apps)\n\n*   [Github](https://github.com/NVIDIA/NeMo-Guardrails)\n*   [Example](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/demo_chain_with_guardrails.py)\n\n##### LangFlow - GUI for LangChain\n\n*   [Github](https://github.com/logspace-ai/langflow)\n*   [Medium](https://cobusgreyling.medium.com/langflow-for-langchain-58c143ba9218)\n*   [LangFlow via HuggingFace](https://huggingface.co/spaces/Logspace/LangFlow)\n\n##### Popular Use Cases Examples\n\n*   [Chatbots](https://python.langchain.com/en/latest/use_cases/chatbots.html)\n*   [Question Answering over Docs](https://python.langchain.com/en/latest/use_cases/question_answering.html)\n*   [More examples of tools and projects: Awesome LangChain](https://github.com/kyrolabs/awesome-langchain)",
    "order": 46,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 8,
    "tags": [
      "nlpllms",
      "embedding",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1449,
      "contentLength": 13491
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#langchain:-build-apps-with-llms",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-llamaindex-47",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "LlamaIndex",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>LlamaIndex is a simple, flexible data framework for connecting custom data sources to LLMs and is most widely used for building RAG applications.</li>\n  <li>It offers data connectors for loading custom data from a variety of sources and data formats (API’s, PDF’s, documents, SQL, etc.) to use with an LLM.</li>\n  <li>It offers data indexing features to store and index your data for different use cases.</li>\n  <li>It provides a query interface that accepts any input prompt over your data and returns a knowledge-augmented response.</li>\n</ul>\n<p><a href=\"https://www.llamaindex.ai\"><img src=\"/primers/ai/assets/LLM/LlamaIndex.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   LlamaIndex is a simple, flexible data framework for connecting custom data sources to LLMs and is most widely used for building RAG applications.\n*   It offers data connectors for loading custom data from a variety of sources and data formats (API’s, PDF’s, documents, SQL, etc.) to use with an LLM.\n*   It offers data indexing features to store and index your data for different use cases.\n*   It provides a query interface that accepts any input prompt over your data and returns a knowledge-augmented response.\n\n[![](/primers/ai/assets/LLM/LlamaIndex.jpg)](https://www.llamaindex.ai)",
    "order": 47,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 87,
      "contentLength": 660
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llamaindex",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-flowise-48",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "Flowise",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>Flowise is an open-source drag &amp; drop tool to build LLM-powered apps, with use-cases such as (but not limited to):\n    <ul>\n      <li>Chat with your PDF and Excel files.</li>\n      <li>Launch customer support chatbots.</li>\n      <li>Turn user input into a full essay through multiple prompts (LLM chain).</li>\n      <li>Explore your codebase via a customized chatbot.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Chat with your PDF and Excel files.</li>\n      <li>Launch customer support chatbots.</li>\n      <li>Turn user input into a full essay through multiple prompts (LLM chain).</li>\n      <li>Explore your codebase via a customized chatbot.</li>\n    </ul>\n<p><a href=\"https://github.com/FlowiseAI/Flowise\"><img src=\"/primers/ai/assets/LLM/flowise.gif\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Flowise is an open-source drag & drop tool to build LLM-powered apps, with use-cases such as (but not limited to):\n    *   Chat with your PDF and Excel files.\n    *   Launch customer support chatbots.\n    *   Turn user input into a full essay through multiple prompts (LLM chain).\n    *   Explore your codebase via a customized chatbot.\n\n*   Chat with your PDF and Excel files.\n*   Launch customer support chatbots.\n*   Turn user input into a full essay through multiple prompts (LLM chain).\n*   Explore your codebase via a customized chatbot.\n\n[![](/primers/ai/assets/LLM/flowise.gif)](https://github.com/FlowiseAI/Flowise)",
    "order": 48,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 90,
      "contentLength": 789
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#flowise",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-ragas-49",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "RAGAS",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2309.15217\">RAGAS: Automated Evaluation of Retrieval Augmented Generation</a> by Es et al. from Exploding Gradients, Cardiff University, and AMPLYFI, <a href=\"https://github.com/explodinggradients/ragas\">RAGAS</a> is a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) systems using a bunch of diverse <a href=\"https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md\">metrics</a>.</li>\n  <li>RAGAS focuses on evaluating the performance of RAG systems in dimensions such as the effectiveness of the retrieval system in providing relevant context, the LLM’s ability to utilize this context, and the overall quality of generation.</li>\n  <li>The framework proposes a suite of metrics to evaluate these dimensions without relying on ground truth human annotations.</li>\n  <li>RAGAS focuses on three quality aspects: Faithfulness, Answer Relevance, and Context Relevance.\n    <ul>\n      <li><strong>Faithfulness</strong>: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo>=</mo><mfrac><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>S</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-751\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1003.08em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-752\"><span class=\"mi\" id=\"MathJax-Span-753\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-755\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.84em, 4.326em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.466em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"texatom\" id=\"MathJax-Span-757\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mo\" id=\"MathJax-Span-759\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-761\"><span class=\"mrow\" id=\"MathJax-Span-762\"><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.73em, 4.326em, -999.997em); top: -3.591em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"texatom\" id=\"MathJax-Span-765\"><span class=\"mrow\" id=\"MathJax-Span-766\"><span class=\"mo\" id=\"MathJax-Span-767\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-769\"><span class=\"mrow\" id=\"MathJax-Span-770\"><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 2.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mo>=</mo><mfrac><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>S</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">F = \\frac{|V|}{|S|}</script>\nwhere, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.3em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"texatom\" id=\"MathJax-Span-774\"><span class=\"mrow\" id=\"MathJax-Span-775\"><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-778\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"mo\" id=\"MathJax-Span-780\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">|V|</script> is the number of statements supported by the context and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>S</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-781\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.1em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"texatom\" id=\"MathJax-Span-783\"><span class=\"mrow\" id=\"MathJax-Span-784\"><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-787\"><span class=\"mrow\" id=\"MathJax-Span-788\"><span class=\"mo\" id=\"MathJax-Span-789\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>S</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-126\">|S|</script> is the total number of statements extracted from the answer.</li>\n      <li><strong>Answer Relevance</strong>: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-790\" style=\"width: 11.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1009.53em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-791\"><span class=\"mi\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-793\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-795\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-796\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-797\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-798\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-799\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.42em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-800\"><span class=\"mrow\" id=\"MathJax-Span-801\"><span class=\"mi\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-803\"><span class=\"mrow\" id=\"MathJax-Span-804\"><span class=\"mi\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-806\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-807\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">sim</span><span class=\"mo\" id=\"MathJax-Span-809\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-810\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-811\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-812\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-814\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-815\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><mtext>sim</mtext><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">AR = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sim}(q, q_i)</script>\nwhere <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-816\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-817\"><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">q</script> is the original question, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>q</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-819\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-820\"><span class=\"msubsup\" id=\"MathJax-Span-821\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-823\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>q</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">q_i</script> are the generated questions, and sim denotes the cosine similarity between their embeddings.</li>\n      <li><strong>Context Relevance</strong>: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi><mi>R</mi><mo>=</mo><mfrac><mtext>number of extracted sentences</mtext><mrow><mtext>total number of sentences in&amp;#xA0;</mtext><mi>c</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 14.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1012.19em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"mi\" id=\"MathJax-Span-826\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-829\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 9.43em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1008.54em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -4.268em;\"><span class=\"mtext\" id=\"MathJax-Span-830\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">number of extracted sentences</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1009.27em, 4.326em, -999.997em); top: -3.643em; left: 50%; margin-left: -4.633em;\"><span class=\"mrow\" id=\"MathJax-Span-831\"><span class=\"mtext\" id=\"MathJax-Span-832\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">total number of sentences in&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-833\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mo\" id=\"MathJax-Span-834\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-835\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-836\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1009.43em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 9.43em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi><mi>R</mi><mo>=</mo><mfrac><mtext>number of extracted sentences</mtext><mrow><mtext>total number of sentences in&nbsp;</mtext><mi>c</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of sentences in } c(q)}</script></li>\n    </ul>\n  </li>\n  <li>The paper validates RAGAS using the WikiEval dataset, demonstrating its alignment with human judgments in evaluating these aspects.</li>\n  <li>The authors argue that RAGAS contributes to faster and more efficient evaluation cycles for RAG systems, which is vital due to the rapid adoption of LLMs.</li>\n  <li>RAGAS is validated using the WikiEval dataset, which includes question-context-answer triples annotated with human judgments for faithfulness, answer relevance, and context relevance.</li>\n  <li>The evaluation shows that RAGAS aligns closely with human judgments, particularly in assessing faithfulness and answer relevance.</li>\n  <li><a href=\"https://github.com/explodinggradients/ragas\">Code</a>.</li>\n</ul>\n<ul>\n      <li><strong>Faithfulness</strong>: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo>=</mo><mfrac><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>S</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-751\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1003.08em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-752\"><span class=\"mi\" id=\"MathJax-Span-753\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-755\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.84em, 4.326em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.466em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"texatom\" id=\"MathJax-Span-757\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mo\" id=\"MathJax-Span-759\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-761\"><span class=\"mrow\" id=\"MathJax-Span-762\"><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.73em, 4.326em, -999.997em); top: -3.591em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"texatom\" id=\"MathJax-Span-765\"><span class=\"mrow\" id=\"MathJax-Span-766\"><span class=\"mo\" id=\"MathJax-Span-767\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-769\"><span class=\"mrow\" id=\"MathJax-Span-770\"><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 2.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mo>=</mo><mfrac><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>S</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">F = \\frac{|V|}{|S|}</script>\nwhere, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.3em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"texatom\" id=\"MathJax-Span-774\"><span class=\"mrow\" id=\"MathJax-Span-775\"><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-778\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"mo\" id=\"MathJax-Span-780\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">|V|</script> is the number of statements supported by the context and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>S</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-781\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1001.1em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"texatom\" id=\"MathJax-Span-783\"><span class=\"mrow\" id=\"MathJax-Span-784\"><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-787\"><span class=\"mrow\" id=\"MathJax-Span-788\"><span class=\"mo\" id=\"MathJax-Span-789\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>S</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-126\">|S|</script> is the total number of statements extracted from the answer.</li>\n      <li><strong>Answer Relevance</strong>: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-790\" style=\"width: 11.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1009.53em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-791\"><span class=\"mi\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-793\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-795\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-796\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-797\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-798\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-799\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.42em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-800\"><span class=\"mrow\" id=\"MathJax-Span-801\"><span class=\"mi\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-803\"><span class=\"mrow\" id=\"MathJax-Span-804\"><span class=\"mi\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-806\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-807\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">sim</span><span class=\"mo\" id=\"MathJax-Span-809\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-810\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-811\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-812\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-814\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-815\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><mtext>sim</mtext><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">AR = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sim}(q, q_i)</script>\nwhere <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-816\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-817\"><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">q</script> is the original question, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>q</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-819\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-820\"><span class=\"msubsup\" id=\"MathJax-Span-821\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-823\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>q</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">q_i</script> are the generated questions, and sim denotes the cosine similarity between their embeddings.</li>\n      <li><strong>Context Relevance</strong>: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi><mi>R</mi><mo>=</mo><mfrac><mtext>number of extracted sentences</mtext><mrow><mtext>total number of sentences in&amp;#xA0;</mtext><mi>c</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 14.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1012.19em, 2.867em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"mi\" id=\"MathJax-Span-826\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-829\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 9.43em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1008.54em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -4.268em;\"><span class=\"mtext\" id=\"MathJax-Span-830\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">number of extracted sentences</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1009.27em, 4.326em, -999.997em); top: -3.643em; left: 50%; margin-left: -4.633em;\"><span class=\"mrow\" id=\"MathJax-Span-831\"><span class=\"mtext\" id=\"MathJax-Span-832\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">total number of sentences in&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-833\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mo\" id=\"MathJax-Span-834\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-835\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-836\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1009.43em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 9.43em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi><mi>R</mi><mo>=</mo><mfrac><mtext>number of extracted sentences</mtext><mrow><mtext>total number of sentences in&nbsp;</mtext><mi>c</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of sentences in } c(q)}</script></li>\n    </ul>\n<p><a href=\"https://github.com/explodinggradients/ragas\"><img src=\"/primers/ai/assets/LLM/ragas.png\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Proposed in [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) by Es et al. from Exploding Gradients, Cardiff University, and AMPLYFI, [RAGAS](https://github.com/explodinggradients/ragas) is a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) systems using a bunch of diverse [metrics](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md).\n*   RAGAS focuses on evaluating the performance of RAG systems in dimensions such as the effectiveness of the retrieval system in providing relevant context, the LLM’s ability to utilize this context, and the overall quality of generation.\n*   The framework proposes a suite of metrics to evaluate these dimensions without relying on ground truth human annotations.\n*   RAGAS focuses on three quality aspects: Faithfulness, Answer Relevance, and Context Relevance.\n    *   **Faithfulness**: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula: F\\=|V||S|F\\=|V||S|F = \\\\frac{|V|}{|S|} where, |V||V||V| is the number of statements supported by the context and |S||S||S| is the total number of statements extracted from the answer.\n    *   **Answer Relevance**: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula: AR\\=1n∑ni\\=1sim(q,qi)AR\\=1n∑i\\=1nsim(q,qi)AR = \\\\frac{1}{n} \\\\sum\\_{i=1}^{n} \\\\text{sim}(q, q\\_i) where qqq is the original question, qiqiq\\_i are the generated questions, and sim denotes the cosine similarity between their embeddings.\n    *   **Context Relevance**: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context: CR\\=number of extracted sentencestotal number of sentences in c(q)CR\\=number of extracted sentencestotal number of sentences in c(q)CR = \\\\frac{\\\\text{number of extracted sentences}}{\\\\text{total number of sentences in } c(q)}\n*   The paper validates RAGAS using the WikiEval dataset, demonstrating its alignment with human judgments in evaluating these aspects.\n*   The authors argue that RAGAS contributes to faster and more efficient evaluation cycles for RAG systems, which is vital due to the rapid adoption of LLMs.\n*   RAGAS is validated using the WikiEval dataset, which includes question-context-answer triples annotated with human judgments for faithfulness, answer relevance, and context relevance.\n*   The evaluation shows that RAGAS aligns closely with human judgments, particularly in assessing faithfulness and answer relevance.\n*   [Code](https://github.com/explodinggradients/ragas).\n\n*   **Faithfulness**: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula: F\\=|V||S|F\\=|V||S|F = \\\\frac{|V|}{|S|} where, |V||V||V| is the number of statements supported by the context and |S||S||S| is the total number of statements extracted from the answer.\n*   **Answer Relevance**: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula: AR\\=1n∑ni\\=1sim(q,qi)AR\\=1n∑i\\=1nsim(q,qi)AR = \\\\frac{1}{n} \\\\sum\\_{i=1}^{n} \\\\text{sim}(q, q\\_i) where qqq is the original question, qiqiq\\_i are the generated questions, and sim denotes the cosine similarity between their embeddings.\n*   **Context Relevance**: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context: CR\\=number of extracted sentencestotal number of sentences in c(q)CR\\=number of extracted sentencestotal number of sentences in c(q)CR = \\\\frac{\\\\text{number of extracted sentences}}{\\\\text{total number of sentences in } c(q)}\n\n[![](/primers/ai/assets/LLM/ragas.png)](https://github.com/explodinggradients/ragas)",
    "order": 49,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 545,
      "contentLength": 45815
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#ragas",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-llama2-accessory-50",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "LLaMA2-Accessory",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/Alpha-VLLM/LLaMA2-Accessorys\">LLaMA2-Accessory</a> is an advanced open-source toolkit for large language models.</li>\n  <li>Evolved from LLaMA-Adapter, they support more datasets, tasks, visual encoders, and efficient optimization methods.</li>\n  <li>Key Features:\n    <ul>\n      <li><strong>Support More Datasets and Tasks:</strong>\n        <ul>\n          <li>Pre-training with <a href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\">RefinedWeb</a> and <a href=\"https://github.com/bigcode-project/starcoder\">StarCoder</a>.</li>\n          <li>Single-modal fine-tuning with <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">Alpaca</a>, <a href=\"https://github.com/domeccleston/sharegpt\">ShareGPT</a>, <a href=\"https://arxiv.org/pdf/2305.11206.pdf\">LIMA</a>, <a href=\"https://github.com/nlpxucan/WizardLM\">WizardLM</a>, <a href=\"https://github.com/thunlp/UltraChat\">UltraChat</a> and <a href=\"https://github.com/OpenLMLab/MOSS\">MOSS</a>.</li>\n          <li>Multi-modal fine-tuning with image-text pairs (<a href=\"https://laion.ai/blog/laion-5b/\">LAION</a>, <a href=\"https://github.com/kakaobrain/coyo-dataset\">COYO</a> and more), interleaved image-text data (<a href=\"https://github.com/allenai/mmc4\">MMC4</a> and <a href=\"https://github.com/huggingface/OBELISC\">OBELISC</a>) and visual instruction data (<a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA</a>, <a href=\"https://github.com/shikras/shikra\">Shrika</a>, <a href=\"https://bard.google.com/\">Bard</a>)</li>\n          <li>LLM for API Control (<a href=\"https://github.com/StevenGrove/GPT4Tools\">GPT4Tools</a> and <a href=\"https://github.com/ShishirPatil/gorilla\">Gorilla</a>).</li>\n        </ul>\n      </li>\n      <li><strong>Efficient Optimization and Deployment</strong>:\n        <ul>\n          <li>Parameter-efficient fine-tuning with <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Zero-init Attention</a> and <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Bias-norm Tuning</a>.</li>\n          <li>Fully Sharded Data Parallel (<a href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\">FSDP</a>), <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash Attention 2</a> and <a href=\"https://github.com/artidoro/qlora\">QLoRA</a>.</li>\n        </ul>\n      </li>\n      <li><strong>🏋️‍♀️Support More Visual Encoders and LLMs</strong>:\n        <ul>\n          <li>Visual Encoders: <a href=\"https://github.com/openai/CLIP\">CLIP</a>, <a href=\"https://github.com/salesforce/LAVIS\">Q-Former</a> and <a href=\"https://github.com/facebookresearch/ImageBind\">ImageBind</a>.</li>\n          <li>LLMs: LLaMA and LLaMA2.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Support More Datasets and Tasks:</strong>\n        <ul>\n          <li>Pre-training with <a href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\">RefinedWeb</a> and <a href=\"https://github.com/bigcode-project/starcoder\">StarCoder</a>.</li>\n          <li>Single-modal fine-tuning with <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">Alpaca</a>, <a href=\"https://github.com/domeccleston/sharegpt\">ShareGPT</a>, <a href=\"https://arxiv.org/pdf/2305.11206.pdf\">LIMA</a>, <a href=\"https://github.com/nlpxucan/WizardLM\">WizardLM</a>, <a href=\"https://github.com/thunlp/UltraChat\">UltraChat</a> and <a href=\"https://github.com/OpenLMLab/MOSS\">MOSS</a>.</li>\n          <li>Multi-modal fine-tuning with image-text pairs (<a href=\"https://laion.ai/blog/laion-5b/\">LAION</a>, <a href=\"https://github.com/kakaobrain/coyo-dataset\">COYO</a> and more), interleaved image-text data (<a href=\"https://github.com/allenai/mmc4\">MMC4</a> and <a href=\"https://github.com/huggingface/OBELISC\">OBELISC</a>) and visual instruction data (<a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA</a>, <a href=\"https://github.com/shikras/shikra\">Shrika</a>, <a href=\"https://bard.google.com/\">Bard</a>)</li>\n          <li>LLM for API Control (<a href=\"https://github.com/StevenGrove/GPT4Tools\">GPT4Tools</a> and <a href=\"https://github.com/ShishirPatil/gorilla\">Gorilla</a>).</li>\n        </ul>\n      </li>\n      <li><strong>Efficient Optimization and Deployment</strong>:\n        <ul>\n          <li>Parameter-efficient fine-tuning with <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Zero-init Attention</a> and <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Bias-norm Tuning</a>.</li>\n          <li>Fully Sharded Data Parallel (<a href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\">FSDP</a>), <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash Attention 2</a> and <a href=\"https://github.com/artidoro/qlora\">QLoRA</a>.</li>\n        </ul>\n      </li>\n      <li><strong>🏋️‍♀️Support More Visual Encoders and LLMs</strong>:\n        <ul>\n          <li>Visual Encoders: <a href=\"https://github.com/openai/CLIP\">CLIP</a>, <a href=\"https://github.com/salesforce/LAVIS\">Q-Former</a> and <a href=\"https://github.com/facebookresearch/ImageBind\">ImageBind</a>.</li>\n          <li>LLMs: LLaMA and LLaMA2.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Pre-training with <a href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\">RefinedWeb</a> and <a href=\"https://github.com/bigcode-project/starcoder\">StarCoder</a>.</li>\n          <li>Single-modal fine-tuning with <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">Alpaca</a>, <a href=\"https://github.com/domeccleston/sharegpt\">ShareGPT</a>, <a href=\"https://arxiv.org/pdf/2305.11206.pdf\">LIMA</a>, <a href=\"https://github.com/nlpxucan/WizardLM\">WizardLM</a>, <a href=\"https://github.com/thunlp/UltraChat\">UltraChat</a> and <a href=\"https://github.com/OpenLMLab/MOSS\">MOSS</a>.</li>\n          <li>Multi-modal fine-tuning with image-text pairs (<a href=\"https://laion.ai/blog/laion-5b/\">LAION</a>, <a href=\"https://github.com/kakaobrain/coyo-dataset\">COYO</a> and more), interleaved image-text data (<a href=\"https://github.com/allenai/mmc4\">MMC4</a> and <a href=\"https://github.com/huggingface/OBELISC\">OBELISC</a>) and visual instruction data (<a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA</a>, <a href=\"https://github.com/shikras/shikra\">Shrika</a>, <a href=\"https://bard.google.com/\">Bard</a>)</li>\n          <li>LLM for API Control (<a href=\"https://github.com/StevenGrove/GPT4Tools\">GPT4Tools</a> and <a href=\"https://github.com/ShishirPatil/gorilla\">Gorilla</a>).</li>\n        </ul>\n<ul>\n          <li>Parameter-efficient fine-tuning with <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Zero-init Attention</a> and <a href=\"https://github.com/OpenGVLab/LLaMA-Adapter\">Bias-norm Tuning</a>.</li>\n          <li>Fully Sharded Data Parallel (<a href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\">FSDP</a>), <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash Attention 2</a> and <a href=\"https://github.com/artidoro/qlora\">QLoRA</a>.</li>\n        </ul>\n<ul>\n          <li>Visual Encoders: <a href=\"https://github.com/openai/CLIP\">CLIP</a>, <a href=\"https://github.com/salesforce/LAVIS\">Q-Former</a> and <a href=\"https://github.com/facebookresearch/ImageBind\">ImageBind</a>.</li>\n          <li>LLMs: LLaMA and LLaMA2.</li>\n        </ul>\n<p><a href=\"https://github.com/Alpha-VLLM/LLaMA2-Accessorys\"><img src=\"/primers/ai/assets/LLM/LLaMA2-Accessory.png\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessorys) is an advanced open-source toolkit for large language models.\n*   Evolved from LLaMA-Adapter, they support more datasets, tasks, visual encoders, and efficient optimization methods.\n*   Key Features:\n    *   **Support More Datasets and Tasks:**\n        *   Pre-training with [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) and [StarCoder](https://github.com/bigcode-project/starcoder).\n        *   Single-modal fine-tuning with [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [ShareGPT](https://github.com/domeccleston/sharegpt), [LIMA](https://arxiv.org/pdf/2305.11206.pdf), [WizardLM](https://github.com/nlpxucan/WizardLM), [UltraChat](https://github.com/thunlp/UltraChat) and [MOSS](https://github.com/OpenLMLab/MOSS).\n        *   Multi-modal fine-tuning with image-text pairs ([LAION](https://laion.ai/blog/laion-5b/), [COYO](https://github.com/kakaobrain/coyo-dataset) and more), interleaved image-text data ([MMC4](https://github.com/allenai/mmc4) and [OBELISC](https://github.com/huggingface/OBELISC)) and visual instruction data ([LLaVA](https://github.com/haotian-liu/LLaVA), [Shrika](https://github.com/shikras/shikra), [Bard](https://bard.google.com/))\n        *   LLM for API Control ([GPT4Tools](https://github.com/StevenGrove/GPT4Tools) and [Gorilla](https://github.com/ShishirPatil/gorilla)).\n    *   **Efficient Optimization and Deployment**:\n        *   Parameter-efficient fine-tuning with [Zero-init Attention](https://github.com/OpenGVLab/LLaMA-Adapter) and [Bias-norm Tuning](https://github.com/OpenGVLab/LLaMA-Adapter).\n        *   Fully Sharded Data Parallel ([FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)), [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) and [QLoRA](https://github.com/artidoro/qlora).\n    *   **🏋️‍♀️Support More Visual Encoders and LLMs**:\n        *   Visual Encoders: [CLIP](https://github.com/openai/CLIP), [Q-Former](https://github.com/salesforce/LAVIS) and [ImageBind](https://github.com/facebookresearch/ImageBind).\n        *   LLMs: LLaMA and LLaMA2.\n\n*   **Support More Datasets and Tasks:**\n    *   Pre-training with [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) and [StarCoder](https://github.com/bigcode-project/starcoder).\n    *   Single-modal fine-tuning with [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [ShareGPT](https://github.com/domeccleston/sharegpt), [LIMA](https://arxiv.org/pdf/2305.11206.pdf), [WizardLM](https://github.com/nlpxucan/WizardLM), [UltraChat](https://github.com/thunlp/UltraChat) and [MOSS](https://github.com/OpenLMLab/MOSS).\n    *   Multi-modal fine-tuning with image-text pairs ([LAION](https://laion.ai/blog/laion-5b/), [COYO](https://github.com/kakaobrain/coyo-dataset) and more), interleaved image-text data ([MMC4](https://github.com/allenai/mmc4) and [OBELISC](https://github.com/huggingface/OBELISC)) and visual instruction data ([LLaVA](https://github.com/haotian-liu/LLaVA), [Shrika](https://github.com/shikras/shikra), [Bard](https://bard.google.com/))\n    *   LLM for API Control ([GPT4Tools](https://github.com/StevenGrove/GPT4Tools) and [Gorilla](https://github.com/ShishirPatil/gorilla)).\n*   **Efficient Optimization and Deployment**:\n    *   Parameter-efficient fine-tuning with [Zero-init Attention](https://github.com/OpenGVLab/LLaMA-Adapter) and [Bias-norm Tuning](https://github.com/OpenGVLab/LLaMA-Adapter).\n    *   Fully Sharded Data Parallel ([FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)), [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) and [QLoRA](https://github.com/artidoro/qlora).\n*   **🏋️‍♀️Support More Visual Encoders and LLMs**:\n    *   Visual Encoders: [CLIP](https://github.com/openai/CLIP), [Q-Former](https://github.com/salesforce/LAVIS) and [ImageBind](https://github.com/facebookresearch/ImageBind).\n    *   LLMs: LLaMA and LLaMA2.\n\n*   Pre-training with [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) and [StarCoder](https://github.com/bigcode-project/starcoder).\n*   Single-modal fine-tuning with [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [ShareGPT](https://github.com/domeccleston/sharegpt), [LIMA](https://arxiv.org/pdf/2305.11206.pdf), [WizardLM](https://github.com/nlpxucan/WizardLM), [UltraChat](https://github.com/thunlp/UltraChat) and [MOSS](https://github.com/OpenLMLab/MOSS).\n*   Multi-modal fine-tuning with image-text pairs ([LAION](https://laion.ai/blog/laion-5b/), [COYO](https://github.com/kakaobrain/coyo-dataset) and more), interleaved image-text data ([MMC4](https://github.com/allenai/mmc4) and [OBELISC](https://github.com/huggingface/OBELISC)) and visual instruction data ([LLaVA](https://github.com/haotian-liu/LLaVA), [Shrika](https://github.com/shikras/shikra), [Bard](https://bard.google.com/))\n*   LLM for API Control ([GPT4Tools](https://github.com/StevenGrove/GPT4Tools) and [Gorilla](https://github.com/ShishirPatil/gorilla)).\n\n*   Parameter-efficient fine-tuning with [Zero-init Attention](https://github.com/OpenGVLab/LLaMA-Adapter) and [Bias-norm Tuning](https://github.com/OpenGVLab/LLaMA-Adapter).\n*   Fully Sharded Data Parallel ([FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)), [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) and [QLoRA](https://github.com/artidoro/qlora).\n\n*   Visual Encoders: [CLIP](https://github.com/openai/CLIP), [Q-Former](https://github.com/salesforce/LAVIS) and [ImageBind](https://github.com/facebookresearch/ImageBind).\n*   LLMs: LLaMA and LLaMA2.\n\n[![](/primers/ai/assets/LLM/LLaMA2-Accessory.png)](https://github.com/Alpha-VLLM/LLaMA2-Accessorys)",
    "order": 50,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "attention",
      "gpt",
      "llm",
      "nlp",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 306,
      "contentLength": 7308
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llama2-accessory",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-llama-factory-51",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "LLaMA Factory",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>LLaMA Factory is a one-stop for easy fine-tuning with LLaMA models. It provides a variety of supported models and training approaches, including LoRA and QLoRA tuning, and supports training on single GPU setups. It boasts faster training speeds and improved performance in tasks like text generation compared to other models.</li>\n  <li>LLaMA Factory also includes features for model training, evaluation, and prediction, as well as a range of datasets and requirements for effective usage.</li>\n  <li><a href=\"https://huggingface.co/spaces/hiyouga/LLaMA-Board\">LLaMA Board</a> offers a comprehensive web UI for getting started with <a href=\"https://github.com/hiyouga/LLaMA-Factory\">LLaMA Factory</a>.</li>\n</ul>\n<p><a href=\"https://github.com/hiyouga/LLaMA-Factory\"><img src=\"/primers/ai/assets/LLM/LLaMA-Factory.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   LLaMA Factory is a one-stop for easy fine-tuning with LLaMA models. It provides a variety of supported models and training approaches, including LoRA and QLoRA tuning, and supports training on single GPU setups. It boasts faster training speeds and improved performance in tasks like text generation compared to other models.\n*   LLaMA Factory also includes features for model training, evaluation, and prediction, as well as a range of datasets and requirements for effective usage.\n*   [LLaMA Board](https://huggingface.co/spaces/hiyouga/LLaMA-Board) offers a comprehensive web UI for getting started with [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory).\n\n[![](/primers/ai/assets/LLM/LLaMA-Factory.jpg)](https://github.com/hiyouga/LLaMA-Factory)",
    "order": 51,
    "orderInChapter": 6,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 90,
      "contentLength": 846
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llama-factory",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-gptcache-52",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "GPTCache",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>Caching, in the context of language models, is storing prompts and their corresponding responses in a database for future use. By caching responses to previously posed questions, LLM-powered apps can deliver faster and cheaper responses, bypassing the need for another LLM API call.</li>\n  <li>GPTCache is a great start and only requires a few lines of code.</li>\n  <li>It works for an exact match, i.e., using the same prompt twice, or for a similar match, i.e., two prompts with the same meaning. For example, “Who was the first US president?” and “Tell me who was the first US president?” are not exact matches but would yield the same answer, therefore saving an API call.</li>\n  <li>The invaluable benefits of caching:\n  (1) Faster and cheaper inference in production, with some queries achieving close-to-zero latency thanks to a cached response\n  (2) Faster and cheaper development cycles, as you don’t incur costs or wait for the response when working with the same prompt repeatedly\n  (3) Having all prompts stored in a database simplifies the process of fine-tuning a language model once you choose to do so, as you can use the stored prompt-response pairs</li>\n  <li>GPTCache also provides metrics such as the cache hit ratio, latency, and recall to gauge how well your cache performs and improve it.</li>\n</ul>\n<p><a href=\"https://github.com/zilliztech/GPTCache\"><img src=\"/primers/ai/assets/LLM/GPTCache.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Caching, in the context of language models, is storing prompts and their corresponding responses in a database for future use. By caching responses to previously posed questions, LLM-powered apps can deliver faster and cheaper responses, bypassing the need for another LLM API call.\n*   GPTCache is a great start and only requires a few lines of code.\n*   It works for an exact match, i.e., using the same prompt twice, or for a similar match, i.e., two prompts with the same meaning. For example, “Who was the first US president?” and “Tell me who was the first US president?” are not exact matches but would yield the same answer, therefore saving an API call.\n*   The invaluable benefits of caching: (1) Faster and cheaper inference in production, with some queries achieving close-to-zero latency thanks to a cached response (2) Faster and cheaper development cycles, as you don’t incur costs or wait for the response when working with the same prompt repeatedly (3) Having all prompts stored in a database simplifies the process of fine-tuning a language model once you choose to do so, as you can use the stored prompt-response pairs\n*   GPTCache also provides metrics such as the cache hit ratio, latency, and recall to gauge how well your cache performs and improve it.\n\n[![](/primers/ai/assets/LLM/GPTCache.jpg)](https://github.com/zilliztech/GPTCache)",
    "order": 52,
    "orderInChapter": 7,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 217,
      "contentLength": 1448
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#gptcache",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-prompt-lookup-decoding-53",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "Prompt Lookup Decoding",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>Prompt Lookup Decoding speeds up LLMs by 2-4x with prompt lookup decoding. The method is effective in summarization, document QA, multi-turn chat, code editing. This method can be used with any decoder model without model changes or external datastore, and with both greedy and sampling techniques.</li>\n  <li>The method is a modified version of speculative decoding where the draft model is replaced with simple string matching in the prompt to generate candidate token sequences. This results in significant speedups (2x-4x) in input-grounded tasks, with no effect on output quality.</li>\n  <li>The intuition of Prompt Lookup Decoding is based on the fact that several LLM use cases where you’re doing input grounded generation (summarization, document QA, multi-turn chat, code editing), there is high n-gram overlap between LLM input (prompt) and LLM output. This could be entity names, phrases, or code chunks that the LLM directly copies from the input while generating the output. Prompt lookup exploits this pattern to speed up autoregressive decoding in LLMs.</li>\n  <li><a href=\"https://github.com/apoorvumang/prompt-lookup-decoding/blob/main/demo-pld.ipynb\">Demo notebook</a>; <a href=\"https://colab.research.google.com/drive/1ovjH1sg3lXWdm5Rx5EEukB9H_PFJVpJ4?usp=sharing\">Colab</a></li>\n</ul>",
    "contentMarkdown": "*   Prompt Lookup Decoding speeds up LLMs by 2-4x with prompt lookup decoding. The method is effective in summarization, document QA, multi-turn chat, code editing. This method can be used with any decoder model without model changes or external datastore, and with both greedy and sampling techniques.\n*   The method is a modified version of speculative decoding where the draft model is replaced with simple string matching in the prompt to generate candidate token sequences. This results in significant speedups (2x-4x) in input-grounded tasks, with no effect on output quality.\n*   The intuition of Prompt Lookup Decoding is based on the fact that several LLM use cases where you’re doing input grounded generation (summarization, document QA, multi-turn chat, code editing), there is high n-gram overlap between LLM input (prompt) and LLM output. This could be entity names, phrases, or code chunks that the LLM directly copies from the input while generating the output. Prompt lookup exploits this pattern to speed up autoregressive decoding in LLMs.\n*   [Demo notebook](https://github.com/apoorvumang/prompt-lookup-decoding/blob/main/demo-pld.ipynb); [Colab](https://colab.research.google.com/drive/1ovjH1sg3lXWdm5Rx5EEukB9H_PFJVpJ4?usp=sharing)",
    "order": 53,
    "orderInChapter": 8,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 169,
      "contentLength": 1315
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#prompt-lookup-decoding",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-axolotl-54",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "Axolotl",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.</li>\n  <li>Features:\n    <ul>\n      <li>Train various HuggingFace models such as Llama, Pythia, Falcon, MPT</li>\n      <li>Supports FullFinetune, LoRA, QLoRA, ReLoRA, and GPTQ</li>\n      <li>Customize configurations using a simple YAML file or CLI overwrite</li>\n      <li>Load different dataset formats, use custom formats, or bring your own tokenized datasets</li>\n      <li>Integrated with XFormer, Flash Attention, ROPE Scaling, and Multipacking</li>\n      <li>Works with single GPU or multiple GPUs via FSDP or DeepSpeed</li>\n      <li>Easily run with Docker locally or on the cloud</li>\n      <li>Log results and optionally checkpoints to Weights &amp; Biases (W&amp;B) or MLflow</li>\n      <li>… and more!</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Train various HuggingFace models such as Llama, Pythia, Falcon, MPT</li>\n      <li>Supports FullFinetune, LoRA, QLoRA, ReLoRA, and GPTQ</li>\n      <li>Customize configurations using a simple YAML file or CLI overwrite</li>\n      <li>Load different dataset formats, use custom formats, or bring your own tokenized datasets</li>\n      <li>Integrated with XFormer, Flash Attention, ROPE Scaling, and Multipacking</li>\n      <li>Works with single GPU or multiple GPUs via FSDP or DeepSpeed</li>\n      <li>Easily run with Docker locally or on the cloud</li>\n      <li>Log results and optionally checkpoints to Weights &amp; Biases (W&amp;B) or MLflow</li>\n      <li>… and more!</li>\n    </ul>",
    "contentMarkdown": "*   Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\n*   Features:\n    *   Train various HuggingFace models such as Llama, Pythia, Falcon, MPT\n    *   Supports FullFinetune, LoRA, QLoRA, ReLoRA, and GPTQ\n    *   Customize configurations using a simple YAML file or CLI overwrite\n    *   Load different dataset formats, use custom formats, or bring your own tokenized datasets\n    *   Integrated with XFormer, Flash Attention, ROPE Scaling, and Multipacking\n    *   Works with single GPU or multiple GPUs via FSDP or DeepSpeed\n    *   Easily run with Docker locally or on the cloud\n    *   Log results and optionally checkpoints to Weights & Biases (W&B) or MLflow\n    *   … and more!\n\n*   Train various HuggingFace models such as Llama, Pythia, Falcon, MPT\n*   Supports FullFinetune, LoRA, QLoRA, ReLoRA, and GPTQ\n*   Customize configurations using a simple YAML file or CLI overwrite\n*   Load different dataset formats, use custom formats, or bring your own tokenized datasets\n*   Integrated with XFormer, Flash Attention, ROPE Scaling, and Multipacking\n*   Works with single GPU or multiple GPUs via FSDP or DeepSpeed\n*   Easily run with Docker locally or on the cloud\n*   Log results and optionally checkpoints to Weights & Biases (W&B) or MLflow\n*   … and more!",
    "order": 54,
    "orderInChapter": 9,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "attention",
      "gpt",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 209,
      "contentLength": 1598
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#axolotl",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-trl---transformer-reinforcement-learning-55",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "TRL - Transformer Reinforcement Learning",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>The <code class=\"language-plaintext highlighter-rouge\">trl</code> library is a full stack library to fine-tune and align transformer language and diffusion models using methods such as Supervised Fine-tuning step (SFT), Reward Modeling (RM) and the Proximal Policy Optimization (PPO) as well as Direct Preference Optimization (DPO).</li>\n  <li>The library is built on top of the <code class=\"language-plaintext highlighter-rouge\">transformers</code> library and thus allows to use any model architecture available there.</li>\n</ul>",
    "contentMarkdown": "*   The `trl` library is a full stack library to fine-tune and align transformer language and diffusion models using methods such as Supervised Fine-tuning step (SFT), Reward Modeling (RM) and the Proximal Policy Optimization (PPO) as well as Direct Preference Optimization (DPO).\n*   The library is built on top of the `transformers` library and thus allows to use any model architecture available there.",
    "order": 55,
    "orderInChapter": 10,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "transformer",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 63,
      "contentLength": 542
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#trl---transformer-reinforcement-learning",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-openrouter-56",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Frameworks",
    "title": "OpenRouter",
    "subtitle": "Frameworks",
    "contentHtml": "<ul>\n  <li>OpenRouter provides a unified API that gives you access to hundreds of AI models through a single endpoint, while automatically handling fallbacks and selecting the most cost-effective options.</li>\n</ul>",
    "contentMarkdown": "*   OpenRouter provides a unified API that gives you access to hundreds of AI models through a single endpoint, while automatically handling fallbacks and selecting the most cost-effective options.",
    "order": 56,
    "orderInChapter": 11,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 29,
      "contentLength": 215
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#openrouter",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-estimate-token-importance-in-llm-prompts-57",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Miscellaneous",
    "title": "Estimate Token Importance in LLM Prompts",
    "subtitle": "Miscellaneous",
    "contentHtml": "<ul>\n  <li>Akin to how Integrated Gradients tells us how much each knob’s adjustment contributed to the final decision, this <a href=\"https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts?utm_source=Andriy_Burkov_Newsletter&amp;utm_medium=Andriy_Burkov_Newsletter\">process by Watchful.io</a> performs ablation and re-embedding, i.e., it removes each token one by one, create a new embedding, and then compares it to the original. This leads to importance estimation where the degree of the resulting difference gives us a rough idea of each token’s importance.</li>\n</ul>\n<p><a href=\"https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts?utm_source=Andriy_Burkov_Newsletter&amp;utm_medium=Andriy_Burkov_Newsletter\"><img src=\"/primers/ai/assets/LLM/Watchful.png\" alt=\"\"></a></p>",
    "contentMarkdown": "*   Akin to how Integrated Gradients tells us how much each knob’s adjustment contributed to the final decision, this [process by Watchful.io](https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts?utm_source=Andriy_Burkov_Newsletter&utm_medium=Andriy_Burkov_Newsletter) performs ablation and re-embedding, i.e., it removes each token one by one, create a new embedding, and then compares it to the original. This leads to importance estimation where the degree of the resulting difference gives us a rough idea of each token’s importance.\n\n[![](/primers/ai/assets/LLM/Watchful.png)](https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts?utm_source=Andriy_Burkov_Newsletter&utm_medium=Andriy_Burkov_Newsletter)",
    "order": 57,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 67,
      "contentLength": 873
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#estimate-token-importance-in-llm-prompts",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-attention-manipulation-to-steer-llm-output-58",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Miscellaneous",
    "title": "Attention Manipulation to Steer LLM Output",
    "subtitle": "Miscellaneous",
    "contentHtml": "<ul>\n  <li>OpenAI and Anthropic have obscured the output probabilities of their LLM’s generated tokens. This affects the interpretability (and thus the safety) of the generated output since “adapting” it at the user-level is impossible without knowing the statistics behind the generation.</li>\n  <li>For instance, Aleph Alpha (a German LLM company) recently announced a feature where you can control the <a href=\"https://docs.aleph-alpha.com/docs/explainability/attention-manipulation/\">emphasis of individual words</a> in a prompt via highlighting to change their effect on the output.</li>\n  <li>It is worth noting that something like this was available through the original GPT-3 API (through manipulating the tokens’ log-probabilities). However, it wasn’t offered in an easy format like Aleph Alpha’s, nor with this level of manipulation. OpenAI and Anthropic removed the feature in the newest releases (for GPT-4 and Claude).</li>\n  <li>Aleph Alpha have a few other <a href=\"https://docs.aleph-alpha.com/docs/explainability/explainability/\">user-level explainability features</a> which are very cool, like info on which specific tokens influenced which part of the output and to what extent. These tools are important for prompt-craft: they take a bit of the intuition out of process in a useful way, as you know upfront which tokens will be more or less impactful to change outputs.</li>\n  <li>This is also a good example of how safety and interpretability research actually lead to product improvements, without need for gain of function through model size. If users have more tools for guiding the model predictably, a model of given size becomes more powerful, while becoming more safe at the same time. This only can happen through researching more about the internals of the model. In this way safety research is profoundly compatible with, and an accelerant for, increasingly powerful product experiences.</li>\n  <li><a href=\"https://arxiv.org/pdf/2301.08110.pdf\">Here’s</a> their paper on Attention Manipulation (AtMan) that talks about suppressing/amplifying the attention of tokens to steer the model’s output. We can also build upon this idea to suppress high entropy words and thus automatically change the next set of tokens and avoid hallucination.</li>\n</ul>",
    "contentMarkdown": "*   OpenAI and Anthropic have obscured the output probabilities of their LLM’s generated tokens. This affects the interpretability (and thus the safety) of the generated output since “adapting” it at the user-level is impossible without knowing the statistics behind the generation.\n*   For instance, Aleph Alpha (a German LLM company) recently announced a feature where you can control the [emphasis of individual words](https://docs.aleph-alpha.com/docs/explainability/attention-manipulation/) in a prompt via highlighting to change their effect on the output.\n*   It is worth noting that something like this was available through the original GPT-3 API (through manipulating the tokens’ log-probabilities). However, it wasn’t offered in an easy format like Aleph Alpha’s, nor with this level of manipulation. OpenAI and Anthropic removed the feature in the newest releases (for GPT-4 and Claude).\n*   Aleph Alpha have a few other [user-level explainability features](https://docs.aleph-alpha.com/docs/explainability/explainability/) which are very cool, like info on which specific tokens influenced which part of the output and to what extent. These tools are important for prompt-craft: they take a bit of the intuition out of process in a useful way, as you know upfront which tokens will be more or less impactful to change outputs.\n*   This is also a good example of how safety and interpretability research actually lead to product improvements, without need for gain of function through model size. If users have more tools for guiding the model predictably, a model of given size becomes more powerful, while becoming more safe at the same time. This only can happen through researching more about the internals of the model. In this way safety research is profoundly compatible with, and an accelerant for, increasingly powerful product experiences.\n*   [Here’s](https://arxiv.org/pdf/2301.08110.pdf) their paper on Attention Manipulation (AtMan) that talks about suppressing/amplifying the attention of tokens to steer the model’s output. We can also build upon this idea to suppress high entropy words and thus automatically change the next set of tokens and avoid hallucination.",
    "order": 58,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "attention",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 320,
      "contentLength": 2279
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#attention-manipulation-to-steer-llm-output",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-strategies-to-get-better-results-using-prompt-engi-59",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Miscellaneous",
    "title": "Strategies to Get Better Results Using Prompt Engineering",
    "subtitle": "Miscellaneous",
    "contentHtml": "<ul>\n  <li>Here are some tactics to get better results when prompting an LLM (summarized from <a href=\"https://platform.openai.com/docs/guides/gpt-best-practices/six-strategies-for-getting-better-results\">source</a>):\n    <ul>\n      <li><strong>Write clear instructions:</strong> the more specific you are with your instructions, the better the output. This could involve specifying the desired length and format or asking the model to adopt a certain persona, such as a world-class nutritionist or sarcastic writer.</li>\n      <li><strong>Provide reference text:</strong> offering reference text can guide the model towards more precise and less hallucinated outputs, much like a student using study notes for an exam.</li>\n      <li><strong>Break down complex tasks:</strong> deconstructing complex tasks into smaller, manageable subtasks can reduce error rates and improve results. For example, an inbound support request can be first addressed with an API call to categorize the user’s message, followed by another call to generate the response based on the category.</li>\n      <li><strong>Encourage the model to think:</strong> asking an LLM to outline its thinking process can help the model reason its way toward more accurate responses.</li>\n      <li><strong>Leverage external tools:</strong> complement GPT’s capabilities by using external tools, such as a text retrieval system or a code execution engine. For example, generating Python code to perform math calculations. You can even use GPT to generate the code that calls external APIs.</li>\n      <li><strong>Evaluate changes systematically:</strong> getting to a performant prompt requires multiple iterations. Establish a comprehensive test suite to ensure changes improve performance and compare model outputs against benchmark answers. The benchmark should represent real-world usage, contain numerous test cases for statistical power, and be easy to automate or repeat. Evaluations can be performed by computers (other LLMs), humans, or a combination of both.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Write clear instructions:</strong> the more specific you are with your instructions, the better the output. This could involve specifying the desired length and format or asking the model to adopt a certain persona, such as a world-class nutritionist or sarcastic writer.</li>\n      <li><strong>Provide reference text:</strong> offering reference text can guide the model towards more precise and less hallucinated outputs, much like a student using study notes for an exam.</li>\n      <li><strong>Break down complex tasks:</strong> deconstructing complex tasks into smaller, manageable subtasks can reduce error rates and improve results. For example, an inbound support request can be first addressed with an API call to categorize the user’s message, followed by another call to generate the response based on the category.</li>\n      <li><strong>Encourage the model to think:</strong> asking an LLM to outline its thinking process can help the model reason its way toward more accurate responses.</li>\n      <li><strong>Leverage external tools:</strong> complement GPT’s capabilities by using external tools, such as a text retrieval system or a code execution engine. For example, generating Python code to perform math calculations. You can even use GPT to generate the code that calls external APIs.</li>\n      <li><strong>Evaluate changes systematically:</strong> getting to a performant prompt requires multiple iterations. Establish a comprehensive test suite to ensure changes improve performance and compare model outputs against benchmark answers. The benchmark should represent real-world usage, contain numerous test cases for statistical power, and be easy to automate or repeat. Evaluations can be performed by computers (other LLMs), humans, or a combination of both.</li>\n    </ul>",
    "contentMarkdown": "*   Here are some tactics to get better results when prompting an LLM (summarized from [source](https://platform.openai.com/docs/guides/gpt-best-practices/six-strategies-for-getting-better-results)):\n    *   **Write clear instructions:** the more specific you are with your instructions, the better the output. This could involve specifying the desired length and format or asking the model to adopt a certain persona, such as a world-class nutritionist or sarcastic writer.\n    *   **Provide reference text:** offering reference text can guide the model towards more precise and less hallucinated outputs, much like a student using study notes for an exam.\n    *   **Break down complex tasks:** deconstructing complex tasks into smaller, manageable subtasks can reduce error rates and improve results. For example, an inbound support request can be first addressed with an API call to categorize the user’s message, followed by another call to generate the response based on the category.\n    *   **Encourage the model to think:** asking an LLM to outline its thinking process can help the model reason its way toward more accurate responses.\n    *   **Leverage external tools:** complement GPT’s capabilities by using external tools, such as a text retrieval system or a code execution engine. For example, generating Python code to perform math calculations. You can even use GPT to generate the code that calls external APIs.\n    *   **Evaluate changes systematically:** getting to a performant prompt requires multiple iterations. Establish a comprehensive test suite to ensure changes improve performance and compare model outputs against benchmark answers. The benchmark should represent real-world usage, contain numerous test cases for statistical power, and be easy to automate or repeat. Evaluations can be performed by computers (other LLMs), humans, or a combination of both.\n\n*   **Write clear instructions:** the more specific you are with your instructions, the better the output. This could involve specifying the desired length and format or asking the model to adopt a certain persona, such as a world-class nutritionist or sarcastic writer.\n*   **Provide reference text:** offering reference text can guide the model towards more precise and less hallucinated outputs, much like a student using study notes for an exam.\n*   **Break down complex tasks:** deconstructing complex tasks into smaller, manageable subtasks can reduce error rates and improve results. For example, an inbound support request can be first addressed with an API call to categorize the user’s message, followed by another call to generate the response based on the category.\n*   **Encourage the model to think:** asking an LLM to outline its thinking process can help the model reason its way toward more accurate responses.\n*   **Leverage external tools:** complement GPT’s capabilities by using external tools, such as a text retrieval system or a code execution engine. For example, generating Python code to perform math calculations. You can even use GPT to generate the code that calls external APIs.\n*   **Evaluate changes systematically:** getting to a performant prompt requires multiple iterations. Establish a comprehensive test suite to ensure changes improve performance and compare model outputs against benchmark answers. The benchmark should represent real-world usage, contain numerous test cases for statistical power, and be easy to automate or repeat. Evaluations can be performed by computers (other LLMs), humans, or a combination of both.",
    "order": 59,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 520,
      "contentLength": 3883
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#strategies-to-get-better-results-using-prompt-engineering",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-the-reversal-curse-llms-trained-on-a-is-b-fail-to--60",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Miscellaneous",
    "title": "The Reversal Curse: LLMs Trained on “A is B” Fail to Learn “B is A”",
    "subtitle": "Miscellaneous",
    "contentHtml": "<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/pdf/2309.12288.pdf\">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a>, the “Reversal Curse” in the context of language models refers to a limitation where models trained on statements in the form “A is B” struggle to infer the reversed statement “B is A.” For example, if a model learns “Olaf Scholz was the ninth Chancellor of Germany,” it may not infer from this that “The ninth Chancellor of Germany was Olaf Scholz.”</li>\n  <li>This phenomenon indicates a basic failure in logical deduction and generalization in language models, as they don’t naturally learn the symmetry in relationships from their training data. The paper presents experiments that highlight this limitation in models like GPT-3 and BERT, suggesting a need for improved training approaches to overcome this issue.</li>\n  <li>The solution to the Reversal Curse proposed in the paper involves a specific method of data augmentation for training language models. Here’s an illustrative example of how this solution would be applied:\n    <ol>\n      <li><strong>Original Training Data:</strong>\n        <ul>\n          <li>Suppose the original training data contains a factual statement like: “The capital of France is Paris.”</li>\n        </ul>\n      </li>\n      <li><strong>Augmented Training Data with Reversed Statements:</strong>\n        <ul>\n          <li>Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”</li>\n          <li>This augmentation explicitly introduces the concept of reversible relationships to the language model.</li>\n        </ul>\n      </li>\n      <li><strong>Training Process:</strong>\n        <ul>\n          <li>The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.</li>\n          <li>During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.</li>\n        </ul>\n      </li>\n      <li><strong>Expected Outcome:</strong>\n        <ul>\n          <li>After being trained on such data, the model is expected to better handle reversible relationships.</li>\n          <li>When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>This method of data augmentation directly addresses the Reversal Curse by teaching the model that the order of elements in certain types of statements can be reversed without changing their meaning. This approach enhances the model’s logical reasoning and generalization capabilities.</li>\n</ul>\n<ol>\n      <li><strong>Original Training Data:</strong>\n        <ul>\n          <li>Suppose the original training data contains a factual statement like: “The capital of France is Paris.”</li>\n        </ul>\n      </li>\n      <li><strong>Augmented Training Data with Reversed Statements:</strong>\n        <ul>\n          <li>Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”</li>\n          <li>This augmentation explicitly introduces the concept of reversible relationships to the language model.</li>\n        </ul>\n      </li>\n      <li><strong>Training Process:</strong>\n        <ul>\n          <li>The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.</li>\n          <li>During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.</li>\n        </ul>\n      </li>\n      <li><strong>Expected Outcome:</strong>\n        <ul>\n          <li>After being trained on such data, the model is expected to better handle reversible relationships.</li>\n          <li>When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>Suppose the original training data contains a factual statement like: “The capital of France is Paris.”</li>\n        </ul>\n<ul>\n          <li>Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”</li>\n          <li>This augmentation explicitly introduces the concept of reversible relationships to the language model.</li>\n        </ul>\n<ul>\n          <li>The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.</li>\n          <li>During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.</li>\n        </ul>\n<ul>\n          <li>After being trained on such data, the model is expected to better handle reversible relationships.</li>\n          <li>When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.</li>\n        </ul>",
    "contentMarkdown": "*   Proposed in [The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”](https://arxiv.org/pdf/2309.12288.pdf), the “Reversal Curse” in the context of language models refers to a limitation where models trained on statements in the form “A is B” struggle to infer the reversed statement “B is A.” For example, if a model learns “Olaf Scholz was the ninth Chancellor of Germany,” it may not infer from this that “The ninth Chancellor of Germany was Olaf Scholz.”\n*   This phenomenon indicates a basic failure in logical deduction and generalization in language models, as they don’t naturally learn the symmetry in relationships from their training data. The paper presents experiments that highlight this limitation in models like GPT-3 and BERT, suggesting a need for improved training approaches to overcome this issue.\n*   The solution to the Reversal Curse proposed in the paper involves a specific method of data augmentation for training language models. Here’s an illustrative example of how this solution would be applied:\n    1.  **Original Training Data:**\n        *   Suppose the original training data contains a factual statement like: “The capital of France is Paris.”\n    2.  **Augmented Training Data with Reversed Statements:**\n        *   Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”\n        *   This augmentation explicitly introduces the concept of reversible relationships to the language model.\n    3.  **Training Process:**\n        *   The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.\n        *   During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.\n    4.  **Expected Outcome:**\n        *   After being trained on such data, the model is expected to better handle reversible relationships.\n        *   When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.\n*   This method of data augmentation directly addresses the Reversal Curse by teaching the model that the order of elements in certain types of statements can be reversed without changing their meaning. This approach enhances the model’s logical reasoning and generalization capabilities.\n\n1.  **Original Training Data:**\n    *   Suppose the original training data contains a factual statement like: “The capital of France is Paris.”\n2.  **Augmented Training Data with Reversed Statements:**\n    *   Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”\n    *   This augmentation explicitly introduces the concept of reversible relationships to the language model.\n3.  **Training Process:**\n    *   The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.\n    *   During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.\n4.  **Expected Outcome:**\n    *   After being trained on such data, the model is expected to better handle reversible relationships.\n    *   When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.\n\n*   Suppose the original training data contains a factual statement like: “The capital of France is Paris.”\n\n*   Along with the original statement, the training data is augmented with its reversed counterpart: “Paris is the capital of France.”\n*   This augmentation explicitly introduces the concept of reversible relationships to the language model.\n\n*   The language model is then trained on this augmented dataset, which includes both the original statements and their reversed versions.\n*   During training, the model learns to understand that these pairs of statements, though structurally different, are semantically equivalent.\n\n*   After being trained on such data, the model is expected to better handle reversible relationships.\n*   When presented with a new statement like “Berlin is the capital of Germany,” the model should be more capable of inferring the reversed statement “The capital of Germany is Berlin” as true.",
    "order": 60,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "bert",
      "gpt",
      "llm",
      "data augmentation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 665,
      "contentLength": 5348
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#the-reversal-curse:-llms-trained-on-“a-is-b”-fail-to-learn-“b-is-a”",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-prompt-generator-61",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Prompt Tools",
    "title": "Prompt Generator",
    "subtitle": "Prompt Tools",
    "contentHtml": "<ul>\n  <li>\n    <p>Announced in <a href=\"https://www.anthropic.com/news/prompt-generator\">Generate better prompts in the developer console</a>.</p>\n  </li>\n  <li>\n    <p><strong>Production-Ready Prompt Generation</strong>: The Anthropic Console now enables users to create production-ready prompt templates with advanced techniques like chain-of-thought reasoning, tailored for effective and reliable outputs.</p>\n  </li>\n  <li>\n    <p><strong>User-Friendly and Customizable</strong>: Designed to assist both beginners and experienced prompt engineers, this tool saves time and allows for easy edits to optimize the generated prompts.</p>\n  </li>\n  <li>\n    <p><strong>Implementation of Best Practices</strong>: Generated prompts incorporate techniques like role setting (e.g., assigning Claude specific roles like a content moderator) and chain-of-thought reasoning to encourage thorough and well-structured responses.</p>\n  </li>\n  <li>\n    <p><strong>Structured and Flexible Formatting</strong>: Templates include XML-tagged variables for clear structure and allow for customized input fields, such as coding language or task-specific data, ensuring flexibility.</p>\n  </li>\n  <li>\n    <p><strong>Examples for Clarity</strong>: Some prompts include sample inputs and outputs to guide Claude toward the desired result, which users can modify for specific formatting requirements.</p>\n  </li>\n  <li>\n    <p><strong>Evaluation and Testing Tool</strong>: Variables in handlebars notation enable uploading policies or datasets to test Claude’s behavior in diverse scenarios, ensuring robust application responses.</p>\n  </li>\n</ul>\n<p>Announced in <a href=\"https://www.anthropic.com/news/prompt-generator\">Generate better prompts in the developer console</a>.</p>\n<p><strong>Production-Ready Prompt Generation</strong>: The Anthropic Console now enables users to create production-ready prompt templates with advanced techniques like chain-of-thought reasoning, tailored for effective and reliable outputs.</p>\n<p><strong>User-Friendly and Customizable</strong>: Designed to assist both beginners and experienced prompt engineers, this tool saves time and allows for easy edits to optimize the generated prompts.</p>\n<p><strong>Implementation of Best Practices</strong>: Generated prompts incorporate techniques like role setting (e.g., assigning Claude specific roles like a content moderator) and chain-of-thought reasoning to encourage thorough and well-structured responses.</p>\n<p><strong>Structured and Flexible Formatting</strong>: Templates include XML-tagged variables for clear structure and allow for customized input fields, such as coding language or task-specific data, ensuring flexibility.</p>\n<p><strong>Examples for Clarity</strong>: Some prompts include sample inputs and outputs to guide Claude toward the desired result, which users can modify for specific formatting requirements.</p>\n<p><strong>Evaluation and Testing Tool</strong>: Variables in handlebars notation enable uploading policies or datasets to test Claude’s behavior in diverse scenarios, ensuring robust application responses.</p>",
    "contentMarkdown": "*   Announced in [Generate better prompts in the developer console](https://www.anthropic.com/news/prompt-generator).\n    \n*   **Production-Ready Prompt Generation**: The Anthropic Console now enables users to create production-ready prompt templates with advanced techniques like chain-of-thought reasoning, tailored for effective and reliable outputs.\n    \n*   **User-Friendly and Customizable**: Designed to assist both beginners and experienced prompt engineers, this tool saves time and allows for easy edits to optimize the generated prompts.\n    \n*   **Implementation of Best Practices**: Generated prompts incorporate techniques like role setting (e.g., assigning Claude specific roles like a content moderator) and chain-of-thought reasoning to encourage thorough and well-structured responses.\n    \n*   **Structured and Flexible Formatting**: Templates include XML-tagged variables for clear structure and allow for customized input fields, such as coding language or task-specific data, ensuring flexibility.\n    \n*   **Examples for Clarity**: Some prompts include sample inputs and outputs to guide Claude toward the desired result, which users can modify for specific formatting requirements.\n    \n*   **Evaluation and Testing Tool**: Variables in handlebars notation enable uploading policies or datasets to test Claude’s behavior in diverse scenarios, ensuring robust application responses.\n    \n\nAnnounced in [Generate better prompts in the developer console](https://www.anthropic.com/news/prompt-generator).\n\n**Production-Ready Prompt Generation**: The Anthropic Console now enables users to create production-ready prompt templates with advanced techniques like chain-of-thought reasoning, tailored for effective and reliable outputs.\n\n**User-Friendly and Customizable**: Designed to assist both beginners and experienced prompt engineers, this tool saves time and allows for easy edits to optimize the generated prompts.\n\n**Implementation of Best Practices**: Generated prompts incorporate techniques like role setting (e.g., assigning Claude specific roles like a content moderator) and chain-of-thought reasoning to encourage thorough and well-structured responses.\n\n**Structured and Flexible Formatting**: Templates include XML-tagged variables for clear structure and allow for customized input fields, such as coding language or task-specific data, ensuring flexibility.\n\n**Examples for Clarity**: Some prompts include sample inputs and outputs to guide Claude toward the desired result, which users can modify for specific formatting requirements.\n\n**Evaluation and Testing Tool**: Variables in handlebars notation enable uploading policies or datasets to test Claude’s behavior in diverse scenarios, ensuring robust application responses.",
    "order": 61,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 337,
      "contentLength": 3115
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#prompt-generator",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-prompt-evaluatorimprover-62",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Prompt Tools",
    "title": "Prompt Evaluator/Improver",
    "subtitle": "Prompt Tools",
    "contentHtml": "<ul>\n  <li>\n    <p>Announced in <a href=\"https://www.anthropic.com/news/prompt-improver\">Improve your prompts in the developer console</a>.</p>\n  </li>\n  <li><strong>Prompt Improver Capabilities</strong>:\n    <ul>\n      <li>Automatically refines prompts with advanced techniques like chain-of-thought reasoning, example standardization, and prefilled directives.</li>\n      <li>Enhances prompt structure and grammar while adapting prompts for optimal Claude performance.</li>\n      <li>Demonstrated to improve accuracy (e.g., 30% increase in multilabel classification tests).</li>\n    </ul>\n  </li>\n  <li><strong>Multi-Shot Example Management</strong>:\n    <ul>\n      <li>Enables structured management of input/output examples directly in the Console.</li>\n      <li>Features Claude-driven synthetic example generation to reduce manual effort.</li>\n      <li>Improves response <strong>accuracy</strong>, <strong>consistency</strong>, and <strong>performance</strong> by providing clear, formatted examples.</li>\n    </ul>\n  </li>\n  <li><strong>Prompt Evaluator and Ideal Outputs</strong>:\n    <ul>\n      <li>Introduces an “ideal output” column for benchmarking responses on a 5-point scale.</li>\n      <li>Facilitates iterative improvement by allowing feedback on prompt performance, with options to test under varied scenarios.</li>\n    </ul>\n  </li>\n  <li><strong>Generate Prompts and Test Cases</strong>:\n    <ul>\n      <li>Simplifies prompt creation with a task-based prompt generator powered by Claude 3.5 Sonnet.</li>\n      <li>Automates test case generation for various real-world scenarios, reducing manual testing efforts.</li>\n    </ul>\n  </li>\n  <li><strong>Test Suite and Response Comparison</strong>:\n    <ul>\n      <li>Provides a centralized platform for running and modifying test cases.</li>\n      <li>Adds side-by-side comparison of multiple prompts to evaluate improvements.</li>\n      <li>Enables subject matter expert grading for a finer evaluation process.</li>\n    </ul>\n  </li>\n</ul>\n<p>Announced in <a href=\"https://www.anthropic.com/news/prompt-improver\">Improve your prompts in the developer console</a>.</p>\n<ul>\n      <li>Automatically refines prompts with advanced techniques like chain-of-thought reasoning, example standardization, and prefilled directives.</li>\n      <li>Enhances prompt structure and grammar while adapting prompts for optimal Claude performance.</li>\n      <li>Demonstrated to improve accuracy (e.g., 30% increase in multilabel classification tests).</li>\n    </ul>\n<ul>\n      <li>Enables structured management of input/output examples directly in the Console.</li>\n      <li>Features Claude-driven synthetic example generation to reduce manual effort.</li>\n      <li>Improves response <strong>accuracy</strong>, <strong>consistency</strong>, and <strong>performance</strong> by providing clear, formatted examples.</li>\n    </ul>\n<ul>\n      <li>Introduces an “ideal output” column for benchmarking responses on a 5-point scale.</li>\n      <li>Facilitates iterative improvement by allowing feedback on prompt performance, with options to test under varied scenarios.</li>\n    </ul>\n<ul>\n      <li>Simplifies prompt creation with a task-based prompt generator powered by Claude 3.5 Sonnet.</li>\n      <li>Automates test case generation for various real-world scenarios, reducing manual testing efforts.</li>\n    </ul>\n<ul>\n      <li>Provides a centralized platform for running and modifying test cases.</li>\n      <li>Adds side-by-side comparison of multiple prompts to evaluate improvements.</li>\n      <li>Enables subject matter expert grading for a finer evaluation process.</li>\n    </ul>",
    "contentMarkdown": "*   Announced in [Improve your prompts in the developer console](https://www.anthropic.com/news/prompt-improver).\n    \n*   **Prompt Improver Capabilities**:\n    *   Automatically refines prompts with advanced techniques like chain-of-thought reasoning, example standardization, and prefilled directives.\n    *   Enhances prompt structure and grammar while adapting prompts for optimal Claude performance.\n    *   Demonstrated to improve accuracy (e.g., 30% increase in multilabel classification tests).\n*   **Multi-Shot Example Management**:\n    *   Enables structured management of input/output examples directly in the Console.\n    *   Features Claude-driven synthetic example generation to reduce manual effort.\n    *   Improves response **accuracy**, **consistency**, and **performance** by providing clear, formatted examples.\n*   **Prompt Evaluator and Ideal Outputs**:\n    *   Introduces an “ideal output” column for benchmarking responses on a 5-point scale.\n    *   Facilitates iterative improvement by allowing feedback on prompt performance, with options to test under varied scenarios.\n*   **Generate Prompts and Test Cases**:\n    *   Simplifies prompt creation with a task-based prompt generator powered by Claude 3.5 Sonnet.\n    *   Automates test case generation for various real-world scenarios, reducing manual testing efforts.\n*   **Test Suite and Response Comparison**:\n    *   Provides a centralized platform for running and modifying test cases.\n    *   Adds side-by-side comparison of multiple prompts to evaluate improvements.\n    *   Enables subject matter expert grading for a finer evaluation process.\n\nAnnounced in [Improve your prompts in the developer console](https://www.anthropic.com/news/prompt-improver).\n\n*   Automatically refines prompts with advanced techniques like chain-of-thought reasoning, example standardization, and prefilled directives.\n*   Enhances prompt structure and grammar while adapting prompts for optimal Claude performance.\n*   Demonstrated to improve accuracy (e.g., 30% increase in multilabel classification tests).\n\n*   Enables structured management of input/output examples directly in the Console.\n*   Features Claude-driven synthetic example generation to reduce manual effort.\n*   Improves response **accuracy**, **consistency**, and **performance** by providing clear, formatted examples.\n\n*   Introduces an “ideal output” column for benchmarking responses on a 5-point scale.\n*   Facilitates iterative improvement by allowing feedback on prompt performance, with options to test under varied scenarios.\n\n*   Simplifies prompt creation with a task-based prompt generator powered by Claude 3.5 Sonnet.\n*   Automates test case generation for various real-world scenarios, reducing manual testing efforts.\n\n*   Provides a centralized platform for running and modifying test cases.\n*   Adds side-by-side comparison of multiple prompts to evaluate improvements.\n*   Enables subject matter expert grading for a finer evaluation process.",
    "order": 62,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 369,
      "contentLength": 3642
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#prompt-evaluator/improver",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-llama-2-responsible-use-guide-63",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Further Reading",
    "title": "Llama 2: Responsible Use Guide",
    "subtitle": "Further Reading",
    "contentHtml": "<ul>\n  <li>The Llama 2 <a href=\"https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf\">Responsible Use Guide</a> offers resources and best practices on how to build downstream LLM-powered products more responsibly.</li>\n  <li>It contains tips for fine-tuning the models, data preparation, mitigating risks, evaluation, red teaming, and a bunch of other resources useful for developers, especially on topics such as dealing with safety issues, hallucinations, adversarial attacks, etc.</li>\n</ul>",
    "contentMarkdown": "*   The Llama 2 [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf) offers resources and best practices on how to build downstream LLM-powered products more responsibly.\n*   It contains tips for fine-tuning the models, data preparation, mitigating risks, evaluation, red teaming, and a bunch of other resources useful for developers, especially on topics such as dealing with safety issues, hallucinations, adversarial attacks, etc.",
    "order": 63,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 58,
      "contentLength": 519
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#llama-2:-responsible-use-guide",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-extended-guide-instruction-tune-llama-2-64",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Further Reading",
    "title": "Extended Guide: Instruction-tune Llama 2",
    "subtitle": "Further Reading",
    "contentHtml": "<ul>\n  <li>This blog <a href=\"https://www.philschmid.de/instruction-tune-llama-2\">post</a> from <a href=\"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/\">Philipp Schmid</a> is a step-by-step guide on instruction-tuning Llama 2.</li>\n  <li>The idea of the blog post is to focus on creating the instruction dataset, which we can then use to fine-tune the base model of Llama 2 to follow our instructions.</li>\n  <li>The extended guide covers:\n    <ul>\n      <li>Define the use case and create a prompt template for instructions.</li>\n      <li>Create an instruction dataset.</li>\n      <li>Add Flash Attention and QLoRA for faster and more efficient training.</li>\n      <li>Instruction-tune Llama 2 using TRL.</li>\n      <li>Test the Model and run Inference.</li>\n    </ul>\n  </li>\n  <li>The guide walks you through a detailed example and where you learn how to fine-tune Llama 7B to generate synthetic instructions, e.g., provide an email to get instructions that could have been used to generate this email. This model can then be used to generate synthetic data for personalizing LLMs, e.g., mimic your email writing.</li>\n</ul>\n<ul>\n      <li>Define the use case and create a prompt template for instructions.</li>\n      <li>Create an instruction dataset.</li>\n      <li>Add Flash Attention and QLoRA for faster and more efficient training.</li>\n      <li>Instruction-tune Llama 2 using TRL.</li>\n      <li>Test the Model and run Inference.</li>\n    </ul>",
    "contentMarkdown": "*   This blog [post](https://www.philschmid.de/instruction-tune-llama-2) from [Philipp Schmid](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/) is a step-by-step guide on instruction-tuning Llama 2.\n*   The idea of the blog post is to focus on creating the instruction dataset, which we can then use to fine-tune the base model of Llama 2 to follow our instructions.\n*   The extended guide covers:\n    *   Define the use case and create a prompt template for instructions.\n    *   Create an instruction dataset.\n    *   Add Flash Attention and QLoRA for faster and more efficient training.\n    *   Instruction-tune Llama 2 using TRL.\n    *   Test the Model and run Inference.\n*   The guide walks you through a detailed example and where you learn how to fine-tune Llama 7B to generate synthetic instructions, e.g., provide an email to get instructions that could have been used to generate this email. This model can then be used to generate synthetic data for personalizing LLMs, e.g., mimic your email writing.\n\n*   Define the use case and create a prompt template for instructions.\n*   Create an instruction dataset.\n*   Add Flash Attention and QLoRA for faster and more efficient training.\n*   Instruction-tune Llama 2 using TRL.\n*   Test the Model and run Inference.",
    "order": 64,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "attention",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 192,
      "contentLength": 1464
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#extended-guide:-instruction-tune-llama-2",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-how-to-make-llms-go-fast-65",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Further Reading",
    "title": "How to Make LLMs Go Fast",
    "subtitle": "Further Reading",
    "contentHtml": "<ul>\n  <li>The post presents an extensive overview of various methods to enhance the performance of LLMs, focusing on aspects like improved hardware utilization and innovative decoding techniques. The goal is to offer a valuable starting point for further exploration of the topics of interest, with an effort to provide links to pertinent research papers and blog posts where relevant.</li>\n  <li>Topics covered:\n    <ul>\n      <li>How inference works</li>\n      <li>Compilers</li>\n      <li>Continuous Batching</li>\n      <li>Quantization and model shrinkage</li>\n      <li>KV caching</li>\n      <li>Speculative decoding</li>\n      <li>Training time optimization</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>How inference works</li>\n      <li>Compilers</li>\n      <li>Continuous Batching</li>\n      <li>Quantization and model shrinkage</li>\n      <li>KV caching</li>\n      <li>Speculative decoding</li>\n      <li>Training time optimization</li>\n    </ul>\n<p><a href=\"https://vgel.me/posts/faster-inference\"><img src=\"/primers/ai/assets/LLM/LLMgofast.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   The post presents an extensive overview of various methods to enhance the performance of LLMs, focusing on aspects like improved hardware utilization and innovative decoding techniques. The goal is to offer a valuable starting point for further exploration of the topics of interest, with an effort to provide links to pertinent research papers and blog posts where relevant.\n*   Topics covered:\n    *   How inference works\n    *   Compilers\n    *   Continuous Batching\n    *   Quantization and model shrinkage\n    *   KV caching\n    *   Speculative decoding\n    *   Training time optimization\n\n*   How inference works\n*   Compilers\n*   Continuous Batching\n*   Quantization and model shrinkage\n*   KV caching\n*   Speculative decoding\n*   Training time optimization\n\n[![](/primers/ai/assets/LLM/LLMgofast.jpg)](https://vgel.me/posts/faster-inference)",
    "order": 65,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 111,
      "contentLength": 1071
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#how-to-make-llms-go-fast",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-merging-llms-66",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "Further Reading",
    "title": "Merging LLMs",
    "subtitle": "Further Reading",
    "contentHtml": "<ul>\n  <li>Merging two LLMs can yield the “best of both worlds”, i.e., an LLM that is proficient at the best performing areas of the individual LLMs. For instance, is -performing 7B param model on the Open LLM Leaderboard\nRemarkably, it also ranks as the 10th best-performing model overall. In just 7B parameters!</li>\n  <li><a href=\"https://huggingface.co/mlabonne/NeuralBeagle14-7B\">NeuralBeagle14-7B</a> is a DPO fine-tune of <a href=\"https://huggingface.co/mlabonne/Beagle14-7B\">mlabonne/Beagle14-7B</a> by <a href=\"https://www.linkedin.com/in/maxime-labonne/\">Maxime Labonne</a> using the <a href=\"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs\">argilla/distilabel-intel-orca-dpo-pairs</a> preference dataset and my DPO notebook from <a href=\"https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac\">this article</a>.</li>\n  <li>It is based on a merge of the following models using <a href=\"https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing\">LazyMergekit</a>:\n    <ul>\n      <li><a href=\"https://huggingface.co/fblgit/UNA-TheBeagle-7b-v1\">fblgit/UNA-TheBeagle-7b-v1</a></li>\n      <li><a href=\"https://huggingface.co/argilla/distilabeled-Marcoro14-7B-slerp\">argilla/distilabeled-Marcoro14-7B-slerp</a></li>\n    </ul>\n  </li>\n  <li><a href=\"https://huggingface.co/spaces/mlabonne/NeuralBeagle14-7B-GGUF-Chat\">Demo (Space)</a>; <a href=\"https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54\">Article about merging models</a>; <a href=\"https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac\">Article about DPO fine-tuning</a></li>\n</ul>\n<ul>\n      <li><a href=\"https://huggingface.co/fblgit/UNA-TheBeagle-7b-v1\">fblgit/UNA-TheBeagle-7b-v1</a></li>\n      <li><a href=\"https://huggingface.co/argilla/distilabeled-Marcoro14-7B-slerp\">argilla/distilabeled-Marcoro14-7B-slerp</a></li>\n    </ul>",
    "contentMarkdown": "*   Merging two LLMs can yield the “best of both worlds”, i.e., an LLM that is proficient at the best performing areas of the individual LLMs. For instance, is -performing 7B param model on the Open LLM Leaderboard Remarkably, it also ranks as the 10th best-performing model overall. In just 7B parameters!\n*   [NeuralBeagle14-7B](https://huggingface.co/mlabonne/NeuralBeagle14-7B) is a DPO fine-tune of [mlabonne/Beagle14-7B](https://huggingface.co/mlabonne/Beagle14-7B) by [Maxime Labonne](https://www.linkedin.com/in/maxime-labonne/) using the [argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs) preference dataset and my DPO notebook from [this article](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac).\n*   It is based on a merge of the following models using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing):\n    *   [fblgit/UNA-TheBeagle-7b-v1](https://huggingface.co/fblgit/UNA-TheBeagle-7b-v1)\n    *   [argilla/distilabeled-Marcoro14-7B-slerp](https://huggingface.co/argilla/distilabeled-Marcoro14-7B-slerp)\n*   [Demo (Space)](https://huggingface.co/spaces/mlabonne/NeuralBeagle14-7B-GGUF-Chat); [Article about merging models](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54); [Article about DPO fine-tuning](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac)\n\n*   [fblgit/UNA-TheBeagle-7b-v1](https://huggingface.co/fblgit/UNA-TheBeagle-7b-v1)\n*   [argilla/distilabeled-Marcoro14-7B-slerp](https://huggingface.co/argilla/distilabeled-Marcoro14-7B-slerp)",
    "order": 66,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 107,
      "contentLength": 1991
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#merging-llms",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-while-training-an-llm-in-a-multi-turn-conversation-67",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "FAQs",
    "title": "While Training an LLM in a Multi-turn Conversational Setting, Which Tokens Do We Calculate Loss On?",
    "subtitle": "FAQs",
    "contentHtml": "<ul>\n  <li>In a multi-turn conversational training setup for a large language model (LLM), <strong>the loss is typically calculated only on the model’s outputs (the assistant’s responses), not on the user’s inputs</strong>. Let’s break this down clearly:</li>\n</ul>\n<h4 id=\"conversation-structure\">Conversation Structure</h4>\n<ul>\n  <li>Suppose a training example looks like this:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">User: Hi, how are you?\nAssistant: I’m doing well, thanks! How can I help you today?\nUser: Tell me about black holes.\nAssistant: Black holes are regions of spacetime where gravity is so strong that nothing can escape...\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">User: Hi, how are you?\nAssistant: I’m doing well, thanks! How can I help you today?\nUser: Tell me about black holes.\nAssistant: Black holes are regions of spacetime where gravity is so strong that nothing can escape...\n</code></pre>\n<h4 id=\"tokenization-and-concatenation\">Tokenization and Concatenation</h4>\n<ul>\n  <li>\n    <p><strong>During training</strong>, the entire multi-turn conversation is usually concatenated into a single token sequence that includes <strong>both special speaker tokens</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">&lt;user&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;assistant&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;end&gt;</code>) <strong>and newline characters (<code class=\"language-plaintext highlighter-rouge\">\\n</code>)</strong> to clearly separate dialogue turns.</p>\n  </li>\n  <li>\n    <p>A typical formatted sequence looks like this:</p>\n  </li>\n</ul>\n<p><strong>During training</strong>, the entire multi-turn conversation is usually concatenated into a single token sequence that includes <strong>both special speaker tokens</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">&lt;user&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;assistant&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;end&gt;</code>) <strong>and newline characters (<code class=\"language-plaintext highlighter-rouge\">\\n</code>)</strong> to clearly separate dialogue turns.</p>\n<p>A typical formatted sequence looks like this:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\">&lt;user&gt; Hi, how are you?\\n\n&lt;assistant&gt; I’m doing well, thanks! How can I help you today?\\n\n&lt;user&gt; Tell me about black holes.\\n\n&lt;assistant&gt; Black holes are regions of spacetime where gravity is so strong that nothing can escape.\\n\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\">&lt;user&gt; Hi, how are you?\\n\n&lt;assistant&gt; I’m doing well, thanks! How can I help you today?\\n\n&lt;user&gt; Tell me about black holes.\\n\n&lt;assistant&gt; Black holes are regions of spacetime where gravity is so strong that nothing can escape.\\n\n</code></pre>\n<ul>\n  <li>The newline characters and speaker tokens are both <strong>fed into the model as input</strong>.</li>\n  <li>They help the model learn where one turn ends and another begins, making the conversational structure explicit.</li>\n</ul>\n<h4 id=\"loss-masking\">Loss Masking</h4>\n<ul>\n  <li>\n    <p>The key idea is <strong>loss masking</strong> — the model should learn to predict only the assistant’s latest response while using all previous dialogue turns as context.</p>\n  </li>\n  <li>\n    <p>Formally, the full token sequence (including all prior user and assistant turns plus the latest assistant response) can be mathematically expressed as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>x</mi><mo>=</mo><mo stretchy=&quot;false&quot;>[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-837\" style=\"width: 16.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.86em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-838\"><span class=\"mi\" id=\"MathJax-Span-839\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-842\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-843\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-844\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-846\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-848\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-849\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-850\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-851\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-853\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-854\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-855\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-856\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-858\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-859\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-860\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-861\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-862\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-864\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-865\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-866\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-868\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-869\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-870\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-871\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-872\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-873\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>x</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub><mo stretchy=\"false\">]</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-131\">x = [x_1, x_2, ..., x_T, y_1, y_2, ..., y_K]</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-874\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-875\"><span class=\"msubsup\" id=\"MathJax-Span-876\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-878\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-884\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-886\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">x_1, ..., x_T</script> are tokens from all prior user and assistant messages, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-887\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-888\"><span class=\"msubsup\" id=\"MathJax-Span-889\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-890\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-891\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-893\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-894\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-895\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-897\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-898\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-899\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">y_1, ..., y_K</script> are tokens of the latest assistant response.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The model predicts a probability distribution:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mi>t</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;lt;</mo><mi>t</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-900\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1008.08em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-901\"><span class=\"mi\" id=\"MathJax-Span-902\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-904\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-905\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-906\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-907\"><span class=\"mrow\" id=\"MathJax-Span-908\"><span class=\"mo\" id=\"MathJax-Span-909\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-910\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-911\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-912\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-913\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-914\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-915\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-916\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-918\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-919\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-920\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-921\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-922\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-924\"><span class=\"mrow\" id=\"MathJax-Span-925\"><span class=\"mo\" id=\"MathJax-Span-926\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&lt;</span><span class=\"mi\" id=\"MathJax-Span-927\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-928\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-134\">P(y_t | x_1, ..., x_T, y_{<t})</script>\n  </li>\n  <li>\n    <p>The loss (cross-entropy) is computed <strong>only</strong> on the tokens of the latest assistant response:</p>\n  </li>\n</ul>\n<p>The key idea is <strong>loss masking</strong> — the model should learn to predict only the assistant’s latest response while using all previous dialogue turns as context.</p>\n<p>Formally, the full token sequence (including all prior user and assistant turns plus the latest assistant response) can be mathematically expressed as:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-874\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-875\"><span class=\"msubsup\" id=\"MathJax-Span-876\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-878\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-884\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-886\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">x_1, ..., x_T</script> are tokens from all prior user and assistant messages, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-887\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-888\"><span class=\"msubsup\" id=\"MathJax-Span-889\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-890\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-891\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-893\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-894\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-895\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-897\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-898\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-899\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>y</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">y_1, ..., y_K</script> are tokens of the latest assistant response.</li>\n    </ul>\n<p>The model predicts a probability distribution:</p>\n<p>The loss (cross-entropy) is computed <strong>only</strong> on the tokens of the latest assistant response:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo>=</mo><mo>&amp;#x2212;</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mi>t</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;lt;</mo><mi>t</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-929\" style=\"width: 16.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.576em, 1013.75em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-930\"><span class=\"texatom\" id=\"MathJax-Span-931\"><span class=\"mrow\" id=\"MathJax-Span-932\"><span class=\"mi\" id=\"MathJax-Span-933\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-934\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-935\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-936\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-937\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-938\"><span class=\"mrow\" id=\"MathJax-Span-939\"><span class=\"mi\" id=\"MathJax-Span-940\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-941\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-942\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-943\"><span class=\"mrow\" id=\"MathJax-Span-944\"><span class=\"mi\" id=\"MathJax-Span-945\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-946\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-947\"></span><span class=\"mi\" id=\"MathJax-Span-948\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-949\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-950\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-951\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-952\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-953\"><span class=\"mrow\" id=\"MathJax-Span-954\"><span class=\"mo\" id=\"MathJax-Span-955\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-956\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-957\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-958\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-959\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-960\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-961\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-962\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-963\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-964\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-965\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-967\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-968\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-969\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-970\"><span class=\"mrow\" id=\"MathJax-Span-971\"><span class=\"mo\" id=\"MathJax-Span-972\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&lt;</span><span class=\"mi\" id=\"MathJax-Span-973\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>All prior turns (both user and assistant messages) are <strong>fed into the model as input context</strong>, but are <strong>excluded from the loss calculation (masked out)</strong>.</p>\n  </li>\n  <li>\n    <p>This ensures the model learns to generate the <strong>next assistant reply</strong> conditioned on the full conversation history, rather than trying to predict or reproduce earlier dialogue turns.</p>\n  </li>\n</ul>\n<p>All prior turns (both user and assistant messages) are <strong>fed into the model as input context</strong>, but are <strong>excluded from the loss calculation (masked out)</strong>.</p>\n<p>This ensures the model learns to generate the <strong>next assistant reply</strong> conditioned on the full conversation history, rather than trying to predict or reproduce earlier dialogue turns.</p>\n<h4 id=\"why-we-do-this\">Why We Do This</h4>\n<ul>\n  <li>We want the model to <strong>generate</strong> the assistant’s next utterance, conditioned on previous user and assistant turns.</li>\n  <li>The user’s tokens are <em>context</em>, not <em>targets</em>.</li>\n  <li>Including user tokens in the loss would make the model try to “predict the user,” which is not desirable.</li>\n</ul>\n<h4 id=\"exceptions\">Exceptions</h4>\n<ul>\n  <li>\n    <p>Some setups differ slightly:</p>\n  </li>\n  <li><strong>Dialogue modeling (e.g., GPT pretraining on mixed text)</strong>: loss may be computed on <em>all</em> tokens since turns are not distinguished.</li>\n  <li><strong>Self-play or data augmentation</strong>: sometimes both sides are model-generated, so both directions may have loss applied.</li>\n  <li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong>: the supervised fine-tuning stage still uses loss only on assistant outputs.</li>\n</ul>\n<p>Some setups differ slightly:</p>\n<h4 id=\"takeaways\">Takeaways</h4>\n<ul>\n  <li>\n    <p>In multi-turn LLM training:</p>\n\n    <ul>\n      <li>The loss is computed <strong>only on assistant tokens</strong>.</li>\n      <li>User tokens are included in the <strong>context</strong>, not in the loss.</li>\n      <li>Implementation uses <strong>loss masking</strong> to achieve this.</li>\n    </ul>\n  </li>\n</ul>\n<p>In multi-turn LLM training:</p>\n<ul>\n      <li>The loss is computed <strong>only on assistant tokens</strong>.</li>\n      <li>User tokens are included in the <strong>context</strong>, not in the loss.</li>\n      <li>Implementation uses <strong>loss masking</strong> to achieve this.</li>\n    </ul>",
    "contentMarkdown": "*   In a multi-turn conversational training setup for a large language model (LLM), **the loss is typically calculated only on the model’s outputs (the assistant’s responses), not on the user’s inputs**. Let’s break this down clearly:\n\n#### Conversation Structure\n\n*   Suppose a training example looks like this:\n\n![](https://aman.ai/images/copy.png)\n\n`User: Hi, how are you? Assistant: I’m doing well, thanks! How can I help you today? User: Tell me about black holes. Assistant: Black holes are regions of spacetime where gravity is so strong that nothing can escape...`\n\n![](https://aman.ai/images/copy.png)\n\n`User: Hi, how are you? Assistant: I’m doing well, thanks! How can I help you today? User: Tell me about black holes. Assistant: Black holes are regions of spacetime where gravity is so strong that nothing can escape...`\n\n#### Tokenization and Concatenation\n\n*   **During training**, the entire multi-turn conversation is usually concatenated into a single token sequence that includes **both special speaker tokens** (e.g., `<user>`, `<assistant>`, `<end>`) **and newline characters (`\\n`)** to clearly separate dialogue turns.\n    \n*   A typical formatted sequence looks like this:\n    \n\n**During training**, the entire multi-turn conversation is usually concatenated into a single token sequence that includes **both special speaker tokens** (e.g., `<user>`, `<assistant>`, `<end>`) **and newline characters (`\\n`)** to clearly separate dialogue turns.\n\nA typical formatted sequence looks like this:\n\n![](https://aman.ai/images/copy.png)\n\n`<user> Hi, how are you?\\n <assistant> I’m doing well, thanks! How can I help you today?\\n <user> Tell me about black holes.\\n <assistant> Black holes are regions of spacetime where gravity is so strong that nothing can escape.\\n`\n\n![](https://aman.ai/images/copy.png)\n\n`<user> Hi, how are you?\\n <assistant> I’m doing well, thanks! How can I help you today?\\n <user> Tell me about black holes.\\n <assistant> Black holes are regions of spacetime where gravity is so strong that nothing can escape.\\n`\n\n*   The newline characters and speaker tokens are both **fed into the model as input**.\n*   They help the model learn where one turn ends and another begins, making the conversational structure explicit.\n\n#### Loss Masking\n\n*   The key idea is **loss masking** — the model should learn to predict only the assistant’s latest response while using all previous dialogue turns as context.\n    \n*   Formally, the full token sequence (including all prior user and assistant turns plus the latest assistant response) can be mathematically expressed as:\n    \n    x\\=\\[x1,x2,...,xT,y1,y2,...,yK\\]x\\=\\[x1,x2,...,xT,y1,y2,...,yK\\]\n    \n    x = \\[x\\_1, x\\_2, ..., x\\_T, y\\_1, y\\_2, ..., y\\_K\\]\n    *   where x1,...,xTx1,...,xTx\\_1, ..., x\\_T are tokens from all prior user and assistant messages, and y1,...,yKy1,...,yKy\\_1, ..., y\\_K are tokens of the latest assistant response.\n*   The model predicts a probability distribution:\n    \n    P(yt|x1,...,xT,y<t)P(yt|x1,...,xT,y<t)\n    \n    P(y\\_t | x\\_1, ..., x\\_T, y\\_{<t})\n*   The loss (cross-entropy) is computed **only** on the tokens of the latest assistant response:\n    \n\nThe key idea is **loss masking** — the model should learn to predict only the assistant’s latest response while using all previous dialogue turns as context.\n\nFormally, the full token sequence (including all prior user and assistant turns plus the latest assistant response) can be mathematically expressed as:\n\n*   where x1,...,xTx1,...,xTx\\_1, ..., x\\_T are tokens from all prior user and assistant messages, and y1,...,yKy1,...,yKy\\_1, ..., y\\_K are tokens of the latest assistant response.\n\nThe model predicts a probability distribution:\n\nThe loss (cross-entropy) is computed **only** on the tokens of the latest assistant response:\n\n\\=−∑t\\=1KlogP(yt|x1,...,xT,y<t)L\\=−∑t\\=1Klog⁡P(yt|x1,...,xT,y<t)\n\n*   All prior turns (both user and assistant messages) are **fed into the model as input context**, but are **excluded from the loss calculation (masked out)**.\n    \n*   This ensures the model learns to generate the **next assistant reply** conditioned on the full conversation history, rather than trying to predict or reproduce earlier dialogue turns.\n    \n\nAll prior turns (both user and assistant messages) are **fed into the model as input context**, but are **excluded from the loss calculation (masked out)**.\n\nThis ensures the model learns to generate the **next assistant reply** conditioned on the full conversation history, rather than trying to predict or reproduce earlier dialogue turns.\n\n#### Why We Do This\n\n*   We want the model to **generate** the assistant’s next utterance, conditioned on previous user and assistant turns.\n*   The user’s tokens are _context_, not _targets_.\n*   Including user tokens in the loss would make the model try to “predict the user,” which is not desirable.\n\n#### Exceptions\n\n*   Some setups differ slightly:\n    \n*   **Dialogue modeling (e.g., GPT pretraining on mixed text)**: loss may be computed on _all_ tokens since turns are not distinguished.\n*   **Self-play or data augmentation**: sometimes both sides are model-generated, so both directions may have loss applied.\n*   **RLHF (Reinforcement Learning from Human Feedback)**: the supervised fine-tuning stage still uses loss only on assistant outputs.\n\nSome setups differ slightly:\n\n#### Takeaways\n\n*   In multi-turn LLM training:\n    \n    *   The loss is computed **only on assistant tokens**.\n    *   User tokens are included in the **context**, not in the loss.\n    *   Implementation uses **loss masking** to achieve this.\n\nIn multi-turn LLM training:\n\n*   The loss is computed **only on assistant tokens**.\n*   User tokens are included in the **context**, not in the loss.\n*   Implementation uses **loss masking** to achieve this.",
    "order": 67,
    "orderInChapter": 1,
    "difficulty": 5,
    "estimatedMinutes": 5,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "reinforcement learning",
      "fine-tuning",
      "data augmentation"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 827,
      "contentLength": 46302
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#while-training-an-llm-in-a-multi-turn-conversational-setting,-which-tokens-do-we-calculate-loss-on?",
    "scrapedAt": "2025-12-28T11:53:26.178Z"
  },
  {
    "id": "ai-LLM-how-do-you-add-a-new-token-to-the-tokenizers-vocab-68",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "FAQs",
    "title": "How Do You Add a New Token to the Tokenizer’s Vocabulary and Model’s Embedding Table?",
    "subtitle": "FAQs",
    "contentHtml": "<ul>\n  <li>\n    <p>When extending a pretrained model to include a new token, you must update both the <strong>tokenizer’s vocabulary</strong> (so it recognizes the new token) and the <strong>model’s embedding table</strong> (so it can learn how to represent it). After adding the token, you typically need to <strong>fine-tune</strong> the model on data where the new token appears, so the model learns its contextual meaning.</p>\n  </li>\n  <li>\n    <p>Below are the steps using Hugging Face Transformers.</p>\n  </li>\n</ul>\n<p>When extending a pretrained model to include a new token, you must update both the <strong>tokenizer’s vocabulary</strong> (so it recognizes the new token) and the <strong>model’s embedding table</strong> (so it can learn how to represent it). After adding the token, you typically need to <strong>fine-tune</strong> the model on data where the new token appears, so the model learns its contextual meaning.</p>\n<p>Below are the steps using Hugging Face Transformers.</p>\n<h4 id=\"steps\">Steps</h4>\n<ol>\n  <li>Load the tokenizer and model.</li>\n  <li>Add the new token(s) using <code class=\"language-plaintext highlighter-rouge\">tokenizer.add_tokens()</code>.</li>\n  <li>Resize the model’s embeddings using <code class=\"language-plaintext highlighter-rouge\">model.resize_token_embeddings(len(tokenizer))</code>.</li>\n  <li>(Optional) Initialize the new embedding based on a similar token.</li>\n  <li>Prepare data containing the new token in context.</li>\n  <li>Tokenize the dataset.</li>\n  <li>Fine-tune the model using a masked language modeling (or other relevant) objective.</li>\n  <li>Save the updated tokenizer and model.</li>\n  <li>Verify that the token works correctly.</li>\n</ol>\n<h4 id=\"mathematical-view\">Mathematical View</h4>\n<ul>\n  <li>\n    <p>Let the original embedding matrix be <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-978\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-979\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-980\"><span class=\"mrow\" id=\"MathJax-Span-981\"><span class=\"mi\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-983\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">E \\in \\mathbb{R}^{V \\times d}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-988\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-989\"><span class=\"mi\" id=\"MathJax-Span-990\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">V</script> is the vocabulary size and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-991\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-992\"><span class=\"mi\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">d</script> is the embedding dimension.</p>\n  </li>\n  <li>\n    <p>After adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-994\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-995\"><span class=\"mi\" id=\"MathJax-Span-996\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-139\">k</script> new tokens, the matrix becomes:</p>\n  </li>\n</ul>\n<p>Let the original embedding matrix be <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-978\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-979\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-980\"><span class=\"mrow\" id=\"MathJax-Span-981\"><span class=\"mi\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-983\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">E \\in \\mathbb{R}^{V \\times d}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-988\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-989\"><span class=\"mi\" id=\"MathJax-Span-990\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">V</script> is the vocabulary size and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-991\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-992\"><span class=\"mi\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">d</script> is the embedding dimension.</p>\n<p>After adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-994\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-995\"><span class=\"mi\" id=\"MathJax-Span-996\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-139\">k</script> new tokens, the matrix becomes:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>E</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>+</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-997\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.63em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-998\"><span class=\"msup\" id=\"MathJax-Span-999\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1000\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1001\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1002\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1003\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1004\"><span class=\"mrow\" id=\"MathJax-Span-1005\"><span class=\"mi\" id=\"MathJax-Span-1006\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1007\"><span class=\"mrow\" id=\"MathJax-Span-1008\"><span class=\"mo\" id=\"MathJax-Span-1009\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1010\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1011\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-1012\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1013\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1014\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1015\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>V</mi><mo>+</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div>\n<ul>\n  <li>The new embeddings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1016\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1017\"><span class=\"msubsup\" id=\"MathJax-Span-1018\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1019\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1020\"><span class=\"mrow\" id=\"MathJax-Span-1021\"><span class=\"mi\" id=\"MathJax-Span-1022\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1023\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1024\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1026\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1027\"><span class=\"mrow\" id=\"MathJax-Span-1028\"><span class=\"mi\" id=\"MathJax-Span-1029\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1030\"><span class=\"mrow\" id=\"MathJax-Span-1031\"><span class=\"mi\" id=\"MathJax-Span-1032\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1033\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">e_{new} \\in \\mathbb{R}^{k \\times d}</script> are initialized randomly or based on similar embeddings:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>=</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>&amp;#x03F5;</mi><mo>,</mo><mspace width=&quot;1em&quot; /><mi>&amp;#x03F5;</mi><mo>&amp;#x223C;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>N</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><msup><mi>&amp;#x03C3;</mi><mn>2</mn></msup><mi>I</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 15.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1013.23em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"msubsup\" id=\"MathJax-Span-1037\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1038\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1039\"><span class=\"mrow\" id=\"MathJax-Span-1040\"><span class=\"mi\" id=\"MathJax-Span-1041\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1042\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1044\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1045\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1046\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1047\"><span class=\"mrow\" id=\"MathJax-Span-1048\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-1050\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-1051\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-1052\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1053\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1054\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1055\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1056\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1057\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1058\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∼</span><span class=\"texatom\" id=\"MathJax-Span-1059\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1060\"><span class=\"mi\" id=\"MathJax-Span-1061\" style=\"font-family: STIXNonUnicode-Italic;\"><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.107em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1062\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1064\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1065\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1066\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1067\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1068\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>=</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>ϵ</mi><mo>,</mo><mspace width=\"1em\"></mspace><mi>ϵ</mi><mo>∼</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">N</mi></mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>During fine-tuning, gradients from the training objective adjust these embeddings so that the model learns the contextual meaning of the new token.</li>\n</ul>\n<h4 id=\"example-bert-based-classifier\">Example: BERT-based Classifier</h4>\n<ul>\n  <li>This section covers a <strong>detailed, end-to-end writeup</strong> showing how to add a new token to the tokenizer and embedding table, and then <strong>fine-tune a model for a text classification task</strong> (instead of masked language modeling).</li>\n  <li>In this case, the new token <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> will act as a <strong>domain marker</strong> — for example, indicating that the text comes from a specific domain or context (e.g., “scientific”, “medical”, or “legal”).</li>\n  <li>The following steps use Hugging Face Transformers with a <strong>BERT-based classifier</strong>. We will add a new token <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code>, resize embeddings, and fine-tune the model on a <strong>classification dataset</strong> where the new token appears in context.</li>\n  <li>\n    <p><strong>Summary of the process</strong>:</p>\n\n    <ol>\n      <li>Load pretrained model and tokenizer.</li>\n      <li>Add new token(s) using <code class=\"language-plaintext highlighter-rouge\">tokenizer.add_tokens()</code>.</li>\n      <li>Resize model embeddings with <code class=\"language-plaintext highlighter-rouge\">model.resize_token_embeddings(len(tokenizer))</code>.</li>\n      <li>(Optional) Initialize new embeddings.</li>\n      <li>Prepare text classification data including the new token.</li>\n      <li>Tokenize and build a dataset.</li>\n      <li>Fine-tune with <code class=\"language-plaintext highlighter-rouge\">Trainer</code> on classification task.</li>\n      <li>Save model and tokenizer.</li>\n      <li>Test the new token’s behavior in predictions.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>Summary of the process</strong>:</p>\n<ol>\n      <li>Load pretrained model and tokenizer.</li>\n      <li>Add new token(s) using <code class=\"language-plaintext highlighter-rouge\">tokenizer.add_tokens()</code>.</li>\n      <li>Resize model embeddings with <code class=\"language-plaintext highlighter-rouge\">model.resize_token_embeddings(len(tokenizer))</code>.</li>\n      <li>(Optional) Initialize new embeddings.</li>\n      <li>Prepare text classification data including the new token.</li>\n      <li>Tokenize and build a dataset.</li>\n      <li>Fine-tune with <code class=\"language-plaintext highlighter-rouge\">Trainer</code> on classification task.</li>\n      <li>Save model and tokenizer.</li>\n      <li>Test the new token’s behavior in predictions.</li>\n    </ol>\n<h4 id=\"load-the-tokenizer-and-model\">Load the Tokenizer and Model</h4>\n<ul>\n  <li>Start by loading your pretrained model and tokenizer.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">,</span> <span class=\"n\">AutoModelForMaskedLM</span>\n\n<span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s\">\"bert-base-uncased\"</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForMaskedLM</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">,</span> <span class=\"n\">AutoModelForMaskedLM</span>\n\n<span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s\">\"bert-base-uncased\"</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForMaskedLM</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>Here we use <strong>BERT</strong> with a masked language modeling (MLM) head as an example. You can use other models (e.g., GPT-2, RoBERTa, etc.) similarly.</li>\n</ul>\n<h4 id=\"add-the-new-tokens\">Add the New Token(s)</h4>\n<ul>\n  <li>Add one or more new tokens to the tokenizer’s vocabulary. This modifies the tokenizer but not the model yet.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"n\">new_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"[NEW_TOKEN]\"</span><span class=\"p\">]</span>\n<span class=\"n\">num_added_tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">add_tokens</span><span class=\"p\">(</span><span class=\"n\">new_tokens</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Added </span><span class=\"si\">{</span><span class=\"n\">num_added_tokens</span><span class=\"si\">}</span><span class=\"s\"> new tokens.\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"n\">new_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"[NEW_TOKEN]\"</span><span class=\"p\">]</span>\n<span class=\"n\">num_added_tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">add_tokens</span><span class=\"p\">(</span><span class=\"n\">new_tokens</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Added </span><span class=\"si\">{</span><span class=\"n\">num_added_tokens</span><span class=\"si\">}</span><span class=\"s\"> new tokens.\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>After this, the tokenizer’s vocabulary includes the new token. However, the model’s embedding table still corresponds to the <strong>old</strong> vocabulary size.</li>\n</ul>\n<h4 id=\"resize-the-models-token-embeddings\">Resize the Model’s Token Embeddings</h4>\n<ul>\n  <li>You must resize the model’s embedding matrix to match the new vocabulary size.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>This adds randomly initialized embeddings for the new token(s). Without fine-tuning, the model won’t know what the new token means.</li>\n</ul>\n<h4 id=\"initialize-the-new-tokens-embedding\">Initialize the New Token’s Embedding</h4>\n<ul>\n  <li>To give the new token a better starting point, you can initialize its embedding similar to an existing one.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">get_input_embeddings</span><span class=\"p\">()</span>\n<span class=\"n\">new_token_id</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"s\">\"[NEW_TOKEN]\"</span><span class=\"p\">)</span>\n<span class=\"n\">cls_id</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"s\">\"[CLS]\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize new token embedding based on [CLS]\n</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">new_token_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">cls_id</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">cls_id</span><span class=\"p\">])</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">get_input_embeddings</span><span class=\"p\">()</span>\n<span class=\"n\">new_token_id</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"s\">\"[NEW_TOKEN]\"</span><span class=\"p\">)</span>\n<span class=\"n\">cls_id</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"s\">\"[CLS]\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize new token embedding based on [CLS]\n</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">new_token_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">cls_id</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">cls_id</span><span class=\"p\">])</span>\n<span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This ensures the new embedding starts in a reasonable region of the embedding space rather than random noise.</li>\n</ul>\n<h4 id=\"prepare-fine-tuning-data\">Prepare Fine-Tuning Data</h4>\n<ul>\n  <li>Fine-tuning is critical to teach the model what the new token represents.</li>\n  <li>\n    <p>For example, suppose you want the model to understand that <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> means something like “special_concept”.</p>\n  </li>\n  <li>\n    <p>You can create synthetic or real training data that uses this token in context.</p>\n  </li>\n  <li>Example data (for Masked Language Modeling):</li>\n</ul>\n<p>For example, suppose you want the model to understand that <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> means something like “special_concept”.</p>\n<p>You can create synthetic or real training data that uses this token in context.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"n\">texts</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s\">\"The [NEW_TOKEN] is an advanced form of artificial intelligence.\"</span><span class=\"p\">,</span>\n    <span class=\"s\">\"In this experiment, we used the [NEW_TOKEN] to enhance predictions.\"</span><span class=\"p\">,</span>\n    <span class=\"s\">\"The [NEW_TOKEN] model achieved state-of-the-art performance.\"</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"n\">texts</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s\">\"The [NEW_TOKEN] is an advanced form of artificial intelligence.\"</span><span class=\"p\">,</span>\n    <span class=\"s\">\"In this experiment, we used the [NEW_TOKEN] to enhance predictions.\"</span><span class=\"p\">,</span>\n    <span class=\"s\">\"The [NEW_TOKEN] model achieved state-of-the-art performance.\"</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n</code></pre>\n<h4 id=\"tokenize-the-data\">Tokenize the Data</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">Dataset</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">CustomTextDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">texts</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__len__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">val</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">CustomTextDataset</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">texts</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">Dataset</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">CustomTextDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">texts</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__len__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">val</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">CustomTextDataset</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">texts</span><span class=\"p\">)</span>\n</code></pre>\n<h4 id=\"fine-tune-the-model-using-transformers\">Fine-Tune the Model (Using <code class=\"language-plaintext Highlighter-rouge\">transformers</code>)</h4>\n<ul>\n  <li>Now fine-tune the model using a masked language modeling objective. This helps it learn how to represent the new token in context.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./fine_tuned_model\"</span><span class=\"p\">,</span>\n    <span class=\"n\">overwrite_output_dir</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"n\">save_steps</span><span class=\"o\">=</span><span class=\"mi\">10_000</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./fine_tuned_model\"</span><span class=\"p\">,</span>\n    <span class=\"n\">overwrite_output_dir</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"n\">save_steps</span><span class=\"o\">=</span><span class=\"mi\">10_000</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>This training loop will adjust the new embedding weights (and potentially others) based on how <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> is used in the text.</li>\n</ul>\n<h4 id=\"fine-tune-the-model-using-pytorch\">Fine-Tune the Model (using PyTorch)</h4>\n<ul>\n  <li>Alternatively, you can fine-tune directly with PyTorch without using Hugging Face <code class=\"language-plaintext highlighter-rouge\">Trainer</code>. This gives more control over the training loop.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.optim</span> <span class=\"kn\">import</span> <span class=\"n\">AdamW</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tqdm</span> <span class=\"kn\">import</span> <span class=\"n\">tqdm</span>\n\n<span class=\"c1\"># Create DataLoader\n</span><span class=\"n\">dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define optimizer\n</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">desc</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">):</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">])</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">loss</span>\n        <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\"> loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.optim</span> <span class=\"kn\">import</span> <span class=\"n\">AdamW</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tqdm</span> <span class=\"kn\">import</span> <span class=\"n\">tqdm</span>\n\n<span class=\"c1\"># Create DataLoader\n</span><span class=\"n\">dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define optimizer\n</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">desc</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">):</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">])</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">loss</span>\n        <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\"> loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This simple PyTorch loop trains the model end-to-end on your dataset so the new token’s embedding gets updated through gradient descent just like the rest of the model parameters.</li>\n</ul>\n<h4 id=\"save-the-updated-model-and-tokenizer\">Save the Updated Model and Tokenizer</h4>\n<ul>\n  <li>Once fine-tuning is complete, save both the tokenizer and model for future use.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./updated_tokenizer\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./updated_model\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./updated_tokenizer\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./updated_model\"</span><span class=\"p\">)</span>\n</code></pre>\n<h4 id=\"verify-the-token-works\">Verify the Token Works</h4>\n<ul>\n  <li>\n    <p>You can now test whether the model recognizes the new token and uses it correctly. The following code:</p>\n\n    <ol>\n      <li>Prints the tokenized IDs and their corresponding tokens, confirming <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> maps to a new unique ID.</li>\n      <li>Runs a forward pass through the model.</li>\n      <li>Decodes the model’s most likely output tokens to verify whether it processes and reproduces <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> correctly.</li>\n    </ol>\n  </li>\n</ul>\n<p>You can now test whether the model recognizes the new token and uses it correctly. The following code:</p>\n<ol>\n      <li>Prints the tokenized IDs and their corresponding tokens, confirming <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> maps to a new unique ID.</li>\n      <li>Runs a forward pass through the model.</li>\n      <li>Decodes the model’s most likely output tokens to verify whether it processes and reproduces <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> correctly.</li>\n    </ol>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"c1\">#### Verify the Token Works\n</span>\n<span class=\"c1\"># You can now test whether the model recognizes the new token and uses it correctly.\n</span>\n<span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">\"The [NEW_TOKEN] achieved great results.\"</span>\n\n<span class=\"c1\"># The tokenizer converts text into model-readable tensors (e.g., token IDs, attention masks)\n# Setting return_tensors=\"pt\" returns PyTorch tensors instead of plain lists.\n</span><span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">input_text</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The tokenizer output is a dictionary with keys like:\n# 'input_ids': tensor of token IDs\n# 'attention_mask': tensor of 1s and 0s marking valid tokens vs padding\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Tokenizer output format:\"</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># When calling model(**inputs), the ** unpacks the dictionary so that\n# each key-value pair is passed as a separate keyword argument:\n# model(input_ids=..., attention_mask=...)\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Print tokenized representation\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Tokenized IDs:\"</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Decoded tokens:\"</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_ids_to_tokens</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># The model output contains 'logits', a tensor of raw predictions over the vocabulary\n# For each token position, argmax(-1) selects the most likely token ID along the vocabulary dimension\n</span><span class=\"n\">predicted_ids</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">logits</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Decode the predicted token IDs back into human-readable text\n# This helps verify how the model interprets and reproduces the new token\n</span><span class=\"n\">decoded_output</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">predicted_ids</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Model output (decoded):\"</span><span class=\"p\">,</span> <span class=\"n\">decoded_output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"c1\">#### Verify the Token Works\n</span>\n<span class=\"c1\"># You can now test whether the model recognizes the new token and uses it correctly.\n</span>\n<span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">\"The [NEW_TOKEN] achieved great results.\"</span>\n\n<span class=\"c1\"># The tokenizer converts text into model-readable tensors (e.g., token IDs, attention masks)\n# Setting return_tensors=\"pt\" returns PyTorch tensors instead of plain lists.\n</span><span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">input_text</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The tokenizer output is a dictionary with keys like:\n# 'input_ids': tensor of token IDs\n# 'attention_mask': tensor of 1s and 0s marking valid tokens vs padding\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Tokenizer output format:\"</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># When calling model(**inputs), the ** unpacks the dictionary so that\n# each key-value pair is passed as a separate keyword argument:\n# model(input_ids=..., attention_mask=...)\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Print tokenized representation\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Tokenized IDs:\"</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Decoded tokens:\"</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">convert_ids_to_tokens</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s\">\"input_ids\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># The model output contains 'logits', a tensor of raw predictions over the vocabulary\n# For each token position, argmax(-1) selects the most likely token ID along the vocabulary dimension\n</span><span class=\"n\">predicted_ids</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">logits</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Decode the predicted token IDs back into human-readable text\n# This helps verify how the model interprets and reproduces the new token\n</span><span class=\"n\">decoded_output</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">predicted_ids</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Model output (decoded):\"</span><span class=\"p\">,</span> <span class=\"n\">decoded_output</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>You should see <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> represented by a unique ID that corresponds to its new embedding.</li>\n</ul>\n<h5 id=\"conceptual-explanation\">Conceptual Explanation</h5>\n<ul>\n  <li>In classification fine-tuning, the model learns to map input sequences to discrete labels.</li>\n  <li>\n    <p>By introducing <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code>, you create a <strong>semantic cue</strong> that helps the model differentiate between domains or contexts.</p>\n  </li>\n  <li>Let the embedding matrix be <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1070\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1071\"><span class=\"mi\" id=\"MathJax-Span-1072\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1074\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1075\"><span class=\"mrow\" id=\"MathJax-Span-1076\"><span class=\"mi\" id=\"MathJax-Span-1077\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1078\"><span class=\"mrow\" id=\"MathJax-Span-1079\"><span class=\"mi\" id=\"MathJax-Span-1080\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1081\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1082\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">E \\in \\mathbb{R}^{V \\times d}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-144-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1083\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1084\"><span class=\"mi\" id=\"MathJax-Span-1085\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-144\">V</script> is vocabulary size and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-145-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1086\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1087\"><span class=\"mi\" id=\"MathJax-Span-1088\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-145\">d</script> is embedding dimension.</li>\n  <li>After adding one token:</li>\n</ul>\n<p>By introducing <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code>, you create a <strong>semantic cue</strong> that helps the model differentiate between domains or contexts.</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>E</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mi>E</mi><mtext>&amp;#xA0;</mtext><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1089\" style=\"width: 12.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1010.47em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1090\"><span class=\"msup\" id=\"MathJax-Span-1091\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1093\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1094\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-1095\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1096\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-1097\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1002.55em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.55em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -1.247em;\"><span class=\"mtd\" id=\"MathJax-Span-1098\"><span class=\"mrow\" id=\"MathJax-Span-1099\"><span class=\"mi\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-1101\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"msubsup\" id=\"MathJax-Span-1102\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1103\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1104\"><span class=\"mrow\" id=\"MathJax-Span-1105\"><span class=\"mi\" id=\"MathJax-Span-1106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1109\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">]</span></span></span><span class=\"mo\" id=\"MathJax-Span-1110\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1111\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1112\"><span class=\"mrow\" id=\"MathJax-Span-1113\"><span class=\"mi\" id=\"MathJax-Span-1114\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1115\"><span class=\"mrow\" id=\"MathJax-Span-1116\"><span class=\"mo\" id=\"MathJax-Span-1117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1118\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-1120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-1121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>E</mi><mo>′</mo></msup><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mi>E</mi><mtext>&nbsp;</mtext><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>V</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div>\n<ul>\n  <li>\n    <p>During fine-tuning, gradient updates modify <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1124\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.67em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1125\"><span class=\"msubsup\" id=\"MathJax-Span-1126\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1128\"><span class=\"mrow\" id=\"MathJax-Span-1129\"><span class=\"mi\" id=\"MathJax-Span-1130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">e_{new}</script> such that:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-148-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mfrac><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>L</mi></mrow><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1133\" style=\"width: 10.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.065em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1009.07em, 3.18em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1134\"><span class=\"msubsup\" id=\"MathJax-Span-1135\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1136\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1137\"><span class=\"mrow\" id=\"MathJax-Span-1138\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1140\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1141\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"msubsup\" id=\"MathJax-Span-1143\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1145\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"mi\" id=\"MathJax-Span-1147\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1150\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mfrac\" id=\"MathJax-Span-1152\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.04em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.518em;\"><span class=\"mrow\" id=\"MathJax-Span-1153\"><span class=\"mi\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">∂</span><span class=\"mi\" id=\"MathJax-Span-1155\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.14em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.039em;\"><span class=\"mrow\" id=\"MathJax-Span-1156\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-family: STIXGeneral-Regular;\">∂</span><span class=\"msubsup\" id=\"MathJax-Span-1158\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1159\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1160\"><span class=\"mrow\" id=\"MathJax-Span-1161\"><span class=\"mi\" id=\"MathJax-Span-1162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.242em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">←</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-148\">e_{new} \\gets e_{new} - \\eta \\frac{\\partial L}{\\partial e_{new}}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1165\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1166\"><span class=\"mi\" id=\"MathJax-Span-1167\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">L</script> is the classification loss (e.g., cross-entropy) and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1168\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1169\"><span class=\"mi\" id=\"MathJax-Span-1170\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">\\eta</script> is the learning rate.</li>\n    </ul>\n  </li>\n  <li>\n    <p>As the model sees <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> in domain-related examples, its embedding adjusts to align with the semantic space corresponding to that label.</p>\n  </li>\n</ul>\n<p>During fine-tuning, gradient updates modify <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1124\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.67em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1125\"><span class=\"msubsup\" id=\"MathJax-Span-1126\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1128\"><span class=\"mrow\" id=\"MathJax-Span-1129\"><span class=\"mi\" id=\"MathJax-Span-1130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">e_{new}</script> such that:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1165\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1166\"><span class=\"mi\" id=\"MathJax-Span-1167\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">L</script> is the classification loss (e.g., cross-entropy) and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1168\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1169\"><span class=\"mi\" id=\"MathJax-Span-1170\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">\\eta</script> is the learning rate.</li>\n    </ul>\n<p>As the model sees <code class=\"language-plaintext highlighter-rouge\">[NEW_TOKEN]</code> in domain-related examples, its embedding adjusts to align with the semantic space corresponding to that label.</p>",
    "contentMarkdown": "*   When extending a pretrained model to include a new token, you must update both the **tokenizer’s vocabulary** (so it recognizes the new token) and the **model’s embedding table** (so it can learn how to represent it). After adding the token, you typically need to **fine-tune** the model on data where the new token appears, so the model learns its contextual meaning.\n    \n*   Below are the steps using Hugging Face Transformers.\n    \n\nWhen extending a pretrained model to include a new token, you must update both the **tokenizer’s vocabulary** (so it recognizes the new token) and the **model’s embedding table** (so it can learn how to represent it). After adding the token, you typically need to **fine-tune** the model on data where the new token appears, so the model learns its contextual meaning.\n\nBelow are the steps using Hugging Face Transformers.\n\n#### Steps\n\n1.  Load the tokenizer and model.\n2.  Add the new token(s) using `tokenizer.add_tokens()`.\n3.  Resize the model’s embeddings using `model.resize_token_embeddings(len(tokenizer))`.\n4.  (Optional) Initialize the new embedding based on a similar token.\n5.  Prepare data containing the new token in context.\n6.  Tokenize the dataset.\n7.  Fine-tune the model using a masked language modeling (or other relevant) objective.\n8.  Save the updated tokenizer and model.\n9.  Verify that the token works correctly.\n\n#### Mathematical View\n\n*   Let the original embedding matrix be E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d}, where VVV is the vocabulary size and ddd is the embedding dimension.\n    \n*   After adding kkk new tokens, the matrix becomes:\n    \n\nLet the original embedding matrix be E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d}, where VVV is the vocabulary size and ddd is the embedding dimension.\n\nAfter adding kkk new tokens, the matrix becomes:\n\nE′∈ℝ(V+k)×dE′∈R(V+k)×d\n\n*   The new embeddings enew∈ℝk×denew∈Rk×de\\_{new} \\\\in \\\\mathbb{R}^{k \\\\times d} are initialized randomly or based on similar embeddings:\n\nenew\\=ebase+ϵ,ϵ∼(0,σ2I)enew\\=ebase+ϵ,ϵ∼N(0,σ2I)\n\n*   During fine-tuning, gradients from the training objective adjust these embeddings so that the model learns the contextual meaning of the new token.\n\n#### Example: BERT-based Classifier\n\n*   This section covers a **detailed, end-to-end writeup** showing how to add a new token to the tokenizer and embedding table, and then **fine-tune a model for a text classification task** (instead of masked language modeling).\n*   In this case, the new token `[NEW_TOKEN]` will act as a **domain marker** — for example, indicating that the text comes from a specific domain or context (e.g., “scientific”, “medical”, or “legal”).\n*   The following steps use Hugging Face Transformers with a **BERT-based classifier**. We will add a new token `[NEW_TOKEN]`, resize embeddings, and fine-tune the model on a **classification dataset** where the new token appears in context.\n*   **Summary of the process**:\n    \n    1.  Load pretrained model and tokenizer.\n    2.  Add new token(s) using `tokenizer.add_tokens()`.\n    3.  Resize model embeddings with `model.resize_token_embeddings(len(tokenizer))`.\n    4.  (Optional) Initialize new embeddings.\n    5.  Prepare text classification data including the new token.\n    6.  Tokenize and build a dataset.\n    7.  Fine-tune with `Trainer` on classification task.\n    8.  Save model and tokenizer.\n    9.  Test the new token’s behavior in predictions.\n\n**Summary of the process**:\n\n1.  Load pretrained model and tokenizer.\n2.  Add new token(s) using `tokenizer.add_tokens()`.\n3.  Resize model embeddings with `model.resize_token_embeddings(len(tokenizer))`.\n4.  (Optional) Initialize new embeddings.\n5.  Prepare text classification data including the new token.\n6.  Tokenize and build a dataset.\n7.  Fine-tune with `Trainer` on classification task.\n8.  Save model and tokenizer.\n9.  Test the new token’s behavior in predictions.\n\n#### Load the Tokenizer and Model\n\n*   Start by loading your pretrained model and tokenizer.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoTokenizer, AutoModelForMaskedLM  model_name = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForMaskedLM.from_pretrained(model_name)`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoTokenizer, AutoModelForMaskedLM  model_name = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForMaskedLM.from_pretrained(model_name)`\n\n*   Here we use **BERT** with a masked language modeling (MLM) head as an example. You can use other models (e.g., GPT-2, RoBERTa, etc.) similarly.\n\n#### Add the New Token(s)\n\n*   Add one or more new tokens to the tokenizer’s vocabulary. This modifies the tokenizer but not the model yet.\n\n![](https://aman.ai/images/copy.png)\n\n`new_tokens = [\"[NEW_TOKEN]\"] num_added_tokens = tokenizer.add_tokens(new_tokens) print(f\"Added {num_added_tokens} new tokens.\")`\n\n![](https://aman.ai/images/copy.png)\n\n`new_tokens = [\"[NEW_TOKEN]\"] num_added_tokens = tokenizer.add_tokens(new_tokens) print(f\"Added {num_added_tokens} new tokens.\")`\n\n*   After this, the tokenizer’s vocabulary includes the new token. However, the model’s embedding table still corresponds to the **old** vocabulary size.\n\n#### Resize the Model’s Token Embeddings\n\n*   You must resize the model’s embedding matrix to match the new vocabulary size.\n\n![](https://aman.ai/images/copy.png)\n\n`model.resize_token_embeddings(len(tokenizer))`\n\n![](https://aman.ai/images/copy.png)\n\n`model.resize_token_embeddings(len(tokenizer))`\n\n*   This adds randomly initialized embeddings for the new token(s). Without fine-tuning, the model won’t know what the new token means.\n\n#### Initialize the New Token’s Embedding\n\n*   To give the new token a better starting point, you can initialize its embedding similar to an existing one.\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  embedding_layer = model.get_input_embeddings() new_token_id = tokenizer.convert_tokens_to_ids(\"[NEW_TOKEN]\") cls_id = tokenizer.convert_tokens_to_ids(\"[CLS]\")  # Initialize new token embedding based on [CLS] embedding_layer.weight.data[new_token_id] = (     embedding_layer.weight.data[cls_id] + 0.01 * torch.randn_like(embedding_layer.weight.data[cls_id]) )`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  embedding_layer = model.get_input_embeddings() new_token_id = tokenizer.convert_tokens_to_ids(\"[NEW_TOKEN]\") cls_id = tokenizer.convert_tokens_to_ids(\"[CLS]\")  # Initialize new token embedding based on [CLS] embedding_layer.weight.data[new_token_id] = (     embedding_layer.weight.data[cls_id] + 0.01 * torch.randn_like(embedding_layer.weight.data[cls_id]) )`\n\n*   This ensures the new embedding starts in a reasonable region of the embedding space rather than random noise.\n\n#### Prepare Fine-Tuning Data\n\n*   Fine-tuning is critical to teach the model what the new token represents.\n*   For example, suppose you want the model to understand that `[NEW_TOKEN]` means something like “special\\_concept”.\n    \n*   You can create synthetic or real training data that uses this token in context.\n    \n*   Example data (for Masked Language Modeling):\n\nFor example, suppose you want the model to understand that `[NEW_TOKEN]` means something like “special\\_concept”.\n\nYou can create synthetic or real training data that uses this token in context.\n\n![](https://aman.ai/images/copy.png)\n\n`texts = [     \"The [NEW_TOKEN] is an advanced form of artificial intelligence.\",     \"In this experiment, we used the [NEW_TOKEN] to enhance predictions.\",     \"The [NEW_TOKEN] model achieved state-of-the-art performance.\", ]`\n\n![](https://aman.ai/images/copy.png)\n\n`texts = [     \"The [NEW_TOKEN] is an advanced form of artificial intelligence.\",     \"In this experiment, we used the [NEW_TOKEN] to enhance predictions.\",     \"The [NEW_TOKEN] model achieved state-of-the-art performance.\", ]`\n\n#### Tokenize the Data\n\n![](https://aman.ai/images/copy.png)\n\n`from torch.utils.data import Dataset  class CustomTextDataset(Dataset):     def __init__(self, tokenizer, texts):         self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")      def __len__(self):         return len(self.encodings.input_ids)      def __getitem__(self, idx):         return {key: val[idx] for key, val in self.encodings.items()}  dataset = CustomTextDataset(tokenizer, texts)`\n\n![](https://aman.ai/images/copy.png)\n\n`from torch.utils.data import Dataset  class CustomTextDataset(Dataset):     def __init__(self, tokenizer, texts):         self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")      def __len__(self):         return len(self.encodings.input_ids)      def __getitem__(self, idx):         return {key: val[idx] for key, val in self.encodings.items()}  dataset = CustomTextDataset(tokenizer, texts)`\n\n#### Fine-Tune the Model (Using `transformers`)\n\n*   Now fine-tune the model using a masked language modeling objective. This helps it learn how to represent the new token in context.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=tokenizer, mlm=True, mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./fine_tuned_model\",     overwrite_output_dir=True,     num_train_epochs=3,     per_device_train_batch_size=8,     save_steps=10_000,     save_total_limit=2,     learning_rate=5e-5, )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=dataset, )  trainer.train()`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=tokenizer, mlm=True, mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./fine_tuned_model\",     overwrite_output_dir=True,     num_train_epochs=3,     per_device_train_batch_size=8,     save_steps=10_000,     save_total_limit=2,     learning_rate=5e-5, )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=dataset, )  trainer.train()`\n\n*   This training loop will adjust the new embedding weights (and potentially others) based on how `[NEW_TOKEN]` is used in the text.\n\n#### Fine-Tune the Model (using PyTorch)\n\n*   Alternatively, you can fine-tune directly with PyTorch without using Hugging Face `Trainer`. This gives more control over the training loop.\n\n![](https://aman.ai/images/copy.png)\n\n`from torch.utils.data import DataLoader from torch.optim import AdamW from tqdm import tqdm  # Create DataLoader dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Define optimizer optimizer = AdamW(model.parameters(), lr=5e-5)  model.train() for epoch in range(3):     for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):         optimizer.zero_grad()         outputs = model(**batch, labels=batch[\"input_ids\"])         loss = outputs.loss         loss.backward()         optimizer.step()     print(f\"Epoch {epoch+1} loss: {loss.item():.4f}\")`\n\n![](https://aman.ai/images/copy.png)\n\n`from torch.utils.data import DataLoader from torch.optim import AdamW from tqdm import tqdm  # Create DataLoader dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Define optimizer optimizer = AdamW(model.parameters(), lr=5e-5)  model.train() for epoch in range(3):     for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):         optimizer.zero_grad()         outputs = model(**batch, labels=batch[\"input_ids\"])         loss = outputs.loss         loss.backward()         optimizer.step()     print(f\"Epoch {epoch+1} loss: {loss.item():.4f}\")`\n\n*   This simple PyTorch loop trains the model end-to-end on your dataset so the new token’s embedding gets updated through gradient descent just like the rest of the model parameters.\n\n#### Save the Updated Model and Tokenizer\n\n*   Once fine-tuning is complete, save both the tokenizer and model for future use.\n\n![](https://aman.ai/images/copy.png)\n\n`tokenizer.save_pretrained(\"./updated_tokenizer\") model.save_pretrained(\"./updated_model\")`\n\n![](https://aman.ai/images/copy.png)\n\n`tokenizer.save_pretrained(\"./updated_tokenizer\") model.save_pretrained(\"./updated_model\")`\n\n#### Verify the Token Works\n\n*   You can now test whether the model recognizes the new token and uses it correctly. The following code:\n    \n    1.  Prints the tokenized IDs and their corresponding tokens, confirming `[NEW_TOKEN]` maps to a new unique ID.\n    2.  Runs a forward pass through the model.\n    3.  Decodes the model’s most likely output tokens to verify whether it processes and reproduces `[NEW_TOKEN]` correctly.\n\nYou can now test whether the model recognizes the new token and uses it correctly. The following code:\n\n1.  Prints the tokenized IDs and their corresponding tokens, confirming `[NEW_TOKEN]` maps to a new unique ID.\n2.  Runs a forward pass through the model.\n3.  Decodes the model’s most likely output tokens to verify whether it processes and reproduces `[NEW_TOKEN]` correctly.\n\n![](https://aman.ai/images/copy.png)\n\n`#### Verify the Token Works # You can now test whether the model recognizes the new token and uses it correctly. input_text = \"The [NEW_TOKEN] achieved great results.\"  # The tokenizer converts text into model-readable tensors (e.g., token IDs, attention masks) # Setting return_tensors=\"pt\" returns PyTorch tensors instead of plain lists. inputs = tokenizer(input_text, return_tensors=\"pt\")  # The tokenizer output is a dictionary with keys like: # 'input_ids': tensor of token IDs # 'attention_mask': tensor of 1s and 0s marking valid tokens vs padding print(\"Tokenizer output format:\", inputs)  # When calling model(**inputs), the ** unpacks the dictionary so that # each key-value pair is passed as a separate keyword argument: # model(input_ids=..., attention_mask=...) outputs = model(**inputs)  # Print tokenized representation print(\"Tokenized IDs:\", inputs[\"input_ids\"]) print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))  # The model output contains 'logits', a tensor of raw predictions over the vocabulary # For each token position, argmax(-1) selects the most likely token ID along the vocabulary dimension predicted_ids = outputs.logits.argmax(-1)  # Decode the predicted token IDs back into human-readable text # This helps verify how the model interprets and reproduces the new token decoded_output = tokenizer.decode(predicted_ids[0])  print(\"Model output (decoded):\", decoded_output)`\n\n![](https://aman.ai/images/copy.png)\n\n`#### Verify the Token Works # You can now test whether the model recognizes the new token and uses it correctly. input_text = \"The [NEW_TOKEN] achieved great results.\"  # The tokenizer converts text into model-readable tensors (e.g., token IDs, attention masks) # Setting return_tensors=\"pt\" returns PyTorch tensors instead of plain lists. inputs = tokenizer(input_text, return_tensors=\"pt\")  # The tokenizer output is a dictionary with keys like: # 'input_ids': tensor of token IDs # 'attention_mask': tensor of 1s and 0s marking valid tokens vs padding print(\"Tokenizer output format:\", inputs)  # When calling model(**inputs), the ** unpacks the dictionary so that # each key-value pair is passed as a separate keyword argument: # model(input_ids=..., attention_mask=...) outputs = model(**inputs)  # Print tokenized representation print(\"Tokenized IDs:\", inputs[\"input_ids\"]) print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))  # The model output contains 'logits', a tensor of raw predictions over the vocabulary # For each token position, argmax(-1) selects the most likely token ID along the vocabulary dimension predicted_ids = outputs.logits.argmax(-1)  # Decode the predicted token IDs back into human-readable text # This helps verify how the model interprets and reproduces the new token decoded_output = tokenizer.decode(predicted_ids[0])  print(\"Model output (decoded):\", decoded_output)`\n\n*   You should see `[NEW_TOKEN]` represented by a unique ID that corresponds to its new embedding.\n\n##### Conceptual Explanation\n\n*   In classification fine-tuning, the model learns to map input sequences to discrete labels.\n*   By introducing `[NEW_TOKEN]`, you create a **semantic cue** that helps the model differentiate between domains or contexts.\n    \n*   Let the embedding matrix be E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d}, where VVV is vocabulary size and ddd is embedding dimension.\n*   After adding one token:\n\nBy introducing `[NEW_TOKEN]`, you create a **semantic cue** that helps the model differentiate between domains or contexts.\n\nE′\\=\\[E enew\\]∈ℝ(V+1)×dE′\\=\\[E enew\\]∈R(V+1)×d\n\n*   During fine-tuning, gradient updates modify enewenewe\\_{new} such that:\n    \n    enew←enew−η∂L∂enewenew←enew−η∂L∂enew\n    \n    e\\_{new} \\\\gets e\\_{new} - \\\\eta \\\\frac{\\\\partial L}{\\\\partial e\\_{new}}\n    *   where LLL is the classification loss (e.g., cross-entropy) and ηη\\\\eta is the learning rate.\n*   As the model sees `[NEW_TOKEN]` in domain-related examples, its embedding adjusts to align with the semantic space corresponding to that label.\n    \n\nDuring fine-tuning, gradient updates modify enewenewe\\_{new} such that:\n\n*   where LLL is the classification loss (e.g., cross-entropy) and ηη\\\\eta is the learning rate.\n\nAs the model sees `[NEW_TOKEN]` in domain-related examples, its embedding adjusts to align with the semantic space corresponding to that label.",
    "order": 68,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 11,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "bert",
      "gpt",
      "gradient descent",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 2072,
      "contentLength": 108152
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#how-do-you-add-a-new-token-to-the-tokenizer’s-vocabulary-and-model’s-embedding-table?",
    "scrapedAt": "2025-12-28T11:53:26.179Z"
  },
  {
    "id": "ai-LLM-how-can-you-extend-a-tokenizers-vocabulary-and-emb-69",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Overview of Large Language Models (LLMs)",
    "articleSlug": "LLM",
    "chapter": "FAQs",
    "title": "How Can You Extend a Tokenizer’s Vocabulary and Embedding Table to Learn a New Language?",
    "subtitle": "FAQs",
    "contentHtml": "<ul>\n  <li>Pretrained models are often trained on one or several major languages, and rely on a <strong>fixed vocabulary</strong> and an <strong>embedding matrix</strong> that map tokens to continuous vector representations.</li>\n  <li>When the model encounters text in a new language, many words or subwords will be <strong>out-of-vocabulary (OOV)</strong>. Without embeddings for these tokens, the model cannot understand or process them effectively. To adapt them to a new language — especially one that was not part of the original pretraining corpus — you must extend both the <strong>tokenizer’s vocabulary</strong> and the <strong>model’s embedding table</strong>.</li>\n  <li>\n    <p>The overall goal is to enable the model to <strong>recognize, represent, and process</strong> text in the new language without losing prior knowledge.</p>\n  </li>\n  <li>\n    <p>To fix this, we must:</p>\n\n    <ol>\n      <li>Extend the tokenizer’s vocabulary to include the new language’s tokens.</li>\n      <li>Expand the embedding table to learn representations for these tokens.</li>\n      <li>Fine-tune the model on monolingual or multilingual corpora containing the new language.</li>\n    </ol>\n  </li>\n</ul>\n<p>The overall goal is to enable the model to <strong>recognize, represent, and process</strong> text in the new language without losing prior knowledge.</p>\n<p>To fix this, we must:</p>\n<ol>\n      <li>Extend the tokenizer’s vocabulary to include the new language’s tokens.</li>\n      <li>Expand the embedding table to learn representations for these tokens.</li>\n      <li>Fine-tune the model on monolingual or multilingual corpora containing the new language.</li>\n    </ol>\n<h4 id=\"tokenizer-extension\">Tokenizer Extension</h4>\n<h5 id=\"collect-text-data-in-the-new-language\">Collect Text Data in the New Language</h5>\n<ul>\n  <li>Gather a large, diverse corpus in the target language.\nIdeally, it should include various domains — news, literature, technical text, social media — to capture the language’s morphology and semantics.</li>\n</ul>\n<h5 id=\"train-or-adapt-the-tokenizer\">Train or Adapt the Tokenizer</h5>\n<ul>\n  <li>There are two main approaches, covered in detail below:\n    <ul>\n      <li>Extend the existing tokenizer.</li>\n      <li>Train a new tokenizer from scratch.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Extend the existing tokenizer.</li>\n      <li>Train a new tokenizer from scratch.</li>\n    </ul>\n<h6 id=\"extend-the-existing-tokenizer\">Extend the Existing Tokenizer</h6>\n<ul>\n  <li>Use the same subword algorithm (e.g., Byte Pair Encoding or WordPiece) as the original model, but train it on both the <strong>existing vocabulary</strong> and the <strong>new corpus</strong>.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"c1\"># Import the AutoTokenizer class from the Hugging Face Transformers library\n</span><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span>\n\n<span class=\"c1\"># Load the original pretrained BERT tokenizer (\"bert-base-uncased\") from Hugging Face\n</span><span class=\"n\">base_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Import low-level tokenizer components from the Tokenizers library\n# These are used to build and train a new tokenizer from scratch or with a new corpus\n</span><span class=\"kn\">from</span> <span class=\"nn\">tokenizers</span> <span class=\"kn\">import</span> <span class=\"n\">Tokenizer</span><span class=\"p\">,</span> <span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">trainers</span><span class=\"p\">,</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">,</span> <span class=\"n\">normalizers</span>\n\n<span class=\"c1\"># Initialize a new tokenizer using the WordPiece model (used by BERT)\n# Specify the unknown token \"[UNK]\" for handling unseen words\n</span><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">Tokenizer</span><span class=\"p\">(</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">WordPiece</span><span class=\"p\">(</span><span class=\"n\">unk_token</span><span class=\"o\">=</span><span class=\"s\">\"[UNK]\"</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Apply the same normalization as BERT (e.g., lowercasing, stripping accents)\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">normalizer</span> <span class=\"o\">=</span> <span class=\"n\">normalizers</span><span class=\"p\">.</span><span class=\"n\">BertNormalizer</span><span class=\"p\">(</span><span class=\"n\">lowercase</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Set the pre-tokenizer to split text the same way BERT does (by whitespace and punctuation)\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pre_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">.</span><span class=\"n\">BertPreTokenizer</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Define a WordPieceTrainer to control how the vocabulary is learned\n# vocab_size sets the total number of tokens in the vocabulary\n# special_tokens defines reserved tokens needed for BERT’s architecture\n</span><span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">trainers</span><span class=\"p\">.</span><span class=\"n\">WordPieceTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">32000</span><span class=\"p\">,</span> \n    <span class=\"n\">special_tokens</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"[PAD]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[UNK]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[CLS]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[SEP]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[MASK]\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Specify the training corpus — a text file containing the new language data\n</span><span class=\"n\">files</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"new_language_corpus.txt\"</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Train the tokenizer on the provided corpus using the defined trainer\n# This step learns the vocabulary and tokenization rules from the text\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">files</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the trained tokenizer to a JSON file for later use or loading in Transformers\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"extended_tokenizer.json\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"c1\"># Import the AutoTokenizer class from the Hugging Face Transformers library\n</span><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span>\n\n<span class=\"c1\"># Load the original pretrained BERT tokenizer (\"bert-base-uncased\") from Hugging Face\n</span><span class=\"n\">base_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Import low-level tokenizer components from the Tokenizers library\n# These are used to build and train a new tokenizer from scratch or with a new corpus\n</span><span class=\"kn\">from</span> <span class=\"nn\">tokenizers</span> <span class=\"kn\">import</span> <span class=\"n\">Tokenizer</span><span class=\"p\">,</span> <span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">trainers</span><span class=\"p\">,</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">,</span> <span class=\"n\">normalizers</span>\n\n<span class=\"c1\"># Initialize a new tokenizer using the WordPiece model (used by BERT)\n# Specify the unknown token \"[UNK]\" for handling unseen words\n</span><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">Tokenizer</span><span class=\"p\">(</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">WordPiece</span><span class=\"p\">(</span><span class=\"n\">unk_token</span><span class=\"o\">=</span><span class=\"s\">\"[UNK]\"</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Apply the same normalization as BERT (e.g., lowercasing, stripping accents)\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">normalizer</span> <span class=\"o\">=</span> <span class=\"n\">normalizers</span><span class=\"p\">.</span><span class=\"n\">BertNormalizer</span><span class=\"p\">(</span><span class=\"n\">lowercase</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Set the pre-tokenizer to split text the same way BERT does (by whitespace and punctuation)\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pre_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">.</span><span class=\"n\">BertPreTokenizer</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Define a WordPieceTrainer to control how the vocabulary is learned\n# vocab_size sets the total number of tokens in the vocabulary\n# special_tokens defines reserved tokens needed for BERT’s architecture\n</span><span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">trainers</span><span class=\"p\">.</span><span class=\"n\">WordPieceTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">32000</span><span class=\"p\">,</span> \n    <span class=\"n\">special_tokens</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"[PAD]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[UNK]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[CLS]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[SEP]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[MASK]\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Specify the training corpus — a text file containing the new language data\n</span><span class=\"n\">files</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"new_language_corpus.txt\"</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Train the tokenizer on the provided corpus using the defined trainer\n# This step learns the vocabulary and tokenization rules from the text\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">files</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the trained tokenizer to a JSON file for later use or loading in Transformers\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"extended_tokenizer.json\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>Then merge the vocabularies of the old and new tokenizers, ensuring that existing tokens retain their IDs.</li>\n</ul>\n<h6 id=\"train-a-new-tokenizer-from-scratch\">Train a New Tokenizer from Scratch</h6>\n<ul>\n  <li>If the target language has a completely different script (e.g., adapting English BERT to Hindi, Arabic, or Chinese), it may be better to train a <strong>new tokenizer</strong> entirely and later align it with the old one through embedding transfer or multilingual training.</li>\n</ul>\n<h4 id=\"expanding-the-models-embedding-table\">Expanding the Model’s Embedding Table</h4>\n<h5 id=\"add-new-tokens\">Add New Tokens</h5>\n<ul>\n  <li>Once you have the new tokenizer vocabulary, identify the tokens that are <strong>not</strong> in the model’s original vocabulary.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModel</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModel</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Assuming 'new_tokenizer' is the updated one\n# 'num_added' helps track how many new tokens were added and can be used to inspect or initialize their embeddings if needed.\n</span><span class=\"n\">num_added</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">new_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">())</span> <span class=\"o\">-</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()))</span>\n\n<span class=\"c1\"># Resize the model's embedding matrix to match the new tokenizer's vocabulary size so it can learn embeddings for these tokens\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">new_tokenizer</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModel</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModel</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Assuming 'new_tokenizer' is the updated one\n# 'num_added' helps track how many new tokens were added and can be used to inspect or initialize their embeddings if needed.\n</span><span class=\"n\">num_added</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">new_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">())</span> <span class=\"o\">-</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()))</span>\n\n<span class=\"c1\"># Resize the model's embedding matrix to match the new tokenizer's vocabulary size so it can learn embeddings for these tokens\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">new_tokenizer</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>This expands the model’s embedding matrix.</li>\n  <li>\n    <p>If the old embedding matrix was <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1171\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1172\"><span class=\"mi\" id=\"MathJax-Span-1173\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1174\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1175\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1176\"><span class=\"mrow\" id=\"MathJax-Span-1177\"><span class=\"mi\" id=\"MathJax-Span-1178\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1179\"><span class=\"mrow\" id=\"MathJax-Span-1180\"><span class=\"mi\" id=\"MathJax-Span-1181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1183\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">E \\in \\mathbb{R}^{V \\times d}</script> (where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1184\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1185\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-152\">V</script> is the vocabulary size and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1187\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1188\"><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">d</script> is the embedding dimension), the new one becomes:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>E</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>+</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1190\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.63em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1191\"><span class=\"msup\" id=\"MathJax-Span-1192\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1193\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1194\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1196\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1197\"><span class=\"mrow\" id=\"MathJax-Span-1198\"><span class=\"mi\" id=\"MathJax-Span-1199\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1200\"><span class=\"mrow\" id=\"MathJax-Span-1201\"><span class=\"mo\" id=\"MathJax-Span-1202\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1203\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-1205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1206\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1207\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>V</mi><mo>+</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-154\">E' \\in \\mathbb{R}^{(V + k) \\times d}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1209\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1210\"><span class=\"mi\" id=\"MathJax-Span-1211\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">k</script> is the number of newly added tokens for the new language.</li>\n    </ul>\n  </li>\n  <li>The new embeddings are randomly initialized and must be learned during fine-tuning.</li>\n</ul>\n<p>If the old embedding matrix was <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1171\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1172\"><span class=\"mi\" id=\"MathJax-Span-1173\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1174\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1175\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1176\"><span class=\"mrow\" id=\"MathJax-Span-1177\"><span class=\"mi\" id=\"MathJax-Span-1178\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1179\"><span class=\"mrow\" id=\"MathJax-Span-1180\"><span class=\"mi\" id=\"MathJax-Span-1181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1183\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">E \\in \\mathbb{R}^{V \\times d}</script> (where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1184\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1185\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-152\">V</script> is the vocabulary size and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1187\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1188\"><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">d</script> is the embedding dimension), the new one becomes:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1209\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1210\"><span class=\"mi\" id=\"MathJax-Span-1211\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">k</script> is the number of newly added tokens for the new language.</li>\n    </ul>\n<h5 id=\"embedding-initialization-strategies\">Embedding Initialization Strategies</h5>\n<ul>\n  <li>To make training more stable and effective, you can initialize new embeddings based on <strong>linguistic or script similarity</strong>.</li>\n</ul>\n<h6 id=\"script-similarity-initialization\">Script Similarity Initialization</h6>\n<ul>\n  <li>\n    <p>If the new language shares a script (e.g., English → Spanish or French), initialize new embeddings as the mean of similar subword embeddings:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi>&amp;#x03F5;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1212\" style=\"width: 11.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1009.64em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1213\"><span class=\"msubsup\" id=\"MathJax-Span-1214\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1215\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1216\"><span class=\"mrow\" id=\"MathJax-Span-1217\"><span class=\"mi\" id=\"MathJax-Span-1218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1220\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-1222\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-1224\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1225\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1226\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-1227\"><span class=\"mrow\" id=\"MathJax-Span-1228\"><span class=\"mi\" id=\"MathJax-Span-1229\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1232\"><span class=\"mrow\" id=\"MathJax-Span-1233\"><span class=\"mi\" id=\"MathJax-Span-1234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1235\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1236\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1237\"><span class=\"mrow\" id=\"MathJax-Span-1238\"><span class=\"mi\" id=\"MathJax-Span-1239\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-1240\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span><span class=\"mi\" id=\"MathJax-Span-1242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1244\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"msubsup\" id=\"MathJax-Span-1245\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1247\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1248\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1249\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi>ϵ</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-156\">e_{new} = \\frac{1}{n} \\sum_{i=1}^{n} e_{similar_i} + \\epsilon</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1250\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.71em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1251\"><span class=\"msubsup\" id=\"MathJax-Span-1252\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1254\"><span class=\"mrow\" id=\"MathJax-Span-1255\"><span class=\"mi\" id=\"MathJax-Span-1256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-1257\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1258\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"msubsup\" id=\"MathJax-Span-1262\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">e_{similar_i}</script> are embeddings of similar tokens (e.g., cognates).</li>\n    </ul>\n  </li>\n</ul>\n<p>If the new language shares a script (e.g., English → Spanish or French), initialize new embeddings as the mean of similar subword embeddings:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1250\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.71em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1251\"><span class=\"msubsup\" id=\"MathJax-Span-1252\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1254\"><span class=\"mrow\" id=\"MathJax-Span-1255\"><span class=\"mi\" id=\"MathJax-Span-1256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-1257\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1258\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"msubsup\" id=\"MathJax-Span-1262\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><msub><mi>r</mi><mi>i</mi></msub></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">e_{similar_i}</script> are embeddings of similar tokens (e.g., cognates).</li>\n    </ul>\n<h6 id=\"cross-lingual-alignment\">Cross-lingual Alignment</h6>\n<ul>\n  <li>For languages with different scripts, you can initialize embeddings using <strong>cross-lingual word alignment</strong> methods (e.g., MUSE or fastText bilingual embeddings). These methods learn a mapping from the new language’s word vectors into the embedding space of the pretrained model.</li>\n</ul>\n<h6 id=\"random-initialization\">Random Initialization</h6>\n<ul>\n  <li>If no alignment data exists, the new embeddings can be left random. Fine-tuning will gradually optimize them.</li>\n</ul>\n<h5 id=\"fine-tuning-on-the-new-language\">Fine-Tuning on the New Language</h5>\n<ul>\n  <li>This is the critical step where the model learns the <strong>semantic and syntactic patterns</strong> of the new language.</li>\n</ul>\n<h6 id=\"pretraining-objective\">Pretraining Objective</h6>\n<ul>\n  <li>\n    <p>If possible, continue training using the <strong>masked language modeling (MLM)</strong> or <strong>causal language modeling (CLM)</strong> objective on the new corpus:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-158-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2208;</mo><mi>M</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo class=&quot;MJX-variant&quot;>&amp;#x2216;</mo><mi>M</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1265\" style=\"width: 13.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.1em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1266\"><span class=\"msubsup\" id=\"MathJax-Span-1267\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1268\"><span class=\"mrow\" id=\"MathJax-Span-1269\"><span class=\"mi\" id=\"MathJax-Span-1270\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1271\"><span class=\"mrow\" id=\"MathJax-Span-1272\"><span class=\"mi\" id=\"MathJax-Span-1273\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1274\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1275\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1276\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1277\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-1278\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-1279\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.3em, 4.273em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1280\"><span class=\"mrow\" id=\"MathJax-Span-1281\"><span class=\"mi\" id=\"MathJax-Span-1282\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-1284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1285\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-1286\"></span><span class=\"mi\" id=\"MathJax-Span-1287\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1288\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1289\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1290\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1291\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1292\"><span class=\"mrow\" id=\"MathJax-Span-1293\"><span class=\"mo\" id=\"MathJax-Span-1294\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1295\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1296\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1297\"><span class=\"mrow\" id=\"MathJax-Span-1298\"><span class=\"mo\" id=\"MathJax-Span-1299\" style=\"font-size: 70.7%; font-family: STIXVariants;\">∖</span><span class=\"mi\" id=\"MathJax-Span-1300\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1301\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>M</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>∈</mo><mi>M</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo class=\"MJX-variant\">∖</mo><mi>M</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-158\">\\mathcal{L}_{MLM} = - \\sum_{i \\in M} \\log P(x_i | x_{\\setminus M})</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>M</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1302\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1303\"><span class=\"texatom\" id=\"MathJax-Span-1304\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"mi\" id=\"MathJax-Span-1306\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">M</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">\\mathcal{M}</script> is the set of masked positions, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo class=&quot;MJX-variant&quot;>&amp;#x2216;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>M</mi></mrow></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1307\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.72em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1308\"><span class=\"msubsup\" id=\"MathJax-Span-1309\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1310\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1311\"><span class=\"mrow\" id=\"MathJax-Span-1312\"><span class=\"mo\" id=\"MathJax-Span-1313\" style=\"font-size: 70.7%; font-family: STIXVariants;\">∖</span><span class=\"texatom\" id=\"MathJax-Span-1314\"><span class=\"mrow\" id=\"MathJax-Span-1315\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo class=\"MJX-variant\">∖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">M</mi></mrow></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">x_{\\setminus \\mathcal{M}}</script> denotes all tokens in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-161-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1317\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1318\"><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-161\">x</script> not at the masked positions (i.e., the unmasked context).</li>\n    </ul>\n  </li>\n  <li>\n    <p>This helps the model develop contextual representations in the new language.</p>\n  </li>\n  <li>\n    <p><strong>Example code (for BERT):</strong></p>\n  </li>\n</ul>\n<p>If possible, continue training using the <strong>masked language modeling (MLM)</strong> or <strong>causal language modeling (CLM)</strong> objective on the new corpus:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>M</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1302\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1303\"><span class=\"texatom\" id=\"MathJax-Span-1304\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"mi\" id=\"MathJax-Span-1306\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">M</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">\\mathcal{M}</script> is the set of masked positions, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo class=&quot;MJX-variant&quot;>&amp;#x2216;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>M</mi></mrow></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1307\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.72em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1308\"><span class=\"msubsup\" id=\"MathJax-Span-1309\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1310\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1311\"><span class=\"mrow\" id=\"MathJax-Span-1312\"><span class=\"mo\" id=\"MathJax-Span-1313\" style=\"font-size: 70.7%; font-family: STIXVariants;\">∖</span><span class=\"texatom\" id=\"MathJax-Span-1314\"><span class=\"mrow\" id=\"MathJax-Span-1315\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo class=\"MJX-variant\">∖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">M</mi></mrow></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">x_{\\setminus \\mathcal{M}}</script> denotes all tokens in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-161-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1317\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1318\"><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-161\">x</script> not at the masked positions (i.e., the unmasked context).</li>\n    </ul>\n<p>This helps the model develop contextual representations in the new language.</p>\n<p><strong>Example code (for BERT):</strong></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">new_tokenizer</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./new_language_model\"</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">new_language_dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">new_tokenizer</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./new_language_model\"</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">new_language_dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre>\n<h6 id=\"multilingual-fine-tuning-optional\">Multilingual Fine-Tuning (Optional)</h6>\n<ul>\n  <li>\n    <p>If you want the model to remain multilingual, fine-tune it jointly on:</p>\n\n    <ul>\n      <li>The original language(s)</li>\n      <li>The new language corpus</li>\n    </ul>\n  </li>\n  <li>\n    <p>This prevents <strong>catastrophic forgetting</strong> of previously learned languages.</p>\n  </li>\n  <li>\n    <p>You can alternate batches or use a mixed dataset.</p>\n  </li>\n  <li>\n    <p>A detailed discourse is available in the <a href=\"../transformers/#multilingual-fine-tuning\">Multilingual Fine-Tuning</a> section of our <a href=\"../transformers\">Transformers</a> primer.</p>\n  </li>\n</ul>\n<p>If you want the model to remain multilingual, fine-tune it jointly on:</p>\n<ul>\n      <li>The original language(s)</li>\n      <li>The new language corpus</li>\n    </ul>\n<p>This prevents <strong>catastrophic forgetting</strong> of previously learned languages.</p>\n<p>You can alternate batches or use a mixed dataset.</p>\n<p>A detailed discourse is available in the <a href=\"../transformers/#multilingual-fine-tuning\">Multilingual Fine-Tuning</a> section of our <a href=\"../transformers\">Transformers</a> primer.</p>\n<h5 id=\"evaluation-and-iterative-refinement\">Evaluation and Iterative Refinement</h5>\n<ul>\n  <li>\n    <p>After fine-tuning:</p>\n\n    <ul>\n      <li>Evaluate on downstream tasks (NER, QA, classification) in the new language.</li>\n      <li>Optionally continue <strong>task-specific fine-tuning</strong> with labeled data in that language.</li>\n      <li>Iterate: retrain tokenizer, extend vocabulary, and fine-tune again as needed.</li>\n    </ul>\n  </li>\n</ul>\n<p>After fine-tuning:</p>\n<ul>\n      <li>Evaluate on downstream tasks (NER, QA, classification) in the new language.</li>\n      <li>Optionally continue <strong>task-specific fine-tuning</strong> with labeled data in that language.</li>\n      <li>Iterate: retrain tokenizer, extend vocabulary, and fine-tune again as needed.</li>\n    </ul>\n<h5 id=\"conceptual-summary\">Conceptual Summary</h5>\n<ul>\n  <li>Tokenizer extension ensures <strong>text coverage</strong> for the new language.</li>\n  <li>Embedding expansion enables <strong>representation learning</strong> for new words.</li>\n  <li>\n    <p>Fine-tuning transfers knowledge from existing embeddings and contextual patterns into the new linguistic domain.</p>\n  </li>\n  <li>\n    <p>Mathematically, the training updates both the new embeddings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-162-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1320\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1321\"><span class=\"msubsup\" id=\"MathJax-Span-1322\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1323\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1324\"><span class=\"mrow\" id=\"MathJax-Span-1325\"><span class=\"mi\" id=\"MathJax-Span-1326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1327\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1328\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-162\">E_{new}</script> and the contextual parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-163-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1329\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1330\"><span class=\"mi\" id=\"MathJax-Span-1331\" style=\"font-family: STIXGeneral-Regular;\">Θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-163\">\\Theta</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-164-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></mrow></munder><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi><mo>&amp;#x223C;</mo><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow></msub><mo stretchy=&quot;false&quot;>[</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1332\" style=\"width: 13.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.36em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1333\"><span class=\"munderover\" id=\"MathJax-Span-1334\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-1335\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.98em, 4.378em, -999.997em); top: -3.331em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1336\"><span class=\"mrow\" id=\"MathJax-Span-1337\"><span class=\"msubsup\" id=\"MathJax-Span-1338\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1339\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-1340\"><span class=\"mrow\" id=\"MathJax-Span-1341\"><span class=\"mi\" id=\"MathJax-Span-1342\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1343\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1344\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1345\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1346\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1347\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1348\"><span class=\"mrow\" id=\"MathJax-Span-1349\"><span class=\"mi\" id=\"MathJax-Span-1350\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1351\"><span class=\"mrow\" id=\"MathJax-Span-1352\"><span class=\"mi\" id=\"MathJax-Span-1353\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1354\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∼</span><span class=\"msubsup\" id=\"MathJax-Span-1355\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1356\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1357\"><span class=\"mrow\" id=\"MathJax-Span-1358\"><span class=\"mi\" id=\"MathJax-Span-1359\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1360\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1361\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1362\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"texatom\" id=\"MathJax-Span-1363\"><span class=\"mrow\" id=\"MathJax-Span-1364\"><span class=\"mi\" id=\"MathJax-Span-1365\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-1366\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1367\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1368\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1369\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Θ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1370\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1371\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1372\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1373\"><span class=\"mrow\" id=\"MathJax-Span-1374\"><span class=\"mi\" id=\"MathJax-Span-1375\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1376\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1377\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1378\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1379\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1380\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1381\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">Θ</mi></mrow></munder><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi><mo>∼</mo><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow></msub><mo stretchy=\"false\">[</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi mathvariant=\"normal\">Θ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-164\">\\min_{E_{new}, \\Theta} \\mathbb{E}_{x \\sim D_{new}} [\\mathcal{L}(f_\\Theta(E_{new}, x))]</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-165-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1383\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"msubsup\" id=\"MathJax-Span-1385\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1386\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1387\"><span class=\"mrow\" id=\"MathJax-Span-1388\"><span class=\"mi\" id=\"MathJax-Span-1389\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1390\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1391\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-165\">D_{new}</script> is the corpus in the new language and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1392\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.89em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1393\"><span class=\"msubsup\" id=\"MathJax-Span-1394\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1396\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Θ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi mathvariant=\"normal\">Θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">f_\\Theta</script> is the model function.</li>\n    </ul>\n  </li>\n</ul>\n<p>Fine-tuning transfers knowledge from existing embeddings and contextual patterns into the new linguistic domain.</p>\n<p>Mathematically, the training updates both the new embeddings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-162-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1320\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1321\"><span class=\"msubsup\" id=\"MathJax-Span-1322\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1323\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1324\"><span class=\"mrow\" id=\"MathJax-Span-1325\"><span class=\"mi\" id=\"MathJax-Span-1326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1327\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1328\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-162\">E_{new}</script> and the contextual parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-163-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1329\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1330\"><span class=\"mi\" id=\"MathJax-Span-1331\" style=\"font-family: STIXGeneral-Regular;\">Θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-163\">\\Theta</script>:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-165-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1383\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"msubsup\" id=\"MathJax-Span-1385\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1386\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1387\"><span class=\"mrow\" id=\"MathJax-Span-1388\"><span class=\"mi\" id=\"MathJax-Span-1389\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1390\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1391\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-165\">D_{new}</script> is the corpus in the new language and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi mathvariant=&quot;normal&quot;>&amp;#x0398;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1392\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.89em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1393\"><span class=\"msubsup\" id=\"MathJax-Span-1394\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1396\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">Θ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi mathvariant=\"normal\">Θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">f_\\Theta</script> is the model function.</li>\n    </ul>\n<h5 id=\"practical-notes\">Practical Notes</h5>\n<ul>\n  <li>Use smaller learning rates (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1397\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1398\"><span class=\"msubsup\" id=\"MathJax-Span-1399\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1400\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1401\"><span class=\"mrow\" id=\"MathJax-Span-1402\"><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1404\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1405\" style=\"width: 4.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.54em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1406\"><span class=\"mn\" id=\"MathJax-Span-1407\" style=\"font-family: STIXGeneral-Regular;\">5</span><span class=\"mo\" id=\"MathJax-Span-1408\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1409\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1410\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1411\"><span class=\"mrow\" id=\"MathJax-Span-1412\"><span class=\"mo\" id=\"MathJax-Span-1413\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1414\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>5</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">5 \\times 10^{-5}</script>) to preserve old knowledge.</li>\n  <li>If the new language is low-resource, leverage <strong>multilingual corpora</strong> or <strong>machine-translated text</strong>.</li>\n  <li>Monitor vocabulary overlap to ensure coverage.</li>\n  <li>Consider <strong>adapter layers</strong> or <strong>LoRA</strong> for efficient fine-tuning on large models.</li>\n</ul>\n<h5 id=\"example-extending-english-bert-to-learn-swahili\">Example: Extending English BERT to Learn Swahili</h5>\n<ul>\n  <li>We will start from <code class=\"language-plaintext highlighter-rouge\">bert-base-uncased</code> (trained on English) and adapt it to include Swahili vocabulary and language patterns.</li>\n</ul>\n<h6 id=\"collect-a-swahili-corpus\">Collect a Swahili Corpus</h6>\n<ul>\n  <li>You need a representative text corpus in Swahili.</li>\n  <li>\n    <p>Sources can include:</p>\n\n    <ul>\n      <li>The <strong>JW300 corpus</strong> (a multilingual dataset with Swahili text)</li>\n      <li><strong>Wikipedia dumps</strong> (<code class=\"language-plaintext highlighter-rouge\">swwiki</code>)</li>\n      <li><strong>CC100</strong> Swahili subset</li>\n      <li>Any domain-specific text data (e.g., government, news, or academic text)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Let’s assume you have a file <code class=\"language-plaintext highlighter-rouge\">swahili_corpus.txt</code> containing Swahili sentences.</p>\n  </li>\n  <li>Example lines:</li>\n</ul>\n<p>Sources can include:</p>\n<ul>\n      <li>The <strong>JW300 corpus</strong> (a multilingual dataset with Swahili text)</li>\n      <li><strong>Wikipedia dumps</strong> (<code class=\"language-plaintext highlighter-rouge\">swwiki</code>)</li>\n      <li><strong>CC100</strong> Swahili subset</li>\n      <li>Any domain-specific text data (e.g., government, news, or academic text)</li>\n    </ul>\n<p>Let’s assume you have a file <code class=\"language-plaintext highlighter-rouge\">swahili_corpus.txt</code> containing Swahili sentences.</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\">Watoto wanapenda kucheza mpira.\nHii ni siku nzuri ya kujifunza.\nTeknolojia inabadilisha maisha yetu.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\">Watoto wanapenda kucheza mpira.\nHii ni siku nzuri ya kujifunza.\nTeknolojia inabadilisha maisha yetu.\n</code></pre>\n<h6 id=\"extend-or-train-a-tokenizer-for-swahili\">Extend or Train a Tokenizer for Swahili</h6>\n<ul>\n  <li>Since Swahili uses the <strong>Latin script</strong>, we can safely <strong>extend</strong> the English tokenizer rather than train one from scratch.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code25\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code25\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tokenizers</span> <span class=\"kn\">import</span> <span class=\"n\">Tokenizer</span><span class=\"p\">,</span> <span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">trainers</span><span class=\"p\">,</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">,</span> <span class=\"n\">normalizers</span>\n\n<span class=\"c1\"># Load the base English tokenizer\n</span><span class=\"n\">base_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train a WordPiece tokenizer on Swahili corpus\n</span><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">Tokenizer</span><span class=\"p\">(</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">WordPiece</span><span class=\"p\">(</span><span class=\"n\">unk_token</span><span class=\"o\">=</span><span class=\"s\">\"[UNK]\"</span><span class=\"p\">))</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">normalizer</span> <span class=\"o\">=</span> <span class=\"n\">normalizers</span><span class=\"p\">.</span><span class=\"n\">BertNormalizer</span><span class=\"p\">(</span><span class=\"n\">lowercase</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pre_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">.</span><span class=\"n\">BertPreTokenizer</span><span class=\"p\">()</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">trainers</span><span class=\"p\">.</span><span class=\"n\">WordPieceTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">32000</span><span class=\"p\">,</span>\n    <span class=\"n\">special_tokens</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"[PAD]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[UNK]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[CLS]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[SEP]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[MASK]\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">files</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"swahili_corpus.txt\"</span><span class=\"p\">]</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">files</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the new tokenizer\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"swahili_tokenizer.json\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code25\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code25\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tokenizers</span> <span class=\"kn\">import</span> <span class=\"n\">Tokenizer</span><span class=\"p\">,</span> <span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">trainers</span><span class=\"p\">,</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">,</span> <span class=\"n\">normalizers</span>\n\n<span class=\"c1\"># Load the base English tokenizer\n</span><span class=\"n\">base_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train a WordPiece tokenizer on Swahili corpus\n</span><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">Tokenizer</span><span class=\"p\">(</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">WordPiece</span><span class=\"p\">(</span><span class=\"n\">unk_token</span><span class=\"o\">=</span><span class=\"s\">\"[UNK]\"</span><span class=\"p\">))</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">normalizer</span> <span class=\"o\">=</span> <span class=\"n\">normalizers</span><span class=\"p\">.</span><span class=\"n\">BertNormalizer</span><span class=\"p\">(</span><span class=\"n\">lowercase</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pre_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">pre_tokenizers</span><span class=\"p\">.</span><span class=\"n\">BertPreTokenizer</span><span class=\"p\">()</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">trainers</span><span class=\"p\">.</span><span class=\"n\">WordPieceTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">32000</span><span class=\"p\">,</span>\n    <span class=\"n\">special_tokens</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"[PAD]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[UNK]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[CLS]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[SEP]\"</span><span class=\"p\">,</span> <span class=\"s\">\"[MASK]\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">files</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"swahili_corpus.txt\"</span><span class=\"p\">]</span>\n<span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">files</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the new tokenizer\n</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"swahili_tokenizer.json\"</span><span class=\"p\">)</span>\n</code></pre>\n<h6 id=\"merge-the-english-and-swahili-vocabularies\">Merge the English and Swahili Vocabularies</h6>\n<ul>\n  <li>Now we merge the two vocabularies — English and Swahili — to create a <strong>joint tokenizer</strong> that supports both languages.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code26\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code26\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertTokenizerFast</span>\n\n<span class=\"n\">swahili_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizerFast</span><span class=\"p\">(</span><span class=\"n\">tokenizer_object</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">english_vocab</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n<span class=\"n\">swahili_vocab</span> <span class=\"o\">=</span> <span class=\"n\">swahili_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Identify new Swahili tokens\n</span><span class=\"n\">new_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tok</span> <span class=\"k\">for</span> <span class=\"n\">tok</span> <span class=\"ow\">in</span> <span class=\"n\">swahili_vocab</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">tok</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">english_vocab</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Add Swahili tokens to the English tokenizer\n</span><span class=\"n\">num_added</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">add_tokens</span><span class=\"p\">(</span><span class=\"n\">new_tokens</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Added </span><span class=\"si\">{</span><span class=\"n\">num_added</span><span class=\"si\">}</span><span class=\"s\"> new Swahili tokens.\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code26\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code26\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertTokenizerFast</span>\n\n<span class=\"n\">swahili_tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizerFast</span><span class=\"p\">(</span><span class=\"n\">tokenizer_object</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">english_vocab</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n<span class=\"n\">swahili_vocab</span> <span class=\"o\">=</span> <span class=\"n\">swahili_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Identify new Swahili tokens\n</span><span class=\"n\">new_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tok</span> <span class=\"k\">for</span> <span class=\"n\">tok</span> <span class=\"ow\">in</span> <span class=\"n\">swahili_vocab</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">tok</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">english_vocab</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Add Swahili tokens to the English tokenizer\n</span><span class=\"n\">num_added</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">add_tokens</span><span class=\"p\">(</span><span class=\"n\">new_tokens</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Added </span><span class=\"si\">{</span><span class=\"n\">num_added</span><span class=\"si\">}</span><span class=\"s\"> new Swahili tokens.\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This step updates the tokenizer’s vocabulary, but not yet the model’s embedding table.</li>\n</ul>\n<h6 id=\"expand-the-models-embedding-table\">Expand the Model’s Embedding Table</h6>\n<ul>\n  <li>Now we align the model’s embedding matrix with the extended tokenizer.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code27\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code27\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForMaskedLM</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForMaskedLM</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Resize embedding table\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code27\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code27\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForMaskedLM</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForMaskedLM</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Resize embedding table\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>This expands the model’s embeddings from the old size (e.g., 30,522 tokens) to include the new Swahili tokens.</li>\n</ul>\n<h6 id=\"initialize-the-new-token-embeddings\">Initialize the New Token Embeddings</h6>\n<ul>\n  <li>To make learning easier, initialize Swahili embeddings based on <strong>similar English embeddings</strong> (since both share Latin roots for many words).</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code28\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code28\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>  <span class=\"c1\"># Import PyTorch for tensor operations\n</span>\n<span class=\"c1\"># Get the model's input embedding layer (where each token has a vector representation)\n</span><span class=\"n\">embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">get_input_embeddings</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Get the tokenizer's vocabulary mapping (token -&gt; integer ID)\n</span><span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Loop through each newly added token that needs an embedding\n</span><span class=\"k\">for</span> <span class=\"n\">tok</span> <span class=\"ow\">in</span> <span class=\"n\">new_tokens</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Retrieve the integer ID for the new token\n</span>    <span class=\"n\">tok_id</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span><span class=\"p\">[</span><span class=\"n\">tok</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Try to find a similar English token based on prefix similarity (first 3 characters)\n</span>    <span class=\"c1\"># This is a heuristic for initializing the new token embedding near a related token\n</span>    <span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">english_vocab</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"n\">tok</span><span class=\"p\">[:</span><span class=\"mi\">3</span><span class=\"p\">])]</span>\n\n    <span class=\"c1\"># If a similar token exists, use its embedding as a base for the new token\n</span>    <span class=\"k\">if</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>\n        <span class=\"n\">base_id</span> <span class=\"o\">=</span> <span class=\"n\">english_vocab</span><span class=\"p\">[</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n\n        <span class=\"c1\"># Initialize the new token's embedding as the base embedding plus small random noise\n</span>        <span class=\"c1\"># The 0.01 factor scales the noise to keep it small — ensuring the new embedding\n</span>        <span class=\"c1\"># stays close to the base token's meaning while introducing slight variation\n</span>        <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">tok_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">base_id</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">base_id</span><span class=\"p\">])</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code28\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code28\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>  <span class=\"c1\"># Import PyTorch for tensor operations\n</span>\n<span class=\"c1\"># Get the model's input embedding layer (where each token has a vector representation)\n</span><span class=\"n\">embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">get_input_embeddings</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Get the tokenizer's vocabulary mapping (token -&gt; integer ID)\n</span><span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">get_vocab</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Loop through each newly added token that needs an embedding\n</span><span class=\"k\">for</span> <span class=\"n\">tok</span> <span class=\"ow\">in</span> <span class=\"n\">new_tokens</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Retrieve the integer ID for the new token\n</span>    <span class=\"n\">tok_id</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span><span class=\"p\">[</span><span class=\"n\">tok</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Try to find a similar English token based on prefix similarity (first 3 characters)\n</span>    <span class=\"c1\"># This is a heuristic for initializing the new token embedding near a related token\n</span>    <span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">english_vocab</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"n\">tok</span><span class=\"p\">[:</span><span class=\"mi\">3</span><span class=\"p\">])]</span>\n\n    <span class=\"c1\"># If a similar token exists, use its embedding as a base for the new token\n</span>    <span class=\"k\">if</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>\n        <span class=\"n\">base_id</span> <span class=\"o\">=</span> <span class=\"n\">english_vocab</span><span class=\"p\">[</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n\n        <span class=\"c1\"># Initialize the new token's embedding as the base embedding plus small random noise\n</span>        <span class=\"c1\"># The 0.01 factor scales the noise to keep it small — ensuring the new embedding\n</span>        <span class=\"c1\"># stays close to the base token's meaning while introducing slight variation\n</span>        <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">tok_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">base_id</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">embedding_layer</span><span class=\"p\">.</span><span class=\"n\">weight</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">base_id</span><span class=\"p\">])</span>\n</code></pre>\n<ul>\n  <li>If there’s no matching English token, the embeddings remain random.</li>\n</ul>\n<h6 id=\"prepare-swahili-fine-tuning-data\">Prepare Swahili Fine-Tuning Data</h6>\n<ul>\n  <li>Now we prepare a dataset for <strong>masked language modeling (MLM)</strong>.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code29\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code29\"><span class=\"c1\"># The dataset returns a dictionary where:\n#   key   → tensor name (e.g., \"input_ids\", \"attention_mask\")\n#   value → corresponding tensor slice for one sample\n#\n# Example:\n# dataset[0] = {\n#   \"input_ids\": tensor([101, 2023, 2003, 1037, 3978, 102]),       # token IDs for the first line\n#   \"attention_mask\": tensor([1, 1, 1, 1, 1, 1])                  # 1s indicate valid tokens\n# }\n</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">Dataset</span>  <span class=\"c1\"># Import the base Dataset class from PyTorch\n</span>\n<span class=\"c1\"># Define a custom dataset class for Swahili text\n</span><span class=\"k\">class</span> <span class=\"nc\">SwahiliDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">file_path</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Read all lines from the corpus file (each line is one training example)\n</span>        <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">,</span> <span class=\"s\">\"r\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s\">\"utf-8\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n            <span class=\"n\">lines</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">().</span><span class=\"n\">splitlines</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Tokenize all lines and convert them into PyTorch tensors\n</span>        <span class=\"c1\"># padding=True → pad all sequences to the same length\n</span>        <span class=\"c1\"># truncation=True → truncate long lines to the model's max length\n</span>        <span class=\"c1\"># return_tensors=\"pt\" → return PyTorch tensors instead of lists\n</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">lines</span><span class=\"p\">,</span> <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__len__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Return the total number of samples in the dataset\n</span>        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Return a dictionary containing token IDs and attention mask for the sample\n</span>        <span class=\"c1\"># This structure matches what transformer models expect during training\n</span>        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">val</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n\n<span class=\"c1\"># Create an instance of the dataset using the base tokenizer and the Swahili corpus\n</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">SwahiliDataset</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">,</span> <span class=\"s\">\"swahili_corpus.txt\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code29\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code29\"><span class=\"c1\"># The dataset returns a dictionary where:\n#   key   → tensor name (e.g., \"input_ids\", \"attention_mask\")\n#   value → corresponding tensor slice for one sample\n#\n# Example:\n# dataset[0] = {\n#   \"input_ids\": tensor([101, 2023, 2003, 1037, 3978, 102]),       # token IDs for the first line\n#   \"attention_mask\": tensor([1, 1, 1, 1, 1, 1])                  # 1s indicate valid tokens\n# }\n</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">Dataset</span>  <span class=\"c1\"># Import the base Dataset class from PyTorch\n</span>\n<span class=\"c1\"># Define a custom dataset class for Swahili text\n</span><span class=\"k\">class</span> <span class=\"nc\">SwahiliDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">file_path</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Read all lines from the corpus file (each line is one training example)\n</span>        <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">,</span> <span class=\"s\">\"r\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s\">\"utf-8\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n            <span class=\"n\">lines</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">().</span><span class=\"n\">splitlines</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Tokenize all lines and convert them into PyTorch tensors\n</span>        <span class=\"c1\"># padding=True → pad all sequences to the same length\n</span>        <span class=\"c1\"># truncation=True → truncate long lines to the model's max length\n</span>        <span class=\"c1\"># return_tensors=\"pt\" → return PyTorch tensors instead of lists\n</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"n\">lines</span><span class=\"p\">,</span> <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__len__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Return the total number of samples in the dataset\n</span>        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Return a dictionary containing token IDs and attention mask for the sample\n</span>        <span class=\"c1\"># This structure matches what transformer models expect during training\n</span>        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">val</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encodings</span><span class=\"p\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n\n<span class=\"c1\"># Create an instance of the dataset using the base tokenizer and the Swahili corpus\n</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">SwahiliDataset</span><span class=\"p\">(</span><span class=\"n\">base_tokenizer</span><span class=\"p\">,</span> <span class=\"s\">\"swahili_corpus.txt\"</span><span class=\"p\">)</span>\n</code></pre>\n<h6 id=\"fine-tune-on-swahili-data\">Fine-Tune on Swahili Data</h6>\n<ul>\n  <li>Use masked language modeling to teach the model how Swahili tokens interact contextually.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code30\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code30\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">base_tokenizer</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./bert_swahili\"</span><span class=\"p\">,</span>\n    <span class=\"n\">overwrite_output_dir</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"n\">save_steps</span><span class=\"o\">=</span><span class=\"mi\">5000</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">logging_steps</span><span class=\"o\">=</span><span class=\"mi\">1000</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code30\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code30\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">,</span> <span class=\"n\">Trainer</span><span class=\"p\">,</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"n\">data_collator</span> <span class=\"o\">=</span> <span class=\"n\">DataCollatorForLanguageModeling</span><span class=\"p\">(</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">base_tokenizer</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">mlm_probability</span><span class=\"o\">=</span><span class=\"mf\">0.15</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">training_args</span> <span class=\"o\">=</span> <span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n    <span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s\">\"./bert_swahili\"</span><span class=\"p\">,</span>\n    <span class=\"n\">overwrite_output_dir</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"n\">save_steps</span><span class=\"o\">=</span><span class=\"mi\">5000</span><span class=\"p\">,</span>\n    <span class=\"n\">save_total_limit</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">logging_steps</span><span class=\"o\">=</span><span class=\"mi\">1000</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">Trainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">training_args</span><span class=\"p\">,</span>\n    <span class=\"n\">data_collator</span><span class=\"o\">=</span><span class=\"n\">data_collator</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>\n    <p>During this stage:</p>\n\n    <ul>\n      <li>The new Swahili embeddings are trained to reflect the syntax and semantics of Swahili.</li>\n      <li>Other model layers are fine-tuned to adjust to new token patterns.</li>\n      <li>The model retains English understanding if the learning rate is small.</li>\n    </ul>\n  </li>\n</ul>\n<p>During this stage:</p>\n<ul>\n      <li>The new Swahili embeddings are trained to reflect the syntax and semantics of Swahili.</li>\n      <li>Other model layers are fine-tuned to adjust to new token patterns.</li>\n      <li>The model retains English understanding if the learning rate is small.</li>\n    </ul>\n<h6 id=\"save-and-test-the-model\">Save and Test the Model</h6>\n<ul>\n  <li>After fine-tuning, save your tokenizer and model.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code31\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code31\"><span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./bert_swahili_tokenizer\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./bert_swahili_model\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code31\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code31\"><span class=\"n\">base_tokenizer</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./bert_swahili_tokenizer\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s\">\"./bert_swahili_model\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>You can now test it:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code32\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code32\"><span class=\"c1\"># The tokenizer output is a dictionary of key–value pairs like:\n# {\n#   \"input_ids\": tensor of token IDs,\n#   \"attention_mask\": tensor indicating which tokens are real (1) vs padding (0)\n# }\n# These are passed to the model using **inputs to unpack them as separate arguments.\n</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s\">\"Watoto wanapenda [MASK] mpira.\"</span>  <span class=\"c1\"># Example text with a [MASK] token for prediction\n</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Tokenize the text and return PyTorch tensors (dict with input_ids, attention_mask)\n</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">inputs</span><span class=\"p\">)</span>  <span class=\"c1\"># Pass tokenized inputs to the model; ** unpacks the dict so the model gets each key as a named argument\n</span></code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code32\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code32\"><span class=\"c1\"># The tokenizer output is a dictionary of key–value pairs like:\n# {\n#   \"input_ids\": tensor of token IDs,\n#   \"attention_mask\": tensor indicating which tokens are real (1) vs padding (0)\n# }\n# These are passed to the model using **inputs to unpack them as separate arguments.\n</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s\">\"Watoto wanapenda [MASK] mpira.\"</span>  <span class=\"c1\"># Example text with a [MASK] token for prediction\n</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">base_tokenizer</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s\">\"pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Tokenize the text and return PyTorch tensors (dict with input_ids, attention_mask)\n</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">inputs</span><span class=\"p\">)</span>  <span class=\"c1\"># Pass tokenized inputs to the model; ** unpacks the dict so the model gets each key as a named argument\n</span></code></pre>\n<ul>\n  <li>The model should begin to predict contextually relevant Swahili words for <code class=\"language-plaintext highlighter-rouge\">[MASK]</code> after fine-tuning.</li>\n</ul>\n<h6 id=\"evaluation-and-further-training\">Evaluation and Further Training</h6>\n<ul>\n  <li>\n    <p>You can evaluate the adapted model by:</p>\n\n    <ul>\n      <li>Computing <strong>perplexity</strong> on Swahili text.</li>\n      <li>Testing on downstream tasks such as <strong>text classification</strong>, <strong>NER</strong>, or <strong>translation</strong>.</li>\n      <li>Continuing multilingual fine-tuning with both English and Swahili to prevent forgetting.</li>\n    </ul>\n  </li>\n</ul>\n<p>You can evaluate the adapted model by:</p>\n<ul>\n      <li>Computing <strong>perplexity</strong> on Swahili text.</li>\n      <li>Testing on downstream tasks such as <strong>text classification</strong>, <strong>NER</strong>, or <strong>translation</strong>.</li>\n      <li>Continuing multilingual fine-tuning with both English and Swahili to prevent forgetting.</li>\n    </ul>\n<h6 id=\"mathematical-summary\">Mathematical Summary</h6>\n<ul>\n  <li>Let the original embedding matrix be <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1415\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1416\"><span class=\"mi\" id=\"MathJax-Span-1417\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1418\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1419\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1420\"><span class=\"mrow\" id=\"MathJax-Span-1421\"><span class=\"mi\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1423\"><span class=\"mrow\" id=\"MathJax-Span-1424\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1426\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1427\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">E \\in \\mathbb{R}^{V \\times d}</script>.</li>\n  <li>After adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-170-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1428\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1429\"><span class=\"mi\" id=\"MathJax-Span-1430\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-170\">k</script> Swahili tokens:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>E</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mi>E</mi><mtext>&amp;#xA0;</mtext><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1431\" style=\"width: 15.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1012.82em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1432\"><span class=\"msup\" id=\"MathJax-Span-1433\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1434\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1435\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-1437\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1438\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-1439\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1002.71em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.71em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -1.352em;\"><span class=\"mtd\" id=\"MathJax-Span-1440\"><span class=\"mrow\" id=\"MathJax-Span-1441\"><span class=\"mi\" id=\"MathJax-Span-1442\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-1443\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"msubsup\" id=\"MathJax-Span-1444\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1445\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1446\"><span class=\"mrow\" id=\"MathJax-Span-1447\"><span class=\"mi\" id=\"MathJax-Span-1448\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1449\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1450\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1451\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">]</span></span></span><span class=\"mo\" id=\"MathJax-Span-1452\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1453\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1454\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1455\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1456\"><span class=\"mrow\" id=\"MathJax-Span-1457\"><span class=\"mi\" id=\"MathJax-Span-1458\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1461\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1462\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1463\"><span class=\"mrow\" id=\"MathJax-Span-1464\"><span class=\"mi\" id=\"MathJax-Span-1465\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1466\"><span class=\"mrow\" id=\"MathJax-Span-1467\"><span class=\"mi\" id=\"MathJax-Span-1468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1470\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>E</mi><mo>′</mo></msup><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mi>E</mi><mtext>&nbsp;</mtext><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div>\n<ul>\n  <li>The fine-tuning process minimizes the masked language modeling loss:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-172-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2208;</mo><mi>M</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><msub><mi>P</mi><mi>&amp;#x03B8;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo class=&quot;MJX-variant&quot;>&amp;#x2216;</mo><mi>M</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1471\" style=\"width: 14.013em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.62em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1472\"><span class=\"msubsup\" id=\"MathJax-Span-1473\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1474\"><span class=\"mrow\" id=\"MathJax-Span-1475\"><span class=\"mi\" id=\"MathJax-Span-1476\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1477\"><span class=\"mrow\" id=\"MathJax-Span-1478\"><span class=\"mi\" id=\"MathJax-Span-1479\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1481\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1482\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1483\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-1484\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-1485\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.3em, 4.273em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1486\"><span class=\"mrow\" id=\"MathJax-Span-1487\"><span class=\"mi\" id=\"MathJax-Span-1488\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1489\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-1490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1491\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-1492\"></span><span class=\"msubsup\" id=\"MathJax-Span-1493\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1494\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1495\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1496\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1497\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1499\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1500\"><span class=\"mrow\" id=\"MathJax-Span-1501\"><span class=\"mo\" id=\"MathJax-Span-1502\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1503\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1505\"><span class=\"mrow\" id=\"MathJax-Span-1506\"><span class=\"mo\" id=\"MathJax-Span-1507\" style=\"font-size: 70.7%; font-family: STIXVariants;\">∖</span><span class=\"mi\" id=\"MathJax-Span-1508\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1509\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>M</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>∈</mo><mi>M</mi></mrow></munder><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo class=\"MJX-variant\">∖</mo><mi>M</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>This allows gradients to update <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-173-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1510\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1511\"><span class=\"msubsup\" id=\"MathJax-Span-1512\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1513\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1514\"><span class=\"mrow\" id=\"MathJax-Span-1515\"><span class=\"mi\" id=\"MathJax-Span-1516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1517\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1518\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-173\">E_{new}</script> so that the model learns meaningful contextual embeddings for Swahili tokens.</li>\n</ul>\n<h6 id=\"key-takeaways\">Key Takeaways</h6>\n<ul>\n  <li>Extend the tokenizer with new language tokens.</li>\n  <li>Resize and optionally initialize the model’s embedding matrix.</li>\n  <li>Fine-tune using a large, diverse monolingual or bilingual corpus.</li>\n  <li>Use multilingual fine-tuning to retain knowledge of other languages.</li>\n  <li>Save and test the updated model for contextual fluency.</li>\n</ul>",
    "contentMarkdown": "*   Pretrained models are often trained on one or several major languages, and rely on a **fixed vocabulary** and an **embedding matrix** that map tokens to continuous vector representations.\n*   When the model encounters text in a new language, many words or subwords will be **out-of-vocabulary (OOV)**. Without embeddings for these tokens, the model cannot understand or process them effectively. To adapt them to a new language — especially one that was not part of the original pretraining corpus — you must extend both the **tokenizer’s vocabulary** and the **model’s embedding table**.\n*   The overall goal is to enable the model to **recognize, represent, and process** text in the new language without losing prior knowledge.\n    \n*   To fix this, we must:\n    \n    1.  Extend the tokenizer’s vocabulary to include the new language’s tokens.\n    2.  Expand the embedding table to learn representations for these tokens.\n    3.  Fine-tune the model on monolingual or multilingual corpora containing the new language.\n\nThe overall goal is to enable the model to **recognize, represent, and process** text in the new language without losing prior knowledge.\n\nTo fix this, we must:\n\n1.  Extend the tokenizer’s vocabulary to include the new language’s tokens.\n2.  Expand the embedding table to learn representations for these tokens.\n3.  Fine-tune the model on monolingual or multilingual corpora containing the new language.\n\n#### Tokenizer Extension\n\n##### Collect Text Data in the New Language\n\n*   Gather a large, diverse corpus in the target language. Ideally, it should include various domains — news, literature, technical text, social media — to capture the language’s morphology and semantics.\n\n##### Train or Adapt the Tokenizer\n\n*   There are two main approaches, covered in detail below:\n    *   Extend the existing tokenizer.\n    *   Train a new tokenizer from scratch.\n\n*   Extend the existing tokenizer.\n*   Train a new tokenizer from scratch.\n\n###### Extend the Existing Tokenizer\n\n*   Use the same subword algorithm (e.g., Byte Pair Encoding or WordPiece) as the original model, but train it on both the **existing vocabulary** and the **new corpus**.\n\n![](https://aman.ai/images/copy.png)\n\n`# Import the AutoTokenizer class from the Hugging Face Transformers library from transformers import AutoTokenizer  # Load the original pretrained BERT tokenizer (\"bert-base-uncased\") from Hugging Face base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Import low-level tokenizer components from the Tokenizers library # These are used to build and train a new tokenizer from scratch or with a new corpus from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers  # Initialize a new tokenizer using the WordPiece model (used by BERT) # Specify the unknown token \"[UNK]\" for handling unseen words tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))  # Apply the same normalization as BERT (e.g., lowercasing, stripping accents) tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)  # Set the pre-tokenizer to split text the same way BERT does (by whitespace and punctuation) tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()  # Define a WordPieceTrainer to control how the vocabulary is learned # vocab_size sets the total number of tokens in the vocabulary # special_tokens defines reserved tokens needed for BERT’s architecture trainer = trainers.WordPieceTrainer(     vocab_size=32000,      special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] )  # Specify the training corpus — a text file containing the new language data files = [\"new_language_corpus.txt\"]  # Train the tokenizer on the provided corpus using the defined trainer # This step learns the vocabulary and tokenization rules from the text tokenizer.train(files, trainer)  # Save the trained tokenizer to a JSON file for later use or loading in Transformers tokenizer.save(\"extended_tokenizer.json\")`\n\n![](https://aman.ai/images/copy.png)\n\n`# Import the AutoTokenizer class from the Hugging Face Transformers library from transformers import AutoTokenizer  # Load the original pretrained BERT tokenizer (\"bert-base-uncased\") from Hugging Face base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Import low-level tokenizer components from the Tokenizers library # These are used to build and train a new tokenizer from scratch or with a new corpus from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers  # Initialize a new tokenizer using the WordPiece model (used by BERT) # Specify the unknown token \"[UNK]\" for handling unseen words tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))  # Apply the same normalization as BERT (e.g., lowercasing, stripping accents) tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)  # Set the pre-tokenizer to split text the same way BERT does (by whitespace and punctuation) tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()  # Define a WordPieceTrainer to control how the vocabulary is learned # vocab_size sets the total number of tokens in the vocabulary # special_tokens defines reserved tokens needed for BERT’s architecture trainer = trainers.WordPieceTrainer(     vocab_size=32000,      special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] )  # Specify the training corpus — a text file containing the new language data files = [\"new_language_corpus.txt\"]  # Train the tokenizer on the provided corpus using the defined trainer # This step learns the vocabulary and tokenization rules from the text tokenizer.train(files, trainer)  # Save the trained tokenizer to a JSON file for later use or loading in Transformers tokenizer.save(\"extended_tokenizer.json\")`\n\n*   Then merge the vocabularies of the old and new tokenizers, ensuring that existing tokens retain their IDs.\n\n###### Train a New Tokenizer from Scratch\n\n*   If the target language has a completely different script (e.g., adapting English BERT to Hindi, Arabic, or Chinese), it may be better to train a **new tokenizer** entirely and later align it with the old one through embedding transfer or multilingual training.\n\n#### Expanding the Model’s Embedding Table\n\n##### Add New Tokens\n\n*   Once you have the new tokenizer vocabulary, identify the tokens that are **not** in the model’s original vocabulary.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoModel  model = AutoModel.from_pretrained(\"bert-base-uncased\")  # Assuming 'new_tokenizer' is the updated one # 'num_added' helps track how many new tokens were added and can be used to inspect or initialize their embeddings if needed. num_added = len(set(new_tokenizer.get_vocab()) - set(base_tokenizer.get_vocab()))  # Resize the model's embedding matrix to match the new tokenizer's vocabulary size so it can learn embeddings for these tokens model.resize_token_embeddings(len(new_tokenizer))`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoModel  model = AutoModel.from_pretrained(\"bert-base-uncased\")  # Assuming 'new_tokenizer' is the updated one # 'num_added' helps track how many new tokens were added and can be used to inspect or initialize their embeddings if needed. num_added = len(set(new_tokenizer.get_vocab()) - set(base_tokenizer.get_vocab()))  # Resize the model's embedding matrix to match the new tokenizer's vocabulary size so it can learn embeddings for these tokens model.resize_token_embeddings(len(new_tokenizer))`\n\n*   This expands the model’s embedding matrix.\n*   If the old embedding matrix was E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d} (where VVV is the vocabulary size and ddd is the embedding dimension), the new one becomes:\n    \n    E′∈ℝ(V+k)×dE′∈R(V+k)×d\n    \n    E' \\\\in \\\\mathbb{R}^{(V + k) \\\\times d}\n    *   where kkk is the number of newly added tokens for the new language.\n*   The new embeddings are randomly initialized and must be learned during fine-tuning.\n\nIf the old embedding matrix was E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d} (where VVV is the vocabulary size and ddd is the embedding dimension), the new one becomes:\n\n*   where kkk is the number of newly added tokens for the new language.\n\n##### Embedding Initialization Strategies\n\n*   To make training more stable and effective, you can initialize new embeddings based on **linguistic or script similarity**.\n\n###### Script Similarity Initialization\n\n*   If the new language shares a script (e.g., English → Spanish or French), initialize new embeddings as the mean of similar subword embeddings:\n    \n    enew\\=1n∑i\\=1nesimilari+ϵenew\\=1n∑i\\=1nesimilari+ϵ\n    \n    e\\_{new} = \\\\frac{1}{n} \\\\sum\\_{i=1}^{n} e\\_{similar\\_i} + \\\\epsilon\n    *   where esimilariesimilarie\\_{similar\\_i} are embeddings of similar tokens (e.g., cognates).\n\nIf the new language shares a script (e.g., English → Spanish or French), initialize new embeddings as the mean of similar subword embeddings:\n\n*   where esimilariesimilarie\\_{similar\\_i} are embeddings of similar tokens (e.g., cognates).\n\n###### Cross-lingual Alignment\n\n*   For languages with different scripts, you can initialize embeddings using **cross-lingual word alignment** methods (e.g., MUSE or fastText bilingual embeddings). These methods learn a mapping from the new language’s word vectors into the embedding space of the pretrained model.\n\n###### Random Initialization\n\n*   If no alignment data exists, the new embeddings can be left random. Fine-tuning will gradually optimize them.\n\n##### Fine-Tuning on the New Language\n\n*   This is the critical step where the model learns the **semantic and syntactic patterns** of the new language.\n\n###### Pretraining Objective\n\n*   If possible, continue training using the **masked language modeling (MLM)** or **causal language modeling (CLM)** objective on the new corpus:\n    \n    MLM\\=−∑i∈MlogP(xi|x∖M)LMLM\\=−∑i∈Mlog⁡P(xi|x∖M)\n    \n    \\\\mathcal{L}\\_{MLM} = - \\\\sum\\_{i \\\\in M} \\\\log P(x\\_i | x\\_{\\\\setminus M})\n    *   where M\\\\mathcal{M} is the set of masked positions, and x∖x∖Mx\\_{\\\\setminus \\\\mathcal{M}} denotes all tokens in xxx not at the masked positions (i.e., the unmasked context).\n*   This helps the model develop contextual representations in the new language.\n    \n*   **Example code (for BERT):**\n    \n\nIf possible, continue training using the **masked language modeling (MLM)** or **causal language modeling (CLM)** objective on the new corpus:\n\n*   where M\\\\mathcal{M} is the set of masked positions, and x∖x∖Mx\\_{\\\\setminus \\\\mathcal{M}} denotes all tokens in xxx not at the masked positions (i.e., the unmasked context).\n\nThis helps the model develop contextual representations in the new language.\n\n**Example code (for BERT):**\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=new_tokenizer,     mlm=True,     mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./new_language_model\",     num_train_epochs=5,     per_device_train_batch_size=16,     learning_rate=5e-5,     save_total_limit=2, )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=new_language_dataset, )  trainer.train()`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=new_tokenizer,     mlm=True,     mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./new_language_model\",     num_train_epochs=5,     per_device_train_batch_size=16,     learning_rate=5e-5,     save_total_limit=2, )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=new_language_dataset, )  trainer.train()`\n\n###### Multilingual Fine-Tuning (Optional)\n\n*   If you want the model to remain multilingual, fine-tune it jointly on:\n    \n    *   The original language(s)\n    *   The new language corpus\n*   This prevents **catastrophic forgetting** of previously learned languages.\n    \n*   You can alternate batches or use a mixed dataset.\n    \n*   A detailed discourse is available in the [Multilingual Fine-Tuning](../transformers/#multilingual-fine-tuning) section of our [Transformers](../transformers) primer.\n    \n\nIf you want the model to remain multilingual, fine-tune it jointly on:\n\n*   The original language(s)\n*   The new language corpus\n\nThis prevents **catastrophic forgetting** of previously learned languages.\n\nYou can alternate batches or use a mixed dataset.\n\nA detailed discourse is available in the [Multilingual Fine-Tuning](../transformers/#multilingual-fine-tuning) section of our [Transformers](../transformers) primer.\n\n##### Evaluation and Iterative Refinement\n\n*   After fine-tuning:\n    \n    *   Evaluate on downstream tasks (NER, QA, classification) in the new language.\n    *   Optionally continue **task-specific fine-tuning** with labeled data in that language.\n    *   Iterate: retrain tokenizer, extend vocabulary, and fine-tune again as needed.\n\nAfter fine-tuning:\n\n*   Evaluate on downstream tasks (NER, QA, classification) in the new language.\n*   Optionally continue **task-specific fine-tuning** with labeled data in that language.\n*   Iterate: retrain tokenizer, extend vocabulary, and fine-tune again as needed.\n\n##### Conceptual Summary\n\n*   Tokenizer extension ensures **text coverage** for the new language.\n*   Embedding expansion enables **representation learning** for new words.\n*   Fine-tuning transfers knowledge from existing embeddings and contextual patterns into the new linguistic domain.\n    \n*   Mathematically, the training updates both the new embeddings EnewEnewE\\_{new} and the contextual parameters ΘΘ\\\\Theta:\n    \n    minEnew,Θ𝔼x∼Dnew\\[(fΘ(Enew,x))\\]minEnew,ΘEx∼Dnew\\[L(fΘ(Enew,x))\\]\n    \n    \\\\min\\_{E\\_{new}, \\\\Theta} \\\\mathbb{E}\\_{x \\\\sim D\\_{new}} \\[\\\\mathcal{L}(f\\_\\\\Theta(E\\_{new}, x))\\]\n    *   where DnewDnewD\\_{new} is the corpus in the new language and fΘfΘf\\_\\\\Theta is the model function.\n\nFine-tuning transfers knowledge from existing embeddings and contextual patterns into the new linguistic domain.\n\nMathematically, the training updates both the new embeddings EnewEnewE\\_{new} and the contextual parameters ΘΘ\\\\Theta:\n\n*   where DnewDnewD\\_{new} is the corpus in the new language and fΘfΘf\\_\\\\Theta is the model function.\n\n##### Practical Notes\n\n*   Use smaller learning rates (e.g., 10−510−510^{-5} to 5×10−55×10−55 \\\\times 10^{-5}) to preserve old knowledge.\n*   If the new language is low-resource, leverage **multilingual corpora** or **machine-translated text**.\n*   Monitor vocabulary overlap to ensure coverage.\n*   Consider **adapter layers** or **LoRA** for efficient fine-tuning on large models.\n\n##### Example: Extending English BERT to Learn Swahili\n\n*   We will start from `bert-base-uncased` (trained on English) and adapt it to include Swahili vocabulary and language patterns.\n\n###### Collect a Swahili Corpus\n\n*   You need a representative text corpus in Swahili.\n*   Sources can include:\n    \n    *   The **JW300 corpus** (a multilingual dataset with Swahili text)\n    *   **Wikipedia dumps** (`swwiki`)\n    *   **CC100** Swahili subset\n    *   Any domain-specific text data (e.g., government, news, or academic text)\n*   Let’s assume you have a file `swahili_corpus.txt` containing Swahili sentences.\n    \n*   Example lines:\n\nSources can include:\n\n*   The **JW300 corpus** (a multilingual dataset with Swahili text)\n*   **Wikipedia dumps** (`swwiki`)\n*   **CC100** Swahili subset\n*   Any domain-specific text data (e.g., government, news, or academic text)\n\nLet’s assume you have a file `swahili_corpus.txt` containing Swahili sentences.\n\n![](https://aman.ai/images/copy.png)\n\n`Watoto wanapenda kucheza mpira. Hii ni siku nzuri ya kujifunza. Teknolojia inabadilisha maisha yetu.`\n\n![](https://aman.ai/images/copy.png)\n\n`Watoto wanapenda kucheza mpira. Hii ni siku nzuri ya kujifunza. Teknolojia inabadilisha maisha yetu.`\n\n###### Extend or Train a Tokenizer for Swahili\n\n*   Since Swahili uses the **Latin script**, we can safely **extend** the English tokenizer rather than train one from scratch.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoTokenizer from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers  # Load the base English tokenizer base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Train a WordPiece tokenizer on Swahili corpus tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\")) tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True) tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()  trainer = trainers.WordPieceTrainer(     vocab_size=32000,     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] )  files = [\"swahili_corpus.txt\"] tokenizer.train(files, trainer)  # Save the new tokenizer tokenizer.save(\"swahili_tokenizer.json\")`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoTokenizer from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers  # Load the base English tokenizer base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Train a WordPiece tokenizer on Swahili corpus tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\")) tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True) tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()  trainer = trainers.WordPieceTrainer(     vocab_size=32000,     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] )  files = [\"swahili_corpus.txt\"] tokenizer.train(files, trainer)  # Save the new tokenizer tokenizer.save(\"swahili_tokenizer.json\")`\n\n###### Merge the English and Swahili Vocabularies\n\n*   Now we merge the two vocabularies — English and Swahili — to create a **joint tokenizer** that supports both languages.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import BertTokenizerFast  swahili_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer) english_vocab = base_tokenizer.get_vocab() swahili_vocab = swahili_tokenizer.get_vocab()  # Identify new Swahili tokens new_tokens = [tok for tok in swahili_vocab.keys() if tok not in english_vocab]  # Add Swahili tokens to the English tokenizer num_added = base_tokenizer.add_tokens(new_tokens) print(f\"Added {num_added} new Swahili tokens.\")`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import BertTokenizerFast  swahili_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer) english_vocab = base_tokenizer.get_vocab() swahili_vocab = swahili_tokenizer.get_vocab()  # Identify new Swahili tokens new_tokens = [tok for tok in swahili_vocab.keys() if tok not in english_vocab]  # Add Swahili tokens to the English tokenizer num_added = base_tokenizer.add_tokens(new_tokens) print(f\"Added {num_added} new Swahili tokens.\")`\n\n*   This step updates the tokenizer’s vocabulary, but not yet the model’s embedding table.\n\n###### Expand the Model’s Embedding Table\n\n*   Now we align the model’s embedding matrix with the extended tokenizer.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoModelForMaskedLM  model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")  # Resize embedding table model.resize_token_embeddings(len(base_tokenizer))`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoModelForMaskedLM  model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")  # Resize embedding table model.resize_token_embeddings(len(base_tokenizer))`\n\n*   This expands the model’s embeddings from the old size (e.g., 30,522 tokens) to include the new Swahili tokens.\n\n###### Initialize the New Token Embeddings\n\n*   To make learning easier, initialize Swahili embeddings based on **similar English embeddings** (since both share Latin roots for many words).\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  # Import PyTorch for tensor operations # Get the model's input embedding layer (where each token has a vector representation) embedding_layer = model.get_input_embeddings()  # Get the tokenizer's vocabulary mapping (token -> integer ID) vocab = base_tokenizer.get_vocab()  # Loop through each newly added token that needs an embedding for tok in new_tokens:     # Retrieve the integer ID for the new token     tok_id = vocab[tok]      # Try to find a similar English token based on prefix similarity (first 3 characters)     # This is a heuristic for initializing the new token embedding near a related token     candidates = [t for t in english_vocab.keys() if t.startswith(tok[:3])]      # If a similar token exists, use its embedding as a base for the new token     if candidates:         base_id = english_vocab[candidates[0]]          # Initialize the new token's embedding as the base embedding plus small random noise         # The 0.01 factor scales the noise to keep it small — ensuring the new embedding         # stays close to the base token's meaning while introducing slight variation         embedding_layer.weight.data[tok_id] = embedding_layer.weight.data[base_id] + 0.01 * torch.randn_like(embedding_layer.weight.data[base_id])`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  # Import PyTorch for tensor operations # Get the model's input embedding layer (where each token has a vector representation) embedding_layer = model.get_input_embeddings()  # Get the tokenizer's vocabulary mapping (token -> integer ID) vocab = base_tokenizer.get_vocab()  # Loop through each newly added token that needs an embedding for tok in new_tokens:     # Retrieve the integer ID for the new token     tok_id = vocab[tok]      # Try to find a similar English token based on prefix similarity (first 3 characters)     # This is a heuristic for initializing the new token embedding near a related token     candidates = [t for t in english_vocab.keys() if t.startswith(tok[:3])]      # If a similar token exists, use its embedding as a base for the new token     if candidates:         base_id = english_vocab[candidates[0]]          # Initialize the new token's embedding as the base embedding plus small random noise         # The 0.01 factor scales the noise to keep it small — ensuring the new embedding         # stays close to the base token's meaning while introducing slight variation         embedding_layer.weight.data[tok_id] = embedding_layer.weight.data[base_id] + 0.01 * torch.randn_like(embedding_layer.weight.data[base_id])`\n\n*   If there’s no matching English token, the embeddings remain random.\n\n###### Prepare Swahili Fine-Tuning Data\n\n*   Now we prepare a dataset for **masked language modeling (MLM)**.\n\n![](https://aman.ai/images/copy.png)\n\n`# The dataset returns a dictionary where: #   key   → tensor name (e.g., \"input_ids\", \"attention_mask\") #   value → corresponding tensor slice for one sample # # Example: # dataset[0] = { #   \"input_ids\": tensor([101, 2023, 2003, 1037, 3978, 102]),       # token IDs for the first line #   \"attention_mask\": tensor([1, 1, 1, 1, 1, 1])                  # 1s indicate valid tokens # } from torch.utils.data import Dataset  # Import the base Dataset class from PyTorch # Define a custom dataset class for Swahili text class SwahiliDataset(Dataset):     def __init__(self, tokenizer, file_path):         # Read all lines from the corpus file (each line is one training example)         with open(file_path, \"r\", encoding=\"utf-8\") as f:             lines = f.read().splitlines()          # Tokenize all lines and convert them into PyTorch tensors         # padding=True → pad all sequences to the same length         # truncation=True → truncate long lines to the model's max length         # return_tensors=\"pt\" → return PyTorch tensors instead of lists         self.encodings = tokenizer(lines, truncation=True, padding=True, return_tensors=\"pt\")      def __len__(self):         # Return the total number of samples in the dataset         return len(self.encodings.input_ids)      def __getitem__(self, idx):         # Return a dictionary containing token IDs and attention mask for the sample         # This structure matches what transformer models expect during training         return {key: val[idx] for key, val in self.encodings.items()}  # Create an instance of the dataset using the base tokenizer and the Swahili corpus dataset = SwahiliDataset(base_tokenizer, \"swahili_corpus.txt\")`\n\n![](https://aman.ai/images/copy.png)\n\n`# The dataset returns a dictionary where: #   key   → tensor name (e.g., \"input_ids\", \"attention_mask\") #   value → corresponding tensor slice for one sample # # Example: # dataset[0] = { #   \"input_ids\": tensor([101, 2023, 2003, 1037, 3978, 102]),       # token IDs for the first line #   \"attention_mask\": tensor([1, 1, 1, 1, 1, 1])                  # 1s indicate valid tokens # } from torch.utils.data import Dataset  # Import the base Dataset class from PyTorch # Define a custom dataset class for Swahili text class SwahiliDataset(Dataset):     def __init__(self, tokenizer, file_path):         # Read all lines from the corpus file (each line is one training example)         with open(file_path, \"r\", encoding=\"utf-8\") as f:             lines = f.read().splitlines()          # Tokenize all lines and convert them into PyTorch tensors         # padding=True → pad all sequences to the same length         # truncation=True → truncate long lines to the model's max length         # return_tensors=\"pt\" → return PyTorch tensors instead of lists         self.encodings = tokenizer(lines, truncation=True, padding=True, return_tensors=\"pt\")      def __len__(self):         # Return the total number of samples in the dataset         return len(self.encodings.input_ids)      def __getitem__(self, idx):         # Return a dictionary containing token IDs and attention mask for the sample         # This structure matches what transformer models expect during training         return {key: val[idx] for key, val in self.encodings.items()}  # Create an instance of the dataset using the base tokenizer and the Swahili corpus dataset = SwahiliDataset(base_tokenizer, \"swahili_corpus.txt\")`\n\n###### Fine-Tune on Swahili Data\n\n*   Use masked language modeling to teach the model how Swahili tokens interact contextually.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=base_tokenizer,     mlm=True,     mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./bert_swahili\",     overwrite_output_dir=True,     num_train_epochs=5,     per_device_train_batch_size=8,     save_steps=5000,     save_total_limit=2,     learning_rate=5e-5,     logging_steps=1000 )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=dataset, )  trainer.train()`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments  data_collator = DataCollatorForLanguageModeling(     tokenizer=base_tokenizer,     mlm=True,     mlm_probability=0.15 )  training_args = TrainingArguments(     output_dir=\"./bert_swahili\",     overwrite_output_dir=True,     num_train_epochs=5,     per_device_train_batch_size=8,     save_steps=5000,     save_total_limit=2,     learning_rate=5e-5,     logging_steps=1000 )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=dataset, )  trainer.train()`\n\n*   During this stage:\n    \n    *   The new Swahili embeddings are trained to reflect the syntax and semantics of Swahili.\n    *   Other model layers are fine-tuned to adjust to new token patterns.\n    *   The model retains English understanding if the learning rate is small.\n\nDuring this stage:\n\n*   The new Swahili embeddings are trained to reflect the syntax and semantics of Swahili.\n*   Other model layers are fine-tuned to adjust to new token patterns.\n*   The model retains English understanding if the learning rate is small.\n\n###### Save and Test the Model\n\n*   After fine-tuning, save your tokenizer and model.\n\n![](https://aman.ai/images/copy.png)\n\n`base_tokenizer.save_pretrained(\"./bert_swahili_tokenizer\") model.save_pretrained(\"./bert_swahili_model\")`\n\n![](https://aman.ai/images/copy.png)\n\n`base_tokenizer.save_pretrained(\"./bert_swahili_tokenizer\") model.save_pretrained(\"./bert_swahili_model\")`\n\n*   You can now test it:\n\n![](https://aman.ai/images/copy.png)\n\n`# The tokenizer output is a dictionary of key–value pairs like: # { #   \"input_ids\": tensor of token IDs, #   \"attention_mask\": tensor indicating which tokens are real (1) vs padding (0) # } # These are passed to the model using **inputs to unpack them as separate arguments. text = \"Watoto wanapenda [MASK] mpira.\"  # Example text with a [MASK] token for prediction inputs = base_tokenizer(text, return_tensors=\"pt\")  # Tokenize the text and return PyTorch tensors (dict with input_ids, attention_mask) outputs = model(**inputs)  # Pass tokenized inputs to the model; ** unpacks the dict so the model gets each key as a named argument`\n\n![](https://aman.ai/images/copy.png)\n\n`# The tokenizer output is a dictionary of key–value pairs like: # { #   \"input_ids\": tensor of token IDs, #   \"attention_mask\": tensor indicating which tokens are real (1) vs padding (0) # } # These are passed to the model using **inputs to unpack them as separate arguments. text = \"Watoto wanapenda [MASK] mpira.\"  # Example text with a [MASK] token for prediction inputs = base_tokenizer(text, return_tensors=\"pt\")  # Tokenize the text and return PyTorch tensors (dict with input_ids, attention_mask) outputs = model(**inputs)  # Pass tokenized inputs to the model; ** unpacks the dict so the model gets each key as a named argument`\n\n*   The model should begin to predict contextually relevant Swahili words for `[MASK]` after fine-tuning.\n\n###### Evaluation and Further Training\n\n*   You can evaluate the adapted model by:\n    \n    *   Computing **perplexity** on Swahili text.\n    *   Testing on downstream tasks such as **text classification**, **NER**, or **translation**.\n    *   Continuing multilingual fine-tuning with both English and Swahili to prevent forgetting.\n\nYou can evaluate the adapted model by:\n\n*   Computing **perplexity** on Swahili text.\n*   Testing on downstream tasks such as **text classification**, **NER**, or **translation**.\n*   Continuing multilingual fine-tuning with both English and Swahili to prevent forgetting.\n\n###### Mathematical Summary\n\n*   Let the original embedding matrix be E∈ℝV×dE∈RV×dE \\\\in \\\\mathbb{R}^{V \\\\times d}.\n*   After adding kkk Swahili tokens:\n\nE′\\=\\[E Enew\\],Enew∈ℝk×dE′\\=\\[E Enew\\],Enew∈Rk×d\n\n*   The fine-tuning process minimizes the masked language modeling loss:\n\nMLM\\=−∑i∈MlogPθ(xi|x∖M)LMLM\\=−∑i∈Mlog⁡Pθ(xi|x∖M)\n\n*   This allows gradients to update EnewEnewE\\_{new} so that the model learns meaningful contextual embeddings for Swahili tokens.\n\n###### Key Takeaways\n\n*   Extend the tokenizer with new language tokens.\n*   Resize and optionally initialize the model’s embedding matrix.\n*   Fine-tune using a large, diverse monolingual or bilingual corpus.\n*   Use multilingual fine-tuning to retain knowledge of other languages.\n*   Save and test the updated model for contextual fluency.",
    "order": 69,
    "orderInChapter": 3,
    "difficulty": 5,
    "estimatedMinutes": 20,
    "tags": [
      "nlpllms",
      "transformer",
      "attention",
      "embedding",
      "bert",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 3839,
      "contentLength": 182832
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/LLM/#how-can-you-extend-a-tokenizer’s-vocabulary-and-embedding-table-to-learn-a-new-language?",
    "scrapedAt": "2025-12-28T11:53:26.179Z"
  }
]