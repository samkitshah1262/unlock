[
  {
    "id": "ai-gan-random-input-1",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "The Generator",
    "title": "Random Input",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Neural networks need some form of input. Normally we input data that we want to do something with, like an instance that we want to classify or make a prediction about. But what do we use as input for a network that outputs entirely new data instances?</li>\n  <li>In its most basic form, a GAN takes random noise as its input. The generator then transforms this noise into a meaningful output. By introducing noise, we can get the GAN to produce a wide variety of data, sampling from different places in the target distribution.</li>\n  <li>Experiments suggest that the distribution of the noise doesn’t matter much, so we can choose something that’s easy to sample from, like a uniform distribution. For convenience the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space.</li>\n  <li>Note that some GANs use non-random input to shape the output. See <a href=\"#gan-variations\">GAN Variations</a>.\nUsing the Discriminator to Train the Generator</li>\n  <li>To train a neural net, we alter the net’s weights to reduce the error or loss of its output. In our GAN, however, the generator is not directly connected to the loss that we’re trying to affect. The generator feeds into the discriminator net, and the discriminator produces the output we’re trying to affect. The generator loss penalizes the generator for producing a sample that the discriminator network classifies as fake.</li>\n  <li>This extra chunk of network must be included in backpropagation. Backpropagation adjusts each weight in the right direction by calculating the weight’s impact on the output — how the output would change if you changed the weight. But the impact of a generator weight depends on the impact of the discriminator weights it feeds into. So backpropagation starts at the output and flows back through the discriminator into the generator.</li>\n  <li>At the same time, we don’t want the discriminator to change during generator training. Trying to hit a moving target would make a hard problem even harder for the generator.</li>\n  <li>So we train the generator with the following procedure:\n    <ul>\n      <li>Sample random noise.</li>\n      <li>Produce generator output from sampled random noise.</li>\n      <li>Get discriminator “real” or “fake” classification for generator output.</li>\n      <li>Calculate loss from discriminator classification.</li>\n      <li>Backpropagate through both the discriminator and generator to obtain gradients.</li>\n      <li>Use gradients to change only the generator weights.</li>\n    </ul>\n  </li>\n  <li>This is one iteration of generator training. In the next section we’ll see how to juggle the training of both the generator and the discriminator.</li>\n</ul>\n<ul>\n      <li>Sample random noise.</li>\n      <li>Produce generator output from sampled random noise.</li>\n      <li>Get discriminator “real” or “fake” classification for generator output.</li>\n      <li>Calculate loss from discriminator classification.</li>\n      <li>Backpropagate through both the discriminator and generator to obtain gradients.</li>\n      <li>Use gradients to change only the generator weights.</li>\n    </ul>",
    "contentMarkdown": "*   Neural networks need some form of input. Normally we input data that we want to do something with, like an instance that we want to classify or make a prediction about. But what do we use as input for a network that outputs entirely new data instances?\n*   In its most basic form, a GAN takes random noise as its input. The generator then transforms this noise into a meaningful output. By introducing noise, we can get the GAN to produce a wide variety of data, sampling from different places in the target distribution.\n*   Experiments suggest that the distribution of the noise doesn’t matter much, so we can choose something that’s easy to sample from, like a uniform distribution. For convenience the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space.\n*   Note that some GANs use non-random input to shape the output. See [GAN Variations](#gan-variations). Using the Discriminator to Train the Generator\n*   To train a neural net, we alter the net’s weights to reduce the error or loss of its output. In our GAN, however, the generator is not directly connected to the loss that we’re trying to affect. The generator feeds into the discriminator net, and the discriminator produces the output we’re trying to affect. The generator loss penalizes the generator for producing a sample that the discriminator network classifies as fake.\n*   This extra chunk of network must be included in backpropagation. Backpropagation adjusts each weight in the right direction by calculating the weight’s impact on the output — how the output would change if you changed the weight. But the impact of a generator weight depends on the impact of the discriminator weights it feeds into. So backpropagation starts at the output and flows back through the discriminator into the generator.\n*   At the same time, we don’t want the discriminator to change during generator training. Trying to hit a moving target would make a hard problem even harder for the generator.\n*   So we train the generator with the following procedure:\n    *   Sample random noise.\n    *   Produce generator output from sampled random noise.\n    *   Get discriminator “real” or “fake” classification for generator output.\n    *   Calculate loss from discriminator classification.\n    *   Backpropagate through both the discriminator and generator to obtain gradients.\n    *   Use gradients to change only the generator weights.\n*   This is one iteration of generator training. In the next section we’ll see how to juggle the training of both the generator and the discriminator.\n\n*   Sample random noise.\n*   Produce generator output from sampled random noise.\n*   Get discriminator “real” or “fake” classification for generator output.\n*   Calculate loss from discriminator classification.\n*   Backpropagate through both the discriminator and generator to obtain gradients.\n*   Use gradients to change only the generator weights.",
    "contentLength": 3189,
    "wordCount": 471,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#random-input"
  },
  {
    "id": "ai-gan-discriminator-training-data-2",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "The Discriminator",
    "title": "Discriminator Training Data",
    "order": 2,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>The discriminator’s training data comes from two sources:</p>\n  </li>\n  <li>Real data instances, such as real pictures of people. The discriminator uses these instances as positive examples during training.</li>\n  <li>Fake data instances created by the generator. The discriminator uses these instances as negative examples during training.</li>\n  <li>In the above figure, the two “Sample” boxes represent these two data sources feeding into the discriminator. During discriminator training the generator does not train. Its weights remain constant while it produces examples for the discriminator to train on.</li>\n</ul>\n<p>The discriminator’s training data comes from two sources:</p>",
    "contentMarkdown": "*   The discriminator’s training data comes from two sources:\n    \n*   Real data instances, such as real pictures of people. The discriminator uses these instances as positive examples during training.\n*   Fake data instances created by the generator. The discriminator uses these instances as negative examples during training.\n*   In the above figure, the two “Sample” boxes represent these two data sources feeding into the discriminator. During discriminator training the generator does not train. Its weights remain constant while it produces examples for the discriminator to train on.\n\nThe discriminator’s training data comes from two sources:",
    "contentLength": 705,
    "wordCount": 95,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#discriminator-training-data"
  },
  {
    "id": "ai-gan-training-the-discriminator-3",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "The Discriminator",
    "title": "Training the Discriminator",
    "order": 3,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>The discriminator connects to two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. We use the generator loss during generator training, as described in the section on <a href=\"#loss-functions\">Loss Functions</a>.</li>\n  <li>During discriminator training:\n    <ol>\n      <li>The discriminator classifies both real data and fake data from the generator.</li>\n      <li>The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.</li>\n      <li>The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.</li>\n    </ol>\n  </li>\n  <li>In the next section we’ll see why the generator loss connects to the discriminator.</li>\n</ul>\n<ol>\n      <li>The discriminator classifies both real data and fake data from the generator.</li>\n      <li>The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.</li>\n      <li>The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.</li>\n    </ol>",
    "contentMarkdown": "*   The discriminator connects to two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. We use the generator loss during generator training, as described in the section on [Loss Functions](#loss-functions).\n*   During discriminator training:\n    1.  The discriminator classifies both real data and fake data from the generator.\n    2.  The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n    3.  The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.\n*   In the next section we’ll see why the generator loss connects to the discriminator.\n\n1.  The discriminator classifies both real data and fake data from the generator.\n2.  The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n3.  The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.",
    "contentLength": 1229,
    "wordCount": 156,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#training-the-discriminator"
  },
  {
    "id": "ai-gan-alternating-training-4",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Training",
    "title": "Alternating Training",
    "order": 4,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>The generator and the discriminator have different training processes. So how do we train the GAN as a whole?</li>\n  <li>GAN training proceeds in alternating periods:\n    <ul>\n      <li>The discriminator trains for one or more epochs.</li>\n      <li>The generator trains for one or more epochs.</li>\n      <li>Repeat steps 1 and 2 to continue to train the generator and discriminator networks.</li>\n    </ul>\n  </li>\n  <li>We keep the generator constant during the discriminator training phase. As discriminator training tries to figure out how to distinguish real data from fake, it has to learn how to recognize the generator’s flaws. That’s a different problem for a thoroughly trained generator than it is for an untrained generator that produces random output.</li>\n  <li>Similarly, we keep the discriminator constant during the generator training phase. Otherwise the generator would be trying to hit a <strong>moving target</strong> and might never converge.</li>\n  <li>It’s this back and forth that allows GANs to tackle otherwise intractable generative problems. We get a toehold in the difficult generative problem by starting with a much simpler classification problem. Conversely, if you can’t train a classifier to tell the difference between real and generated data even for the initial random generator output, you can’t get the GAN training started.</li>\n</ul>\n<ul>\n      <li>The discriminator trains for one or more epochs.</li>\n      <li>The generator trains for one or more epochs.</li>\n      <li>Repeat steps 1 and 2 to continue to train the generator and discriminator networks.</li>\n    </ul>",
    "contentMarkdown": "*   The generator and the discriminator have different training processes. So how do we train the GAN as a whole?\n*   GAN training proceeds in alternating periods:\n    *   The discriminator trains for one or more epochs.\n    *   The generator trains for one or more epochs.\n    *   Repeat steps 1 and 2 to continue to train the generator and discriminator networks.\n*   We keep the generator constant during the discriminator training phase. As discriminator training tries to figure out how to distinguish real data from fake, it has to learn how to recognize the generator’s flaws. That’s a different problem for a thoroughly trained generator than it is for an untrained generator that produces random output.\n*   Similarly, we keep the discriminator constant during the generator training phase. Otherwise the generator would be trying to hit a **moving target** and might never converge.\n*   It’s this back and forth that allows GANs to tackle otherwise intractable generative problems. We get a toehold in the difficult generative problem by starting with a much simpler classification problem. Conversely, if you can’t train a classifier to tell the difference between real and generated data even for the initial random generator output, you can’t get the GAN training started.\n\n*   The discriminator trains for one or more epochs.\n*   The generator trains for one or more epochs.\n*   Repeat steps 1 and 2 to continue to train the generator and discriminator networks.",
    "contentLength": 1625,
    "wordCount": 237,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#alternating-training"
  },
  {
    "id": "ai-gan-convergence-5",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Training",
    "title": "Convergence",
    "order": 5,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>As the generator improves with training, the discriminator performance gets worse because the discriminator can’t easily tell the difference between real and fake. If the generator succeeds perfectly, then the <strong>discriminator has a 50% accuracy</strong>. In effect, the discriminator <strong>flips a coin to make its prediction</strong>.</li>\n  <li>This progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its own quality may collapse.</li>\n  <li>For a GAN, convergence is often a fleeting, rather than stable, state.</li>\n</ul>",
    "contentMarkdown": "*   As the generator improves with training, the discriminator performance gets worse because the discriminator can’t easily tell the difference between real and fake. If the generator succeeds perfectly, then the **discriminator has a 50% accuracy**. In effect, the discriminator **flips a coin to make its prediction**.\n*   This progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its own quality may collapse.\n*   For a GAN, convergence is often a fleeting, rather than stable, state.",
    "contentLength": 782,
    "wordCount": 113,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#convergence"
  },
  {
    "id": "ai-gan-one-loss-function-or-two-6",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "One Loss Function or Two?",
    "order": 6,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>A GAN can have two loss functions: one for generator training and one for discriminator training. How can two loss functions work together to reflect a distance measure between probability distributions?</li>\n  <li>In the loss schemes we’ll look at here, the generator and discriminator losses derive from a single measure of distance between probability distributions. In both of these schemes, however, the generator can only affect one term in the distance measure: the term that reflects the distribution of the fake data. So during generator training we drop the other term, which reflects the distribution of the real data.</li>\n  <li>The generator and discriminator losses look different in the end, even though they derive from a single formula.</li>\n</ul>",
    "contentMarkdown": "*   A GAN can have two loss functions: one for generator training and one for discriminator training. How can two loss functions work together to reflect a distance measure between probability distributions?\n*   In the loss schemes we’ll look at here, the generator and discriminator losses derive from a single measure of distance between probability distributions. In both of these schemes, however, the generator can only affect one term in the distance measure: the term that reflects the distribution of the fake data. So during generator training we drop the other term, which reflects the distribution of the real data.\n*   The generator and discriminator losses look different in the end, even though they derive from a single formula.",
    "contentLength": 775,
    "wordCount": 119,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#one-loss-function-or-two?"
  },
  {
    "id": "ai-gan-minimax-loss-7",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "Minimax Loss",
    "order": 7,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>In the <a href=\"https://arxiv.org/abs/1406.2661\">paper</a> that introduced GANs, the generator tries to minimize the following function while the discriminator tries to maximize it:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi></mrow></msub><mo stretchy=&quot;false&quot;>[</mo><mi>log</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo><mo>+</mo><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>z</mi></mrow></msub><mo stretchy=&quot;false&quot;>[</mo><mi>log</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 17.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1014.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-33\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular;\">log</span><span class=\"mo\" id=\"MathJax-Span-38\"></span><span class=\"mo\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">]</span><span class=\"mo\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-47\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-49\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular;\">log</span><span class=\"mo\" id=\"MathJax-Span-54\"></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">D</span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>+</mo><msub><mi>E</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>z</mi></mrow></msub><mo stretchy=\"false\">[</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>where,\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-67\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">D(x)</script> is the discriminator’s estimate of the probability that real data instance <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-73\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> is real.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mi>x</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-76\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-77\"><span class=\"msubsup\" id=\"MathJax-Span-78\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mi>x</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">E_x</script> is the expected value over all real data instances.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-81\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-82\"><span class=\"mi\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">G(z)</script> is the generator’s output when given noise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>z</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.451em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">z</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>z</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">z</script>.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">D(G(z))</script> is the discriminator’s estimate of the probability that a fake instance is real.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mi>z</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"msubsup\" id=\"MathJax-Span-101\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mi>z</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">E_z</script> is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-104\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-105\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">G(z)</script>).</li>\n      <li>The formula derives from the cross-entropy between the real and generated distributions.</li>\n    </ul>\n  </li>\n  <li>The generator can’t directly affect the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.8em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mo\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">log(D(x))</script> term in the function, so, for the generator, minimizing the loss is equivalent to minimizing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 8.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.72em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"mi\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">D</span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">log(1 - D(G(z)))</script>.</li>\n</ul>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-67\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">D(x)</script> is the discriminator’s estimate of the probability that real data instance <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-73\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> is real.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mi>x</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-76\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-77\"><span class=\"msubsup\" id=\"MathJax-Span-78\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mi>x</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">E_x</script> is the expected value over all real data instances.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-81\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-82\"><span class=\"mi\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">G(z)</script> is the generator’s output when given noise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>z</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.451em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">z</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>z</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">z</script>.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">D(G(z))</script> is the discriminator’s estimate of the probability that a fake instance is real.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mi>z</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"msubsup\" id=\"MathJax-Span-101\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>E</mi><mi>z</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">E_z</script> is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-104\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-105\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">G(z)</script>).</li>\n      <li>The formula derives from the cross-entropy between the real and generated distributions.</li>\n    </ul>",
    "contentMarkdown": "*   In the [paper](https://arxiv.org/abs/1406.2661) that introduced GANs, the generator tries to minimize the following function while the discriminator tries to maximize it:\n\nEx\\[log(D(x))\\]+Ez\\[log(1−D(G(z)))\\]Ex\\[log⁡(D(x))\\]+Ez\\[log⁡(1−D(G(z)))\\]\n\n*   where,\n    *   D(x)D(x)D(x) is the discriminator’s estimate of the probability that real data instance xxx is real.\n    *   ExExE\\_x is the expected value over all real data instances.\n    *   G(z)G(z)G(z) is the generator’s output when given noise zzz.\n    *   D(G(z))D(G(z))D(G(z)) is the discriminator’s estimate of the probability that a fake instance is real.\n    *   EzEzE\\_z is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)G(z)G(z)).\n    *   The formula derives from the cross-entropy between the real and generated distributions.\n*   The generator can’t directly affect the log(D(x))log(D(x))log(D(x)) term in the function, so, for the generator, minimizing the loss is equivalent to minimizing log(1−D(G(z)))log(1−D(G(z)))log(1 - D(G(z))).\n\n*   D(x)D(x)D(x) is the discriminator’s estimate of the probability that real data instance xxx is real.\n*   ExExE\\_x is the expected value over all real data instances.\n*   G(z)G(z)G(z) is the generator’s output when given noise zzz.\n*   D(G(z))D(G(z))D(G(z)) is the discriminator’s estimate of the probability that a fake instance is real.\n*   EzEzE\\_z is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)G(z)G(z)).\n*   The formula derives from the cross-entropy between the real and generated distributions.",
    "contentLength": 40164,
    "wordCount": 229,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#minimax-loss"
  },
  {
    "id": "ai-gan-modified-minimax-loss-8",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "Modified Minimax Loss",
    "order": 8,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>The original GAN paper notes that the above minimax loss function can cause the GAN to get stuck in the early stages of GAN training when the discriminator’s job is very easy. The paper therefore suggests modifying the generator loss so that the generator tries to maximize log D(G(z)).</li>\n</ul>",
    "contentMarkdown": "*   The original GAN paper notes that the above minimax loss function can cause the GAN to get stuck in the early stages of GAN training when the discriminator’s job is very easy. The paper therefore suggests modifying the generator loss so that the generator tries to maximize log D(G(z)).",
    "contentLength": 308,
    "wordCount": 50,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#modified-minimax-loss"
  },
  {
    "id": "ai-gan-wasserstein-loss-9",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "Wasserstein Loss",
    "order": 9,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Wasserstein loss serves as the default loss function of several libraries including TF-GAN.</li>\n  <li>This loss function depends on a modification of the GAN scheme (called “Wasserstein GAN” or “WGAN”) in which the discriminator does not actually classify instances. For each instance it outputs a number. This number does not have to be less than one or greater than 0, so we can’t use 0.5 as a threshold to decide whether an instance is real or fake. Discriminator training just tries to make the output bigger for real instances than for fake instances.</li>\n  <li>Because it can’t really discriminate between real and fake, the WGAN discriminator is actually called a “critic” instead of a “discriminator”. This distinction has theoretical importance, but for practical purposes we can treat it as an acknowledgement that the inputs to the loss functions don’t have to be probabilities.</li>\n  <li>The loss functions themselves are deceptively simple:</li>\n  <li>Critic Loss:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 7.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.15em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">D</span><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The discriminator tries to maximize this function. In other words, it tries to maximize the difference between its output on real instances and its output on fake instances.</li>\n  <li>Generator Loss:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-151\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-152\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The generator tries to maximize this function. In other words, It tries to maximize the discriminator’s output for its fake instances.</li>\n  <li>where,\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-160\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">D(x)</script> is the critic’s output for a real instance.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">G(z)</script> is the generator’s output when given noise z.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D(G(z))</script> is the critic’s output for a fake instance.</li>\n      <li>The output of critic <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D</script> does not have to be between 1 and 0.</li>\n      <li>The formulas derive from the earth mover distance between the real and generated distributions.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-160\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">D(x)</script> is the critic’s output for a real instance.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">G(z)</script> is the generator’s output when given noise z.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mo stretchy=&quot;false&quot;>(</mo><mi>G</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D(G(z))</script> is the critic’s output for a fake instance.</li>\n      <li>The output of critic <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D</script> does not have to be between 1 and 0.</li>\n      <li>The formulas derive from the earth mover distance between the real and generated distributions.</li>\n    </ul>",
    "contentMarkdown": "*   Wasserstein loss serves as the default loss function of several libraries including TF-GAN.\n*   This loss function depends on a modification of the GAN scheme (called “Wasserstein GAN” or “WGAN”) in which the discriminator does not actually classify instances. For each instance it outputs a number. This number does not have to be less than one or greater than 0, so we can’t use 0.5 as a threshold to decide whether an instance is real or fake. Discriminator training just tries to make the output bigger for real instances than for fake instances.\n*   Because it can’t really discriminate between real and fake, the WGAN discriminator is actually called a “critic” instead of a “discriminator”. This distinction has theoretical importance, but for practical purposes we can treat it as an acknowledgement that the inputs to the loss functions don’t have to be probabilities.\n*   The loss functions themselves are deceptively simple:\n*   Critic Loss:\n\nD(x)−D(G(z))D(x)−D(G(z))\n\n*   The discriminator tries to maximize this function. In other words, it tries to maximize the difference between its output on real instances and its output on fake instances.\n*   Generator Loss:\n\nD(G(z))D(G(z))\n\n*   The generator tries to maximize this function. In other words, It tries to maximize the discriminator’s output for its fake instances.\n*   where,\n    *   D(x)D(x)D(x) is the critic’s output for a real instance.\n    *   G(z)G(z)G(z) is the generator’s output when given noise z.\n    *   D(G(z))D(G(z))D(G(z)) is the critic’s output for a fake instance.\n    *   The output of critic DDD does not have to be between 1 and 0.\n    *   The formulas derive from the earth mover distance between the real and generated distributions.\n\n*   D(x)D(x)D(x) is the critic’s output for a real instance.\n*   G(z)G(z)G(z) is the generator’s output when given noise z.\n*   D(G(z))D(G(z))D(G(z)) is the critic’s output for a fake instance.\n*   The output of critic DDD does not have to be between 1 and 0.\n*   The formulas derive from the earth mover distance between the real and generated distributions.",
    "contentLength": 19950,
    "wordCount": 333,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#wasserstein-loss"
  },
  {
    "id": "ai-gan-requirements-10",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "Requirements",
    "order": 10,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>The theoretical justification for the Wasserstein GAN (or WGAN) requires that the weights throughout the GAN be clipped so that they remain within a constrained range.</li>\n</ul>",
    "contentMarkdown": "*   The theoretical justification for the Wasserstein GAN (or WGAN) requires that the weights throughout the GAN be clipped so that they remain within a constrained range.",
    "contentLength": 189,
    "wordCount": 27,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#requirements"
  },
  {
    "id": "ai-gan-benefits-11",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "Loss Functions",
    "title": "Benefits",
    "order": 11,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Wasserstein GANs are less vulnerable to getting stuck than minimax-based GANs, and avoid problems with vanishing gradients. The earth mover distance also has the advantage of being a true metric: a measure of distance in a space of probability distributions. Cross-entropy is not a metric in this sense.</li>\n</ul>",
    "contentMarkdown": "*   Wasserstein GANs are less vulnerable to getting stuck than minimax-based GANs, and avoid problems with vanishing gradients. The earth mover distance also has the advantage of being a true metric: a measure of distance in a space of probability distributions. Cross-entropy is not a metric in this sense.",
    "contentLength": 325,
    "wordCount": 49,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#benefits"
  },
  {
    "id": "ai-gan-progressive-gans-12",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Progressive GANs",
    "order": 12,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>In a progressive GAN, the generator’s first layers produce very low resolution images, and subsequent layers add details. This technique allows the GAN to train more quickly than comparable non-progressive GANs, and produces higher resolution images.</li>\n  <li>For more information see <a href=\"https://arxiv.org/abs/1710.10196\">Karras et al, 2017</a>.</li>\n</ul>",
    "contentMarkdown": "*   In a progressive GAN, the generator’s first layers produce very low resolution images, and subsequent layers add details. This technique allows the GAN to train more quickly than comparable non-progressive GANs, and produces higher resolution images.\n*   For more information see [Karras et al, 2017](https://arxiv.org/abs/1710.10196).",
    "contentLength": 375,
    "wordCount": 46,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#progressive-gans"
  },
  {
    "id": "ai-gan-conditional-gans-13",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Conditional GANs",
    "order": 13,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Conditional GANs train on a labeled data set and let you specify the label for each generated instance. For example, an unconditional MNIST GAN would produce random digits, while a conditional MNIST GAN would let you specify which digit the GAN should generate.</li>\n  <li>Instead of modeling the joint probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-184\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">P(X, Y)</script>, conditional GANs model the conditional probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>&amp;#x2223;</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-192\" style=\"width: 4.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.6em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-193\"><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-199\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo>∣</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">P(X \\mid Y)</script>.</li>\n  <li>For more information about conditional GANs, see <a href=\"https://arxiv.org/abs/1411.1784\">Mirza et al, 2014</a>.</li>\n</ul>",
    "contentMarkdown": "*   Conditional GANs train on a labeled data set and let you specify the label for each generated instance. For example, an unconditional MNIST GAN would produce random digits, while a conditional MNIST GAN would let you specify which digit the GAN should generate.\n*   Instead of modeling the joint probability P(X,Y)P(X,Y)P(X, Y), conditional GANs model the conditional probability P(X∣Y)P(X∣Y)P(X \\\\mid Y).\n*   For more information about conditional GANs, see [Mirza et al, 2014](https://arxiv.org/abs/1411.1784).",
    "contentLength": 4596,
    "wordCount": 74,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/gan/#conditional-gans"
  },
  {
    "id": "ai-gan-image-to-image-translation-14",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Image-to-Image Translation",
    "order": 14,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Image-to-Image translation GANs take an image as input and map it to a generated output image with different properties. For example, we can take a mask image with blob of color in the shape of a car, and the GAN can fill in the shape with photorealistic car details.</li>\n  <li>Similarly, you can train an image-to-image GAN to take sketches of handbags and turn them into photorealistic images of handbags:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/gans/img2img.png\" alt=\"\"></p>\n<ul>\n  <li>The figure above shows a 3x3 table of pictures of handbags. Each row shows a different handbag style. In each row, the leftmost image is a simple line drawing, of a handbag, the middle image is a photo of a real handbag, and the rightmost image is a photorealistic picture generated by a GAN. The three columns are labeled ‘Input’, ‘Ground Truth’, and ‘output’.</li>\n  <li>In these cases, the loss is a weighted combination of the usual discriminator-based loss and a pixel-wise loss that penalizes the generator for departing from the source image.</li>\n  <li>For more information, see <a href=\"https://arxiv.org/abs/1611.07004\">Isola et al, 2016</a>.</li>\n</ul>",
    "contentMarkdown": "*   Image-to-Image translation GANs take an image as input and map it to a generated output image with different properties. For example, we can take a mask image with blob of color in the shape of a car, and the GAN can fill in the shape with photorealistic car details.\n*   Similarly, you can train an image-to-image GAN to take sketches of handbags and turn them into photorealistic images of handbags:\n\n![](/primers/ai/assets/gans/img2img.png)\n\n*   The figure above shows a 3x3 table of pictures of handbags. Each row shows a different handbag style. In each row, the leftmost image is a simple line drawing, of a handbag, the middle image is a photo of a real handbag, and the rightmost image is a photorealistic picture generated by a GAN. The three columns are labeled ‘Input’, ‘Ground Truth’, and ‘output’.\n*   In these cases, the loss is a weighted combination of the usual discriminator-based loss and a pixel-wise loss that penalizes the generator for departing from the source image.\n*   For more information, see [Isola et al, 2016](https://arxiv.org/abs/1611.07004).",
    "contentLength": 1167,
    "wordCount": 175,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/gan/#image-to-image-translation"
  },
  {
    "id": "ai-gan-cyclegan-15",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "CycleGAN",
    "order": 15,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>CycleGANs learn to transform images from one set into images that could plausibly belong to another set. For example, a CycleGAN produced the righthand image below when given the lefthand image as input. It took an image of a horse and turned it into an image of a zebra. The following image shows a horse running, and a second image that’s identical in all respeccts except that the horse is a zebra:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/gans/cyclegan.png\" alt=\"\"></p>\n<ul>\n  <li>The training data for the CycleGAN is simply two sets of images (in this case, a set of horse images and a set of zebra images). The system requires no labels or pairwise correspondences between images.</li>\n  <li>For more information see <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf\">Zhu et al, 2017</a>, which illustrates the use of CycleGAN to perform image-to-image translation without paired data.</li>\n</ul>",
    "contentMarkdown": "*   CycleGANs learn to transform images from one set into images that could plausibly belong to another set. For example, a CycleGAN produced the righthand image below when given the lefthand image as input. It took an image of a horse and turned it into an image of a zebra. The following image shows a horse running, and a second image that’s identical in all respeccts except that the horse is a zebra:\n\n![](/primers/ai/assets/gans/cyclegan.png)\n\n*   The training data for the CycleGAN is simply two sets of images (in this case, a set of horse images and a set of zebra images). The system requires no labels or pairwise correspondences between images.\n*   For more information see [Zhu et al, 2017](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf), which illustrates the use of CycleGAN to perform image-to-image translation without paired data.",
    "contentLength": 989,
    "wordCount": 133,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/gan/#cyclegan"
  },
  {
    "id": "ai-gan-text-to-image-synthesis-16",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Text-to-Image Synthesis",
    "order": 16,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>Text-to-image GANs take text as input and produce images that are plausible and described by the text. For example, the flower image below was produced by feeding a text description to a GAN.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/gans/tti.jpg\" alt=\"\"></p>\n<ul>\n  <li>Note that in this system the GAN can only produce images from a small set of classes.</li>\n  <li>For more information, see <a href=\"https://arxiv.org/abs/1612.03242\">Zhang et al, 2016</a>.</li>\n</ul>",
    "contentMarkdown": "*   Text-to-image GANs take text as input and produce images that are plausible and described by the text. For example, the flower image below was produced by feeding a text description to a GAN.\n\n![](/primers/ai/assets/gans/tti.jpg)\n\n*   Note that in this system the GAN can only produce images from a small set of classes.\n*   For more information, see [Zhang et al, 2016](https://arxiv.org/abs/1612.03242).",
    "contentLength": 482,
    "wordCount": 62,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/gan/#text-to-image-synthesis"
  },
  {
    "id": "ai-gan-super-resolution-17",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Super-resolution",
    "order": 17,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Super-resolution GANs increase the resolution of images, adding detail where necessary to fill in blurry areas. For example, the blurry middle image below is a downsampled version of the original image on the left. Given the blurry image, a GAN produced the sharper image on the right:</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n  <thead>\n    <tr>\n      <th class=\"tg-hcenter-valign-first\">Original</th>\n      <th class=\"tg-hcenter-valign-first\">Blurred</th>\n      <th class=\"tg-hcenter-valign-second\">Restored with GAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/superres_orig.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/superres_blurry.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n      <td class=\"tg-tcenter-valign-second\"><img src=\"../assets/gans/superres_gan.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<table class=\"tg\">\n  <thead>\n    <tr>\n      <th class=\"tg-hcenter-valign-first\">Original</th>\n      <th class=\"tg-hcenter-valign-first\">Blurred</th>\n      <th class=\"tg-hcenter-valign-second\">Restored with GAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/superres_orig.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/superres_blurry.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n      <td class=\"tg-tcenter-valign-second\"><img src=\"../assets/gans/superres_gan.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\" height=\"170px\" weight=\"115px\"></td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li>Note that the original image shows a painting of a girl wearing an <strong>elaborate headdress</strong>. The headband of the headdress is knit in a <strong>complex pattern</strong>. Given a blurry version of the painting, a sharp, clear version is obtained at the output of the GAN.</li>\n  <li>The GAN-generated image looks very similar to the original image, but some of the details of the patterns on her headdress and clothing are subtly different - for example, if you look closely at the headband you’ll see that the GAN didn’t reproduce the starburst pattern from the original. Instead, it made up its own plausible pattern to replace the pattern erased by the down-sampling.</li>\n  <li>For more information, see <a href=\"https://arxiv.org/pdf/1609.04802.pdf\">Ledig et al, 2017</a>.</li>\n</ul>",
    "contentMarkdown": "*   Super-resolution GANs increase the resolution of images, adding detail where necessary to fill in blurry areas. For example, the blurry middle image below is a downsampled version of the original image on the left. Given the blurry image, a GAN produced the sharper image on the right:\n\nOriginal\n\nBlurred\n\nRestored with GAN\n\n![](../assets/gans/superres_orig.png)\n\n![](../assets/gans/superres_blurry.png)\n\n![](../assets/gans/superres_gan.png)\n\nOriginal\n\nBlurred\n\nRestored with GAN\n\n![](../assets/gans/superres_orig.png)\n\n![](../assets/gans/superres_blurry.png)\n\n![](../assets/gans/superres_gan.png)\n\n*   Note that the original image shows a painting of a girl wearing an **elaborate headdress**. The headband of the headdress is knit in a **complex pattern**. Given a blurry version of the painting, a sharp, clear version is obtained at the output of the GAN.\n*   The GAN-generated image looks very similar to the original image, but some of the details of the patterns on her headdress and clothing are subtly different - for example, if you look closely at the headband you’ll see that the GAN didn’t reproduce the starburst pattern from the original. Instead, it made up its own plausible pattern to replace the pattern erased by the down-sampling.\n*   For more information, see [Ledig et al, 2017](https://arxiv.org/pdf/1609.04802.pdf).",
    "contentLength": 2890,
    "wordCount": 185,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/gan/#super-resolution"
  },
  {
    "id": "ai-gan-face-inpainting-18",
    "articleSlug": "gan",
    "articleTitle": "Generative Adversarial Networks (GANs)",
    "category": "Algorithms/Architecture",
    "chapter": "GAN Variations",
    "title": "Face Inpainting",
    "order": 18,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>GANs have been used for the semantic image inpainting task. In the inpainting task, chunks of an image are blacked out, and the system tries to fill in the missing chunks.</li>\n  <li><a href=\"\">Yeh et al, 2017</a> used a GAN to outperform other techniques for inpainting images of faces. Shown below are a set of images where each image is a photo of a face with some areas replaced with black. Each image is a photo of a face identical to one of the images in the ‘Input’ column, except that there are no black areas.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n  <thead>\n    <tr>\n      <th class=\"tg-hcenter-valign-first\">Input</th>\n      <th class=\"tg-hcenter-valign-second\">GAN Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/inpainting_in.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\"></td>\n      <td class=\"tg-tcenter-valign-second\"><img src=\"../assets/gans/inpainting_out.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\"></td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<table class=\"tg\">\n  <thead>\n    <tr>\n      <th class=\"tg-hcenter-valign-first\">Input</th>\n      <th class=\"tg-hcenter-valign-second\">GAN Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td class=\"tg-tcenter-valign-first\"><img src=\"../assets/gans/inpainting_in.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\"></td>\n      <td class=\"tg-tcenter-valign-second\"><img src=\"../assets/gans/inpainting_out.png\" align=\"center\" style=\"background-color: #fff; padding: 0px 0px 0px 0px;\"></td>\n    </tr>\n  </tbody>\n</table>",
    "contentMarkdown": "*   GANs have been used for the semantic image inpainting task. In the inpainting task, chunks of an image are blacked out, and the system tries to fill in the missing chunks.\n*   Yeh et al, 2017 used a GAN to outperform other techniques for inpainting images of faces. Shown below are a set of images where each image is a photo of a face with some areas replaced with black. Each image is a photo of a face identical to one of the images in the ‘Input’ column, except that there are no black areas.\n\nInput\n\nGAN Output\n\n![](../assets/gans/inpainting_in.png)\n\n![](../assets/gans/inpainting_out.png)\n\nInput\n\nGAN Output\n\n![](../assets/gans/inpainting_in.png)\n\n![](../assets/gans/inpainting_out.png)",
    "contentLength": 1672,
    "wordCount": 106,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/gan/#face-inpainting"
  }
]