[
  {
    "id": "ai-GeminiEmbedding-background-and-scope-1",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Background and Scope",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<p>This primer summarizes the model design, training objective, data pipeline, and evaluation protocol of the Gemini Embedding model as described in <a href=\"https://arxiv.org/abs/2503.07891\">the paper</a>. It begins from the transformer-encoder view, presents the exact pooling/projection mechanics and loss, then covers the two-stage training recipe, data generation/filtering/mining, and evaluation setup.</p>\n<p><img src=\"/primers/ai/assets/gembed/1.png\" alt=\"\"></p>",
    "contentMarkdown": "This primer summarizes the model design, training objective, data pipeline, and evaluation protocol of the Gemini Embedding model as described in [the paper](https://arxiv.org/abs/2503.07891). It begins from the transformer-encoder view, presents the exact pooling/projection mechanics and loss, then covers the two-stage training recipe, data generation/filtering/mining, and evaluation setup.\n\n![](/primers/ai/assets/gembed/1.png)",
    "contentLength": 470,
    "wordCount": 48,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#background-and-scope"
  },
  {
    "id": "ai-GeminiEmbedding-encoder-architecture-2",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Encoder Architecture",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<p>Suppose the input sequence is</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>T</mi><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>t</mi><mi>L</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 9.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mn\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-10\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mn\" id=\"MathJax-Span-12\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-16\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>T</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>t</mi><mi>L</mi></msub><mo stretchy=\"false\">)</mo><mo>,</mo></math></span></span></div>\n<p>a sequence of $L$ tokens after tokenization. Each token $t_i$ is mapped to an embedding vector and fed into a <strong>bidirectional transformer encoder</strong> $M$ (initialized from Gemini). The encoder outputs contextualized token embeddings:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>T</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>M</mi><mo stretchy=&quot;false&quot;>(</mo><mi>T</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>L</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mi>M</mi></msub></mrow></msup><mo>.</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-21\" style=\"width: 11.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.9em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"msubsup\" id=\"MathJax-Span-23\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-25\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mtext\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-34\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-35\"><span class=\"mrow\" id=\"MathJax-Span-36\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-38\"><span class=\"mrow\" id=\"MathJax-Span-39\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-42\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>T</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>M</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>L</mi><mo>×</mo><msub><mi>d</mi><mi>M</mi></msub></mrow></msup><mo>.</mo></math></span></span></div>\n<p>Here, $d_M$ is the hidden size of the encoder. Each row $T_{\\text{embed}}[i]$ corresponds to a token, but crucially, it is <strong>contextualized</strong>: the vector does not represent only the identity of $t_i$, but also how $t_i$ relates to every other token in the sequence.</p>\n<p>This contextualization arises from the transformer’s self-attention mechanism, where each token embedding attends to the others and integrates information from them. As a result:</p>\n<ul>\n  <li>The vector for “bank” in <em>“river bank”</em> is pulled toward meanings related to water and geography.</li>\n  <li>The vector for “bank” in <em>“savings bank”</em> is shifted toward finance-related meanings.\nBoth embeddings come from the same word, but they live in different parts of the hidden space because of their different contexts.</li>\n</ul>\n<h4 id=\"mean-pooling\">Mean Pooling</h4>\n<p>The transformer outputs one vector per token, but for retrieval and similarity tasks, we need one <strong>sentence-level embedding</strong>. To do this, Gemini Embedding applies <strong>mean pooling</strong> across the token dimension:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>L</mi></mrow></munderover><msub><mi>T</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo stretchy=&quot;false&quot;>[</mo><mi>i</mi><mo stretchy=&quot;false&quot;>]</mo><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>d</mi><mi>M</mi></msub></mrow></msup><mo>.</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-46\" style=\"width: 15.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.315em, 1012.87em, 3.648em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-47\"><span class=\"msubsup\" id=\"MathJax-Span-48\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mtext\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-54\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-57\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-59\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.42em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-64\"><span class=\"mrow\" id=\"MathJax-Span-65\"><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-69\"><span class=\"mrow\" id=\"MathJax-Span-70\"><span class=\"mtext\" id=\"MathJax-Span-71\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Regular;\">]</span><span class=\"mo\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-76\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-77\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-80\"><span class=\"mrow\" id=\"MathJax-Span-81\"><span class=\"msubsup\" id=\"MathJax-Span-82\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-83\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>L</mi></mrow></munderover><msub><mi>T</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>d</mi><mi>M</mi></msub></mrow></msup><mo>.</mo></math></span></span></div>\n<p>Mean pooling computes the centroid of all token vectors, treating them as points in a high-dimensional space.</p>\n<ul>\n  <li><strong>Intuitively</strong>, imagine every token embedding as a coordinate in $\\mathbb{R}^{d_M}$. Mean pooling takes the geometric center of these points.</li>\n  <li><strong>Theoretically</strong>, it is permutation-invariant: the result does not depend on token order (though token order already influenced the contextual vectors).</li>\n  <li><strong>Statistically</strong>, it reduces variance and noise by averaging over all positions.</li>\n</ul>\n<p><strong>Example:</strong>\nIn <em>“The cat sat on the mat,”</em> the words “cat,” “sat,” and “mat” each contribute their contextual embeddings. Averaging ensures the final representation reflects the sentence as a whole, rather than any single dominant word.</p>\n<h4 id=\"alternatives-to-mean-pooling\">Alternatives to Mean Pooling</h4>\n<p>The paper chooses mean pooling deliberately. Other options include:</p>\n<ul>\n  <li><strong>[CLS] pooling:</strong> use the representation of a special [CLS] token. This works in models like BERT, but performance often depends heavily on how the [CLS] token is trained. It may encode classification-specific signals rather than pure semantics.</li>\n  <li><strong>Max pooling:</strong> take the maximum along each embedding dimension across tokens. This highlights the most salient features but can discard complementary information.</li>\n  <li><strong>Attention pooling:</strong> learn a weighted average where some tokens contribute more. This can be powerful, but it introduces additional complexity and often requires supervision.</li>\n</ul>\n<p>Mean pooling, in contrast, is simple, robust, and empirically stable across languages, tasks, and domains. It avoids overfitting to task-specific markers and produces embeddings that generalize well.</p>\n<h4 id=\"linear-projection\">Linear Projection</h4>\n<p>The pooled embedding is then passed through a <strong>linear projection</strong>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>E</mi><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>W</mi><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>b</mi><mspace width=&quot;1em&quot; /><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup><mo>,</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-86\" style=\"width: 18.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1015.63em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-87\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-92\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-94\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"mtext\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-100\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-102\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"mtext\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">b</span><span class=\"mspace\" id=\"MathJax-Span-107\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-109\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-110\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>E</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>W</mi><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>b</mi><mspace width=\"1em\"></mspace><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>d</mi></msup><mo>,</mo></math></span></span></div>\n<p>where $W \\in \\mathbb{R}^{d \\times d_M}$ and $b \\in \\mathbb{R}^d$.</p>\n<p>This projection accomplishes two important functions:</p>\n<ol>\n  <li><strong>Dimensional control:</strong> it allows the model to output embeddings of a chosen dimension $d$ (e.g., 768, 1536, 3072) without changing the encoder architecture.</li>\n  <li><strong>Task alignment:</strong> it decouples the encoder’s hidden space (optimized for representation learning) from the final embedding space (optimized for similarity tasks).</li>\n</ol>",
    "contentMarkdown": "Suppose the input sequence is\n\nT\\=(t1,t2,…,tL),T\\=(t1,t2,…,tL),\n\na sequence of $L$ tokens after tokenization. Each token $t\\_i$ is mapped to an embedding vector and fed into a **bidirectional transformer encoder** $M$ (initialized from Gemini). The encoder outputs contextualized token embeddings:\n\nTembed\\=M(T)∈ℝL×dM.Tembed\\=M(T)∈RL×dM.\n\nHere, $d\\_M$ is the hidden size of the encoder. Each row $T\\_{\\\\text{embed}}\\[i\\]$ corresponds to a token, but crucially, it is **contextualized**: the vector does not represent only the identity of $t\\_i$, but also how $t\\_i$ relates to every other token in the sequence.\n\nThis contextualization arises from the transformer’s self-attention mechanism, where each token embedding attends to the others and integrates information from them. As a result:\n\n*   The vector for “bank” in _“river bank”_ is pulled toward meanings related to water and geography.\n*   The vector for “bank” in _“savings bank”_ is shifted toward finance-related meanings. Both embeddings come from the same word, but they live in different parts of the hidden space because of their different contexts.\n\n#### Mean Pooling\n\nThe transformer outputs one vector per token, but for retrieval and similarity tasks, we need one **sentence-level embedding**. To do this, Gemini Embedding applies **mean pooling** across the token dimension:\n\nPembed\\=1L∑i\\=1LTembed\\[i\\]∈ℝdM.Pembed\\=1L∑i\\=1LTembed\\[i\\]∈RdM.\n\nMean pooling computes the centroid of all token vectors, treating them as points in a high-dimensional space.\n\n*   **Intuitively**, imagine every token embedding as a coordinate in $\\\\mathbb{R}^{d\\_M}$. Mean pooling takes the geometric center of these points.\n*   **Theoretically**, it is permutation-invariant: the result does not depend on token order (though token order already influenced the contextual vectors).\n*   **Statistically**, it reduces variance and noise by averaging over all positions.\n\n**Example:** In _“The cat sat on the mat,”_ the words “cat,” “sat,” and “mat” each contribute their contextual embeddings. Averaging ensures the final representation reflects the sentence as a whole, rather than any single dominant word.\n\n#### Alternatives to Mean Pooling\n\nThe paper chooses mean pooling deliberately. Other options include:\n\n*   **\\[CLS\\] pooling:** use the representation of a special \\[CLS\\] token. This works in models like BERT, but performance often depends heavily on how the \\[CLS\\] token is trained. It may encode classification-specific signals rather than pure semantics.\n*   **Max pooling:** take the maximum along each embedding dimension across tokens. This highlights the most salient features but can discard complementary information.\n*   **Attention pooling:** learn a weighted average where some tokens contribute more. This can be powerful, but it introduces additional complexity and often requires supervision.\n\nMean pooling, in contrast, is simple, robust, and empirically stable across languages, tasks, and domains. It avoids overfitting to task-specific markers and produces embeddings that generalize well.\n\n#### Linear Projection\n\nThe pooled embedding is then passed through a **linear projection**:\n\nE\\=f(Pembed)\\=WPembed+b∈ℝd,E\\=f(Pembed)\\=WPembed+b∈Rd,\n\nwhere $W \\\\in \\\\mathbb{R}^{d \\\\times d\\_M}$ and $b \\\\in \\\\mathbb{R}^d$.\n\nThis projection accomplishes two important functions:\n\n1.  **Dimensional control:** it allows the model to output embeddings of a chosen dimension $d$ (e.g., 768, 1536, 3072) without changing the encoder architecture.\n2.  **Task alignment:** it decouples the encoder’s hidden space (optimized for representation learning) from the final embedding space (optimized for similarity tasks).",
    "contentLength": 29645,
    "wordCount": 502,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#encoder-architecture"
  },
  {
    "id": "ai-GeminiEmbedding-pipeline-overview-3",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Pipeline Overview",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ol>\n  <li><strong>Transformer encoder:</strong> contextualizes each token, producing vectors that encode both token identity and its relationships with the entire sequence.</li>\n  <li><strong>Mean pooling:</strong> aggregates all contextualized vectors into one sentence-level representation by averaging.</li>\n  <li><strong>Linear projection:</strong> reshapes this representation into the target embedding dimension, ensuring flexibility and task alignment.</li>\n</ol>\n<p>The final vector (E) is the output embedding used for retrieval, clustering, or classification.</p>",
    "contentMarkdown": "1.  **Transformer encoder:** contextualizes each token, producing vectors that encode both token identity and its relationships with the entire sequence.\n2.  **Mean pooling:** aggregates all contextualized vectors into one sentence-level representation by averaging.\n3.  **Linear projection:** reshapes this representation into the target embedding dimension, ensuring flexibility and task alignment.\n\nThe final vector (E) is the output embedding used for retrieval, clustering, or classification.",
    "contentLength": 574,
    "wordCount": 63,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#pipeline-overview"
  },
  {
    "id": "ai-GeminiEmbedding-training-objective-contrastive-nce-with-in-batch-n-4",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Training Objective (contrastive NCE with In-batch Negatives)",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<p>The Gemini embedding model is trained with a <strong>contrastive noise-contrastive estimation (NCE) loss</strong>. The goal is to pull queries and their correct targets closer together in the embedding space, while pushing them away from incorrect or irrelevant candidates.</p>\n<h4 id=\"training-example-structure\">Training Example Structure</h4>\n<p>Each training instance consists of:</p>\n<ul>\n  <li>A <strong>task string</strong> $t$, which tells the model what kind of problem it is solving (e.g., “question answering,” “retrieval,” “classification”).</li>\n  <li>A <strong>query</strong> $q_i$, such as a natural language question or a code snippet.</li>\n  <li>A <strong>positive target</strong> $p_i^{+}$, the correct answer passage or label.</li>\n  <li>Optionally, a <strong>hard negative</strong> $p_i^{-}$, which looks superficially similar to the positive but is actually incorrect.</li>\n</ul>\n<h4 id=\"embedding-step\">Embedding Step</h4>\n<p>The encoder maps both queries and targets into vectors using the same architecture described before (transformer → mean pooling → projection). For query $q_i$:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><mtext>mean\\_pool</mtext><mrow><mo>(</mo><mrow><mi>M</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2295;</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 13.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1011.46em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"msubsup\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">mean\\_pool</span><span class=\"mrow\" id=\"MathJax-Span-122\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-123\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⊕</span><span class=\"msubsup\" id=\"MathJax-Span-129\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-133\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><mtext>mean\\_pool</mtext><mrow><mo>(</mo><mrow><mi>M</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo>⊕</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></math></span></span></div>\n<p>and for positives/negatives:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>p</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x00B1;</mo></mrow></msubsup><mo>=</mo><mtext>mean\\_pool</mtext><mrow><mo>(</mo><mrow><mi>M</mi><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x00B1;</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-134\" style=\"width: 12.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1010.63em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"msubsup\" id=\"MathJax-Span-136\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.221em, -999.997em); top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-138\"><span class=\"mrow\" id=\"MathJax-Span-139\"><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">±</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">mean\\_pool</span><span class=\"mrow\" id=\"MathJax-Span-144\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-145\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-149\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.221em, -999.997em); top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-151\"><span class=\"mrow\" id=\"MathJax-Span-152\"><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">±</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-156\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>p</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>±</mo></mrow></msubsup><mo>=</mo><mtext>mean\\_pool</mtext><mrow><mo>(</mo><mrow><mi>M</mi><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>±</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></math></span></span></div>\n<p>Here $t \\oplus q_i$ denotes the concatenation of the task string and the query.</p>\n<h4 id=\"similarity-function\">Similarity Function</h4>\n<p>To compare vectors, the model uses <strong>cosine similarity</strong>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><msup><mi>x</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mi>y</mi></mrow><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>x</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>y</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.523em, 1008.13em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mtext\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">sim</span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-166\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1001.46em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.727em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"msubsup\" id=\"MathJax-Span-168\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.87em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.508em;\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">‖</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 3.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>sim</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><msup><mi>x</mi><mi mathvariant=\"normal\">⊤</mi></msup><mi>y</mi></mrow><mrow><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>x</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>y</mi><mo fence=\"false\" stretchy=\"false\">‖</mo></mrow></mfrac></math></span></span></div>\n<p>Cosine similarity measures the angle between two vectors, ignoring their length. This is particularly useful for embeddings, where direction in space matters more than raw magnitude.</p>\n<h4 id=\"loss-function\">Loss Function</h4>\n<p>For a batch of size $B$, the training uses <strong>in-batch negatives</strong>: every other pair in the same batch serves as a negative example for the current query. This greatly increases training efficiency, since one batch provides $B^2$ comparisons instead of just $B$.</p>\n<p>With temperature $\\tau$, the per-example loss is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mi>i</mi></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mfrac><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>+</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></mrow><mi>&amp;#x03C4;</mi></mfrac><mo>)</mo></mrow></mrow><mrow><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn mathvariant=&quot;bold&quot;>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mtext>hard neg present</mtext><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></mrow></msub><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></mrow><mi>&amp;#x03C4;</mi></mfrac><mo>)</mo></mrow><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></munderover><mtext>mask</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=&quot;false&quot;>)</mo><mi>exp</mi><mo>&amp;#x2061;</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>j</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>+</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></mrow><mi>&amp;#x03C4;</mi></mfrac><mo>)</mo></mrow></mrow></mfrac><mo>.</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-179\" style=\"width: 36.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 30.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-0.258em, 1030.47em, 4.273em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"msubsup\" id=\"MathJax-Span-181\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-182\"><span class=\"mrow\" id=\"MathJax-Span-183\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-189\"></span><span class=\"mfrac\" id=\"MathJax-Span-190\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 25.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.659em, 1005.84em, 4.846em, -999.997em); top: -5.154em; left: 50%; margin-left: -2.966em;\"><span class=\"mrow\" id=\"MathJax-Span-191\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-193\"></span><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mo\" id=\"MathJax-Span-195\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-196\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.336em, 1002.92em, 4.378em, -999.997em); top: -4.633em; left: 50%; margin-left: -1.456em;\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mtext\" id=\"MathJax-Span-198\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">sim</span><span class=\"mo\" id=\"MathJax-Span-199\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-200\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-202\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-203\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-204\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.596em, 1000.42em, 4.169em, -999.997em); top: -4.268em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-206\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mo\" id=\"MathJax-Span-208\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.21em, 4.169em, -999.997em); top: -3.799em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-212\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.607em, 1025.42em, 4.846em, -999.997em); top: -2.758em; left: 50%; margin-left: -12.758em;\"><span class=\"mrow\" id=\"MathJax-Span-213\"><span class=\"msubsup\" id=\"MathJax-Span-214\"><span style=\"display: inline-block; position: relative; width: 5.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-215\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"mn\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral; font-weight: bold;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-218\"><span class=\"mrow\" id=\"MathJax-Span-219\"><span class=\"mo\" id=\"MathJax-Span-220\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">{</span><span class=\"mtext\" id=\"MathJax-Span-221\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">hard neg present</span><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">}</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-224\"></span><span class=\"mrow\" id=\"MathJax-Span-225\"><span class=\"mo\" id=\"MathJax-Span-226\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-227\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1002.92em, 4.378em, -999.997em); top: -4.633em; left: 50%; margin-left: -1.456em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mtext\" id=\"MathJax-Span-229\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">sim</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-231\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-233\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-235\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-236\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.596em, 1000.42em, 4.221em, -999.997em); top: -4.32em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-237\"><span class=\"mrow\" id=\"MathJax-Span-238\"><span class=\"mo\" id=\"MathJax-Span-239\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">−</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.21em, 4.169em, -999.997em); top: -3.799em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-240\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-245\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-247\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.326em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-250\"><span class=\"mrow\" id=\"MathJax-Span-251\"><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-253\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">mask</span><span class=\"mo\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-262\"></span><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mo\" id=\"MathJax-Span-264\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-265\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.336em, 1002.92em, 4.482em, -999.997em); top: -4.737em; left: 50%; margin-left: -1.456em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mtext\" id=\"MathJax-Span-267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">sim</span><span class=\"mo\" id=\"MathJax-Span-268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-269\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-271\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-273\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-274\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.596em, 1000.42em, 4.169em, -999.997em); top: -4.268em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-275\"><span class=\"mrow\" id=\"MathJax-Span-276\"><span class=\"mo\" id=\"MathJax-Span-277\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.21em, 4.273em, -999.997em); top: -3.799em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-281\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1025.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 25.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Regular;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.372em; border-left: 0px solid; width: 0px; height: 5.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mi>i</mi></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>+</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>τ</mi></mfrac><mo>)</mo></mrow></mrow><mrow><msub><mrow class=\"MJX-TeXAtom-ORD\"><mn mathvariant=\"bold\">1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo fence=\"false\" stretchy=\"false\">{</mo><mtext>hard neg present</mtext><mo fence=\"false\" stretchy=\"false\">}</mo></mrow></msub><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>τ</mi></mfrac><mo>)</mo></mrow><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>B</mi></mrow></munderover><mtext>mask</mtext><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>p</mi><mi>j</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>+</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>τ</mi></mfrac><mo>)</mo></mrow></mrow></mfrac><mo>.</mo></math></span></span></div>\n<p>The total batch loss is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-283\" style=\"width: 6.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1005.58em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"texatom\" id=\"MathJax-Span-285\"><span class=\"mrow\" id=\"MathJax-Span-286\"><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-289\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.31em;\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">B</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-292\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-293\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-294\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-297\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-298\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.42em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-299\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-302\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-303\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>B</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mi>i</mi></msub></math></span></span></div>\n<h4 id=\"intuition\">Intuition</h4>\n<ul>\n  <li><strong>Numerator:</strong> the similarity between the query and its correct positive. The loss is minimized when this similarity is high.</li>\n  <li><strong>Denominator:</strong> the sum of similarities with negatives (all other positives in the batch, plus any explicit hard negative). The loss is minimized when these similarities are low.</li>\n  <li><strong>Temperature $\\tau$:</strong> controls how sharply the softmax weights similarities. A smaller $\\tau$ makes the model focus more strongly on the hardest negatives.</li>\n</ul>\n<h4 id=\"role-of-hard-negatives\">Role of Hard Negatives</h4>\n<p>Hard negatives are particularly tricky examples (e.g., a passage that looks like the right answer but isn’t). By explicitly including them in the denominator, the model learns fine-grained distinctions.</p>\n<p>However, the paper notes that adding too many hard negatives can hurt, the model starts overfitting to distinguishing subtle noise rather than learning broad semantic separation.</p>\n<h4 id=\"example\">Example</h4>\n<p>Suppose we are training on the query:</p>\n<blockquote>\n  <p><em>“Who wrote the play Hamlet?”</em></p>\n</blockquote>\n<p><em>“Who wrote the play Hamlet?”</em></p>\n<ul>\n  <li>Positive $p_i^{+}$: <em>“William Shakespeare authored Hamlet, one of his most famous tragedies.”</em></li>\n  <li>Hard negative $p_i^{-}$: <em>“Hamlet is one of the most frequently performed plays in the world.”</em></li>\n</ul>\n<p>The hard negative is topically correct (it’s about Hamlet), but it doesn’t answer the question. Training pushes the query embedding closer to the positive and away from both the hard negative and other batch candidates (e.g., answers about Moby-Dick or The Odyssey).</p>\n<h4 id=\"masking-and-same-tower-negatives\">Masking and “same-tower Negatives”</h4>\n<p>The <strong>mask</strong> function ensures degenerate cases are excluded:</p>\n<ul>\n  <li>For classification, the same label appearing in the batch is not counted as a negative.</li>\n  <li>“Same-tower negatives” (examples encoded by the same side of the model) are omitted, since they often produce false negatives that degrade performance.</li>\n</ul>",
    "contentMarkdown": "The Gemini embedding model is trained with a **contrastive noise-contrastive estimation (NCE) loss**. The goal is to pull queries and their correct targets closer together in the embedding space, while pushing them away from incorrect or irrelevant candidates.\n\n#### Training Example Structure\n\nEach training instance consists of:\n\n*   A **task string** $t$, which tells the model what kind of problem it is solving (e.g., “question answering,” “retrieval,” “classification”).\n*   A **query** $q\\_i$, such as a natural language question or a code snippet.\n*   A **positive target** $p\\_i^{+}$, the correct answer passage or label.\n*   Optionally, a **hard negative** $p\\_i^{-}$, which looks superficially similar to the positive but is actually incorrect.\n\n#### Embedding Step\n\nThe encoder maps both queries and targets into vectors using the same architecture described before (transformer → mean pooling → projection). For query $q\\_i$:\n\nqi\\=mean\\\\\\_pool(M(t⊕qi))qi\\=mean\\\\\\_pool(M(t⊕qi))\n\nand for positives/negatives:\n\np±i\\=mean\\\\\\_pool(M(p±i))pi±\\=mean\\\\\\_pool(M(pi±))\n\nHere $t \\\\oplus q\\_i$ denotes the concatenation of the task string and the query.\n\n#### Similarity Function\n\nTo compare vectors, the model uses **cosine similarity**:\n\nsim(x,y)\\=x⊤y‖x‖‖y‖sim(x,y)\\=x⊤y‖x‖‖y‖\n\nCosine similarity measures the angle between two vectors, ignoring their length. This is particularly useful for embeddings, where direction in space matters more than raw magnitude.\n\n#### Loss Function\n\nFor a batch of size $B$, the training uses **in-batch negatives**: every other pair in the same batch serves as a negative example for the current query. This greatly increases training efficiency, since one batch provides $B^2$ comparisons instead of just $B$.\n\nWith temperature $\\\\tau$, the per-example loss is:\n\ni\\=−logexp(sim(qi,p+i)τ)1{hard neg present}exp(sim(qi,p−i)τ)+∑Bj\\=1mask(i,j)exp(sim(qi,p+j)τ).Li\\=−log⁡exp⁡(sim(qi,pi+)τ)1{hard neg present}exp⁡(sim(qi,pi−)τ)+∑j\\=1Bmask(i,j)exp⁡(sim(qi,pj+)τ).\n\nThe total batch loss is:\n\n\\=1B∑i\\=1BiL\\=1B∑i\\=1BLi\n\n#### Intuition\n\n*   **Numerator:** the similarity between the query and its correct positive. The loss is minimized when this similarity is high.\n*   **Denominator:** the sum of similarities with negatives (all other positives in the batch, plus any explicit hard negative). The loss is minimized when these similarities are low.\n*   **Temperature $\\\\tau$:** controls how sharply the softmax weights similarities. A smaller $\\\\tau$ makes the model focus more strongly on the hardest negatives.\n\n#### Role of Hard Negatives\n\nHard negatives are particularly tricky examples (e.g., a passage that looks like the right answer but isn’t). By explicitly including them in the denominator, the model learns fine-grained distinctions.\n\nHowever, the paper notes that adding too many hard negatives can hurt, the model starts overfitting to distinguishing subtle noise rather than learning broad semantic separation.\n\n#### Example\n\nSuppose we are training on the query:\n\n> _“Who wrote the play Hamlet?”_\n\n_“Who wrote the play Hamlet?”_\n\n*   Positive $p\\_i^{+}$: _“William Shakespeare authored Hamlet, one of his most famous tragedies.”_\n*   Hard negative $p\\_i^{-}$: _“Hamlet is one of the most frequently performed plays in the world.”_\n\nThe hard negative is topically correct (it’s about Hamlet), but it doesn’t answer the question. Training pushes the query embedding closer to the positive and away from both the hard negative and other batch candidates (e.g., answers about Moby-Dick or The Odyssey).\n\n#### Masking and “same-tower Negatives”\n\nThe **mask** function ensures degenerate cases are excluded:\n\n*   For classification, the same label appearing in the batch is not counted as a negative.\n*   “Same-tower negatives” (examples encoded by the same side of the model) are omitted, since they often produce false negatives that degrade performance.",
    "contentLength": 46830,
    "wordCount": 533,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#training-objective-(contrastive-nce-with-in-batch-negatives)"
  },
  {
    "id": "ai-GeminiEmbedding-multi-resolution-training-for-multiple-output-dime-5",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Multi-resolution Training for Multiple Output Dimensions (MRL)",
    "order": 5,
    "orderInChapter": 5,
    "contentHtml": "<p>A challenge in embedding models is that different applications have different requirements for embedding dimensionality.</p>\n<ul>\n  <li><strong>High-dimensional embeddings</strong> (e.g., 3072-dim) capture more information and perform better on fine-grained semantic tasks, but they are computationally expensive to store, index, and compare.</li>\n  <li><strong>Low-dimensional embeddings</strong> (e.g., 768-dim) are cheaper and faster, but may sacrifice accuracy.</li>\n</ul>\n<p>Normally, one would train and maintain multiple models for different embedding sizes. Gemini Embedding avoids this duplication by using <strong>Matryoshka Representation Learning (MRL)</strong>, where a single model can serve multiple embedding sizes at once.</p>\n<h4 id=\"how-mrl-works\">How MRL Works</h4>\n<p>The idea is to train the embedding space such that <strong>prefix slices</strong> of the full embedding vector are themselves valid embeddings.</p>\n<ul>\n  <li>Suppose the model outputs a full embedding $E \\in \\mathbb{R}^{3072}$.</li>\n  <li>With MRL, the first 768 dimensions $E_{1:768}$ should already be a strong embedding; likewise the first 1536 dimensions $E_{1:1536}$.</li>\n  <li>This is achieved by adding <strong>multiple contrastive losses</strong> during training, one per prefix.</li>\n</ul>\n<p>Formally, if $\\mathcal{L}^{d}$ denotes the contrastive loss computed on the first $d$ dimensions of the embedding, then the total loss is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>MRL</mtext></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x2208;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>768</mn><mo>,</mo><mn>1536</mn><mo>,</mo><mn>3072</mn><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></mrow></munder><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 12.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1010.63em, 3.701em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"msubsup\" id=\"MathJax-Span-309\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-310\"><span class=\"mrow\" id=\"MathJax-Span-311\"><span class=\"mi\" id=\"MathJax-Span-312\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-313\"><span class=\"mrow\" id=\"MathJax-Span-314\"><span class=\"mtext\" id=\"MathJax-Span-315\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">MRL</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-317\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 2.242em;\"><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1005.73em, 4.378em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-319\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-322\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">{</span><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">768</span><span class=\"mo\" id=\"MathJax-Span-325\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1536</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-328\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3072</span><span class=\"mo\" id=\"MathJax-Span-329\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">}</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-330\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-331\"><span class=\"mrow\" id=\"MathJax-Span-332\"><span class=\"mi\" id=\"MathJax-Span-333\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-334\"><span class=\"mrow\" id=\"MathJax-Span-335\"><span class=\"mi\" id=\"MathJax-Span-336\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 2.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>MRL</mtext></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>∈</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>768</mn><mo>,</mo><mn>1536</mn><mo>,</mo><mn>3072</mn><mo fence=\"false\" stretchy=\"false\">}</mo></mrow></munder><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msup></math></span></span></div>\n<p>This forces the model to distribute semantic information across the vector in a layered way, like a nested set of dolls (hence “Matryoshka”).</p>\n<h4 id=\"intuition-1\">Intuition</h4>\n<ul>\n  <li><strong>Efficiency:</strong> At inference time, the same trained model can output 768-, 1536-, or 3072-dim embeddings by truncating.</li>\n  <li><strong>Flexibility:</strong> Users can trade accuracy for speed depending on their application, without retraining.</li>\n  <li><strong>Representation sharing:</strong> Lower-dimensional embeddings act as compressed summaries, while higher dimensions capture finer distinctions.</li>\n</ul>",
    "contentMarkdown": "A challenge in embedding models is that different applications have different requirements for embedding dimensionality.\n\n*   **High-dimensional embeddings** (e.g., 3072-dim) capture more information and perform better on fine-grained semantic tasks, but they are computationally expensive to store, index, and compare.\n*   **Low-dimensional embeddings** (e.g., 768-dim) are cheaper and faster, but may sacrifice accuracy.\n\nNormally, one would train and maintain multiple models for different embedding sizes. Gemini Embedding avoids this duplication by using **Matryoshka Representation Learning (MRL)**, where a single model can serve multiple embedding sizes at once.\n\n#### How MRL Works\n\nThe idea is to train the embedding space such that **prefix slices** of the full embedding vector are themselves valid embeddings.\n\n*   Suppose the model outputs a full embedding $E \\\\in \\\\mathbb{R}^{3072}$.\n*   With MRL, the first 768 dimensions $E\\_{1:768}$ should already be a strong embedding; likewise the first 1536 dimensions $E\\_{1:1536}$.\n*   This is achieved by adding **multiple contrastive losses** during training, one per prefix.\n\nFormally, if $\\\\mathcal{L}^{d}$ denotes the contrastive loss computed on the first $d$ dimensions of the embedding, then the total loss is:\n\nMRL\\=∑d∈{768,1536,3072}dLMRL\\=∑d∈{768,1536,3072}Ld\n\nThis forces the model to distribute semantic information across the vector in a layered way, like a nested set of dolls (hence “Matryoshka”).\n\n#### Intuition\n\n*   **Efficiency:** At inference time, the same trained model can output 768-, 1536-, or 3072-dim embeddings by truncating.\n*   **Flexibility:** Users can trade accuracy for speed depending on their application, without retraining.\n*   **Representation sharing:** Lower-dimensional embeddings act as compressed summaries, while higher dimensions capture finer distinctions.",
    "contentLength": 8245,
    "wordCount": 251,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#multi-resolution-training-for-multiple-output-dimensions-(mrl)"
  },
  {
    "id": "ai-GeminiEmbedding-two-stage-training-recipe-6",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Two-stage Training Recipe",
    "order": 6,
    "orderInChapter": 6,
    "contentHtml": "<p>Gemini Embedding training is not a single pass. Instead, it follows a <strong>two-stage curriculum</strong>: pre-finetuning followed by finetuning, with an additional ensemble-like averaging step called “model soup.”</p>\n<h4 id=\"1-pre-finetuning\">1. Pre-finetuning</h4>\n<ul>\n  <li><strong>Purpose:</strong> Adapt the Gemini language model (trained for autoregressive generation) into an encoder suitable for embeddings.</li>\n  <li><strong>Data:</strong> A very large, noisy collection of (query, target) pairs. These are not carefully curated but provide broad coverage.</li>\n  <li><strong>Batch size:</strong> Very large. Larger batches help stabilize the contrastive loss, since each batch provides many in-batch negatives.</li>\n  <li><strong>Negatives:</strong> Hard negatives are <strong>not</strong> used at this stage; the model relies on random in-batch negatives.</li>\n  <li><strong>Duration:</strong> This stage is run for significantly longer than finetuning, allowing the model to acquire a general embedding capability.</li>\n</ul>\n<p>Think of this as <strong>roughly sculpting the embedding space</strong>: not perfect, but shaped enough to separate broadly correct vs. incorrect matches.</p>\n<h4 id=\"2-finetuning\">2. Finetuning</h4>\n<ul>\n  <li><strong>Purpose:</strong> Refine the pre-finetuned encoder on carefully chosen datasets to maximize performance on downstream tasks.</li>\n  <li>\n    <p><strong>Mixture composition:</strong> Designed to balance</p>\n\n    <ul>\n      <li><strong>task diversity</strong> (e.g., retrieval, classification, clustering),</li>\n      <li><strong>language diversity</strong> (over 250 languages), and</li>\n      <li><strong>code retrieval</strong> (so embeddings work well on source code).</li>\n    </ul>\n  </li>\n  <li><strong>Batch size:</strong> Smaller than pre-finetuning. Each batch is drawn from a <strong>single dataset</strong>, which sharpens the contrastive signal by forcing the model to discriminate examples within that domain.</li>\n  <li><strong>Hyperparameters:</strong> Learning rates, mixture weights, and other factors are tuned systematically (e.g., grid search).</li>\n  <li><strong>Outputs:</strong> Multiple candidate checkpoints, each representing a different mixture/hyperparameter setting.</li>\n</ul>\n<p><strong>Mixture composition:</strong> Designed to balance</p>\n<ul>\n      <li><strong>task diversity</strong> (e.g., retrieval, classification, clustering),</li>\n      <li><strong>language diversity</strong> (over 250 languages), and</li>\n      <li><strong>code retrieval</strong> (so embeddings work well on source code).</li>\n    </ul>\n<p>This stage is like <strong>polishing the embedding space</strong>: ensuring the model captures nuanced semantic differences across tasks and languages.</p>\n<h4 id=\"3-model-soup\">3. Model Soup</h4>\n<p>After finetuning, the team doesn’t just pick one checkpoint. Instead, they apply a <strong>parameter averaging strategy</strong> known as “model soup”:</p>\n<ul>\n  <li>Multiple finetuned checkpoints (from different runs, hyperparameters, or datasets) are combined by averaging their weights.</li>\n  <li>Averaging can be uniform (simple mean) or weighted (emphasizing stronger runs).</li>\n  <li>The result is a model that often generalizes better than any individual checkpoint, smoothing out idiosyncrasies from specific runs.</li>\n</ul>\n<p>This final step can be thought of as <strong>ensembling without extra cost</strong>: a single averaged model retains much of the diversity of multiple fine-tuned ones.</p>",
    "contentMarkdown": "Gemini Embedding training is not a single pass. Instead, it follows a **two-stage curriculum**: pre-finetuning followed by finetuning, with an additional ensemble-like averaging step called “model soup.”\n\n#### 1\\. Pre-finetuning\n\n*   **Purpose:** Adapt the Gemini language model (trained for autoregressive generation) into an encoder suitable for embeddings.\n*   **Data:** A very large, noisy collection of (query, target) pairs. These are not carefully curated but provide broad coverage.\n*   **Batch size:** Very large. Larger batches help stabilize the contrastive loss, since each batch provides many in-batch negatives.\n*   **Negatives:** Hard negatives are **not** used at this stage; the model relies on random in-batch negatives.\n*   **Duration:** This stage is run for significantly longer than finetuning, allowing the model to acquire a general embedding capability.\n\nThink of this as **roughly sculpting the embedding space**: not perfect, but shaped enough to separate broadly correct vs. incorrect matches.\n\n#### 2\\. Finetuning\n\n*   **Purpose:** Refine the pre-finetuned encoder on carefully chosen datasets to maximize performance on downstream tasks.\n*   **Mixture composition:** Designed to balance\n    \n    *   **task diversity** (e.g., retrieval, classification, clustering),\n    *   **language diversity** (over 250 languages), and\n    *   **code retrieval** (so embeddings work well on source code).\n*   **Batch size:** Smaller than pre-finetuning. Each batch is drawn from a **single dataset**, which sharpens the contrastive signal by forcing the model to discriminate examples within that domain.\n*   **Hyperparameters:** Learning rates, mixture weights, and other factors are tuned systematically (e.g., grid search).\n*   **Outputs:** Multiple candidate checkpoints, each representing a different mixture/hyperparameter setting.\n\n**Mixture composition:** Designed to balance\n\n*   **task diversity** (e.g., retrieval, classification, clustering),\n*   **language diversity** (over 250 languages), and\n*   **code retrieval** (so embeddings work well on source code).\n\nThis stage is like **polishing the embedding space**: ensuring the model captures nuanced semantic differences across tasks and languages.\n\n#### 3\\. Model Soup\n\nAfter finetuning, the team doesn’t just pick one checkpoint. Instead, they apply a **parameter averaging strategy** known as “model soup”:\n\n*   Multiple finetuned checkpoints (from different runs, hyperparameters, or datasets) are combined by averaging their weights.\n*   Averaging can be uniform (simple mean) or weighted (emphasizing stronger runs).\n*   The result is a model that often generalizes better than any individual checkpoint, smoothing out idiosyncrasies from specific runs.\n\nThis final step can be thought of as **ensembling without extra cost**: a single averaged model retains much of the diversity of multiple fine-tuned ones.",
    "contentLength": 3528,
    "wordCount": 393,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#two-stage-training-recipe"
  },
  {
    "id": "ai-GeminiEmbedding-data-pipeline-enhancements-using-gemini-7",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Data Pipeline Enhancements Using Gemini",
    "order": 7,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Synthetic data generation</strong>\nAdditional retrieval and classification data are generated with few-shot prompting. For retrieval, the paper extends prior synthetic-query pipelines (e.g., FRet and SWIM-IR adaptations) to generate queries for web passages, followed by an automatic rating step to filter low-quality generations. For classification, synthetic datasets (e.g., counterfactual, sentiment, reviews) are produced. Multi-stage prompting (e.g., conditioning on synthetic user/product/movie metadata and sampling from longer candidate lists) is used to improve realism and diversity.</p>\n  </li>\n  <li>\n    <p><strong>LLM-based data filtering</strong>\nHuman-annotated retrieval datasets can contain incorrect positives/negatives. The paper uses Gemini to score and filter out low-quality examples via few-shot prompting for data quality assessment.</p>\n  </li>\n  <li>\n    <p><strong>Hard negative mining</strong>\nA Gemini-initialized embedding model is first trained without hard negatives. For each query, a set of nearest-neighbor candidates is retrieved. Each candidate is then scored with two prompting strategies (graded classification and query likelihood), and the scores are fused using Reciprocal Rank Fusion (RRF). The lowest-scoring neighbor among these fused ranks is selected as the hard negative. The paper observes that adding some hard negatives improves retrieval metrics, but adding too many leads to overfitting.</p>\n  </li>\n</ul>\n<p><strong>Synthetic data generation</strong>\nAdditional retrieval and classification data are generated with few-shot prompting. For retrieval, the paper extends prior synthetic-query pipelines (e.g., FRet and SWIM-IR adaptations) to generate queries for web passages, followed by an automatic rating step to filter low-quality generations. For classification, synthetic datasets (e.g., counterfactual, sentiment, reviews) are produced. Multi-stage prompting (e.g., conditioning on synthetic user/product/movie metadata and sampling from longer candidate lists) is used to improve realism and diversity.</p>\n<p><strong>LLM-based data filtering</strong>\nHuman-annotated retrieval datasets can contain incorrect positives/negatives. The paper uses Gemini to score and filter out low-quality examples via few-shot prompting for data quality assessment.</p>\n<p><strong>Hard negative mining</strong>\nA Gemini-initialized embedding model is first trained without hard negatives. For each query, a set of nearest-neighbor candidates is retrieved. Each candidate is then scored with two prompting strategies (graded classification and query likelihood), and the scores are fused using Reciprocal Rank Fusion (RRF). The lowest-scoring neighbor among these fused ranks is selected as the hard negative. The paper observes that adding some hard negatives improves retrieval metrics, but adding too many leads to overfitting.</p>",
    "contentMarkdown": "*   **Synthetic data generation** Additional retrieval and classification data are generated with few-shot prompting. For retrieval, the paper extends prior synthetic-query pipelines (e.g., FRet and SWIM-IR adaptations) to generate queries for web passages, followed by an automatic rating step to filter low-quality generations. For classification, synthetic datasets (e.g., counterfactual, sentiment, reviews) are produced. Multi-stage prompting (e.g., conditioning on synthetic user/product/movie metadata and sampling from longer candidate lists) is used to improve realism and diversity.\n    \n*   **LLM-based data filtering** Human-annotated retrieval datasets can contain incorrect positives/negatives. The paper uses Gemini to score and filter out low-quality examples via few-shot prompting for data quality assessment.\n    \n*   **Hard negative mining** A Gemini-initialized embedding model is first trained without hard negatives. For each query, a set of nearest-neighbor candidates is retrieved. Each candidate is then scored with two prompting strategies (graded classification and query likelihood), and the scores are fused using Reciprocal Rank Fusion (RRF). The lowest-scoring neighbor among these fused ranks is selected as the hard negative. The paper observes that adding some hard negatives improves retrieval metrics, but adding too many leads to overfitting.\n    \n\n**Synthetic data generation** Additional retrieval and classification data are generated with few-shot prompting. For retrieval, the paper extends prior synthetic-query pipelines (e.g., FRet and SWIM-IR adaptations) to generate queries for web passages, followed by an automatic rating step to filter low-quality generations. For classification, synthetic datasets (e.g., counterfactual, sentiment, reviews) are produced. Multi-stage prompting (e.g., conditioning on synthetic user/product/movie metadata and sampling from longer candidate lists) is used to improve realism and diversity.\n\n**LLM-based data filtering** Human-annotated retrieval datasets can contain incorrect positives/negatives. The paper uses Gemini to score and filter out low-quality examples via few-shot prompting for data quality assessment.\n\n**Hard negative mining** A Gemini-initialized embedding model is first trained without hard negatives. For each query, a set of nearest-neighbor candidates is retrieved. Each candidate is then scored with two prompting strategies (graded classification and query likelihood), and the scores are fused using Reciprocal Rank Fusion (RRF). The lowest-scoring neighbor among these fused ranks is selected as the hard negative. The paper observes that adding some hard negatives improves retrieval metrics, but adding too many leads to overfitting.",
    "contentLength": 2905,
    "wordCount": 361,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#data-pipeline-enhancements-using-gemini"
  },
  {
    "id": "ai-GeminiEmbedding-evaluation-setup-and-findings-paper-summary-8",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Evaluation Setup and Findings (paper Summary)",
    "order": 8,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>The model is evaluated across Massive Multilingual Text Embedding Benchmark (MMTEB), including multilingual, English, and code tracks; and on cross-lingual retrieval benchmarks XOR-Retrieve and XTREME-UP.</li>\n  <li>The paper reports state-of-the-art aggregate performance on MTEB(Multilingual) at the time of submission, and strong results on MTEB(Eng, v2), MTEB(Code), and cross-lingual retrieval.</li>\n  <li>The ablations show: pre-finetuning materially improves results; mixtures emphasizing task diversity are especially important; multilingual finetuning most helps long-tail languages; filtering MIRACL training data improves retrieval in many languages; and hard-negative counts should be moderated to avoid overfitting.</li>\n</ul>",
    "contentMarkdown": "*   The model is evaluated across Massive Multilingual Text Embedding Benchmark (MMTEB), including multilingual, English, and code tracks; and on cross-lingual retrieval benchmarks XOR-Retrieve and XTREME-UP.\n*   The paper reports state-of-the-art aggregate performance on MTEB(Multilingual) at the time of submission, and strong results on MTEB(Eng, v2), MTEB(Code), and cross-lingual retrieval.\n*   The ablations show: pre-finetuning materially improves results; mixtures emphasizing task diversity are especially important; multilingual finetuning most helps long-tail languages; filtering MIRACL training data improves retrieval in many languages; and hard-negative counts should be moderated to avoid overfitting.",
    "contentLength": 750,
    "wordCount": 89,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#evaluation-setup-and-findings-(paper-summary)"
  },
  {
    "id": "ai-GeminiEmbedding-practical-shape-of-the-model-interface-as-describe-9",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Practical Shape of the Model Interface (as Described)",
    "order": 9,
    "orderInChapter": 9,
    "contentHtml": "<ul>\n  <li><strong>Inputs</strong>\nText sequences with an optional task string prefix (t) can be encoded.</li>\n  <li><strong>Encoder</strong>\nBidirectional transformer (M) (initialized from Gemini).</li>\n  <li><strong>Pooling</strong>\nMean pooling over tokens.</li>\n  <li><strong>Projection</strong>\nLinear map $f:\\mathbb{R}^{d_M} \\to \\mathbb{R}^{d}$ with $d \\in {768,1536,3072}$ supported via MRL.</li>\n  <li><strong>Similarity</strong>\nCosine similarity $\\text{sim}(x,y)=\\frac{x^\\top y}{\\lVert x\\rVert \\lVert y\\rVert}$.</li>\n  <li><strong>Loss</strong>\nContrastive NCE with in-batch negatives, an optional explicit hard negative, masking for classification-style batches, and temperature $\\tau$.</li>\n  <li><strong>Training recipe</strong>\nPre-finetuning → finetuning mixtures → model soup.</li>\n</ul>",
    "contentMarkdown": "*   **Inputs** Text sequences with an optional task string prefix (t) can be encoded.\n*   **Encoder** Bidirectional transformer (M) (initialized from Gemini).\n*   **Pooling** Mean pooling over tokens.\n*   **Projection** Linear map $f:\\\\mathbb{R}^{d\\_M} \\\\to \\\\mathbb{R}^{d}$ with $d \\\\in {768,1536,3072}$ supported via MRL.\n*   **Similarity** Cosine similarity $\\\\text{sim}(x,y)=\\\\frac{x^\\\\top y}{\\\\lVert x\\\\rVert \\\\lVert y\\\\rVert}$.\n*   **Loss** Contrastive NCE with in-batch negatives, an optional explicit hard negative, masking for classification-style batches, and temperature $\\\\tau$.\n*   **Training recipe** Pre-finetuning → finetuning mixtures → model soup.",
    "contentLength": 803,
    "wordCount": 80,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#practical-shape-of-the-model-interface-(as-described)"
  },
  {
    "id": "ai-GeminiEmbedding-full-loss-written-end-to-end-with-temperature-and--10",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Full Loss Written End-to-end (with Temperature and Masking)",
    "order": 10,
    "orderInChapter": 10,
    "contentHtml": "<p>Let:</p>\n<ul>\n  <li>$S_i^+ = \\exp(\\text{sim}(q_i,p_i^{+})/\\tau)$</li>\n  <li>$S_i^- = \\mathbf{1}_{{\\text{hard neg present}}}\\exp(\\text{sim}(q_i,p_i^{-})/\\tau)$</li>\n  <li>$S_{ij} = \\text{mask}(i,j)\\exp(\\text{sim}(q_i,p_j^{+})/\\tau)$</li>\n</ul>\n<p>Then</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></munderover><mrow><mo>[</mo><mrow><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mfrac><msubsup><mi>S</mi><mi>i</mi><mo>+</mo></msubsup><mrow><msubsup><mi>S</mi><mi>i</mi><mo>&amp;#x2212;</mo></msubsup><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></munderover><msub><mi>S</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-337\" style=\"width: 16.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.065em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1013.75em, 5.628em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-338\"><span class=\"texatom\" id=\"MathJax-Span-339\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-343\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-344\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.31em;\"><span class=\"mi\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Italic;\">B</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-346\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-347\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-348\"><span class=\"mrow\" id=\"MathJax-Span-349\"><span class=\"mi\" id=\"MathJax-Span-350\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-351\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-352\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.42em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-353\"><span class=\"mrow\" id=\"MathJax-Span-354\"><span class=\"mi\" id=\"MathJax-Span-355\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-356\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-357\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-358\"><span class=\"mo\" id=\"MathJax-Span-359\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-361\"></span><span class=\"mfrac\" id=\"MathJax-Span-362\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 5.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.076em, 1001.1em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -0.57em;\"><span class=\"msubsup\" id=\"MathJax-Span-363\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.221em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-365\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1005.52em, 4.638em, -999.997em); top: -3.07em; left: 50%; margin-left: -2.758em;\"><span class=\"mrow\" id=\"MathJax-Span-367\"><span class=\"msubsup\" id=\"MathJax-Span-368\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.273em, -999.997em); top: -4.477em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-370\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-372\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-373\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-374\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-375\"><span class=\"mrow\" id=\"MathJax-Span-376\"><span class=\"mi\" id=\"MathJax-Span-377\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.326em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-378\"><span class=\"mrow\" id=\"MathJax-Span-379\"><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-381\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-382\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-383\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-385\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"mi\" id=\"MathJax-Span-387\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-389\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>B</mi></mrow></munderover><mrow><mo>[</mo><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mfrac><msubsup><mi>S</mi><mi>i</mi><mo>+</mo></msubsup><mrow><msubsup><mi>S</mi><mi>i</mi><mo>−</mo></msubsup><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>B</mi></mrow></munderover><msub><mi>S</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><mo>]</mo></mrow></math></span></span></div>\n<p>Where:</p>\n<ul>\n  <li>$\\mathcal{L}$ is the total batch loss</li>\n  <li>$S_i^+$ is the similarity with the positive example</li>\n  <li>$S_i^-$ is the similarity with the hard negative (if present)</li>\n  <li>$S_{ij}$ is the similarity with other examples in the batch</li>\n</ul>\n<p>With MRL, this loss is computed on several prefix slices of the embedding and summed.</p>",
    "contentMarkdown": "Let:\n\n*   $S\\_i^+ = \\\\exp(\\\\text{sim}(q\\_i,p\\_i^{+})/\\\\tau)$\n*   $S\\_i^- = \\\\mathbf{1}\\_{{\\\\text{hard neg present}}}\\\\exp(\\\\text{sim}(q\\_i,p\\_i^{-})/\\\\tau)$\n*   $S\\_{ij} = \\\\text{mask}(i,j)\\\\exp(\\\\text{sim}(q\\_i,p\\_j^{+})/\\\\tau)$\n\nThen\n\n\\=1B∑i\\=1B\\[−logS+iS−i+∑Bj\\=1Sij\\]L\\=1B∑i\\=1B\\[−log⁡Si+Si−+∑j\\=1BSij\\]\n\nWhere:\n\n*   $\\\\mathcal{L}$ is the total batch loss\n*   $S\\_i^+$ is the similarity with the positive example\n*   $S\\_i^-$ is the similarity with the hard negative (if present)\n*   $S\\_{ij}$ is the similarity with other examples in the batch\n\nWith MRL, this loss is computed on several prefix slices of the embedding and summed.",
    "contentLength": 12979,
    "wordCount": 71,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#full-loss-written-end-to-end-(with-temperature-and-masking)"
  },
  {
    "id": "ai-GeminiEmbedding-closing-summary-11",
    "articleSlug": "GeminiEmbedding",
    "articleTitle": "Gemini Embedding",
    "category": "Models",
    "chapter": "Gemini Embedding",
    "title": "Closing Summary",
    "order": 11,
    "orderInChapter": 11,
    "contentHtml": "<p>The paper’s core design can be read succinctly as: Gemini-initialized bidirectional transformer encoder → mean pooling → linear projection; trained with a contrastive NCE loss using in-batch negatives plus carefully mined hard negatives; multi-resolution losses to support multiple output sizes in one model; two-stage training and model-soup averaging; and data quality improvements via synthetic generation, LLM filtering, and RRF-based hard-negative mining. The reported evaluations show strong multilingual, English, code, and cross-lingual retrieval performance under this recipe.</p>",
    "contentMarkdown": "The paper’s core design can be read succinctly as: Gemini-initialized bidirectional transformer encoder → mean pooling → linear projection; trained with a contrastive NCE loss using in-batch negatives plus carefully mined hard negatives; multi-resolution losses to support multiple output sizes in one model; two-stage training and model-soup averaging; and data quality improvements via synthetic generation, LLM filtering, and RRF-based hard-negative mining. The reported evaluations show strong multilingual, English, code, and cross-lingual retrieval performance under this recipe.",
    "contentLength": 592,
    "wordCount": 76,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/GeminiEmbedding/#closing-summary"
  }
]