[
  {
    "id": "ai-activation-leaky-relu-1",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation",
    "chapter": "Rectified Linear Unit (ReLU):",
    "title": "Leaky ReLU:",
    "subtitle": "Rectified Linear Unit (ReLU):",
    "contentHtml": "<ul>\n  <li>Leaky ReLU is a variant of ReLU that introduces a small, non-zero slope for negative inputs. This prevents complete saturation of negative values and is useful in scenarios where sparse gradients may occur, such as training generative adversarial networks (GANs).</li>\n  <li>It is defined as max(αx, x), where x is the input and α is a small positive constant.</li>\n  <li>“Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training.</li>\n  <li>This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks.”<a href=\"https://paperswithcode.com/method/leaky-relu\">Source</a></li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B1;</mo></mrow><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 14.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.77em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">m</span><span class=\"mi\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-111\"><span class=\"mrow\" id=\"MathJax-Span-112\"><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular;\">α</span></span></span><span class=\"mi\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>α</mo></mrow><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<p><img src=\"/primers/ai/assets/activation/8.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Leaky ReLU is a variant of ReLU that introduces a small, non-zero slope for negative inputs. This prevents complete saturation of negative values and is useful in scenarios where sparse gradients may occur, such as training generative adversarial networks (GANs).\n*   It is defined as max(αx, x), where x is the input and α is a small positive constant.\n*   “Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training.\n*   This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks.”[Source](https://paperswithcode.com/method/leaky-relu)\n\nLeakyReLU(x)\\=max(αx,x)LeakyReLU(x)\\=max(αx,x)\n\n![](/primers/ai/assets/activation/8.png)",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 131,
      "contentLength": 5571
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation/#leaky-relu:",
    "scrapedAt": "2025-12-28T11:49:07.299Z"
  }
]