[
  {
    "id": "ai-activation-functions-sigmoid-1",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "Sigmoid",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<ul>\n  <li>The Sigmoid function is used for binary classification. It squashes a vector in the range (0, 1). It is applied independently to each element of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">s</script>. It is also called the logistic function (since it is used in logistic regression for binary classification).</li>\n</ul>\n<p><img src=\"/primers/ai/assets/activation-functions/sigmoid.png\" alt=\"\"></p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 7.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1006.51em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-8\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-10\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-15\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.284em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.18em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mn\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-20\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-25\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-27\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.28em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.284em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow></msup></mrow></mfrac></math></span></span></div>\n<h4 id=\"pros\">Pros</h4>\n<ul>\n  <li>Utilized in binary classification.</li>\n  <li>Offers an output that can be interpreted as a probability value since it is non-negative and in the range (0, 1).</li>\n</ul>",
    "contentMarkdown": "*   The Sigmoid function is used for binary classification. It squashes a vector in the range (0, 1). It is applied independently to each element of sss. It is also called the logistic function (since it is used in logistic regression for binary classification).\n\n![](/primers/ai/assets/activation-functions/sigmoid.png)\n\nf(si)\\=11+e−sif(si)\\=11+e−si\n\n#### Pros\n\n*   Utilized in binary classification.\n*   Offers an output that can be interpreted as a probability value since it is non-negative and in the range (0, 1).",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 75,
      "contentLength": 7684
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#sigmoid",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-softmax-2",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "Softmax",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<ul>\n  <li>The Softmax function is a generalization of the sigmoid function for multi-class classification. In other words, use sigmoid for binary classification and softmax for multiclass classification.\nSoftmax is a function, not a loss. It squashes a vector in the range (0, 1) and all the resulting elements sum up to 1. It is applied to the output scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-30\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">s</script>. As elements represent a class, they can be interpreted as class probabilities.</li>\n  <li>\n    <p>The Softmax function cannot be applied independently to each <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"msubsup\" id=\"MathJax-Span-35\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">s_i</script>, since it depends on all elements of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;bold-italic&quot;>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-38\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1000.32em, 2.659em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-39\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\boldsymbol{s}</script>. For a given class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-41\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"msubsup\" id=\"MathJax-Span-43\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">s_i</script>, the Softmax function can be computed as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><msub><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>s</mi><mi>i</mi></msub></mrow></msup><mrow><munderover><mo>&amp;#x2211;</mo><mi>j</mi><mi>C</mi></munderover><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>s</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-46\" style=\"width: 7.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1006.04em, 3.701em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-47\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"msubsup\" id=\"MathJax-Span-51\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-55\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-58\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"msubsup\" id=\"MathJax-Span-60\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.919em, 1002.66em, 4.638em, -999.997em); top: -3.07em; left: 50%; margin-left: -1.352em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"munderover\" id=\"MathJax-Span-64\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.326em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"mi\" id=\"MathJax-Span-67\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-68\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-70\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"msubsup\" id=\"MathJax-Span-72\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.82em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.815em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>s</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>s</mi><mi>i</mi></msub></mrow></msup><mrow><munderover><mo>∑</mo><mi>j</mi><mi>C</mi></munderover><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>s</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-7\">f(s)_i=\\frac{e^{s_i}}{\\sum_j^C e^{s_j}}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-75\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"msubsup\" id=\"MathJax-Span-77\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">s_j</script> are the scores inferred by the net for each class in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-80\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-81\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">C</script>. Note that the Softmax activation for a class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"msubsup\" id=\"MathJax-Span-85\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">s_i</script> depends on all the scores in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-88\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-89\"><span class=\"mi\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">s</script>.</li>\n    </ul>\n  </li>\n  <li>Activation functions are used to transform vectors before computing the loss in the training phase. In testing, when the loss is no longer applied, activation functions are also used to get the CNN outputs.</li>\n</ul>\n<p>The Softmax function cannot be applied independently to each <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"msubsup\" id=\"MathJax-Span-35\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">s_i</script>, since it depends on all elements of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;bold-italic&quot;>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-38\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1000.32em, 2.659em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-39\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\boldsymbol{s}</script>. For a given class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-41\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"msubsup\" id=\"MathJax-Span-43\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">s_i</script>, the Softmax function can be computed as:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-75\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"msubsup\" id=\"MathJax-Span-77\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">s_j</script> are the scores inferred by the net for each class in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-80\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-81\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">C</script>. Note that the Softmax activation for a class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-83\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"msubsup\" id=\"MathJax-Span-85\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">s_i</script> depends on all the scores in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-88\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-89\"><span class=\"mi\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">s</script>.</li>\n    </ul>\n<h4 id=\"pros-1\">Pros</h4>\n<ul>\n  <li>Utilized in multi-class classification.</li>\n</ul>",
    "contentMarkdown": "*   The Softmax function is a generalization of the sigmoid function for multi-class classification. In other words, use sigmoid for binary classification and softmax for multiclass classification. Softmax is a function, not a loss. It squashes a vector in the range (0, 1) and all the resulting elements sum up to 1. It is applied to the output scores sss. As elements represent a class, they can be interpreted as class probabilities.\n*   The Softmax function cannot be applied independently to each sisis\\_i, since it depends on all elements of ss\\\\boldsymbol{s}. For a given class sisis\\_i, the Softmax function can be computed as:\n    \n    f(s)i\\=esi∑Cjesjf(s)i\\=esi∑jCesj\n    \n    f(s)\\_i=\\\\frac{e^{s\\_i}}{\\\\sum\\_j^C e^{s\\_j}}\n    *   where sjsjs\\_j are the scores inferred by the net for each class in CCC. Note that the Softmax activation for a class sisis\\_i depends on all the scores in sss.\n*   Activation functions are used to transform vectors before computing the loss in the training phase. In testing, when the loss is no longer applied, activation functions are also used to get the CNN outputs.\n\nThe Softmax function cannot be applied independently to each sisis\\_i, since it depends on all elements of ss\\\\boldsymbol{s}. For a given class sisis\\_i, the Softmax function can be computed as:\n\n*   where sjsjs\\_j are the scores inferred by the net for each class in CCC. Note that the Softmax activation for a class sisis\\_i depends on all the scores in sss.\n\n#### Pros\n\n*   Utilized in multi-class classification.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "cnn",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 240,
      "contentLength": 32990
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#softmax",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-relu-rectified-linear-unit-3",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "ReLU (Rectified Linear Unit)",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<h4 id=\"pros-2\">Pros</h4>\n<ul>\n  <li>Due to sparsity, there is less time and space complexity compared to the sigmoid.</li>\n  <li>Avoids the vanishing gradient problem.</li>\n</ul>\n<h4 id=\"cons\">Cons</h4>\n<ul>\n  <li>Introduces the concept of the “dead ReLU problem,” which refers to network elements that are probably never updated with new values. This can also cause issues from time to time. In a way, this is also an advantage.</li>\n  <li>Does not avoid the exploding gradient problem.</li>\n</ul>",
    "contentMarkdown": "#### Pros\n\n*   Due to sparsity, there is less time and space complexity compared to the sigmoid.\n*   Avoids the vanishing gradient problem.\n\n#### Cons\n\n*   Introduces the concept of the “dead ReLU problem,” which refers to network elements that are probably never updated with new values. This can also cause issues from time to time. In a way, this is also an advantage.\n*   Does not avoid the exploding gradient problem.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 72,
      "contentLength": 499
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#relu-(rectified-linear-unit)",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-elu-exponential-linear-unit-4",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "ELU (Exponential Linear Unit)",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<h4 id=\"pros-3\">Pros</h4>\n<ul>\n  <li>Avoids the dead ReLU problem.</li>\n  <li>Enables the network to nudge weights and biases in the desired directions by producing negative outputs.</li>\n  <li>When calculating the gradient, create activations rather than having them be zero.</li>\n</ul>\n<h4 id=\"cons-1\">Cons</h4>\n<ul>\n  <li>Increases computing time due to the use of an exponential operation.</li>\n  <li>Does not avoid the exploding gradient problem.</li>\n  <li>The alpha value is not learned by the neural network.</li>\n</ul>",
    "contentMarkdown": "#### Pros\n\n*   Avoids the dead ReLU problem.\n*   Enables the network to nudge weights and biases in the desired directions by producing negative outputs.\n*   When calculating the gradient, create activations rather than having them be zero.\n\n#### Cons\n\n*   Increases computing time due to the use of an exponential operation.\n*   Does not avoid the exploding gradient problem.\n*   The alpha value is not learned by the neural network.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "neural network",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 71,
      "contentLength": 527
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#elu-(exponential-linear-unit)",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-leaky-relu-leaky-rectified-linear-unit-5",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "Leaky ReLU (Leaky Rectified Linear Unit)",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<h4 id=\"pros-4\">Pros</h4>\n<ul>\n  <li>Since we allow a tiny gradient when computing the derivative, we avoid the dead ReLU problem, just like ELU.</li>\n  <li>Faster to compute than ELU, because no exponential operation is included.</li>\n</ul>\n<h4 id=\"cons-2\">Cons</h4>\n<ul>\n  <li>Does not avoid the exploding gradient problem.</li>\n  <li>The alpha value is not learned by the neural network.</li>\n  <li>When differentiated, it becomes a linear function, whereas ELU is partially linear and partially nonlinear.</li>\n</ul>",
    "contentMarkdown": "#### Pros\n\n*   Since we allow a tiny gradient when computing the derivative, we avoid the dead ReLU problem, just like ELU.\n*   Faster to compute than ELU, because no exponential operation is included.\n\n#### Cons\n\n*   Does not avoid the exploding gradient problem.\n*   The alpha value is not learned by the neural network.\n*   When differentiated, it becomes a linear function, whereas ELU is partially linear and partially nonlinear.",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "neural network"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 71,
      "contentLength": 520
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#leaky-relu-(leaky-rectified-linear-unit)",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-selu-scaled-exponential-linear-unit-6",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "SELU (Scaled Exponential Linear Unit)",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<ul>\n  <li>If utilized, keep in mind that the LeCun Normal weight initialization approach is necessary for the SELU function and that Alpha Dropout is a unique variant that must be used if dropout is desired.</li>\n</ul>\n<h4 id=\"pros-5\">Pros</h4>\n<ul>\n  <li>The SELU activation is self-normalizing, hence the neural network converges more quickly than external normalization.</li>\n  <li>Vanishing and exploding gradient problems are impossible.</li>\n</ul>\n<h4 id=\"cons-3\">Cons</h4>\n<ul>\n  <li>Works best for sequential network architectures. If your architecture has skipped connections, self-normalization will not be guaranteed, hence better performance is not guaranteed.</li>\n</ul>",
    "contentMarkdown": "*   If utilized, keep in mind that the LeCun Normal weight initialization approach is necessary for the SELU function and that Alpha Dropout is a unique variant that must be used if dropout is desired.\n\n#### Pros\n\n*   The SELU activation is self-normalizing, hence the neural network converges more quickly than external normalization.\n*   Vanishing and exploding gradient problems are impossible.\n\n#### Cons\n\n*   Works best for sequential network architectures. If your architecture has skipped connections, self-normalization will not be guaranteed, hence better performance is not guaranteed.",
    "order": 6,
    "orderInChapter": 6,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "neural network",
      "activation",
      "dropout"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 87,
      "contentLength": 684
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#selu-(scaled-exponential-linear-unit)",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-gelu-gaussian-error-linear-unit-7",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "GELU (Gaussian Error Linear Unit)",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<h4 id=\"pros-6\">Pros</h4>\n<ul>\n  <li>Appears to be cutting-edge in NLP, particularly in Transformer models.</li>\n  <li>Avoids vanishing gradient problem</li>\n</ul>\n<h4 id=\"cons-4\">Cons</h4>\n<ul>\n  <li>Fairly new in practical use, although introduced in 2016.</li>\n</ul>",
    "contentMarkdown": "#### Pros\n\n*   Appears to be cutting-edge in NLP, particularly in Transformer models.\n*   Avoids vanishing gradient problem\n\n#### Cons\n\n*   Fairly new in practical use, although introduced in 2016.",
    "order": 7,
    "orderInChapter": 7,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "transformer",
      "nlp"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 30,
      "contentLength": 269
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#gelu-(gaussian-error-linear-unit)",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  },
  {
    "id": "ai-activation-functions-citation-8",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Activation Functions",
    "articleSlug": "activation-functions",
    "chapter": "Types of Activation Functions",
    "title": "Citation",
    "subtitle": "Types of Activation Functions",
    "contentHtml": "<p>If you found our work useful, please cite it as:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">@article{Chadha2020DistilledActFunctions,\n  title   = {Activation Functions},\n  author  = {Chadha, Aman and Jain, Vinija},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">@article{Chadha2020DistilledActFunctions,\n  title   = {Activation Functions},\n  author  = {Chadha, Aman and Jain, Vinija},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n</code></pre>",
    "contentMarkdown": "If you found our work useful, please cite it as:\n\n![](https://aman.ai/images/copy.png)\n\n`@article{Chadha2020DistilledActFunctions,   title   = {Activation Functions},   author  = {Chadha, Aman and Jain, Vinija},   journal = {Distilled AI},   year    = {2020},   note    = {\\url{https://aman.ai}} }`\n\n![](https://aman.ai/images/copy.png)\n\n`@article{Chadha2020DistilledActFunctions,   title   = {Activation Functions},   author  = {Chadha, Aman and Jain, Vinija},   journal = {Distilled AI},   year    = {2020},   note    = {\\url{https://aman.ai}} }`",
    "order": 8,
    "orderInChapter": 8,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "activation"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 58,
      "contentLength": 1132
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/activation-functions/#citation",
    "scrapedAt": "2025-12-28T11:48:56.796Z"
  }
]