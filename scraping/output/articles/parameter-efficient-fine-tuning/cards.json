[
  {
    "id": "ai-parameter-efficient-fine-tuning-practical-use-case-1",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Advantages",
    "title": "Practical Use-case",
    "subtitle": "Advantages",
    "contentHtml": "<ul>\n  <li>Credits to the below section go to <a href=\"https://www.linkedin.com/in/pranaypasula/\">Pranay Pasula</a>.</li>\n  <li>PEFT obviates the need for 40 or 80GB A100s to make use of powerful LLMs. In other words, you can fine-tune 10B+ parameter LLMs for your desired task for free or on cheap consumer GPUs.</li>\n  <li>Using PEFT methods like <a href=\"#low-rank-adaptation-lora\">LoRA</a>, especially 4-bit quantized base models via <a href=\"#QLoRA\">QLoRA</a>, you can fine-tune 10B+ parameter LLMs that are 30-40GB in size on 16GB GPUs. If it’s out of your budget to buy a 16GB GPU/TPU, Google Colab occasionally offers a 16GB VRAM Tesla T4 for free. Remember to save your model checkpoints every now and then and reload them as necessary, in the event of a Colab disconnect/kernel crash.</li>\n  <li>If you’re fine-tuning on a single task, the base models are already so expressive that you need only a few (~10s-100s) of examples to perform well on this task. With PEFT via LoRA, you need to train only a trivial fraction (in this case, 0.08%), and though the weights are stored as 4-bit, computations are still done at 16-bit.</li>\n  <li>Note that while a good amount of VRAM is still needed for the fine-tuning process, using PEFT, with a small enough batch size, and little gradient accumulation, can do the trick while still retaining ‘<code class=\"language-plaintext highlighter-rouge\">FP16</code>’ computation. In some cases, the performance on the fine-tuned task can be comparable to that of a fine-tuned 16-bit model.</li>\n  <li>Key takeaway: You can fine-tune powerful LLMs to perform well on a desired task using free compute. Use a &lt;10B parameter model, which is still huge, and use quantization, PEFT, checkpointing, and provide a small training set, and you can quickly fine-tune this model for your use case.</li>\n</ul>",
    "contentMarkdown": "*   Credits to the below section go to [Pranay Pasula](https://www.linkedin.com/in/pranaypasula/).\n*   PEFT obviates the need for 40 or 80GB A100s to make use of powerful LLMs. In other words, you can fine-tune 10B+ parameter LLMs for your desired task for free or on cheap consumer GPUs.\n*   Using PEFT methods like [LoRA](#low-rank-adaptation-lora), especially 4-bit quantized base models via [QLoRA](#QLoRA), you can fine-tune 10B+ parameter LLMs that are 30-40GB in size on 16GB GPUs. If it’s out of your budget to buy a 16GB GPU/TPU, Google Colab occasionally offers a 16GB VRAM Tesla T4 for free. Remember to save your model checkpoints every now and then and reload them as necessary, in the event of a Colab disconnect/kernel crash.\n*   If you’re fine-tuning on a single task, the base models are already so expressive that you need only a few (~10s-100s) of examples to perform well on this task. With PEFT via LoRA, you need to train only a trivial fraction (in this case, 0.08%), and though the weights are stored as 4-bit, computations are still done at 16-bit.\n*   Note that while a good amount of VRAM is still needed for the fine-tuning process, using PEFT, with a small enough batch size, and little gradient accumulation, can do the trick while still retaining ‘`FP16`’ computation. In some cases, the performance on the fine-tuned task can be comparable to that of a fine-tuned 16-bit model.\n*   Key takeaway: You can fine-tune powerful LLMs to perform well on a desired task using free compute. Use a <10B parameter model, which is still huge, and use quantization, PEFT, checkpointing, and provide a small training set, and you can quickly fine-tune this model for your use case.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 283,
      "contentLength": 1844
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#practical-use-case",
    "scrapedAt": "2025-12-28T11:49:57.381Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-prompt-modifications-2",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "PEFT Methods",
    "title": "Prompt Modifications",
    "subtitle": "PEFT Methods",
    "contentHtml": "<h4 id=\"soft-prompt-tuning\">Soft Prompt Tuning</h4>\n<ul>\n  <li>First introduced in the <a href=\"https://aclanthology.org/2021.emnlp-main.243.pdf\">The Power of Scale for Parameter-Efficient Prompt Tuning</a>; this paper by Lester et al. introduces a simple yet effective method called soft prompt tuning, which prepends a trainable tensor to the model’s input embeddings, essentially creating a soft prompt to condition frozen LLMs to perform specific downstream tasks. Unlike the discrete text prompts, soft prompts are learned through backpropagation and can be fine-tuned to incorporate signals from any number of labeled examples.</li>\n  <li>Soft prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pre-trained model.</li>\n  <li>The authors show that prompt tuning outperforms few-shot learning by a large margin, and becomes more competitive with scale.</li>\n  <li>This is an interesting approach that can help to effectively use a single frozen model for multi-task serving.</li>\n  <li>Model tuning requires making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude – assuming a prompt length of 5 tokens.</li>\n  <li>Thus, instead of using discrete text prompts, prompt tuning employs soft prompts. Soft prompts are learnable and conditioned through backpropagation, making them adaptable for specific tasks.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/PromptTuning.jpg\" alt=\"\"></p>\n<ul>\n  <li>Prompt Tuning offers many benefits such as:\n    <ul>\n      <li>Memory-Efficiency: Prompt tuning dramatically reduces memory requirements. For instance, while a T5 “XXL” model necessitates 11 billion parameters for each task-specific model, prompt-tuned models need a mere 20,480 parameters (assuming a prompt length of 5 tokens).</li>\n      <li>Versatility: Enables the use of a single frozen model for multi-task operations.</li>\n      <li>Performance: Outshines few-shot learning and becomes more competitive as the scale grows.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Memory-Efficiency: Prompt tuning dramatically reduces memory requirements. For instance, while a T5 “XXL” model necessitates 11 billion parameters for each task-specific model, prompt-tuned models need a mere 20,480 parameters (assuming a prompt length of 5 tokens).</li>\n      <li>Versatility: Enables the use of a single frozen model for multi-task operations.</li>\n      <li>Performance: Outshines few-shot learning and becomes more competitive as the scale grows.</li>\n    </ul>\n<h4 id=\"soft-prompt-vs-prompting\">Soft Prompt vs. Prompting</h4>\n<ul>\n  <li>Soft prompt tuning and prompting a model with extra context are both methods designed to guide a model’s behavior for specific tasks, but they operate in different ways. Here’s how they differ:</li>\n</ul>\n<ol>\n  <li><strong>Mechanism</strong>:\n    <ul>\n      <li><strong>Soft Prompt Tuning</strong>: This involves introducing trainable parameters (soft prompts) that are concatenated or added to the model’s input embeddings. These soft prompts are learned during the fine-tuning process and are adjusted through backpropagation to condition the model to produce desired outputs for specific tasks.</li>\n      <li><strong>Prompting with Extra Context</strong>: This method involves feeding the model with handcrafted or predefined text prompts that provide additional context. There’s no explicit fine-tuning; instead, the model leverages its pre-trained knowledge to produce outputs based on the provided context. This method is common in few-shot learning scenarios where the model is given a few examples as prompts and then asked to generalize to a new example.</li>\n    </ul>\n  </li>\n  <li><strong>Trainability</strong>:\n    <ul>\n      <li><strong>Soft Prompt Tuning</strong>: The soft prompts are trainable. They get adjusted during the fine-tuning process to optimize the model’s performance on the target task.</li>\n      <li><strong>Prompting with Extra Context</strong>: The prompts are static and not trainable. They’re designed (often manually) to give the model the necessary context for the desired task.</li>\n    </ul>\n  </li>\n  <li><strong>Use Case</strong>:\n    <ul>\n      <li><strong>Soft Prompt Tuning</strong>: This method is particularly useful when there’s a need to adapt a pre-trained model to various downstream tasks without adding significant computational overhead. Since the soft prompts are learned and optimized, they can capture nuanced information necessary for the task.</li>\n      <li><strong>Prompting with Extra Context</strong>: This is often used when fine-tuning isn’t feasible or when working with models in a zero-shot or few-shot setting. It’s a way to leverage the vast knowledge contained in large pre-trained models by just guiding their behavior with carefully crafted prompts.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li><strong>Soft Prompt Tuning</strong>: This involves introducing trainable parameters (soft prompts) that are concatenated or added to the model’s input embeddings. These soft prompts are learned during the fine-tuning process and are adjusted through backpropagation to condition the model to produce desired outputs for specific tasks.</li>\n      <li><strong>Prompting with Extra Context</strong>: This method involves feeding the model with handcrafted or predefined text prompts that provide additional context. There’s no explicit fine-tuning; instead, the model leverages its pre-trained knowledge to produce outputs based on the provided context. This method is common in few-shot learning scenarios where the model is given a few examples as prompts and then asked to generalize to a new example.</li>\n    </ul>\n<ul>\n      <li><strong>Soft Prompt Tuning</strong>: The soft prompts are trainable. They get adjusted during the fine-tuning process to optimize the model’s performance on the target task.</li>\n      <li><strong>Prompting with Extra Context</strong>: The prompts are static and not trainable. They’re designed (often manually) to give the model the necessary context for the desired task.</li>\n    </ul>\n<ul>\n      <li><strong>Soft Prompt Tuning</strong>: This method is particularly useful when there’s a need to adapt a pre-trained model to various downstream tasks without adding significant computational overhead. Since the soft prompts are learned and optimized, they can capture nuanced information necessary for the task.</li>\n      <li><strong>Prompting with Extra Context</strong>: This is often used when fine-tuning isn’t feasible or when working with models in a zero-shot or few-shot setting. It’s a way to leverage the vast knowledge contained in large pre-trained models by just guiding their behavior with carefully crafted prompts.</li>\n    </ul>\n<ul>\n  <li>In essence, while both methods use prompts to guide the model, soft prompt tuning involves learning and adjusting these prompts, whereas prompting with extra context involves using static, handcrafted prompts to guide the model’s behavior.</li>\n</ul>\n<h4 id=\"prefix-tuning\">Prefix Tuning</h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2101.00190\">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>, prefix-tuning is a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix).</li>\n  <li>Instead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM’s original parameters are kept frozen while the prefix parameters are updated.</li>\n  <li>Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”.</li>\n  <li>The figure below from the paper shows that fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires storing a full model copy for each task. They propose prefix-tuning (bottom), which freezes the Transformer parameters and only optimizes the prefix (the red prefix blocks). Consequently, prefix-tuning only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/Prefix-Tuning.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>They apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. They find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.</p>\n  </li>\n  <li>\n    <p>The image below <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html\">(source)</a> illustrate how in prefix tuning, trainable tensors are addted to each transformer block instead of only in the input embedding.\n<img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/prefixtuningraschka.png\" alt=\"\"></p>\n  </li>\n</ul>\n<p>They apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. They find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.</p>\n<p>The image below <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html\">(source)</a> illustrate how in prefix tuning, trainable tensors are addted to each transformer block instead of only in the input embedding.\n<img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/prefixtuningraschka.png\" alt=\"\"></p>\n<h4 id=\"hard-prompt-tuning\">Hard Prompt Tuning</h4>\n<ul>\n  <li>Hard prompt tuning directly modifies the input prompt to the model. This can involve a vast multitude of things such as:\n    <ul>\n      <li>We can add examples of outputs we expect from the prompt</li>\n      <li>We can add tags specifically relating to our task at hand</li>\n    </ul>\n  </li>\n  <li>In essence, it is just the modification of the string input, or prompt, to the model.</li>\n</ul>\n<ul>\n      <li>We can add examples of outputs we expect from the prompt</li>\n      <li>We can add tags specifically relating to our task at hand</li>\n    </ul>",
    "contentMarkdown": "#### Soft Prompt Tuning\n\n*   First introduced in the [The Power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf); this paper by Lester et al. introduces a simple yet effective method called soft prompt tuning, which prepends a trainable tensor to the model’s input embeddings, essentially creating a soft prompt to condition frozen LLMs to perform specific downstream tasks. Unlike the discrete text prompts, soft prompts are learned through backpropagation and can be fine-tuned to incorporate signals from any number of labeled examples.\n*   Soft prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pre-trained model.\n*   The authors show that prompt tuning outperforms few-shot learning by a large margin, and becomes more competitive with scale.\n*   This is an interesting approach that can help to effectively use a single frozen model for multi-task serving.\n*   Model tuning requires making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude – assuming a prompt length of 5 tokens.\n*   Thus, instead of using discrete text prompts, prompt tuning employs soft prompts. Soft prompts are learnable and conditioned through backpropagation, making them adaptable for specific tasks.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/PromptTuning.jpg)\n\n*   Prompt Tuning offers many benefits such as:\n    *   Memory-Efficiency: Prompt tuning dramatically reduces memory requirements. For instance, while a T5 “XXL” model necessitates 11 billion parameters for each task-specific model, prompt-tuned models need a mere 20,480 parameters (assuming a prompt length of 5 tokens).\n    *   Versatility: Enables the use of a single frozen model for multi-task operations.\n    *   Performance: Outshines few-shot learning and becomes more competitive as the scale grows.\n\n*   Memory-Efficiency: Prompt tuning dramatically reduces memory requirements. For instance, while a T5 “XXL” model necessitates 11 billion parameters for each task-specific model, prompt-tuned models need a mere 20,480 parameters (assuming a prompt length of 5 tokens).\n*   Versatility: Enables the use of a single frozen model for multi-task operations.\n*   Performance: Outshines few-shot learning and becomes more competitive as the scale grows.\n\n#### Soft Prompt vs. Prompting\n\n*   Soft prompt tuning and prompting a model with extra context are both methods designed to guide a model’s behavior for specific tasks, but they operate in different ways. Here’s how they differ:\n\n1.  **Mechanism**:\n    *   **Soft Prompt Tuning**: This involves introducing trainable parameters (soft prompts) that are concatenated or added to the model’s input embeddings. These soft prompts are learned during the fine-tuning process and are adjusted through backpropagation to condition the model to produce desired outputs for specific tasks.\n    *   **Prompting with Extra Context**: This method involves feeding the model with handcrafted or predefined text prompts that provide additional context. There’s no explicit fine-tuning; instead, the model leverages its pre-trained knowledge to produce outputs based on the provided context. This method is common in few-shot learning scenarios where the model is given a few examples as prompts and then asked to generalize to a new example.\n2.  **Trainability**:\n    *   **Soft Prompt Tuning**: The soft prompts are trainable. They get adjusted during the fine-tuning process to optimize the model’s performance on the target task.\n    *   **Prompting with Extra Context**: The prompts are static and not trainable. They’re designed (often manually) to give the model the necessary context for the desired task.\n3.  **Use Case**:\n    *   **Soft Prompt Tuning**: This method is particularly useful when there’s a need to adapt a pre-trained model to various downstream tasks without adding significant computational overhead. Since the soft prompts are learned and optimized, they can capture nuanced information necessary for the task.\n    *   **Prompting with Extra Context**: This is often used when fine-tuning isn’t feasible or when working with models in a zero-shot or few-shot setting. It’s a way to leverage the vast knowledge contained in large pre-trained models by just guiding their behavior with carefully crafted prompts.\n\n*   **Soft Prompt Tuning**: This involves introducing trainable parameters (soft prompts) that are concatenated or added to the model’s input embeddings. These soft prompts are learned during the fine-tuning process and are adjusted through backpropagation to condition the model to produce desired outputs for specific tasks.\n*   **Prompting with Extra Context**: This method involves feeding the model with handcrafted or predefined text prompts that provide additional context. There’s no explicit fine-tuning; instead, the model leverages its pre-trained knowledge to produce outputs based on the provided context. This method is common in few-shot learning scenarios where the model is given a few examples as prompts and then asked to generalize to a new example.\n\n*   **Soft Prompt Tuning**: The soft prompts are trainable. They get adjusted during the fine-tuning process to optimize the model’s performance on the target task.\n*   **Prompting with Extra Context**: The prompts are static and not trainable. They’re designed (often manually) to give the model the necessary context for the desired task.\n\n*   **Soft Prompt Tuning**: This method is particularly useful when there’s a need to adapt a pre-trained model to various downstream tasks without adding significant computational overhead. Since the soft prompts are learned and optimized, they can capture nuanced information necessary for the task.\n*   **Prompting with Extra Context**: This is often used when fine-tuning isn’t feasible or when working with models in a zero-shot or few-shot setting. It’s a way to leverage the vast knowledge contained in large pre-trained models by just guiding their behavior with carefully crafted prompts.\n\n*   In essence, while both methods use prompts to guide the model, soft prompt tuning involves learning and adjusting these prompts, whereas prompting with extra context involves using static, handcrafted prompts to guide the model’s behavior.\n\n#### Prefix Tuning\n\n*   Proposed in [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190), prefix-tuning is a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix).\n*   Instead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM’s original parameters are kept frozen while the prefix parameters are updated.\n*   Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”.\n*   The figure below from the paper shows that fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires storing a full model copy for each task. They propose prefix-tuning (bottom), which freezes the Transformer parameters and only optimizes the prefix (the red prefix blocks). Consequently, prefix-tuning only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/Prefix-Tuning.jpg)\n\n*   They apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. They find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\n    \n*   The image below [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html) illustrate how in prefix tuning, trainable tensors are addted to each transformer block instead of only in the input embedding. ![](/primers/ai/assets/parameter-efficient-fine-tuning/prefixtuningraschka.png)\n    \n\nThey apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. They find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\n\nThe image below [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html) illustrate how in prefix tuning, trainable tensors are addted to each transformer block instead of only in the input embedding. ![](/primers/ai/assets/parameter-efficient-fine-tuning/prefixtuningraschka.png)\n\n#### Hard Prompt Tuning\n\n*   Hard prompt tuning directly modifies the input prompt to the model. This can involve a vast multitude of things such as:\n    *   We can add examples of outputs we expect from the prompt\n    *   We can add tags specifically relating to our task at hand\n*   In essence, it is just the modification of the string input, or prompt, to the model.\n\n*   We can add examples of outputs we expect from the prompt\n*   We can add tags specifically relating to our task at hand",
    "order": 2,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 7,
    "tags": [
      "datatraining",
      "transformer",
      "embedding",
      "gpt",
      "llm",
      "nlp",
      "backpropagation",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1395,
      "contentLength": 10947
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#prompt-modifications",
    "scrapedAt": "2025-12-28T11:49:57.381Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-adapters-3",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "PEFT Methods",
    "title": "Adapters",
    "subtitle": "PEFT Methods",
    "contentHtml": "<ul>\n  <li>Adapter layers, often termed “Adapters”, add minimal additional parameters to the pretrained model. These adapters are inserted between existing layers of the network.</li>\n  <li>Adapters is a PEFT technique shown to achieve similar performance as compared to tuning the top layers while requiring as fewer parameters as two orders of magnitude.</li>\n  <li>Adapter-based tuning simply inserts new modules called “adapter modules” between the layers of the pre-trained network.</li>\n  <li>The image below <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html\">(source)</a> illustrates this concept for the transformer block:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/adapterraschka.png\" alt=\"\"></p>\n<ul>\n  <li>During fine-tuning, only the parameters of these adapter layers are updated, while the original model parameters are kept fixed. This results in a model with a small number of additional parameters that are task-specific.</li>\n  <li>Keeping the full PT model frozen, these modules are the only optimizable ones while fine-tuning – this means only a very few parameters are introduced per task yielding “compact” models.</li>\n  <li>They offer many benefits such as:\n    <ul>\n      <li>Parameter-Efficiency: By keeping the main model frozen and only updating the adapter layers, a minimal number of parameters are added per task. This results in compact models that are memory-efficient.</li>\n      <li>Performance: Despite the small parameter footprint, adapters often achieve performance comparable to conventional fine-tuning.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The adapter module consists of two fully connected layers with a bottleneck structure. This structure is inspired by autoencoders, which are designed to encode information into a compressed representation and then decode it back to its original form.</p>\n  </li>\n  <li>Here’s how the parameter efficiency is achieved:</li>\n</ul>\n<ul>\n      <li>Parameter-Efficiency: By keeping the main model frozen and only updating the adapter layers, a minimal number of parameters are added per task. This results in compact models that are memory-efficient.</li>\n      <li>Performance: Despite the small parameter footprint, adapters often achieve performance comparable to conventional fine-tuning.</li>\n    </ul>\n<p>The adapter module consists of two fully connected layers with a bottleneck structure. This structure is inspired by autoencoders, which are designed to encode information into a compressed representation and then decode it back to its original form.</p>\n<ol>\n  <li>\n    <p><strong>Bottleneck Structure</strong>: The first layer of the adapter reduces the dimensionality of the input (e.g., from 1024 to 24 dimensions). This drastic reduction means that the information from the original 1024 dimensions must be compressed into just 24 dimensions. The second layer then projects these 24 dimensions back to the original 1024 dimensions.</p>\n  </li>\n  <li>\n    <p><strong>Reduction in Parameters</strong>: This bottleneck approach significantly reduces the number of parameters. In your example, the total number of parameters introduced by the adapter is 49,152 (from the computation 1024x24 + 24x1024). If we were to use a single fully connected layer to project a 1024-dimensional input to a 1024-dimensional output directly, it would require 1,048,576 parameters (1024x1024).</p>\n  </li>\n  <li>\n    <p><strong>Efficiency Analysis</strong>: By using the adapter approach, the number of parameters is substantially lower. Comparing 49,152 parameters to 1,048,576 parameters shows a dramatic reduction, making the adapter much more efficient in terms of parameter usage.</p>\n  </li>\n  <li>\n    <p><strong>Why is this Beneficial?</strong>: This efficiency is particularly beneficial when fine-tuning large pre-trained models. Instead of retraining or adapting the entire network (which would be computationally expensive and memory-intensive), adapters allow for targeted adjustments with far fewer additional parameters. This makes the process more manageable and practical, especially when resources are limited.</p>\n  </li>\n</ol>\n<p><strong>Bottleneck Structure</strong>: The first layer of the adapter reduces the dimensionality of the input (e.g., from 1024 to 24 dimensions). This drastic reduction means that the information from the original 1024 dimensions must be compressed into just 24 dimensions. The second layer then projects these 24 dimensions back to the original 1024 dimensions.</p>\n<p><strong>Reduction in Parameters</strong>: This bottleneck approach significantly reduces the number of parameters. In your example, the total number of parameters introduced by the adapter is 49,152 (from the computation 1024x24 + 24x1024). If we were to use a single fully connected layer to project a 1024-dimensional input to a 1024-dimensional output directly, it would require 1,048,576 parameters (1024x1024).</p>\n<p><strong>Efficiency Analysis</strong>: By using the adapter approach, the number of parameters is substantially lower. Comparing 49,152 parameters to 1,048,576 parameters shows a dramatic reduction, making the adapter much more efficient in terms of parameter usage.</p>\n<p><strong>Why is this Beneficial?</strong>: This efficiency is particularly beneficial when fine-tuning large pre-trained models. Instead of retraining or adapting the entire network (which would be computationally expensive and memory-intensive), adapters allow for targeted adjustments with far fewer additional parameters. This makes the process more manageable and practical, especially when resources are limited.</p>\n<ul>\n  <li>The adapter’s bottleneck structure allows it to achieve similar functionality (adapting the model to new tasks or data) as a full-sized layer would, but with a significantly reduced number of parameters. This efficiency makes adapters a popular choice for fine-tuning large pre-trained models in a resource-effective manner.</li>\n</ul>\n<h4 id=\"what-is-an-adapter-module\">What is an Adapter Module?</h4>\n<ul>\n  <li>Let’s look at the application of the adapter module in the transformer architecture in three points:\n    <ul>\n      <li>The adapter module (right) first projects the original <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">d</script>-dimensional features into a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">m</script>-dimensional vector, applies a non-linearity, and then projects it back to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-40\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">d</script> dimensions.</li>\n      <li>As can be seen, the module features a skip-connection - With it in place, when the parameters of the projection layers are initialized to near-zero which eventually leads to near identity initialization of the module. This is required for stable fine-tuning and is intuitive as with it, we essentially do not disturb the learning from pre-training.</li>\n      <li>In a transformer block (left), the adapter is applied directly to the outputs of each of the layers (attention and feedforward).</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The adapter module (right) first projects the original <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">d</script>-dimensional features into a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">m</script>-dimensional vector, applies a non-linearity, and then projects it back to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-40\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">d</script> dimensions.</li>\n      <li>As can be seen, the module features a skip-connection - With it in place, when the parameters of the projection layers are initialized to near-zero which eventually leads to near identity initialization of the module. This is required for stable fine-tuning and is intuitive as with it, we essentially do not disturb the learning from pre-training.</li>\n      <li>In a transformer block (left), the adapter is applied directly to the outputs of each of the layers (attention and feedforward).</li>\n    </ul>\n<h4 id=\"how-do-you-decide-the-value-of-m\">How Do You Decide the Value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-43\" style=\"width: 0.96em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.794em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.585em, 1000.79em, 2.294em, -999.998em); top: -2.165em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.169em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.048em; border-left: 0px solid; width: 0px; height: 0.653em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">m</script>?</h4>\n<ul>\n  <li>The size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-46\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-47\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">m</script> in the Adapter module determines the number of optimizable parameters and hence poses a parameter vs performance tradeoff.</li>\n  <li>The original paper experimentally investigates that the performance remains fairly stable across varying adapter sizes <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-49\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">m</script> and hence for a given model a fixed size can be used for all downstream tasks.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/adapter.jpeg\" alt=\"\"></p>\n<h4 id=\"llama-adapters\"><a href=\"https://arxiv.org/abs/2303.16199\">LLaMA-Adapters</a></h4>\n<ul>\n  <li>\n    <p>This paper introduces an efficient fine-tuning method called LLaMA-Adapter. This method is designed to adapt the LLaMA model into an instruction-following model with high efficiency in terms of resource usage and time. Key aspects of this paper include:</p>\n\n    <ol>\n      <li>\n        <p><strong>Parameter Efficiency</strong>: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.</p>\n      </li>\n      <li>\n        <p><strong>Learnable Adaption Prompts</strong>: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.</p>\n      </li>\n      <li>\n        <p><strong>Zero-initialized Attention Mechanism</strong>: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.</p>\n      </li>\n      <li>\n        <p><strong>Generalization and Multi-modal Reasoning</strong>: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>In summary, the LLaMA-Adapter represents a significant advancement in the field of parameter-efficient fine-tuning of large LLMs. Its innovative use of learnable adaption prompts and zero-initialized attention mechanism provides a highly efficient method for adapting pre-trained models to new tasks and domains, including multi-modal applications.</p>\n  </li>\n  <li>\n    <p>The image below <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html\">(source)</a> illustrates this concept below.\n<img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/Llamaadapter.png\" alt=\"\"></p>\n  </li>\n</ul>\n<p>This paper introduces an efficient fine-tuning method called LLaMA-Adapter. This method is designed to adapt the LLaMA model into an instruction-following model with high efficiency in terms of resource usage and time. Key aspects of this paper include:</p>\n<ol>\n      <li>\n        <p><strong>Parameter Efficiency</strong>: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.</p>\n      </li>\n      <li>\n        <p><strong>Learnable Adaption Prompts</strong>: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.</p>\n      </li>\n      <li>\n        <p><strong>Zero-initialized Attention Mechanism</strong>: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.</p>\n      </li>\n      <li>\n        <p><strong>Generalization and Multi-modal Reasoning</strong>: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.</p>\n      </li>\n    </ol>\n<p><strong>Parameter Efficiency</strong>: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.</p>\n<p><strong>Learnable Adaption Prompts</strong>: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.</p>\n<p><strong>Zero-initialized Attention Mechanism</strong>: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.</p>\n<p><strong>Generalization and Multi-modal Reasoning</strong>: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.</p>\n<p>In summary, the LLaMA-Adapter represents a significant advancement in the field of parameter-efficient fine-tuning of large LLMs. Its innovative use of learnable adaption prompts and zero-initialized attention mechanism provides a highly efficient method for adapting pre-trained models to new tasks and domains, including multi-modal applications.</p>\n<p>The image below <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html\">(source)</a> illustrates this concept below.\n<img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/Llamaadapter.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Adapter layers, often termed “Adapters”, add minimal additional parameters to the pretrained model. These adapters are inserted between existing layers of the network.\n*   Adapters is a PEFT technique shown to achieve similar performance as compared to tuning the top layers while requiring as fewer parameters as two orders of magnitude.\n*   Adapter-based tuning simply inserts new modules called “adapter modules” between the layers of the pre-trained network.\n*   The image below [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html) illustrates this concept for the transformer block:\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/adapterraschka.png)\n\n*   During fine-tuning, only the parameters of these adapter layers are updated, while the original model parameters are kept fixed. This results in a model with a small number of additional parameters that are task-specific.\n*   Keeping the full PT model frozen, these modules are the only optimizable ones while fine-tuning – this means only a very few parameters are introduced per task yielding “compact” models.\n*   They offer many benefits such as:\n    *   Parameter-Efficiency: By keeping the main model frozen and only updating the adapter layers, a minimal number of parameters are added per task. This results in compact models that are memory-efficient.\n    *   Performance: Despite the small parameter footprint, adapters often achieve performance comparable to conventional fine-tuning.\n*   The adapter module consists of two fully connected layers with a bottleneck structure. This structure is inspired by autoencoders, which are designed to encode information into a compressed representation and then decode it back to its original form.\n    \n*   Here’s how the parameter efficiency is achieved:\n\n*   Parameter-Efficiency: By keeping the main model frozen and only updating the adapter layers, a minimal number of parameters are added per task. This results in compact models that are memory-efficient.\n*   Performance: Despite the small parameter footprint, adapters often achieve performance comparable to conventional fine-tuning.\n\nThe adapter module consists of two fully connected layers with a bottleneck structure. This structure is inspired by autoencoders, which are designed to encode information into a compressed representation and then decode it back to its original form.\n\n1.  **Bottleneck Structure**: The first layer of the adapter reduces the dimensionality of the input (e.g., from 1024 to 24 dimensions). This drastic reduction means that the information from the original 1024 dimensions must be compressed into just 24 dimensions. The second layer then projects these 24 dimensions back to the original 1024 dimensions.\n    \n2.  **Reduction in Parameters**: This bottleneck approach significantly reduces the number of parameters. In your example, the total number of parameters introduced by the adapter is 49,152 (from the computation 1024x24 + 24x1024). If we were to use a single fully connected layer to project a 1024-dimensional input to a 1024-dimensional output directly, it would require 1,048,576 parameters (1024x1024).\n    \n3.  **Efficiency Analysis**: By using the adapter approach, the number of parameters is substantially lower. Comparing 49,152 parameters to 1,048,576 parameters shows a dramatic reduction, making the adapter much more efficient in terms of parameter usage.\n    \n4.  **Why is this Beneficial?**: This efficiency is particularly beneficial when fine-tuning large pre-trained models. Instead of retraining or adapting the entire network (which would be computationally expensive and memory-intensive), adapters allow for targeted adjustments with far fewer additional parameters. This makes the process more manageable and practical, especially when resources are limited.\n    \n\n**Bottleneck Structure**: The first layer of the adapter reduces the dimensionality of the input (e.g., from 1024 to 24 dimensions). This drastic reduction means that the information from the original 1024 dimensions must be compressed into just 24 dimensions. The second layer then projects these 24 dimensions back to the original 1024 dimensions.\n\n**Reduction in Parameters**: This bottleneck approach significantly reduces the number of parameters. In your example, the total number of parameters introduced by the adapter is 49,152 (from the computation 1024x24 + 24x1024). If we were to use a single fully connected layer to project a 1024-dimensional input to a 1024-dimensional output directly, it would require 1,048,576 parameters (1024x1024).\n\n**Efficiency Analysis**: By using the adapter approach, the number of parameters is substantially lower. Comparing 49,152 parameters to 1,048,576 parameters shows a dramatic reduction, making the adapter much more efficient in terms of parameter usage.\n\n**Why is this Beneficial?**: This efficiency is particularly beneficial when fine-tuning large pre-trained models. Instead of retraining or adapting the entire network (which would be computationally expensive and memory-intensive), adapters allow for targeted adjustments with far fewer additional parameters. This makes the process more manageable and practical, especially when resources are limited.\n\n*   The adapter’s bottleneck structure allows it to achieve similar functionality (adapting the model to new tasks or data) as a full-sized layer would, but with a significantly reduced number of parameters. This efficiency makes adapters a popular choice for fine-tuning large pre-trained models in a resource-effective manner.\n\n#### What is an Adapter Module?\n\n*   Let’s look at the application of the adapter module in the transformer architecture in three points:\n    *   The adapter module (right) first projects the original ddd\\-dimensional features into a smaller mmm\\-dimensional vector, applies a non-linearity, and then projects it back to ddd dimensions.\n    *   As can be seen, the module features a skip-connection - With it in place, when the parameters of the projection layers are initialized to near-zero which eventually leads to near identity initialization of the module. This is required for stable fine-tuning and is intuitive as with it, we essentially do not disturb the learning from pre-training.\n    *   In a transformer block (left), the adapter is applied directly to the outputs of each of the layers (attention and feedforward).\n\n*   The adapter module (right) first projects the original ddd\\-dimensional features into a smaller mmm\\-dimensional vector, applies a non-linearity, and then projects it back to ddd dimensions.\n*   As can be seen, the module features a skip-connection - With it in place, when the parameters of the projection layers are initialized to near-zero which eventually leads to near identity initialization of the module. This is required for stable fine-tuning and is intuitive as with it, we essentially do not disturb the learning from pre-training.\n*   In a transformer block (left), the adapter is applied directly to the outputs of each of the layers (attention and feedforward).\n\n#### How Do You Decide the Value of mmm?\n\n*   The size mmm in the Adapter module determines the number of optimizable parameters and hence poses a parameter vs performance tradeoff.\n*   The original paper experimentally investigates that the performance remains fairly stable across varying adapter sizes mmm and hence for a given model a fixed size can be used for all downstream tasks.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/adapter.jpeg)\n\n#### [LLaMA-Adapters](https://arxiv.org/abs/2303.16199)\n\n*   This paper introduces an efficient fine-tuning method called LLaMA-Adapter. This method is designed to adapt the LLaMA model into an instruction-following model with high efficiency in terms of resource usage and time. Key aspects of this paper include:\n    \n    1.  **Parameter Efficiency**: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.\n        \n    2.  **Learnable Adaption Prompts**: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.\n        \n    3.  **Zero-initialized Attention Mechanism**: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.\n        \n    4.  **Generalization and Multi-modal Reasoning**: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.\n        \n*   In summary, the LLaMA-Adapter represents a significant advancement in the field of parameter-efficient fine-tuning of large LLMs. Its innovative use of learnable adaption prompts and zero-initialized attention mechanism provides a highly efficient method for adapting pre-trained models to new tasks and domains, including multi-modal applications.\n    \n*   The image below [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html) illustrates this concept below. ![](/primers/ai/assets/parameter-efficient-fine-tuning/Llamaadapter.png)\n    \n\nThis paper introduces an efficient fine-tuning method called LLaMA-Adapter. This method is designed to adapt the LLaMA model into an instruction-following model with high efficiency in terms of resource usage and time. Key aspects of this paper include:\n\n1.  **Parameter Efficiency**: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.\n    \n2.  **Learnable Adaption Prompts**: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.\n    \n3.  **Zero-initialized Attention Mechanism**: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.\n    \n4.  **Generalization and Multi-modal Reasoning**: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.\n    \n\n**Parameter Efficiency**: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.\n\n**Learnable Adaption Prompts**: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.\n\n**Zero-initialized Attention Mechanism**: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.\n\n**Generalization and Multi-modal Reasoning**: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.\n\nIn summary, the LLaMA-Adapter represents a significant advancement in the field of parameter-efficient fine-tuning of large LLMs. Its innovative use of learnable adaption prompts and zero-initialized attention mechanism provides a highly efficient method for adapting pre-trained models to new tasks and domains, including multi-modal applications.\n\nThe image below [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html) illustrates this concept below. ![](/primers/ai/assets/parameter-efficient-fine-tuning/Llamaadapter.png)",
    "order": 3,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 10,
    "tags": [
      "datatraining",
      "transformer",
      "attention",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 1952,
      "contentLength": 26570
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#adapters",
    "scrapedAt": "2025-12-28T11:49:57.381Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-reparameterization-4",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "PEFT Methods",
    "title": "Reparameterization",
    "subtitle": "PEFT Methods",
    "contentHtml": "<h4 id=\"low-rank-adaptation-lora\">Low-Rank Adaptation (LoRA)</h4>\n<h5 id=\"background\">Background</h5>\n<h6 id=\"rank-of-a-matrix\">Rank of a Matrix</h6>\n<ul>\n  <li>The rank of a matrix is a measure of the number of linearly independent rows or columns in the matrix.</li>\n  <li>If a matrix has rank 1, it means all rows or all columns can be represented as multiples of each other, so there’s essentially only one unique “direction” in the data.</li>\n  <li>A full-rank matrix has rank equal to the smallest of its dimensions (number of rows or columns), meaning all rows and columns are independent.</li>\n  <li>\n    <p>On a related note, a matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. For more, refer <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\">Wikipedia: Rank</a>.</p>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>Consider the following 3x3 matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-52\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">A</script>:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-55\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.034em, 1006.88em, 6.253em, -999.997em); top: -4.372em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-59\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-60\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎡<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtable\" id=\"MathJax-Span-61\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.346em, 1000.47em, 6.253em, -999.997em); top: -4.581em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-62\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"mn\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-71\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mn\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-80\"><span class=\"mrow\" id=\"MathJax-Span-81\"><span class=\"mn\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">7</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1000.47em, 6.253em, -999.997em); top: -4.581em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-65\"><span class=\"mrow\" id=\"MathJax-Span-66\"><span class=\"mn\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mn\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mn\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1000.47em, 6.253em, -999.997em); top: -4.581em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-68\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mn\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-77\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mn\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-86\"><span class=\"mrow\" id=\"MathJax-Span-87\"><span class=\"mn\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular;\">9</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎤<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎦<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.122em; border-left: 0px solid; width: 0px; height: 4.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-19\">A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}</script>\n  </li>\n  <li>\n    <p><strong>Step-by-Step to Determine the Rank</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Row Reduction</strong>: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.</p>\n\n        <p>After row-reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">A</script>, we get:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>&amp;#x2212;</mo><mn>3</mn></mtd><mtd><mo>&amp;#x2212;</mo><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-93\" style=\"width: 9.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.034em, 1008.23em, 6.253em, -999.997em); top: -4.372em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-94\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-97\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-98\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎡<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtable\" id=\"MathJax-Span-99\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.107em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.346em, 1000.47em, 6.253em, -999.997em); top: -4.581em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-100\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mn\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-109\"><span class=\"mrow\" id=\"MathJax-Span-110\"><span class=\"mn\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-120\"><span class=\"mrow\" id=\"MathJax-Span-121\"><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1001.1em, 6.253em, -999.997em); top: -4.581em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-103\"><span class=\"mrow\" id=\"MathJax-Span-104\"><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.1em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.57em;\"><span class=\"mtd\" id=\"MathJax-Span-112\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-123\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mn\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1001.15em, 6.253em, -999.997em); top: -4.581em; left: 3.961em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-106\"><span class=\"mrow\" id=\"MathJax-Span-107\"><span class=\"mn\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.15em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.57em;\"><span class=\"mtd\" id=\"MathJax-Span-116\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"mo\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-126\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mn\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎤<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎦<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.122em; border-left: 0px solid; width: 0px; height: 4.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>−</mo><mn>3</mn></mtd><mtd><mo>−</mo><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-21\">A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & 0 \\end{bmatrix}</script>\n      </li>\n      <li>\n        <p><strong>Count Independent Rows</strong>: Now we look at the rows with non-zero entries:</p>\n        <ul>\n          <li>The first row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-130\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-131\"><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">2</span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">3</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">[1, 2, 3]</script> is non-zero.</li>\n          <li>The second row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>3</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>6</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mo>−</mo><mn>3</mn><mo>,</mo><mo>−</mo><mn>6</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">[0, -3, -6]</script> is also non-zero and independent of the first row.</li>\n          <li>The third row is all zeros, which does not contribute to the rank.</li>\n        </ul>\n\n        <p>Since there are two non-zero, independent rows in the row echelon form, the rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">A</script> is 2.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Explanation</strong>:</p>\n\n    <ul>\n      <li>The rank of 2 indicates that only two rows or columns in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-153\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-154\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">A</script> contain unique information, and the third row (or column) can be derived from a combination of the other two. Essentially, this matrix can be thought of as existing in a 2-dimensional space rather than a full 3-dimensional space, despite its 3x3 size.</li>\n    </ul>\n  </li>\n  <li><strong>In summary:</strong>\n    <ul>\n      <li>The rank of matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-156\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"mi\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">A</script> is 2.</li>\n      <li>This rank tells us the matrix’s actual dimensionality in terms of its independent information.</li>\n    </ul>\n  </li>\n</ul>\n<p>On a related note, a matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. For more, refer <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\">Wikipedia: Rank</a>.</p>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>Consider the following 3x3 matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-52\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">A</script>:</li>\n    </ul>\n<p><strong>Step-by-Step to Determine the Rank</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Row Reduction</strong>: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.</p>\n\n        <p>After row-reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">A</script>, we get:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>&amp;#x2212;</mo><mn>3</mn></mtd><mtd><mo>&amp;#x2212;</mo><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-93\" style=\"width: 9.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.034em, 1008.23em, 6.253em, -999.997em); top: -4.372em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-94\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-97\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-98\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎡<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎢<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtable\" id=\"MathJax-Span-99\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.107em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.346em, 1000.47em, 6.253em, -999.997em); top: -4.581em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-100\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mn\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-109\"><span class=\"mrow\" id=\"MathJax-Span-110\"><span class=\"mn\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-120\"><span class=\"mrow\" id=\"MathJax-Span-121\"><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1001.1em, 6.253em, -999.997em); top: -4.581em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-103\"><span class=\"mrow\" id=\"MathJax-Span-104\"><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.1em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.57em;\"><span class=\"mtd\" id=\"MathJax-Span-112\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-123\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mn\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span><span style=\"position: absolute; clip: rect(2.346em, 1001.15em, 6.253em, -999.997em); top: -4.581em; left: 3.961em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -5.414em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-106\"><span class=\"mrow\" id=\"MathJax-Span-107\"><span class=\"mn\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.15em, 4.326em, -999.997em); top: -3.956em; left: 50%; margin-left: -0.57em;\"><span class=\"mtd\" id=\"MathJax-Span-116\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"mo\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-126\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mn\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"vertical-align: 2.19em;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -3.331em; left: 0em;\">⎤<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.414em; left: 0em;\">⎦<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -2.341em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXSizeOneSym; position: absolute; top: -1.404em; left: 0em;\">⎥<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.122em; border-left: 0px solid; width: 0px; height: 4.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>−</mo><mn>3</mn></mtd><mtd><mo>−</mo><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-21\">A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & 0 \\end{bmatrix}</script>\n      </li>\n      <li>\n        <p><strong>Count Independent Rows</strong>: Now we look at the rows with non-zero entries:</p>\n        <ul>\n          <li>The first row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-130\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-131\"><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">2</span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">3</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">[1, 2, 3]</script> is non-zero.</li>\n          <li>The second row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>3</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>6</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mo>−</mo><mn>3</mn><mo>,</mo><mo>−</mo><mn>6</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">[0, -3, -6]</script> is also non-zero and independent of the first row.</li>\n          <li>The third row is all zeros, which does not contribute to the rank.</li>\n        </ul>\n\n        <p>Since there are two non-zero, independent rows in the row echelon form, the rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">A</script> is 2.</p>\n      </li>\n    </ol>\n<p><strong>Row Reduction</strong>: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.</p>\n<p>After row-reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">A</script>, we get:</p>\n<p><strong>Count Independent Rows</strong>: Now we look at the rows with non-zero entries:</p>\n<ul>\n          <li>The first row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-130\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-131\"><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">2</span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">3</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">[1, 2, 3]</script> is non-zero.</li>\n          <li>The second row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>3</mn><mo>,</mo><mo>&amp;#x2212;</mo><mn>6</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mn\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">−</span><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mo>−</mo><mn>3</mn><mo>,</mo><mo>−</mo><mn>6</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">[0, -3, -6]</script> is also non-zero and independent of the first row.</li>\n          <li>The third row is all zeros, which does not contribute to the rank.</li>\n        </ul>\n<p>Since there are two non-zero, independent rows in the row echelon form, the rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">A</script> is 2.</p>\n<p><strong>Explanation</strong>:</p>\n<ul>\n      <li>The rank of 2 indicates that only two rows or columns in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-153\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-154\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">A</script> contain unique information, and the third row (or column) can be derived from a combination of the other two. Essentially, this matrix can be thought of as existing in a 2-dimensional space rather than a full 3-dimensional space, despite its 3x3 size.</li>\n    </ul>\n<ul>\n      <li>The rank of matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-156\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"mi\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">A</script> is 2.</li>\n      <li>This rank tells us the matrix’s actual dimensionality in terms of its independent information.</li>\n    </ul>\n<h6 id=\"related-rank-of-a-tensor\">Related: Rank of a Tensor</h6>\n<ul>\n  <li>While LoRA injects trainable low-rank matrices, it is important to understand rank in the context of tensors as well.</li>\n  <li>\n    <p>The rank of a tensor refers to the number of dimensions in the tensor. This is different from the rank of a matrix, which relates to the number of linearly independent rows or columns. For tensors, rank simply tells us how many dimensions or axes the tensor has.</p>\n  </li>\n  <li>\n    <p><strong>Explanation with Examples</strong>:</p>\n\n    <ol>\n      <li><strong>Scalar (Rank 0 Tensor)</strong>:\n        <ul>\n          <li>A scalar is a single number with no dimensions.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">5</code> or <code class=\"language-plaintext highlighter-rouge\">3.14</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">()</code> (no dimensions)</li>\n          <li><strong>Rank</strong>: 0</li>\n        </ul>\n      </li>\n      <li><strong>Vector (Rank 1 Tensor)</strong>:\n        <ul>\n          <li>A vector is a one-dimensional array of numbers.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">[3, 7, 2]</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(3,)</code> (one dimension with 3 elements)</li>\n          <li><strong>Rank</strong>: 1</li>\n        </ul>\n      </li>\n      <li><strong>Matrix (Rank 2 Tensor)</strong>:\n        <ul>\n          <li>A matrix is a two-dimensional array of numbers, like a table.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-159\" style=\"width: 6.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1004.9em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mo\" id=\"MathJax-Span-162\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-163\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-164\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mn\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-173\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mn\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-167\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mn\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-176\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mn\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-170\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mn\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-179\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 3)</code> (two dimensions: 2 rows, 3 columns)</li>\n          <li><strong>Rank</strong>: 2</li>\n        </ul>\n      </li>\n      <li><strong>3D Tensor (Rank 3 Tensor)</strong>:\n        <ul>\n          <li>A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 16.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1013.49em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mo\" id=\"MathJax-Span-186\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-187\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 50%; margin-left: -6.143em;\"><span class=\"mtd\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mrow\" id=\"MathJax-Span-190\"><span class=\"mo\" id=\"MathJax-Span-191\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-192\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-193\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mn\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-196\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mn\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-205\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"mn\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-199\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mn\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span><span class=\"mo\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mrow\" id=\"MathJax-Span-213\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-214\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-215\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-216\"><span class=\"mrow\" id=\"MathJax-Span-217\"><span class=\"mn\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">7</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-225\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.89em, 4.951em, -999.997em); top: -4.008em; left: 2.138em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-219\"><span class=\"mrow\" id=\"MathJax-Span-220\"><span class=\"mn\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-228\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mn\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">11</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 4.273em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-222\"><span class=\"mrow\" id=\"MathJax-Span-223\"><span class=\"mn\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Regular;\">9</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-231\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mn\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">12</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix},\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 2, 3)</code> (three dimensions: 2 matrices, each with 2 rows and 3 columns)</li>\n          <li><strong>Rank</strong>: 3</li>\n        </ul>\n      </li>\n      <li><strong>4D Tensor (Rank 4 Tensor)</strong>:\n        <ul>\n          <li>A 4D tensor might represent multiple “stacks” of 3D tensors.</li>\n          <li>Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions <code class=\"language-plaintext highlighter-rouge\">[batch size, channels, height, width]</code>.</li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(10, 3, 64, 64)</code> for a batch of 10 images, each with 3 color channels and a resolution of 64x64.</li>\n          <li><strong>Rank</strong>: 4</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>General Rule</strong>:\n    <ul>\n      <li><strong>Rank</strong> = Number of dimensions (or axes) of the tensor.</li>\n    </ul>\n  </li>\n  <li><strong>Why Rank Matters</strong>:\n    <ul>\n      <li>The rank of a tensor tells us about its structural complexity and the data it can represent. Higher-rank tensors can represent more complex data structures, which is essential in fields like deep learning, physics simulations, and data science for handling multi-dimensional data.</li>\n    </ul>\n  </li>\n</ul>\n<p>The rank of a tensor refers to the number of dimensions in the tensor. This is different from the rank of a matrix, which relates to the number of linearly independent rows or columns. For tensors, rank simply tells us how many dimensions or axes the tensor has.</p>\n<p><strong>Explanation with Examples</strong>:</p>\n<ol>\n      <li><strong>Scalar (Rank 0 Tensor)</strong>:\n        <ul>\n          <li>A scalar is a single number with no dimensions.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">5</code> or <code class=\"language-plaintext highlighter-rouge\">3.14</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">()</code> (no dimensions)</li>\n          <li><strong>Rank</strong>: 0</li>\n        </ul>\n      </li>\n      <li><strong>Vector (Rank 1 Tensor)</strong>:\n        <ul>\n          <li>A vector is a one-dimensional array of numbers.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">[3, 7, 2]</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(3,)</code> (one dimension with 3 elements)</li>\n          <li><strong>Rank</strong>: 1</li>\n        </ul>\n      </li>\n      <li><strong>Matrix (Rank 2 Tensor)</strong>:\n        <ul>\n          <li>A matrix is a two-dimensional array of numbers, like a table.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-159\" style=\"width: 6.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1004.9em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mo\" id=\"MathJax-Span-162\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-163\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-164\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mn\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-173\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mn\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-167\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mn\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-176\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mn\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-170\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mn\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-179\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 3)</code> (two dimensions: 2 rows, 3 columns)</li>\n          <li><strong>Rank</strong>: 2</li>\n        </ul>\n      </li>\n      <li><strong>3D Tensor (Rank 3 Tensor)</strong>:\n        <ul>\n          <li>A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 16.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1013.49em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mo\" id=\"MathJax-Span-186\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-187\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 50%; margin-left: -6.143em;\"><span class=\"mtd\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mrow\" id=\"MathJax-Span-190\"><span class=\"mo\" id=\"MathJax-Span-191\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-192\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-193\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mn\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-196\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mn\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-205\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"mn\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-199\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mn\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span><span class=\"mo\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mrow\" id=\"MathJax-Span-213\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-214\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-215\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-216\"><span class=\"mrow\" id=\"MathJax-Span-217\"><span class=\"mn\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">7</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-225\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.89em, 4.951em, -999.997em); top: -4.008em; left: 2.138em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-219\"><span class=\"mrow\" id=\"MathJax-Span-220\"><span class=\"mn\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-228\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mn\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">11</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 4.273em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-222\"><span class=\"mrow\" id=\"MathJax-Span-223\"><span class=\"mn\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Regular;\">9</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-231\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mn\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">12</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix},\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 2, 3)</code> (three dimensions: 2 matrices, each with 2 rows and 3 columns)</li>\n          <li><strong>Rank</strong>: 3</li>\n        </ul>\n      </li>\n      <li><strong>4D Tensor (Rank 4 Tensor)</strong>:\n        <ul>\n          <li>A 4D tensor might represent multiple “stacks” of 3D tensors.</li>\n          <li>Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions <code class=\"language-plaintext highlighter-rouge\">[batch size, channels, height, width]</code>.</li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(10, 3, 64, 64)</code> for a batch of 10 images, each with 3 color channels and a resolution of 64x64.</li>\n          <li><strong>Rank</strong>: 4</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>A scalar is a single number with no dimensions.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">5</code> or <code class=\"language-plaintext highlighter-rouge\">3.14</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">()</code> (no dimensions)</li>\n          <li><strong>Rank</strong>: 0</li>\n        </ul>\n<ul>\n          <li>A vector is a one-dimensional array of numbers.</li>\n          <li>Example: <code class=\"language-plaintext highlighter-rouge\">[3, 7, 2]</code></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(3,)</code> (one dimension with 3 elements)</li>\n          <li><strong>Rank</strong>: 1</li>\n        </ul>\n<ul>\n          <li>A matrix is a two-dimensional array of numbers, like a table.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-159\" style=\"width: 6.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1004.9em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mo\" id=\"MathJax-Span-162\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-163\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-164\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mn\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-173\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mn\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-167\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mn\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-176\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mn\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-170\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mn\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-179\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 3)</code> (two dimensions: 2 rows, 3 columns)</li>\n          <li><strong>Rank</strong>: 2</li>\n        </ul>\n<ul>\n          <li>A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.</li>\n          <li>Example:\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 16.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1013.49em, 5.263em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mo\" id=\"MathJax-Span-186\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-187\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.398em, 1012.14em, 5.159em, -999.997em); top: -4.008em; left: 50%; margin-left: -6.143em;\"><span class=\"mtd\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mrow\" id=\"MathJax-Span-190\"><span class=\"mo\" id=\"MathJax-Span-191\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-192\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-193\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mn\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-196\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mn\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-205\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"mn\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.47em, 4.951em, -999.997em); top: -4.008em; left: 3.232em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-199\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mn\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">6</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span><span class=\"mo\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mrow\" id=\"MathJax-Span-213\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-214\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">[</span></span><span class=\"mtable\" id=\"MathJax-Span-215\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-216\"><span class=\"mrow\" id=\"MathJax-Span-217\"><span class=\"mn\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">7</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-225\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.89em, 4.951em, -999.997em); top: -4.008em; left: 2.138em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-219\"><span class=\"mrow\" id=\"MathJax-Span-220\"><span class=\"mn\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-228\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mn\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">11</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1000.99em, 4.951em, -999.997em); top: -4.008em; left: 4.273em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mtd\" id=\"MathJax-Span-222\"><span class=\"mrow\" id=\"MathJax-Span-223\"><span class=\"mn\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Regular;\">9</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -3.227em; left: 50%; margin-left: -0.518em;\"><span class=\"mtd\" id=\"MathJax-Span-231\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mn\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">12</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable rowspacing=\"4pt\" columnspacing=\"1em\"><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr><mtr><mtd><mn>10</mn></mtd><mtd><mn>11</mn></mtd><mtd><mn>12</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix},\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{bmatrix}</script></li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(2, 2, 3)</code> (three dimensions: 2 matrices, each with 2 rows and 3 columns)</li>\n          <li><strong>Rank</strong>: 3</li>\n        </ul>\n<ul>\n          <li>A 4D tensor might represent multiple “stacks” of 3D tensors.</li>\n          <li>Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions <code class=\"language-plaintext highlighter-rouge\">[batch size, channels, height, width]</code>.</li>\n          <li>Shape: <code class=\"language-plaintext highlighter-rouge\">(10, 3, 64, 64)</code> for a batch of 10 images, each with 3 color channels and a resolution of 64x64.</li>\n          <li><strong>Rank</strong>: 4</li>\n        </ul>\n<ul>\n      <li><strong>Rank</strong> = Number of dimensions (or axes) of the tensor.</li>\n    </ul>\n<ul>\n      <li>The rank of a tensor tells us about its structural complexity and the data it can represent. Higher-rank tensors can represent more complex data structures, which is essential in fields like deep learning, physics simulations, and data science for handling multi-dimensional data.</li>\n    </ul>\n<h5 id=\"overview-1\">Overview</h5>\n<ul>\n  <li>\n    <p><strong>Intrinsic Rank Hypothesis</strong>:</p>\n\n    <ul>\n      <li>Low-Rank Adaptation (LoRA) is motivated by the hypothesis that the updates to a model’s weights during task-specific adaptation exhibit a low “intrinsic rank.” This suggests that the weight changes required for effective adaptation are inherently low-dimensional. Thus, LoRA constrains these updates by representing them through low-rank decomposition matrices, enabling efficient adaptation without fine-tuning all model parameters.</li>\n      <li>\n        <p>As illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-239\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">B</script>, which capture the adaptation. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-242\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">A</script> (the <strong>down-projection matrix</strong>) projects the input into a lower-dimensional subspace, while <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-245\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">B</script> (the <strong>up-projection matrix</strong>) maps it back to the original dimension. Per the <a href=\"https://arxiv.org/pdf/2106.09685\">LoRA paper</a> by Hu et al. (2021), the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">A</script> is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">B</script> is initialized to zeros so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-254\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-255\"><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">\\Delta W = B A</script> starts as the zero matrix. During training, the product of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-261\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mi\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">B</script> forms a low-rank update matrix that is added to the original, pre-trained weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">W</script> to produce the adapted model output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-270\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-271\"><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">h</script>. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">W</script> frozen.</p>\n\n        <p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg\" alt=\"\"></p>\n\n        <ul>\n          <li>This product, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-276\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-277\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">BA</script>, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">B</script> is a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-283\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>×</mo><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">d \\times r</script> matrix and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">A</script> is an <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-291\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>×</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">r \\times d</script> matrix, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">r</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-299\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">d</script>, the resulting product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-302\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-303\"><span class=\"mi\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">BA</script> will have a maximum rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-306\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-307\"><span class=\"mi\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">r</script>, regardless of the dimensions of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-309\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-310\"><span class=\"mi\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">d</script>. This means the update to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">W</script> is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.</li>\n          <li>For example, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mn>1000</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-315\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1000</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>=</mo><mn>1000</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">d = 1000</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">r = 2</script>, the update matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">BA</script> will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-329\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-330\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">BA</script> is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-333\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-334\"><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">W</script> frozen.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Process</strong>:\n    <ul>\n      <li>LoRA fundamentally changes the approach to fine-tuning large neural networks by introducing a method to decompose high-dimensional weight matrices into lower-dimensional forms, preserving essential information while reducing computational load. Put simply, LoRA efficiently fine-tunes large-scale neural networks by introducing trainable low-rank matrices, simplifying the model’s complexity while retaining its robust learning capabilities.</li>\n      <li>LoRA is similar to methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), leveraging the observation that weight updates during fine-tuning are often rank-deficient. By imposing a low-rank constraint on these updates, LoRA enables a lightweight adaptation process that captures the most critical directions or “components” in the weight space. This means the model can retain essential knowledge from its pre-training phase while focusing on the specific nuances of the new task, resulting in efficient adaptation with a smaller memory and computational footprint.</li>\n      <li>During backpropagation, only the low-rank LoRA matrices (A and B) receive gradient updates; the original pretrained weights remain frozen. Gradients flow through the combined effective weight (W = W_0 + BA), but only the LoRA-specific parameters (A and B) have <code class=\"language-plaintext highlighter-rouge\">requires_grad=True</code>, ensuring that only these low-rank components are optimized while the base model weights remain unchanged.</li>\n    </ul>\n  </li>\n  <li><strong>Application</strong>:\n    <ul>\n      <li>LoRA’s primary application is in the efficient fine-tuning of large neural networks, particularly in transformer-based architectures. In practice, LoRA identifies key dimensions within the original weight matrix that are crucial for a specific task, significantly reducing the adaptation’s dimensionality.</li>\n      <li>Instead of fine-tuning all weights, which is computationally prohibitive for large models, LoRA introduces two trainable low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-336\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-337\"><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">B</script>, in specific layers of the transformer (e.g., in the query and value projections of the attention mechanism). These matrices are optimized during training while keeping the core model parameters frozen. This architecture adjustment means that only a fraction of the original parameters are actively updated, thus lowering memory usage and enabling faster computations.</li>\n      <li>By focusing only on the most relevant dimensions for each task, LoRA enables the deployment of large models on hardware with limited resources and facilitates task-switching by updating only the low-rank matrices without retraining the entire model.</li>\n    </ul>\n  </li>\n  <li><strong>Benefits</strong>:\n    <ul>\n      <li>LoRA provides several advantages that make it particularly suitable for practical use in industry and research settings where resource constraints are a concern. The primary benefit is in memory and computational efficiency.</li>\n      <li>By keeping the majority of the model’s parameters frozen and only updating the low-rank matrices, LoRA significantly reduces the number of trainable parameters, leading to lower memory consumption and faster training times. This reduction in training complexity also means that fewer GPUs or lower-spec hardware can be used to fine-tune large models, broadening accessibility.</li>\n      <li>Additionally, LoRA avoids introducing any additional latency during inference because, once the low-rank matrices have been trained, they can be merged back into the pre-trained weights without altering the architecture.</li>\n      <li>This setup makes LoRA ideal for environments where models need to switch between tasks frequently, as only the task-specific low-rank weights need to be loaded, allowing the model to quickly adapt to new tasks or datasets with minimal overhead.</li>\n    </ul>\n  </li>\n  <li><strong>In Summary</strong>:\n    <ul>\n      <li>LoRA represents a highly efficient approach to fine-tuning, striking a balance between the depth of knowledge encapsulated in large pre-trained models and the need for targeted adaptation to new tasks or data.</li>\n      <li>By leveraging the low intrinsic rank of weight updates, LoRA retains the model’s robustness and generalization capability while allowing for quick, cost-effective adaptations. This approach redefines efficiency in the era of massive LLMs, making it feasible to use and adapt large-scale pre-trained models across various applications and domains without the extensive computational burden associated with traditional fine-tuning.</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora2.png\" alt=\"\"></p>\n  </li>\n  <li>As a recap of traditional finetuning vs. LoRA <a href=\"https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html\">(source)</a>:</li>\n</ul>\n<p><strong>Intrinsic Rank Hypothesis</strong>:</p>\n<ul>\n      <li>Low-Rank Adaptation (LoRA) is motivated by the hypothesis that the updates to a model’s weights during task-specific adaptation exhibit a low “intrinsic rank.” This suggests that the weight changes required for effective adaptation are inherently low-dimensional. Thus, LoRA constrains these updates by representing them through low-rank decomposition matrices, enabling efficient adaptation without fine-tuning all model parameters.</li>\n      <li>\n        <p>As illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-239\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">B</script>, which capture the adaptation. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-242\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">A</script> (the <strong>down-projection matrix</strong>) projects the input into a lower-dimensional subspace, while <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-245\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">B</script> (the <strong>up-projection matrix</strong>) maps it back to the original dimension. Per the <a href=\"https://arxiv.org/pdf/2106.09685\">LoRA paper</a> by Hu et al. (2021), the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">A</script> is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">B</script> is initialized to zeros so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-254\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-255\"><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">\\Delta W = B A</script> starts as the zero matrix. During training, the product of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-261\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mi\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">B</script> forms a low-rank update matrix that is added to the original, pre-trained weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">W</script> to produce the adapted model output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-270\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-271\"><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">h</script>. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">W</script> frozen.</p>\n\n        <p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg\" alt=\"\"></p>\n\n        <ul>\n          <li>This product, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-276\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-277\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">BA</script>, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">B</script> is a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-283\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>×</mo><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">d \\times r</script> matrix and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">A</script> is an <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-291\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>×</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">r \\times d</script> matrix, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">r</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-299\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">d</script>, the resulting product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-302\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-303\"><span class=\"mi\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">BA</script> will have a maximum rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-306\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-307\"><span class=\"mi\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">r</script>, regardless of the dimensions of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-309\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-310\"><span class=\"mi\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">d</script>. This means the update to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">W</script> is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.</li>\n          <li>For example, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mn>1000</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-315\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1000</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>=</mo><mn>1000</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">d = 1000</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">r = 2</script>, the update matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">BA</script> will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-329\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-330\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">BA</script> is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-333\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-334\"><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">W</script> frozen.</li>\n        </ul>\n      </li>\n    </ul>\n<p>As illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-239\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">B</script>, which capture the adaptation. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-242\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">A</script> (the <strong>down-projection matrix</strong>) projects the input into a lower-dimensional subspace, while <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-245\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">B</script> (the <strong>up-projection matrix</strong>) maps it back to the original dimension. Per the <a href=\"https://arxiv.org/pdf/2106.09685\">LoRA paper</a> by Hu et al. (2021), the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">A</script> is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">B</script> is initialized to zeros so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-254\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-255\"><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">\\Delta W = B A</script> starts as the zero matrix. During training, the product of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-261\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mi\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">B</script> forms a low-rank update matrix that is added to the original, pre-trained weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">W</script> to produce the adapted model output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-270\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-271\"><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">h</script>. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">W</script> frozen.</p>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg\" alt=\"\"></p>\n<ul>\n          <li>This product, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-276\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-277\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">BA</script>, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">B</script> is a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-283\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>×</mo><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">d \\times r</script> matrix and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">A</script> is an <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-291\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>×</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">r \\times d</script> matrix, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">r</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-299\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">d</script>, the resulting product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-302\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-303\"><span class=\"mi\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">BA</script> will have a maximum rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-306\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-307\"><span class=\"mi\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">r</script>, regardless of the dimensions of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-309\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-310\"><span class=\"mi\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">d</script>. This means the update to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">W</script> is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.</li>\n          <li>For example, if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mn>1000</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-315\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1000</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>=</mo><mn>1000</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">d = 1000</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">r = 2</script>, the update matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">BA</script> will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-329\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-330\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">BA</script> is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-333\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-334\"><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">W</script> frozen.</li>\n        </ul>\n<ul>\n      <li>LoRA fundamentally changes the approach to fine-tuning large neural networks by introducing a method to decompose high-dimensional weight matrices into lower-dimensional forms, preserving essential information while reducing computational load. Put simply, LoRA efficiently fine-tunes large-scale neural networks by introducing trainable low-rank matrices, simplifying the model’s complexity while retaining its robust learning capabilities.</li>\n      <li>LoRA is similar to methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), leveraging the observation that weight updates during fine-tuning are often rank-deficient. By imposing a low-rank constraint on these updates, LoRA enables a lightweight adaptation process that captures the most critical directions or “components” in the weight space. This means the model can retain essential knowledge from its pre-training phase while focusing on the specific nuances of the new task, resulting in efficient adaptation with a smaller memory and computational footprint.</li>\n      <li>During backpropagation, only the low-rank LoRA matrices (A and B) receive gradient updates; the original pretrained weights remain frozen. Gradients flow through the combined effective weight (W = W_0 + BA), but only the LoRA-specific parameters (A and B) have <code class=\"language-plaintext highlighter-rouge\">requires_grad=True</code>, ensuring that only these low-rank components are optimized while the base model weights remain unchanged.</li>\n    </ul>\n<ul>\n      <li>LoRA’s primary application is in the efficient fine-tuning of large neural networks, particularly in transformer-based architectures. In practice, LoRA identifies key dimensions within the original weight matrix that are crucial for a specific task, significantly reducing the adaptation’s dimensionality.</li>\n      <li>Instead of fine-tuning all weights, which is computationally prohibitive for large models, LoRA introduces two trainable low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-336\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-337\"><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">B</script>, in specific layers of the transformer (e.g., in the query and value projections of the attention mechanism). These matrices are optimized during training while keeping the core model parameters frozen. This architecture adjustment means that only a fraction of the original parameters are actively updated, thus lowering memory usage and enabling faster computations.</li>\n      <li>By focusing only on the most relevant dimensions for each task, LoRA enables the deployment of large models on hardware with limited resources and facilitates task-switching by updating only the low-rank matrices without retraining the entire model.</li>\n    </ul>\n<ul>\n      <li>LoRA provides several advantages that make it particularly suitable for practical use in industry and research settings where resource constraints are a concern. The primary benefit is in memory and computational efficiency.</li>\n      <li>By keeping the majority of the model’s parameters frozen and only updating the low-rank matrices, LoRA significantly reduces the number of trainable parameters, leading to lower memory consumption and faster training times. This reduction in training complexity also means that fewer GPUs or lower-spec hardware can be used to fine-tune large models, broadening accessibility.</li>\n      <li>Additionally, LoRA avoids introducing any additional latency during inference because, once the low-rank matrices have been trained, they can be merged back into the pre-trained weights without altering the architecture.</li>\n      <li>This setup makes LoRA ideal for environments where models need to switch between tasks frequently, as only the task-specific low-rank weights need to be loaded, allowing the model to quickly adapt to new tasks or datasets with minimal overhead.</li>\n    </ul>\n<ul>\n      <li>LoRA represents a highly efficient approach to fine-tuning, striking a balance between the depth of knowledge encapsulated in large pre-trained models and the need for targeted adaptation to new tasks or data.</li>\n      <li>By leveraging the low intrinsic rank of weight updates, LoRA retains the model’s robustness and generalization capability while allowing for quick, cost-effective adaptations. This approach redefines efficiency in the era of massive LLMs, making it feasible to use and adapt large-scale pre-trained models across various applications and domains without the extensive computational burden associated with traditional fine-tuning.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora2.png\" alt=\"\"></p>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/1.png\" alt=\"\"></p>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/2.png\" alt=\"\"></p>\n<h5 id=\"advantages-1\">Advantages</h5>\n<h6 id=\"parameter-efficiency\">Parameter Efficiency</h6>\n<ul>\n  <li>Compared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the number of trainable parameters by 10,000 times. Specifically, this means that LoRA only fine-tunes approximately 0.01% of the parameters of the original model.</li>\n  <li>The below table from the LoRA paper indicates that for GPT-3 with LoRA, we see that we only fine-tune <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mn>4.7</mn><mn>175255</mn></mfrac><mo>&amp;#x00D7;</mo><mn>100</mn><mo>=</mo><mn>0.002</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-342\" style=\"width: 11.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1009.27em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-343\"><span class=\"mfrac\" id=\"MathJax-Span-344\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.84em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mn\" id=\"MathJax-Span-345\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4.7</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.09em, 4.169em, -999.997em); top: -3.591em; left: 50%; margin-left: -1.039em;\"><span class=\"mn\" id=\"MathJax-Span-346\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">175255</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.242em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-347\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">100</span><span class=\"mo\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.002</span><span class=\"mi\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Regular;\">%</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mn>4.7</mn><mn>175255</mn></mfrac><mo>×</mo><mn>100</mn><mo>=</mo><mn>0.002</mn><mi mathvariant=\"normal\">%</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">\\frac{4.7}{175255} \\times 100 = 0.002\\%</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mn>38</mn><mn>175255</mn></mfrac><mo>&amp;#x00D7;</mo><mn>100</mn><mo>=</mo><mn>0.02</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-352\" style=\"width: 10.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1008.75em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-353\"><span class=\"mfrac\" id=\"MathJax-Span-354\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.68em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.362em;\"><span class=\"mn\" id=\"MathJax-Span-355\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.09em, 4.169em, -999.997em); top: -3.591em; left: 50%; margin-left: -1.039em;\"><span class=\"mn\" id=\"MathJax-Span-356\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">175255</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.242em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-358\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">100</span><span class=\"mo\" id=\"MathJax-Span-359\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.02</span><span class=\"mi\" id=\"MathJax-Span-361\" style=\"font-family: STIXGeneral-Regular;\">%</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mn>38</mn><mn>175255</mn></mfrac><mo>×</mo><mn>100</mn><mo>=</mo><mn>0.02</mn><mi mathvariant=\"normal\">%</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">\\frac{38}{175255} \\times 100 = 0.02\\%</script> parameters.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/param-efficiency.jpg\" alt=\"\"></p>\n<h6 id=\"gpu-memory-and-storage-savings\">GPU Memory (and Storage) Savings</h6>\n<ul>\n  <li>Compared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the GPU memory requirement by 3 times. Specifically, this means that LoRA fine-tunes the original model with 33% of the memory.</li>\n  <li>For a large Transformer trained with Adam, LoRA reduces VRAM usage by up to two-thirds by avoiding the need to store optimizer states for the frozen parameters. On GPT-3 175B, VRAM consumption during training drops from 1.2TB to 350GB. When adapting only the query and value projection matrices with a rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-362\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-363\"><span class=\"mi\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-365\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">r = 4</script>, the checkpoint size decreases significantly from approximately 350GB to 35MB. This efficiency allows training with significantly fewer GPUs and avoids I/O bottlenecks.</li>\n</ul>\n<h6 id=\"efficient-task-switching\">Efficient Task Switching</h6>\n<ul>\n  <li>Task switching is more cost-effective as only the LoRA weights need swapping, enabling the creation of numerous customized models that can be dynamically swapped on machines storing the pre-trained weights in VRAM.</li>\n</ul>\n<h6 id=\"faster-training-speed\">Faster Training Speed</h6>\n<ul>\n  <li>Training speed also improves by 25% compared to full fine-tuning, as the gradient calculation for the vast majority of the parameters is unnecessary.</li>\n</ul>\n<h6 id=\"no-additional-inference-latency\">No Additional Inference Latency</h6>\n<ul>\n  <li>LoRA ensures no additional inference latency when deployed in production by allowing explicit computation and storage of the combined weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 7.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.89em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-370\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-371\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-372\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-373\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-374\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span><span class=\"mi\" id=\"MathJax-Span-376\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">W = W_0 + BA</script>. During inference, this approach uses the pre-computed matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-377\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-378\"><span class=\"mi\" id=\"MathJax-Span-379\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">W</script>, which includes the original pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-380\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-381\"><span class=\"msubsup\" id=\"MathJax-Span-382\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-384\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">W_0</script> and the low-rank adaptation matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-385\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"mi\" id=\"MathJax-Span-387\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">B</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-388\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-389\"><span class=\"mi\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">A</script>. This method eliminates the need for dynamic computations during inference.</li>\n  <li>When switching to another downstream task, the pre-trained weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-391\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-392\"><span class=\"msubsup\" id=\"MathJax-Span-393\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">W_0</script> can be quickly restored by subtracting the current low-rank product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-396\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-397\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">BA</script> and adding the new task-specific low-rank product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>B</mi><mo>&amp;#x2032;</mo></msup><msup><mi>A</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-400\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1001.88em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-401\"><span class=\"msup\" id=\"MathJax-Span-402\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Italic;\">B</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-404\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msup\" id=\"MathJax-Span-405\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral-Italic;\">A</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mo\" id=\"MathJax-Span-407\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>B</mi><mo>′</mo></msup><msup><mi>A</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">B' A'</script>. This operation incurs minimal memory overhead and allows for efficient task switching without impacting inference speed. By merging the low-rank matrices with the pre-trained weights in advance, LoRA avoids the extra computational burden during real-time inference (unlike adapters), ensuring latency remains on par with that of fully fine-tuned models.</li>\n</ul>\n<h5 id=\"limitations\">Limitations</h5>\n<ul>\n  <li>While LoRA offers significant advantages in terms of parameter efficiency and memory savings, it also has some limitations. One notable limitation is the complexity involved in batching inputs for different tasks when using distinct low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-408\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"mi\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">B</script>. If the goal is to absorb <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-414\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-415\"><span class=\"mi\" id=\"MathJax-Span-416\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-417\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-418\"><span class=\"mi\" id=\"MathJax-Span-419\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">B</script> into the combined weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-420\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-421\"><span class=\"mi\" id=\"MathJax-Span-422\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">W</script> to avoid additional inference latency, it becomes challenging to batch inputs from different tasks in a single forward pass. This is because each task would require a different set of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-423\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-424\"><span class=\"mi\" id=\"MathJax-Span-425\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-426\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-427\"><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">B</script> matrices, complicating the batching process.</li>\n  <li>Additionally, although it is possible to avoid merging the weights and dynamically select the appropriate LoRA modules for each sample in a batch, this approach is more feasible in scenarios where latency is not a critical concern. This workaround does not fully address the need for seamless integration when low-latency inference is required across multiple tasks.</li>\n  <li>In summary, while LoRA provides a highly efficient adaptation method, the complexity in handling multiple tasks simultaneously and the need for careful management of low-rank matrices during batching are important considerations for its practical deployment.</li>\n</ul>\n<h5 id=\"hyperparameters\">Hyperparameters</h5>\n<ul>\n  <li>LoRA-specific hyperparameters include rank (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-429\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-430\"><span class=\"mi\" id=\"MathJax-Span-431\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">r</script>) and alpha (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-432\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-433\"><span class=\"mi\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">\\alpha</script>). Others, while still used for LoRA-based fine-tuning, such as learning rate (lr), dropout probability (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-435\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-436\"><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">p</script>), and batch size (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-438\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-439\"><span class=\"mi\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">N</script>), are more generic to deep learning-based model training/fine-tuning. Here’s a detailed explanation of each:</li>\n</ul>\n<h6 id=\"rank-r\">Rank (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-441\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-442\"><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">r</script>)</h6>\n<ul>\n  <li>\n    <p><strong>Description</strong>: In LoRA, instead of fine-tuning the full weight matrix, the weight updates are modeled as a low-rank approximation. Specifically, the weight update matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-444\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.72em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">\\Delta W</script> is decomposed into two smaller matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-448\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-449\"><span class=\"mi\" id=\"MathJax-Span-450\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-451\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-452\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-453\"><span class=\"mrow\" id=\"MathJax-Span-454\"><span class=\"mi\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-456\"><span class=\"mrow\" id=\"MathJax-Span-457\"><span class=\"mi\" id=\"MathJax-Span-458\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">A \\in \\mathbb{R}^{d \\times r}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-461\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.75em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-462\"><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-464\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-465\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">B \\in \\mathbb{R}^{r \\times k}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-474\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-475\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">r</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-477\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-478\"><span class=\"mi\" id=\"MathJax-Span-479\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">d</script> or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-480\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-481\"><span class=\"mi\" id=\"MathJax-Span-482\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">k</script>. The rank (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-483\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-484\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">r</script>) of matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-486\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-487\"><span class=\"mi\" id=\"MathJax-Span-488\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-489\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-490\"><span class=\"mi\" id=\"MathJax-Span-491\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">B</script> – one of the core hyperparameters in LoRA – represents the rank of the low-rank decomposition applied to the weight matrices. The new weight is then modeled as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-492\" style=\"width: 14.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1012.14em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-493\"><span class=\"mi\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-496\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-498\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-502\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-503\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-504\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-505\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-507\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">A</span><span class=\"mo\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>A</mi><mo>⋅</mo><mi>B</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-91\">W = W_0 + \\Delta W = W_0 + A \\cdot B</script>\n  </li>\n  <li><strong>Role</strong>: The rank controls the dimensionality of the low-rank matrices and hence the number of additional parameters introduced during fine-tuning.</li>\n  <li>\n    <p><strong>Interpretation</strong>: Lower values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-510\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-511\"><span class=\"mi\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">r</script> will impose stronger restrictions on how much the weight matrices can adapt, potentially limiting the model’s flexibility but greatly reducing the computational and memory footprint. Higher values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-513\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-514\"><span class=\"mi\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">r</script> allow for more expressive updates but increase the number of parameters and computation required.</p>\n  </li>\n  <li>\n    <p><strong>Equation</strong>: In matrix form, for any original weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-516\" style=\"width: 5.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.43em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-517\"><span class=\"msubsup\" id=\"MathJax-Span-518\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-520\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-522\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-523\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-526\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"mi\" id=\"MathJax-Span-528\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-530\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">W_0 \\in \\mathbb{R}^{d \\times k}</script>, the adapted weight update is expressed as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-531\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.26em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-532\"><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-534\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mo\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>A</mi><mo>⋅</mo><mi>B</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-95\">\\Delta W = A \\cdot B</script>\n\n    <ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-539\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-540\"><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-542\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-543\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-544\"><span class=\"mrow\" id=\"MathJax-Span-545\"><span class=\"mi\" id=\"MathJax-Span-546\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-547\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mi\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-550\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-551\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">A \\in \\mathbb{R}^{d \\times r}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-552\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.75em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-553\"><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-556\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-557\"><span class=\"mrow\" id=\"MathJax-Span-558\"><span class=\"mi\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-560\"><span class=\"mrow\" id=\"MathJax-Span-561\"><span class=\"mi\" id=\"MathJax-Span-562\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-563\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">B \\in \\mathbb{R}^{r \\times k}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi><mo>,</mo><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-565\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.23em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mi\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-568\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-569\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi><mo>,</mo><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">r \\ll d, k</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Typical Values</strong>: 2–16, depending on the size of the model and the complexity of the task. In most tasks, a small rank (e.g., 4 or 8) provides a good trade-off between performance and efficiency.</p>\n  </li>\n  <li>\n    <p><strong>Higher Values</strong>: For more complex tasks, larger models, or cases where the pretrained model diverges significantly from the specialized task, higher rank values (e.g., 32, 64, or 128) may be used. Examples include:</p>\n\n    <ul>\n      <li>Adapting a general language model to legal contract review, where formal, domain-specific syntax and terminology dominate.</li>\n      <li>Fine-tuning for biomedical question answering or clinical note summarization, which involves specialized jargon not well represented in general corpora.</li>\n      <li>Tuning for code generation in a low-resource or proprietary programming language.</li>\n      <li>\n        <p>Adapting to historical or archaic language for cultural heritage and digitization tasks.</p>\n      </li>\n      <li>These scenarios benefit from higher-rank LoRA modules due to the substantial gap between the pretraining data and the target domain, requiring more capacity to learn meaningful adaptations.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Description</strong>: In LoRA, instead of fine-tuning the full weight matrix, the weight updates are modeled as a low-rank approximation. Specifically, the weight update matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-444\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.72em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">\\Delta W</script> is decomposed into two smaller matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-448\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-449\"><span class=\"mi\" id=\"MathJax-Span-450\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-451\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-452\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-453\"><span class=\"mrow\" id=\"MathJax-Span-454\"><span class=\"mi\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-456\"><span class=\"mrow\" id=\"MathJax-Span-457\"><span class=\"mi\" id=\"MathJax-Span-458\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">A \\in \\mathbb{R}^{d \\times r}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-461\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.75em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-462\"><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-464\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-465\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">B \\in \\mathbb{R}^{r \\times k}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-474\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-475\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">r</script> is much smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-477\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-478\"><span class=\"mi\" id=\"MathJax-Span-479\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">d</script> or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-480\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-481\"><span class=\"mi\" id=\"MathJax-Span-482\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">k</script>. The rank (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-483\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-484\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">r</script>) of matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-486\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-487\"><span class=\"mi\" id=\"MathJax-Span-488\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-489\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-490\"><span class=\"mi\" id=\"MathJax-Span-491\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">B</script> – one of the core hyperparameters in LoRA – represents the rank of the low-rank decomposition applied to the weight matrices. The new weight is then modeled as:</p>\n<p><strong>Interpretation</strong>: Lower values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-510\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-511\"><span class=\"mi\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">r</script> will impose stronger restrictions on how much the weight matrices can adapt, potentially limiting the model’s flexibility but greatly reducing the computational and memory footprint. Higher values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-513\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-514\"><span class=\"mi\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">r</script> allow for more expressive updates but increase the number of parameters and computation required.</p>\n<p><strong>Equation</strong>: In matrix form, for any original weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-516\" style=\"width: 5.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.43em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-517\"><span class=\"msubsup\" id=\"MathJax-Span-518\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-520\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-522\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-523\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-526\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"mi\" id=\"MathJax-Span-528\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-530\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">W_0 \\in \\mathbb{R}^{d \\times k}</script>, the adapted weight update is expressed as:</p>\n<ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-539\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-540\"><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-542\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-543\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-544\"><span class=\"mrow\" id=\"MathJax-Span-545\"><span class=\"mi\" id=\"MathJax-Span-546\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-547\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mi\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-550\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-551\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">A \\in \\mathbb{R}^{d \\times r}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-552\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.75em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-553\"><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-556\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-557\"><span class=\"mrow\" id=\"MathJax-Span-558\"><span class=\"mi\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-560\"><span class=\"mrow\" id=\"MathJax-Span-561\"><span class=\"mi\" id=\"MathJax-Span-562\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-563\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">B \\in \\mathbb{R}^{r \\times k}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi><mo>,</mo><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-565\" style=\"width: 3.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.23em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mi\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-568\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-569\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi><mo>,</mo><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">r \\ll d, k</script>.</li>\n    </ul>\n<p><strong>Typical Values</strong>: 2–16, depending on the size of the model and the complexity of the task. In most tasks, a small rank (e.g., 4 or 8) provides a good trade-off between performance and efficiency.</p>\n<p><strong>Higher Values</strong>: For more complex tasks, larger models, or cases where the pretrained model diverges significantly from the specialized task, higher rank values (e.g., 32, 64, or 128) may be used. Examples include:</p>\n<ul>\n      <li>Adapting a general language model to legal contract review, where formal, domain-specific syntax and terminology dominate.</li>\n      <li>Fine-tuning for biomedical question answering or clinical note summarization, which involves specialized jargon not well represented in general corpora.</li>\n      <li>Tuning for code generation in a low-resource or proprietary programming language.</li>\n      <li>\n        <p>Adapting to historical or archaic language for cultural heritage and digitization tasks.</p>\n      </li>\n      <li>These scenarios benefit from higher-rank LoRA modules due to the substantial gap between the pretraining data and the target domain, requiring more capacity to learn meaningful adaptations.</li>\n    </ul>\n<p>Adapting to historical or archaic language for cultural heritage and digitization tasks.</p>\n<h6 id=\"scaling-factor-alpha\">Scaling Factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-572\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-573\"><span class=\"mi\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">\\alpha</script>)</h6>\n<ul>\n  <li>\n    <p><strong>Description</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-575\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"mi\" id=\"MathJax-Span-577\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">\\alpha</script> is a scaling factor applied to the LoRA updates. Specifically, it scales the low-rank updates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-578\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.35em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-579\"><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-581\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>⋅</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">A \\cdot B</script> before adding them to the base weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-583\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-584\"><span class=\"msubsup\" id=\"MathJax-Span-585\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">W_0</script>. The weight update rule becomes:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&amp;#x03B1;</mi><mi>r</mi></mfrac><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-588\" style=\"width: 11.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1009.38em, 3.076em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-589\"><span class=\"mi\" id=\"MathJax-Span-590\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-592\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-594\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-595\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mfrac\" id=\"MathJax-Span-596\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-599\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span><span class=\"mo\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>α</mi><mi>r</mi></mfrac><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>A</mi><mo>⋅</mo><mi>B</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-103\">W = W_0 + \\frac{\\alpha}{r} \\cdot (A \\cdot B)</script>\n  </li>\n  <li>\n    <p><strong>Role</strong>: The purpose of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-605\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-606\"><span class=\"mi\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">\\alpha</script> is to control the magnitude of the low-rank updates to prevent the model from diverging too far from the pre-trained weights. By dividing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-608\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-609\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">\\alpha</script> by the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-611\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-612\"><span class=\"mi\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">r</script>, LoRA ensures that the update magnitude is normalized according to the size of the low-rank decomposition. This is crucial because a larger rank would introduce more freedom for updates, and the division by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-614\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">r</script> keeps the updates in check.</p>\n  </li>\n  <li>\n    <p><strong>Interpretation</strong>: A higher <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-617\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"mi\" id=\"MathJax-Span-619\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">\\alpha</script> means that the low-rank updates will have a larger impact on the final weight, while a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-620\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-621\"><span class=\"mi\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">\\alpha</script> means the low-rank updates will contribute less to the adapted model. The division by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-623\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">r</script> helps keep the effect of the low-rank update consistent across different choices of rank.</p>\n  </li>\n  <li>\n    <p><strong>Equation</strong>: The weight update is now written as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mfrac><mi>&amp;#x03B1;</mi><mi>r</mi></mfrac><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-626\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1007.76em, 3.076em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-627\"><span class=\"mi\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-631\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mi\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-637\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-638\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span><span class=\"mo\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mfrac><mi>α</mi><mi>r</mi></mfrac><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>A</mi><mo>⋅</mo><mi>B</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-111\">\\Delta W = \\frac{\\alpha}{r} \\cdot (A \\cdot B)</script>\n  </li>\n  <li>\n    <p><strong>Typical Values</strong>: Common values for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">\\alpha</script> are in the range of 1–32. The typical recommendation is to set <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>=</mo><mfrac><mi>r</mi><mtext>base rank</mtext></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-643\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-644\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-647\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.71em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.352em;\"><span class=\"mtext\" id=\"MathJax-Span-649\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">base rank</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.82em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.815em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi><mo>=</mo><mfrac><mi>r</mi><mtext>base rank</mtext></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">\\alpha = \\frac{r}{\\text{base rank}}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>base rank</mtext></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-650\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.8em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-651\"><span class=\"mtext\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Regular;\">base rank</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>base rank</mtext></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">\\text{base rank}</script> is a predetermined scale for the model.</p>\n  </li>\n</ul>\n<p><strong>Description</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-575\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"mi\" id=\"MathJax-Span-577\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">\\alpha</script> is a scaling factor applied to the LoRA updates. Specifically, it scales the low-rank updates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x22C5;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-578\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.35em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-579\"><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-581\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>⋅</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">A \\cdot B</script> before adding them to the base weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-583\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-584\"><span class=\"msubsup\" id=\"MathJax-Span-585\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">W_0</script>. The weight update rule becomes:</p>\n<p><strong>Role</strong>: The purpose of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-605\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-606\"><span class=\"mi\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">\\alpha</script> is to control the magnitude of the low-rank updates to prevent the model from diverging too far from the pre-trained weights. By dividing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-608\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-609\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">\\alpha</script> by the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-611\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-612\"><span class=\"mi\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">r</script>, LoRA ensures that the update magnitude is normalized according to the size of the low-rank decomposition. This is crucial because a larger rank would introduce more freedom for updates, and the division by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-614\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">r</script> keeps the updates in check.</p>\n<p><strong>Interpretation</strong>: A higher <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-617\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"mi\" id=\"MathJax-Span-619\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">\\alpha</script> means that the low-rank updates will have a larger impact on the final weight, while a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-620\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-621\"><span class=\"mi\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">\\alpha</script> means the low-rank updates will contribute less to the adapted model. The division by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-623\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">r</script> helps keep the effect of the low-rank update consistent across different choices of rank.</p>\n<p><strong>Equation</strong>: The weight update is now written as:</p>\n<p><strong>Typical Values</strong>: Common values for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">\\alpha</script> are in the range of 1–32. The typical recommendation is to set <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>=</mo><mfrac><mi>r</mi><mtext>base rank</mtext></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-643\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-644\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-647\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.71em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.352em;\"><span class=\"mtext\" id=\"MathJax-Span-649\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">base rank</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.82em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.815em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi><mo>=</mo><mfrac><mi>r</mi><mtext>base rank</mtext></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">\\alpha = \\frac{r}{\\text{base rank}}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>base rank</mtext></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-650\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.8em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-651\"><span class=\"mtext\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Regular;\">base rank</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>base rank</mtext></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">\\text{base rank}</script> is a predetermined scale for the model.</p>\n<h6 id=\"dropout-probability-p\">Dropout Probability (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-653\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-654\"><span class=\"mi\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-115\">p</script>)</h6>\n<ul>\n  <li>\n    <p><strong>Description</strong>: <a href=\"../dropout\">Dropout</a> is a regularization technique used to prevent overfitting, and it is applied in the LoRA framework as well. The dropout probability (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"mi\" id=\"MathJax-Span-658\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">p</script>) refers to the probability with which a particular element in the low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-659\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-660\"><span class=\"mi\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-662\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-663\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">B</script> is randomly set to zero during training. Dropout is typically used to reduce overfitting by introducing noise during training.</p>\n\n    <ul>\n      <li>\n        <p><strong>Role</strong>: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: Higher values of dropout probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-665\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">p</script> imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-668\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">p</script> imply less regularization and could potentially lead to overfitting on small datasets.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The dropout operation is typically represented as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>A</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>A</mi><mo>&amp;#x2299;</mo><mtext>Bernoulli</mtext><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-671\" style=\"width: 15.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1012.82em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-672\"><span class=\"msubsup\" id=\"MathJax-Span-673\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Italic;\">A</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-675\"><span class=\"mrow\" id=\"MathJax-Span-676\"><span class=\"mi\" id=\"MathJax-Span-677\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-679\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-683\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mo\" id=\"MathJax-Span-686\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⊙</span><span class=\"mtext\" id=\"MathJax-Span-687\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Bernoulli</span><span class=\"mo\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-689\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-690\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-692\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>A</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>A</mi><mo>⊙</mo><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-121\">A_{dropped} = A \\odot \\text{Bernoulli}(1-p)</script>\n\n        <ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mo\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">\\odot</script> denotes element-wise multiplication, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Bernoulli</mtext><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-696\" style=\"width: 7.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.46em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-697\"><span class=\"mtext\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Regular;\">Bernoulli</span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-701\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">\\text{Bernoulli}(1-p)</script> is a binary mask where each element is independently drawn from a Bernoulli distribution with probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mn\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-707\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>−</mo><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">1 - p</script>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Dropout probabilities <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-709\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">p</script> are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Description</strong>: <a href=\"../dropout\">Dropout</a> is a regularization technique used to prevent overfitting, and it is applied in the LoRA framework as well. The dropout probability (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"mi\" id=\"MathJax-Span-658\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">p</script>) refers to the probability with which a particular element in the low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-659\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-660\"><span class=\"mi\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-662\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-663\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">B</script> is randomly set to zero during training. Dropout is typically used to reduce overfitting by introducing noise during training.</p>\n<ul>\n      <li>\n        <p><strong>Role</strong>: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: Higher values of dropout probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-665\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">p</script> imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-668\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">p</script> imply less regularization and could potentially lead to overfitting on small datasets.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The dropout operation is typically represented as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>A</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>A</mi><mo>&amp;#x2299;</mo><mtext>Bernoulli</mtext><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-671\" style=\"width: 15.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1012.82em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-672\"><span class=\"msubsup\" id=\"MathJax-Span-673\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Italic;\">A</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-675\"><span class=\"mrow\" id=\"MathJax-Span-676\"><span class=\"mi\" id=\"MathJax-Span-677\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-679\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-683\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mo\" id=\"MathJax-Span-686\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⊙</span><span class=\"mtext\" id=\"MathJax-Span-687\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Bernoulli</span><span class=\"mo\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-689\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-690\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-692\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>A</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>A</mi><mo>⊙</mo><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-121\">A_{dropped} = A \\odot \\text{Bernoulli}(1-p)</script>\n\n        <ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mo\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">\\odot</script> denotes element-wise multiplication, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Bernoulli</mtext><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-696\" style=\"width: 7.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.46em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-697\"><span class=\"mtext\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Regular;\">Bernoulli</span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-701\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">\\text{Bernoulli}(1-p)</script> is a binary mask where each element is independently drawn from a Bernoulli distribution with probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mn\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-707\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>−</mo><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">1 - p</script>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Dropout probabilities <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-709\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">p</script> are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.</p>\n      </li>\n    </ul>\n<p><strong>Role</strong>: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.</p>\n<p><strong>Interpretation</strong>: Higher values of dropout probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-665\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">p</script> imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-668\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">p</script> imply less regularization and could potentially lead to overfitting on small datasets.</p>\n<p><strong>Equation</strong>: The dropout operation is typically represented as:</p>\n<ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"mo\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">\\odot</script> denotes element-wise multiplication, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>Bernoulli</mtext><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-696\" style=\"width: 7.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.46em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-697\"><span class=\"mtext\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Regular;\">Bernoulli</span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-701\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">\\text{Bernoulli}(1-p)</script> is a binary mask where each element is independently drawn from a Bernoulli distribution with probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x2212;</mo><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mn\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-707\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>−</mo><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">1 - p</script>.</li>\n        </ul>\n<p><strong>Typical Values</strong>: Dropout probabilities <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-709\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">p</script> are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.</p>\n<h6 id=\"learning-rate-eta\">Learning Rate (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-712\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-713\"><span class=\"mi\" id=\"MathJax-Span-714\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-126\">\\eta</script>)</h6>\n<ul>\n  <li>\n    <p><strong>Description</strong>: The learning rate is a fundamental hyperparameter in any optimization process, and it determines the step size at which the model’s parameters are updated during training. In the context of LoRA, it controls the update of the low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-715\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-716\"><span class=\"mi\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-718\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-719\"><span class=\"mi\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">B</script> rather than the full model weights.</p>\n\n    <ul>\n      <li>\n        <p><strong>Role</strong>: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-721\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-722\"><span class=\"mi\" id=\"MathJax-Span-723\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">\\alpha</script> is large.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The update to the low-rank parameters follows the standard gradient descent update rule:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03B8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>&amp;#x03B8;</mi><mi>t</mi></msub><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mo>&amp;#x22C5;</mo><msub><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mi>L</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-724\" style=\"width: 9.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.97em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-725\"><span class=\"msubsup\" id=\"MathJax-Span-726\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-727\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-728\"><span class=\"mrow\" id=\"MathJax-Span-729\"><span class=\"mi\" id=\"MathJax-Span-730\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-731\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-732\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-733\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-734\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-735\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-736\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-738\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mo\" id=\"MathJax-Span-739\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-740\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-742\"><span class=\"mrow\" id=\"MathJax-Span-743\"><span class=\"mi\" id=\"MathJax-Span-744\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>θ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mo>⋅</mo><msub><mi mathvariant=\"normal\">∇</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mi>L</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-130\">\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} L</script>\n\n        <p>Where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">L</script> is the loss function, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"msubsup\" id=\"MathJax-Span-751\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-752\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-753\"><span class=\"mrow\" id=\"MathJax-Span-754\"><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">∇</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">\\nabla_{\\theta} L</script> is the gradient of the loss with respect to the low-rank parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">\\theta</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-760\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-761\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">\\eta</script> is the learning rate.</p>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Learning rates for LoRA typically range from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-763\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"msubsup\" id=\"MathJax-Span-765\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-766\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-767\"><span class=\"mrow\" id=\"MathJax-Span-768\"><span class=\"mo\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-135\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>3</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-771\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-772\"><span class=\"msubsup\" id=\"MathJax-Span-773\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-775\"><span class=\"mrow\" id=\"MathJax-Span-776\"><span class=\"mo\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-778\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>3</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">10^{-3}</script>, depending on the model, the task, and the scale of adaptation needed.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Description</strong>: The learning rate is a fundamental hyperparameter in any optimization process, and it determines the step size at which the model’s parameters are updated during training. In the context of LoRA, it controls the update of the low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-715\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-716\"><span class=\"mi\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-718\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-719\"><span class=\"mi\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">B</script> rather than the full model weights.</p>\n<ul>\n      <li>\n        <p><strong>Role</strong>: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-721\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-722\"><span class=\"mi\" id=\"MathJax-Span-723\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">\\alpha</script> is large.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The update to the low-rank parameters follows the standard gradient descent update rule:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03B8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>&amp;#x03B8;</mi><mi>t</mi></msub><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mo>&amp;#x22C5;</mo><msub><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mi>L</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-724\" style=\"width: 9.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.97em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-725\"><span class=\"msubsup\" id=\"MathJax-Span-726\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-727\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-728\"><span class=\"mrow\" id=\"MathJax-Span-729\"><span class=\"mi\" id=\"MathJax-Span-730\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-731\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-732\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-733\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-734\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-735\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-736\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-738\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mo\" id=\"MathJax-Span-739\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-740\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-742\"><span class=\"mrow\" id=\"MathJax-Span-743\"><span class=\"mi\" id=\"MathJax-Span-744\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>θ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mo>⋅</mo><msub><mi mathvariant=\"normal\">∇</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mi>L</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-130\">\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} L</script>\n\n        <p>Where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">L</script> is the loss function, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"msubsup\" id=\"MathJax-Span-751\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-752\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-753\"><span class=\"mrow\" id=\"MathJax-Span-754\"><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">∇</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">\\nabla_{\\theta} L</script> is the gradient of the loss with respect to the low-rank parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">\\theta</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-760\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-761\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">\\eta</script> is the learning rate.</p>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Learning rates for LoRA typically range from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-763\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"msubsup\" id=\"MathJax-Span-765\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-766\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-767\"><span class=\"mrow\" id=\"MathJax-Span-768\"><span class=\"mo\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-135\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>3</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-771\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-772\"><span class=\"msubsup\" id=\"MathJax-Span-773\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-775\"><span class=\"mrow\" id=\"MathJax-Span-776\"><span class=\"mo\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-778\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>3</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">10^{-3}</script>, depending on the model, the task, and the scale of adaptation needed.</p>\n      </li>\n    </ul>\n<p><strong>Role</strong>: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.</p>\n<p><strong>Interpretation</strong>: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-721\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-722\"><span class=\"mi\" id=\"MathJax-Span-723\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">\\alpha</script> is large.</p>\n<p><strong>Equation</strong>: The update to the low-rank parameters follows the standard gradient descent update rule:</p>\n<p>Where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">L</script> is the loss function, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"msubsup\" id=\"MathJax-Span-751\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-752\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-753\"><span class=\"mrow\" id=\"MathJax-Span-754\"><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">∇</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">\\nabla_{\\theta} L</script> is the gradient of the loss with respect to the low-rank parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">\\theta</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-760\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-761\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">\\eta</script> is the learning rate.</p>\n<p><strong>Typical Values</strong>: Learning rates for LoRA typically range from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-763\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"msubsup\" id=\"MathJax-Span-765\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-766\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-767\"><span class=\"mrow\" id=\"MathJax-Span-768\"><span class=\"mo\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-135\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>3</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-771\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-772\"><span class=\"msubsup\" id=\"MathJax-Span-773\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-775\"><span class=\"mrow\" id=\"MathJax-Span-776\"><span class=\"mo\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-778\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>3</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">10^{-3}</script>, depending on the model, the task, and the scale of adaptation needed.</p>\n<h6 id=\"batch-size-n\">Batch Size (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-779\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-780\"><span class=\"mi\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">N</script>)</h6>\n<ul>\n  <li>\n    <p><strong>Description</strong>: The batch size is the number of examples that are passed through the model at one time before updating the weights. It is a crucial hyperparameter for stabilizing the training process.</p>\n\n    <ul>\n      <li>\n        <p><strong>Role</strong>: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The loss for a given batch of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-782\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-783\"><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">N</script> is averaged over the batch:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>batch</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>L</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-785\" style=\"width: 8.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1006.98em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-786\"><span class=\"msubsup\" id=\"MathJax-Span-787\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-789\"><span class=\"mrow\" id=\"MathJax-Span-790\"><span class=\"mtext\" id=\"MathJax-Span-791\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">batch</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-793\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-795\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-796\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-797\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-798\"><span class=\"mrow\" id=\"MathJax-Span-799\"><span class=\"mi\" id=\"MathJax-Span-800\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-801\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-803\"><span class=\"mrow\" id=\"MathJax-Span-804\"><span class=\"mi\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-806\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-807\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>batch</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>L</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-139\">L_{\\text{batch}} = \\frac{1}{N} \\sum_{i=1}^{N} L_i</script>\n\n        <ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-809\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"msubsup\" id=\"MathJax-Span-811\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-140\">L_i</script> is the loss for the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-814\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-815\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">i</script>-th example in the batch.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Description</strong>: The batch size is the number of examples that are passed through the model at one time before updating the weights. It is a crucial hyperparameter for stabilizing the training process.</p>\n<ul>\n      <li>\n        <p><strong>Role</strong>: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.</p>\n      </li>\n      <li>\n        <p><strong>Interpretation</strong>: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.</p>\n      </li>\n      <li>\n        <p><strong>Equation</strong>: The loss for a given batch of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-782\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-783\"><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">N</script> is averaged over the batch:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>batch</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>L</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-785\" style=\"width: 8.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1006.98em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-786\"><span class=\"msubsup\" id=\"MathJax-Span-787\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-789\"><span class=\"mrow\" id=\"MathJax-Span-790\"><span class=\"mtext\" id=\"MathJax-Span-791\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">batch</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-793\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-795\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-796\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-797\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-798\"><span class=\"mrow\" id=\"MathJax-Span-799\"><span class=\"mi\" id=\"MathJax-Span-800\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-801\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-803\"><span class=\"mrow\" id=\"MathJax-Span-804\"><span class=\"mi\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-806\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-807\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>batch</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>L</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-139\">L_{\\text{batch}} = \\frac{1}{N} \\sum_{i=1}^{N} L_i</script>\n\n        <ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-809\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"msubsup\" id=\"MathJax-Span-811\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-140\">L_i</script> is the loss for the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-814\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-815\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">i</script>-th example in the batch.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Typical Values</strong>: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.</p>\n      </li>\n    </ul>\n<p><strong>Role</strong>: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.</p>\n<p><strong>Interpretation</strong>: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.</p>\n<p><strong>Equation</strong>: The loss for a given batch of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-782\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-783\"><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">N</script> is averaged over the batch:</p>\n<ul>\n          <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-809\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"msubsup\" id=\"MathJax-Span-811\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-140\">L_i</script> is the loss for the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-814\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-815\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">i</script>-th example in the batch.</li>\n        </ul>\n<p><strong>Typical Values</strong>: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.</p>\n<h6 id=\"summary\">Summary</h6>\n<ul>\n  <li>The main hyperparameters involved in LoRA—rank (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-817\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-818\"><span class=\"mi\" id=\"MathJax-Span-819\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-142\">r</script>), alpha (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-820\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-821\"><span class=\"mi\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">\\alpha</script>), dropout probability (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-144-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-823\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-824\"><span class=\"mi\" id=\"MathJax-Span-825\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-144\">p</script>), learning rate (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-145-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-826\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-827\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-145\">\\eta</script>), and batch size (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-829\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-830\"><span class=\"mi\" id=\"MathJax-Span-831\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-146\">N</script>)—are crucial for controlling the behavior and effectiveness of LoRA. By adjusting these parameters, LoRA can offer an efficient way to fine-tune large pre-trained models with significantly reduced computational costs and memory usage while maintaining competitive performance. Each of these hyperparameters impacts the trade-off between model flexibility, computational efficiency, and training stability.</li>\n  <li>These hyperparameters are interconnected, especially scaling factor and rank; changes in one can require adjustments in others; more on this in the section on <a href=\"#is-there-a-relationship-between-setting-scaling-factor-and-rank-in-lora\">Is There a Relationship Between Setting Scaling Factor and Rank in LoRA?</a>. Effective tuning of these parameters is critical for leveraging LoRA’s capabilities to adapt large models without extensive retraining.</li>\n</ul>\n<h5 id=\"how-does-having-a-low-rank-matrix-in-lora-help-the-fine-tuning-process\">How Does Having a Low-rank Matrix in LoRA Help the Fine-tuning Process?</h5>\n<ul>\n  <li>In LoRA, a low-rank matrix is a matrix with a rank significantly smaller than its full dimensionality, which enables efficient and focused adjustments to model parameters. This lightweight adaptation mechanism allows large LLMs to learn new tasks without overfitting by capturing only the most essential adjustments, thus optimizing both information representation and parameter efficiency.</li>\n</ul>\n<h6 id=\"what-is-a-low-rank-matrix\">What is a Low-rank Matrix?</h6>\n<ul>\n  <li>A matrix is considered low-rank when its rank (the number of independent rows or columns) is much smaller than its dimensions. For example, a 1000x1000 matrix with rank 10 is low-rank because only 10 of its rows or columns contain unique information, and the others can be derived from these. This smaller rank indicates that the matrix contains a limited variety of independent patterns or directions, meaning it has a reduced capacity to capture complex relationships.</li>\n</ul>\n<h6 id=\"low-rank-in-lora-context\">Low-Rank in LoRA Context</h6>\n<ul>\n  <li>In LoRA, low-rank matrices are introduced to fine-tune large LLMs with fewer trainable parameters. Here’s how it works:\n    <ol>\n      <li><strong>Adding Low-Rank Matrices</strong>: LoRA adds small, low-rank matrices to the model’s layers (typically linear or attention layers). These matrices serve as “adaptation” layers that adjust the original layer’s output.</li>\n      <li><strong>Freezing the Original Weights</strong>: The original model weights remain frozen during fine-tuning. Only the low-rank matrices are trained, which reduces the number of parameters to update.</li>\n    </ol>\n  </li>\n  <li>By limiting the rank of these new matrices, LoRA effectively limits the number of patterns they can represent. For instance, a rank-5 matrix in a high-dimensional space can only capture 5 independent directions, which forces the model to learn only essential, low-dimensional adjustments without becoming too complex.</li>\n</ul>\n<ol>\n      <li><strong>Adding Low-Rank Matrices</strong>: LoRA adds small, low-rank matrices to the model’s layers (typically linear or attention layers). These matrices serve as “adaptation” layers that adjust the original layer’s output.</li>\n      <li><strong>Freezing the Original Weights</strong>: The original model weights remain frozen during fine-tuning. Only the low-rank matrices are trained, which reduces the number of parameters to update.</li>\n    </ol>\n<h6 id=\"example\">Example</h6>\n<ul>\n  <li>Suppose we have a pre-trained model layer represented by a 512x512 matrix (common in large LLMs). Instead of fine-tuning this large matrix directly, LoRA adds two low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-832\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-833\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-148-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-835\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"mi\" id=\"MathJax-Span-837\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-148\">B</script>, with dimensions 512x10 and 10x512, respectively. Here:\n    <ul>\n      <li>The product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-838\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-839\"><span class=\"mi\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">A \\times B</script> has a rank of 10, much smaller than 512.</li>\n      <li>This product effectively adds a low-rank adaptation to the original layer, allowing it to adjust its output in just a few key directions (10 in this case), rather than making unrestricted adjustments.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-838\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-839\"><span class=\"mi\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">A \\times B</script> has a rank of 10, much smaller than 512.</li>\n      <li>This product effectively adds a low-rank adaptation to the original layer, allowing it to adjust its output in just a few key directions (10 in this case), rather than making unrestricted adjustments.</li>\n    </ul>\n<h6 id=\"why-rank-matters\">Why Rank Matters</h6>\n<ul>\n  <li>The rank of the LoRA matrices directly affects the model’s ability to learn task-specific patterns:\n    <ul>\n      <li><strong>Lower Rank</strong>: Imposes a strong constraint on the model, which helps it generalize better and reduces the risk of overfitting.</li>\n      <li><strong>Higher Rank</strong>: Provides more flexibility but also increases the risk of overfitting, as the model can learn more complex adjustments that may fit the fine-tuning data too closely.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Lower Rank</strong>: Imposes a strong constraint on the model, which helps it generalize better and reduces the risk of overfitting.</li>\n      <li><strong>Higher Rank</strong>: Provides more flexibility but also increases the risk of overfitting, as the model can learn more complex adjustments that may fit the fine-tuning data too closely.</li>\n    </ul>\n<h5 id=\"how-does-low-rank-constraint-introduced-by-lora-inherently-act-as-a-form-of-regularization-especially-for-the-lower-layers-of-the-model\">How Does Low-rank Constraint Introduced by LoRA Inherently Act As a Form of Regularization, Especially for the Lower Layers of the Model?</h5>\n<ul>\n  <li>In LoRA, the low-rank constraint serves as a built-in regularization mechanism by limiting the model’s flexibility during fine-tuning. This constraint especially impacts lower layers, which are designed to capture general, foundational features. By further restricting these layers, LoRA minimizes their adaptation to task-specific data, reducing the risk of overfitting. This regularization preserves the model’s foundational knowledge in the lower layers, while allowing the higher layers—where task-specific adjustments are most beneficial—to adapt more freely.</li>\n</ul>\n<h6 id=\"low-rank-constraint-as-regularization\">Low-Rank Constraint As Regularization</h6>\n<ol>\n  <li>\n    <p><strong>Low-Rank Matrices Limit Complexity</strong>: By adding only low-rank matrices to the model’s layers, LoRA restricts the model’s capacity to represent highly complex, task-specific patterns. A low-rank matrix has fewer independent “directions” or dimensions in which it can vary. This means that the model, even when fine-tuned, can only make adjustments within a constrained range, learning broad, generalizable patterns rather than memorizing specific details of the training data. This limited capacity serves as a form of regularization, preventing the model from overfitting.</p>\n  </li>\n  <li>\n    <p><strong>Reduced Sensitivity to Noisy Patterns</strong>: Low-rank matrices inherently ignore minor or highly detailed variations in the training data, focusing only on dominant, overarching patterns. This makes LoRA less sensitive to the idiosyncrasies of the fine-tuning dataset, enhancing the model’s robustness and generalization ability.</p>\n  </li>\n</ol>\n<p><strong>Low-Rank Matrices Limit Complexity</strong>: By adding only low-rank matrices to the model’s layers, LoRA restricts the model’s capacity to represent highly complex, task-specific patterns. A low-rank matrix has fewer independent “directions” or dimensions in which it can vary. This means that the model, even when fine-tuned, can only make adjustments within a constrained range, learning broad, generalizable patterns rather than memorizing specific details of the training data. This limited capacity serves as a form of regularization, preventing the model from overfitting.</p>\n<p><strong>Reduced Sensitivity to Noisy Patterns</strong>: Low-rank matrices inherently ignore minor or highly detailed variations in the training data, focusing only on dominant, overarching patterns. This makes LoRA less sensitive to the idiosyncrasies of the fine-tuning dataset, enhancing the model’s robustness and generalization ability.</p>\n<h6 id=\"effect-on-lower-layers\">Effect on Lower Layers</h6>\n<ul>\n  <li>The <strong>lower layers</strong> of a neural network, especially in a transformer model, are primarily responsible for extracting general-purpose features from the input data. In LLMs, for example:\n    <ul>\n      <li>Lower layers capture basic syntactic relationships, such as sentence structure and word dependencies.</li>\n      <li>These layers learn representations that are widely applicable across tasks and domains.</li>\n    </ul>\n  </li>\n  <li>Because these lower layers are already optimized to represent broad, generalizable patterns from pre-training, they are naturally less flexible and more constrained in what they capture compared to higher layers, which focus on more task-specific details. Adding a low-rank constraint in LoRA further reinforces this effect:</li>\n</ul>\n<ul>\n      <li>Lower layers capture basic syntactic relationships, such as sentence structure and word dependencies.</li>\n      <li>These layers learn representations that are widely applicable across tasks and domains.</li>\n    </ul>\n<ol>\n  <li>\n    <p><strong>Enhanced Regularization on Lower Layers</strong>: Since lower layers are already constrained to capture only general patterns, the addition of a low-rank constraint essentially adds a second layer of regularization. This means that these layers become even less likely to adapt in ways that would compromise their general-purpose functionality. The low-rank constraint reinforces their role as foundational feature extractors, preserving their generalization capability while preventing overfitting on the specific details of the fine-tuning data.</p>\n  </li>\n  <li>\n    <p><strong>Minimal Disruption of Pre-Trained Knowledge</strong>: The low-rank adaptation in LoRA ensures that lower layers maintain the knowledge they acquired during pre-training. Because these layers are regularized by the low-rank constraint, they are less likely to overfit to new data patterns introduced during fine-tuning. This preservation of pre-trained knowledge is crucial for maintaining the model’s transferability to other tasks or domains, as lower layers retain their broad, foundational representations.</p>\n  </li>\n</ol>\n<p><strong>Enhanced Regularization on Lower Layers</strong>: Since lower layers are already constrained to capture only general patterns, the addition of a low-rank constraint essentially adds a second layer of regularization. This means that these layers become even less likely to adapt in ways that would compromise their general-purpose functionality. The low-rank constraint reinforces their role as foundational feature extractors, preserving their generalization capability while preventing overfitting on the specific details of the fine-tuning data.</p>\n<p><strong>Minimal Disruption of Pre-Trained Knowledge</strong>: The low-rank adaptation in LoRA ensures that lower layers maintain the knowledge they acquired during pre-training. Because these layers are regularized by the low-rank constraint, they are less likely to overfit to new data patterns introduced during fine-tuning. This preservation of pre-trained knowledge is crucial for maintaining the model’s transferability to other tasks or domains, as lower layers retain their broad, foundational representations.</p>\n<h6 id=\"why-this-matters-for-generalization\">Why This Matters for Generalization</h6>\n<ul>\n  <li>When fine-tuning with LoRA:\n    <ul>\n      <li><strong>Higher Layers Adapt More Easily</strong>: Higher layers, being closer to the output, are more adaptable and can more readily accommodate task-specific changes introduced during fine-tuning.</li>\n      <li><strong>Lower Layers Remain Generalized</strong>: Lower layers, reinforced by the low-rank constraint, retain their focus on general patterns. This balanced approach helps the model generalize well to unseen data because the lower layers still provide robust, general-purpose representations while the higher layers adapt to the new task.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Higher Layers Adapt More Easily</strong>: Higher layers, being closer to the output, are more adaptable and can more readily accommodate task-specific changes introduced during fine-tuning.</li>\n      <li><strong>Lower Layers Remain Generalized</strong>: Lower layers, reinforced by the low-rank constraint, retain their focus on general patterns. This balanced approach helps the model generalize well to unseen data because the lower layers still provide robust, general-purpose representations while the higher layers adapt to the new task.</li>\n    </ul>\n<h5 id=\"how-does-lora-help-avoid-catastrophic-forgetting\">How Does LoRA Help Avoid Catastrophic Forgetting?</h5>\n<ul>\n  <li>\n    <p>LoRA helps prevent catastrophic forgetting by fine-tuning large pre-trained models in a way that preserves their foundational knowledge while allowing for task-specific adaptations. Catastrophic forgetting occurs when fine-tuning neural networks, particularly large pre-trained models, causes them to overwrite or disrupt previously learned information, reducing performance on earlier tasks. LoRA mitigates this risk through a few key strategies:</p>\n\n    <ul>\n      <li><strong>Freezing Original Weights</strong>: The core model parameters remain untouched, preserving the base knowledge and preventing interference.</li>\n      <li><strong>Introducing Low-Rank Matrices</strong>: These matrices have limited capacity, focusing solely on task-specific adjustments, which allows the model to adapt to new tasks without losing general knowledge.</li>\n      <li><strong>Targeting Specific Layers</strong>: LoRA typically modifies higher attention layers, avoiding disruption to fundamental representations in lower layers.</li>\n      <li><strong>Parameter-Efficient, Modular Adaptation</strong>: LoRA’s modular design allows for reversible, task-specific adjustments, making it suitable for flexible multi-task and continual learning.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Through this approach, LoRA enables large models to adapt efficiently to new tasks while retaining previously learned information, which is especially valuable for applications requiring retention of prior knowledge.</p>\n  </li>\n</ul>\n<p>LoRA helps prevent catastrophic forgetting by fine-tuning large pre-trained models in a way that preserves their foundational knowledge while allowing for task-specific adaptations. Catastrophic forgetting occurs when fine-tuning neural networks, particularly large pre-trained models, causes them to overwrite or disrupt previously learned information, reducing performance on earlier tasks. LoRA mitigates this risk through a few key strategies:</p>\n<ul>\n      <li><strong>Freezing Original Weights</strong>: The core model parameters remain untouched, preserving the base knowledge and preventing interference.</li>\n      <li><strong>Introducing Low-Rank Matrices</strong>: These matrices have limited capacity, focusing solely on task-specific adjustments, which allows the model to adapt to new tasks without losing general knowledge.</li>\n      <li><strong>Targeting Specific Layers</strong>: LoRA typically modifies higher attention layers, avoiding disruption to fundamental representations in lower layers.</li>\n      <li><strong>Parameter-Efficient, Modular Adaptation</strong>: LoRA’s modular design allows for reversible, task-specific adjustments, making it suitable for flexible multi-task and continual learning.</li>\n    </ul>\n<p>Through this approach, LoRA enables large models to adapt efficiently to new tasks while retaining previously learned information, which is especially valuable for applications requiring retention of prior knowledge.</p>\n<h6 id=\"freezing-the-original-weights\">Freezing the Original Weights</h6>\n<ul>\n  <li>One of the core aspects of LoRA is that it freezes the original model weights and adds new, low-rank matrices that handle the fine-tuning process:\n    <ul>\n      <li>The frozen original weights retain the model’s general knowledge from pre-training. This means that core information, patterns, and representations acquired from extensive pre-training on large datasets remain unaffected.</li>\n      <li>Since only the low-rank matrices are adjusted for the new task, there is no direct modification of the original weights. This minimizes the risk of overwriting or disrupting the knowledge captured in those weights.</li>\n    </ul>\n  </li>\n  <li>By keeping the original parameters intact, LoRA avoids catastrophic forgetting in a way that typical fine-tuning (where the original weights are updated) does not.</li>\n</ul>\n<ul>\n      <li>The frozen original weights retain the model’s general knowledge from pre-training. This means that core information, patterns, and representations acquired from extensive pre-training on large datasets remain unaffected.</li>\n      <li>Since only the low-rank matrices are adjusted for the new task, there is no direct modification of the original weights. This minimizes the risk of overwriting or disrupting the knowledge captured in those weights.</li>\n    </ul>\n<h6 id=\"low-rank-adaptation-layers-for-task-specific-adjustments\">Low-Rank Adaptation Layers for Task-Specific Adjustments</h6>\n<ul>\n  <li>LoRA introduces low-rank matrices as additional layers to the model, which have the following properties:\n    <ul>\n      <li><strong>Limited Capacity</strong>: Low-rank matrices have a constrained capacity to represent new information, which forces them to focus only on essential, task-specific adaptations. This means they cannot significantly alter the underlying model’s behavior, preserving the broader general knowledge.</li>\n      <li><strong>Focused Adaptation</strong>: By adding task-specific information via low-rank matrices rather than altering the model’s entire parameter space, LoRA ensures that the new task-specific changes are confined to these auxiliary matrices. This helps the model adapt to new tasks without losing its prior knowledge.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Limited Capacity</strong>: Low-rank matrices have a constrained capacity to represent new information, which forces them to focus only on essential, task-specific adaptations. This means they cannot significantly alter the underlying model’s behavior, preserving the broader general knowledge.</li>\n      <li><strong>Focused Adaptation</strong>: By adding task-specific information via low-rank matrices rather than altering the model’s entire parameter space, LoRA ensures that the new task-specific changes are confined to these auxiliary matrices. This helps the model adapt to new tasks without losing its prior knowledge.</li>\n    </ul>\n<h6 id=\"layer-specific-impact\">Layer-Specific Impact</h6>\n<ul>\n  <li>LoRA often targets specific layers in the model, commonly the attention layers:\n    <ul>\n      <li><strong>Higher Attention Layers</strong>: These layers (closer to the output) are responsible for more task-specific representations and are typically the ones modified by LoRA. This selective adaptation means that the deeper, more task-general features in lower layers are left intact, reducing the risk of catastrophic forgetting.</li>\n      <li><strong>Minimal Lower-Layer Impact</strong>: Since lower layers (closer to the input) remain unchanged or minimally affected, the model retains the general-purpose, foundational features learned during pre-training, which are crucial for generalization.</li>\n    </ul>\n  </li>\n  <li>This selective impact allows LoRA to introduce new, task-specific representations while preserving fundamental information, balancing new task learning with knowledge retention.</li>\n</ul>\n<ul>\n      <li><strong>Higher Attention Layers</strong>: These layers (closer to the output) are responsible for more task-specific representations and are typically the ones modified by LoRA. This selective adaptation means that the deeper, more task-general features in lower layers are left intact, reducing the risk of catastrophic forgetting.</li>\n      <li><strong>Minimal Lower-Layer Impact</strong>: Since lower layers (closer to the input) remain unchanged or minimally affected, the model retains the general-purpose, foundational features learned during pre-training, which are crucial for generalization.</li>\n    </ul>\n<h6 id=\"parameter-efficient-fine-tuning\">Parameter-Efficient Fine-Tuning</h6>\n<ul>\n  <li>LoRA is designed for parameter-efficient fine-tuning, meaning it uses a fraction of the parameters that traditional fine-tuning would require:\n    <ul>\n      <li>LoRA adds only a small number of new parameters through the low-rank matrices. This efficiency keeps the model changes lightweight, making it less likely to interfere with the original model’s representations.</li>\n      <li>The low-rank constraint also regularizes the fine-tuning process, helping to prevent overfitting to the new task, which can indirectly support retention of general knowledge. Overfitting can cause catastrophic forgetting if the model becomes too specialized, as it loses flexibility in dealing with tasks beyond the fine-tuning data.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>LoRA adds only a small number of new parameters through the low-rank matrices. This efficiency keeps the model changes lightweight, making it less likely to interfere with the original model’s representations.</li>\n      <li>The low-rank constraint also regularizes the fine-tuning process, helping to prevent overfitting to the new task, which can indirectly support retention of general knowledge. Overfitting can cause catastrophic forgetting if the model becomes too specialized, as it loses flexibility in dealing with tasks beyond the fine-tuning data.</li>\n    </ul>\n<h6 id=\"easy-reversibility\">Easy Reversibility</h6>\n<ul>\n  <li>Since LoRA’s approach is to add new matrices rather than alter the original model’s weights, it makes it easy to revert the model to its original state or apply it to different tasks:\n    <ul>\n      <li>The low-rank matrices can be removed or swapped out without affecting the base model. This modularity allows for rapid switching between tasks or models, making it easy to adapt the model to different tasks while maintaining the pre-trained knowledge.</li>\n      <li>This adaptability is particularly useful for multi-task learning or continual learning, as it allows LoRA-enhanced models to apply distinct low-rank adaptations for different tasks without compromising the model’s underlying pre-trained knowledge.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The low-rank matrices can be removed or swapped out without affecting the base model. This modularity allows for rapid switching between tasks or models, making it easy to adapt the model to different tasks while maintaining the pre-trained knowledge.</li>\n      <li>This adaptability is particularly useful for multi-task learning or continual learning, as it allows LoRA-enhanced models to apply distinct low-rank adaptations for different tasks without compromising the model’s underlying pre-trained knowledge.</li>\n    </ul>\n<h6 id=\"modular-and-reusable-adapters\">Modular and Reusable Adapters</h6>\n<ul>\n  <li>With LoRA, fine-tuning for different tasks can be achieved by creating different low-rank matrices for each new task:\n    <ul>\n      <li>These modular, reusable matrices enable task-specific tuning without overwriting previous adaptations or the original model. This is especially valuable for applications where the model needs to perform multiple tasks or domains interchangeably.</li>\n      <li>By associating each task with its own set of low-rank matrices, LoRA enables the model to maintain knowledge across tasks without interference, effectively circumventing catastrophic forgetting.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>These modular, reusable matrices enable task-specific tuning without overwriting previous adaptations or the original model. This is especially valuable for applications where the model needs to perform multiple tasks or domains interchangeably.</li>\n      <li>By associating each task with its own set of low-rank matrices, LoRA enables the model to maintain knowledge across tasks without interference, effectively circumventing catastrophic forgetting.</li>\n    </ul>\n<h5 id=\"how-does-multiplication-of-two-low-rank-matrices-in-lora-lead-to-lower-attention-layers-being-impacted-less-than-higher-attention-layers\">How Does Multiplication of Two Low-rank Matrices in LoRA Lead to Lower Attention Layers Being Impacted Less Than Higher Attention Layers?</h5>\n<ul>\n  <li>In LoRA, the use of low-rank matrices enables efficient, controlled updates by selectively applying them to specific layers—primarily in the higher attention layers rather than the lower ones. This targeted approach allows the model to adjust effectively to task-specific nuances in these higher layers, which capture more complex patterns and contextual information, while preserving the general features encoded in the lower layers. By focusing fine-tuning efforts on the higher layers, LoRA minimizes overfitting and retains foundational knowledge from pre-training, making it an efficient and effective fine-tuning strategy.</li>\n</ul>\n<h6 id=\"role-of-low-rank-matrices-in-lora\">Role of Low-Rank Matrices in LoRA</h6>\n<ul>\n  <li>LoRA adds two low-rank matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-843\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-844\"><span class=\"mi\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-846\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-847\"><span class=\"mi\" id=\"MathJax-Span-848\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">B</script>, to certain layers, typically in the form:\n  <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>new</mtext></mrow></msub><mo>=</mo><mi>W</mi><mo>+</mo><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-849\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.87em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-850\"><span class=\"msubsup\" id=\"MathJax-Span-851\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-853\"><span class=\"mrow\" id=\"MathJax-Span-854\"><span class=\"mtext\" id=\"MathJax-Span-855\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">new</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-856\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-858\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-859\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">A</span><span class=\"mo\" id=\"MathJax-Span-860\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-861\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>new</mtext></mrow></msub><mo>=</mo><mi>W</mi><mo>+</mo><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-152\">W_{\\text{new}} = W + A \\times B</script>\n    <ul>\n      <li>where:\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-862\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-863\"><span class=\"mi\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">W</script> is the original (frozen) weight matrix in the model layer.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-865\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-866\"><span class=\"mi\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-868\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-869\"><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">B</script> are low-rank matrices (with ranks much smaller than the original dimensionality of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-871\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-872\"><span class=\"mi\" id=\"MathJax-Span-873\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">W</script>), creating a low-rank adaptation.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>The product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-874\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-875\"><span class=\"mi\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-878\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">A \\times B</script> has a limited rank and thus introduces only a restricted adjustment to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-158-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-879\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-880\"><span class=\"mi\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-158\">W</script>. This adjustment constrains the layer to learn only a few independent patterns rather than a full set of complex, task-specific transformations.</li>\n</ul>\n<ul>\n      <li>where:\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-862\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-863\"><span class=\"mi\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">W</script> is the original (frozen) weight matrix in the model layer.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-865\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-866\"><span class=\"mi\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-868\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-869\"><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">B</script> are low-rank matrices (with ranks much smaller than the original dimensionality of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-871\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-872\"><span class=\"mi\" id=\"MathJax-Span-873\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">W</script>), creating a low-rank adaptation.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-862\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-863\"><span class=\"mi\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">W</script> is the original (frozen) weight matrix in the model layer.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-865\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-866\"><span class=\"mi\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-868\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-869\"><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">B</script> are low-rank matrices (with ranks much smaller than the original dimensionality of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-871\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-872\"><span class=\"mi\" id=\"MathJax-Span-873\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">W</script>), creating a low-rank adaptation.</li>\n        </ul>\n<h6 id=\"higher-attention-layers-task-specific-focus\">Higher Attention Layers: Task-Specific Focus</h6>\n<ul>\n  <li>In large models, higher attention layers (closer to the output) tend to capture task-specific, abstract features, while lower attention layers (closer to the input) capture general, reusable patterns. By applying LoRA-based fine-tuning primarily to higher attention layers:</li>\n  <li>The model’s low-rank adaptation focuses on high-level, task-specific adjustments rather than modifying general representations.</li>\n  <li>Higher layers, which already deal with more specific information, are more sensitive to the small adjustments made by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-882\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-883\"><span class=\"mi\" id=\"MathJax-Span-884\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-886\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">A \\times B</script> since they directly influence task-related outputs.</li>\n  <li>In practice, LoRA-based fine-tuning modifies these higher layers more significantly because these layers are more directly responsible for adapting the model to new tasks. Lower layers, in contrast, require less task-specific adjustment and retain their general-purpose features.</li>\n</ul>\n<h6 id=\"limited-capacity-of-low-rank-matrices-and-layer-impact\">Limited Capacity of Low-Rank Matrices and Layer Impact</h6>\n<ul>\n  <li>The low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-887\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-888\"><span class=\"mi\" id=\"MathJax-Span-889\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-161-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-890\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-891\"><span class=\"mi\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-161\">B</script> have limited expressive power (due to their low rank), meaning they can only introduce a small number of directional adjustments in the weight space. This limited capacity aligns well with higher layers because:</li>\n  <li>Higher layers don’t need drastic changes but rather subtle adjustments to fine-tune the model to specific tasks.</li>\n  <li>The constraint imposed by low-rank matrices helps avoid overfitting by restricting the number of learned patterns, which is ideal for the high-level, abstract representations in higher layers.</li>\n  <li>For lower layers, which capture broad, general-purpose features, such limited adjustments don’t significantly impact the model. Lower layers still operate with the general features learned during pre-training, while higher layers adapt to task-specific details.</li>\n</ul>\n<h6 id=\"why-lower-layers-are-less-affected\">Why Lower Layers are Less Affected</h6>\n<ul>\n  <li>Lower layers in the attention stack are less impacted by LoRA’s low-rank updates because:</li>\n  <li>They are often not fine-tuned at all in LoRA-based setups, preserving the general features learned during pre-training.</li>\n  <li>Even when fine-tuned with low-rank matrices, the limited capacity of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-162-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-893\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-894\"><span class=\"mi\" id=\"MathJax-Span-895\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-897\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-162\">A \\times B</script> is not sufficient to drastically alter their broader, foundational representations.</li>\n</ul>\n<h5 id=\"in-lora-why-is-a-initialized-using-a-gaussian-and-b-set-to-0\">In LoRA, Why is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-163-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-898\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"mi\" id=\"MathJax-Span-900\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-163\">A</script> Initialized Using a Gaussian and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-164-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-901\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-902\"><span class=\"mi\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-164\">B</script> Set to 0?</h5>\n<ul>\n  <li>In LoRA, the initialization strategy where matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-165-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-904\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-905\"><span class=\"mi\" id=\"MathJax-Span-906\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-165\">A</script> is initialized with a Gaussian distribution and matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-907\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-908\"><span class=\"mi\" id=\"MathJax-Span-909\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">B</script> is set to zero is crucial for ensuring a smooth integration of the adaptation with minimal initial disruption to the pre-trained model. This approach is designed with specific goals in mind:</li>\n</ul>\n<h6 id=\"preserving-initial-model-behavior\">Preserving Initial Model Behavior</h6>\n<ul>\n  <li><strong>Rationale</strong>: By setting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-910\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-911\"><span class=\"mi\" id=\"MathJax-Span-912\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">B</script> to zero, the product <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-913\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-914\"><span class=\"mi\" id=\"MathJax-Span-915\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-916\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-918\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mi\" id=\"MathJax-Span-919\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">\\Delta W = BA</script> initially equals zero. This means that the adapted weights do not alter the original pre-trained weights at the beginning of the training process.</li>\n  <li><strong>Impact</strong>: This preserves the behavior of the original model at the start of fine-tuning, allowing the model to maintain its pre-trained performance and stability. The model begins adaptation from a known good state, reducing the risk of drastic initial performance drops.</li>\n</ul>\n<h6 id=\"gradual-learning-and-adaptation\">Gradual Learning and Adaptation</h6>\n<ul>\n  <li><strong>Rationale</strong>: Starting with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-920\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.44em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-921\"><span class=\"mi\" id=\"MathJax-Span-922\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-924\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-925\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">\\Delta W = 0</script> allows the model to gradually adapt through the updates to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-170-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-926\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-927\"><span class=\"mi\" id=\"MathJax-Span-928\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-170\">B</script> during training. This gradual adjustment is less likely to destabilize the model than a sudden, large change would.</li>\n  <li><strong>Impact</strong>: As <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-929\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-930\"><span class=\"mi\" id=\"MathJax-Span-931\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-171\">B</script> starts updating from zero, any changes in the model’s behavior are introduced slowly. This controlled adaptation is beneficial for training dynamics, as it allows the model to incrementally learn how to incorporate new information effectively without losing valuable prior knowledge.</li>\n</ul>\n<h6 id=\"ensuring-controlled-updates\">Ensuring Controlled Updates</h6>\n<ul>\n  <li><strong>Rationale</strong>: Gaussian initialization of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-172-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-932\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-933\"><span class=\"mi\" id=\"MathJax-Span-934\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-172\">A</script> provides a set of initial values that, while random, are statistically regularized by the properties of the Gaussian distribution (such as having a mean of zero and a defined variance). This regularity helps in providing a balanced and predictable set of initial conditions for the adaptation process.</li>\n  <li><strong>Impact</strong>: The Gaussian distribution helps ensure that the values in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-173-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-935\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-936\"><span class=\"mi\" id=\"MathJax-Span-937\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-173\">A</script> are neither too large nor too biased in any direction, which could lead to disproportionate influence on the updates when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-174-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-938\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-939\"><span class=\"mi\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-174\">B</script> begins to change. This helps in maintaining a stable and effective learning process.</li>\n</ul>\n<h6 id=\"focused-adaptation\">Focused Adaptation</h6>\n<ul>\n  <li><strong>Rationale</strong>: The low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-175-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-941\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-942\"><span class=\"mi\" id=\"MathJax-Span-943\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-175\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-176-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-944\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-945\"><span class=\"mi\" id=\"MathJax-Span-946\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-176\">B</script> are intended to capture the most essential aspects of the new data or tasks relative to the model’s existing capabilities. By starting with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-177-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>=</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-947\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-948\"><span class=\"mi\" id=\"MathJax-Span-949\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-950\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-951\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>=</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-177\">B = 0</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-178-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-952\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-953\"><span class=\"mi\" id=\"MathJax-Span-954\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-178\">A</script> initialized randomly, the learning focuses on identifying and optimizing only those aspects that truly need adaptation, as opposed to re-learning aspects that the model already performs well.</li>\n  <li>\n    <p><strong>Impact</strong>: This focus helps optimize training efficiency by directing computational resources and learning efforts towards making meaningful updates that enhance the model’s capabilities in specific new areas.</p>\n  </li>\n  <li>This initialization strategy supports the overall goal of LoRA: to adapt large, pre-trained models efficiently with minimal resource expenditure and without compromising the foundational strengths of the original model. This approach ensures that any new learning builds on and complements the existing pre-trained model structure.</li>\n</ul>\n<p><strong>Impact</strong>: This focus helps optimize training efficiency by directing computational resources and learning efforts towards making meaningful updates that enhance the model’s capabilities in specific new areas.</p>\n<h5 id=\"for-a-given-task-how-do-we-determine-whether-to-fine-tune-the-attention-layers-or-feed-forward-layers\">For a Given Task, How Do We Determine Whether to Fine-tune the Attention Layers or Feed-forward Layers?</h5>\n<ul>\n  <li>Deciding whether to fine-tune the attention layers or the feed-forward (MLP) layers in a model adapted using LoRA involves several considerations. These include the nature of the task, the model architecture, and the distribution of parameters between attention and feed-forward layers.</li>\n  <li>Note that the LoRA paper originally only adapted the attention weights for downstream tasks and froze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency. Thus, the number of attention weights relative to feed-forward weights can impact the choice of .</li>\n  <li>Here are some key factors to guide this decision:</li>\n</ul>\n<h6 id=\"nature-of-the-task\">Nature of the Task</h6>\n<ul>\n  <li><strong>Task Requirements</strong>: Attention mechanisms are particularly effective for tasks that benefit from modeling relationships between different parts of the input, such as sequence-to-sequence tasks or tasks requiring contextual understanding. If the task demands strong relational reasoning or context sensitivity, fine-tuning attention layers might be more beneficial.</li>\n  <li><strong>Feed-Forward Layer Role</strong>: MLPs generally focus on transforming the representation at individual positions without considering other positions. They are effective for tasks requiring more substantial non-linear transformation of features. If the task demands significant feature transformation at individual positions, MLPs may need adaptation.</li>\n</ul>\n<h6 id=\"model-architecture\">Model Architecture</h6>\n<ul>\n  <li><strong>Proportion of Parameters</strong>: In transformer architectures, MLPs typically contain a larger number of parameters compared to attention mechanisms (of the order of 2x to 5x). For example, in standard configurations like those seen in BERT or GPT models, the MLPs can contain around three times more parameters than the attention layers.</li>\n  <li><strong>Impact on Efficiency</strong>: Because MLPs are parameter-heavy, fine-tuning them can significantly increase the number of trainable parameters, impacting training efficiency and computational requirements. If parameter efficiency is a priority, you might opt to adapt only the attention layers, as originally done in the LoRA approach.</li>\n</ul>\n<h6 id=\"computational-constraints\">Computational Constraints</h6>\n<ul>\n  <li><strong>Resource Availability</strong>: The decision can also be influenced by available computational resources. Adapting attention layers only can save computational resources and training time, making it a preferable option when resources are limited.</li>\n  <li><strong>Balance of Adaptation and Performance</strong>: If computational resources allow, experimenting with both components can be useful to understand which contributes more to performance improvements on specific tasks.</li>\n</ul>\n<h6 id=\"empirical-testing\">Empirical Testing</h6>\n<ul>\n  <li><strong>A/B Testing</strong>: One effective way to determine the optimal strategy for a specific model and task is to conduct empirical tests where you fine-tune the attention layers alone, the MLP layers alone, and both together in different experiments to compare the performance impacts.</li>\n  <li><strong>Performance Metrics</strong>: Monitoring key performance metrics specific to the task during these tests will guide which components are more critical to fine-tune.</li>\n</ul>\n<h6 id=\"task-specific-research-and-insights\">Task-Specific Research and Insights</h6>\n<ul>\n  <li>\n    <p><strong>Literature and Benchmarks</strong>: Insights from research papers and benchmarks on similar tasks can provide guidelines on what has worked well historically in similar scenarios. For example, tasks that require nuanced understanding of input relationships (like question answering or summarization) might benefit more from tuning attention mechanisms.</p>\n  </li>\n  <li>\n    <p>In summary, the choice between tuning attention or MLP layers depends on the specific demands of the task, the model’s architecture, the balance of parameters, and empirical results. Considering these aspects can help in making a decision that optimizes both performance and efficiency.</p>\n  </li>\n</ul>\n<p><strong>Literature and Benchmarks</strong>: Insights from research papers and benchmarks on similar tasks can provide guidelines on what has worked well historically in similar scenarios. For example, tasks that require nuanced understanding of input relationships (like question answering or summarization) might benefit more from tuning attention mechanisms.</p>\n<p>In summary, the choice between tuning attention or MLP layers depends on the specific demands of the task, the model’s architecture, the balance of parameters, and empirical results. Considering these aspects can help in making a decision that optimizes both performance and efficiency.</p>\n<h5 id=\"assuming-were-fine-tuning-attention-weights-which-specific-attention-weight-matrices-should-we-apply-lora-to\">Assuming We’re Fine-tuning Attention Weights, Which Specific Attention Weight Matrices Should We Apply LoRA To?</h5>\n<ul>\n  <li>The question of which attention weight matrices in the transformer architecture should be adapted using LoRA to optimize performance on downstream tasks is central to maximizing the effectiveness of parameter usage, especially when dealing with large models like GPT-3. Based on the findings reported in the LoRA paper and the specific experiment mentioned, here’s a detailed explanation and recommendation:</li>\n</ul>\n<h6 id=\"context-and-setup\">Context and Setup</h6>\n<ul>\n  <li>The LoRA paper explores the adaptation of various weight matrices within the self-attention module of GPT-3 under a limited parameter budget. With a constraint of 18 million trainable parameters, the authors tested different configurations of adapting the weights associated with the query (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-179-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-955\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-956\"><span class=\"msubsup\" id=\"MathJax-Span-957\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-959\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-179\">W_q</script>), key (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-180-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-960\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-961\"><span class=\"msubsup\" id=\"MathJax-Span-962\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-963\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-964\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-180\">W_k</script>), value (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-181-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-965\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-966\"><span class=\"msubsup\" id=\"MathJax-Span-967\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-969\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-181\">W_v</script>), and output (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-182-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-970\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-971\"><span class=\"msubsup\" id=\"MathJax-Span-972\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-973\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-182\">W_o</script>) matrices. This setup allows for a comparison of the effectiveness of adapting different combinations of weights at varying ranks.</li>\n</ul>\n<h6 id=\"experimental-findings\">Experimental Findings</h6>\n<ul>\n  <li><strong>Parameter Allocation</strong>: The experiment considered adapting individual weight types at a rank of 8 and combinations of weights at lower ranks (4 and 2) due to the fixed parameter budget. This arrangement allowed assessing whether it is more beneficial to distribute the available parameters across multiple weight types or concentrate them on fewer weights at a higher rank.</li>\n  <li><strong>Performance Metrics</strong>: The validation accuracies on the WikiSQL and MultiNLI datasets served as the primary performance indicators. The results show varying degrees of success depending on which weights were adapted and how the ranks were distributed. The table below from the LoRA paper shows validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-183-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"msubsup\" id=\"MathJax-Span-977\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-978\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-183\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-184-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-980\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-981\"><span class=\"msubsup\" id=\"MathJax-Span-982\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-983\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-984\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-184\">W_v</script> gives the best performance overall. They find the standard deviation across random seeds to be consistent for a given dataset, which they report in the first column.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora_which_weights.jpg\" alt=\"\"></p>\n<h6 id=\"key-results-and-recommendations\">Key Results and Recommendations</h6>\n<ul>\n  <li><strong>Single vs. Multiple Weight Adaptation</strong>: Adapting single weight matrices (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-185-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-985\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-986\"><span class=\"msubsup\" id=\"MathJax-Span-987\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-988\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-989\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-185\">W_q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-186-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-990\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-991\"><span class=\"msubsup\" id=\"MathJax-Span-992\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-994\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-186\">W_k</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-187-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-995\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-996\"><span class=\"msubsup\" id=\"MathJax-Span-997\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-998\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-999\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-187\">W_v</script>, or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-188-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1000\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1001\"><span class=\"msubsup\" id=\"MathJax-Span-1002\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1003\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1004\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-188\">W_o</script> individually) at a higher rank generally resulted in lower performance compared to adapting combinations of weights at a reduced rank. Specifically, putting all parameters in ∆<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-189-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1005\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1006\"><span class=\"msubsup\" id=\"MathJax-Span-1007\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1008\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1009\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-189\">W_q</script> or ∆<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-190-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1010\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1011\"><span class=\"msubsup\" id=\"MathJax-Span-1012\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1013\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1014\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-190\">W_k</script> alone did not yield optimal results.</li>\n  <li><strong>Optimal Combination</strong>: The combination of adapting both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-191-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1015\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1016\"><span class=\"msubsup\" id=\"MathJax-Span-1017\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1018\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1019\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-191\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-192-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1020\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1021\"><span class=\"msubsup\" id=\"MathJax-Span-1022\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1023\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1024\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-192\">W_v</script> at a rank of 4 emerged as the most effective strategy, achieving the highest validation accuracies on both datasets. This suggests a balanced approach to distributing the parameter budget across multiple types of attention weights, rather than focusing on a single type, leads to better performance.</li>\n  <li><strong>Effectiveness of Rank Distribution</strong>: The result indicates that a lower rank (such as 4) is sufficient to capture essential adaptations in the weights, making it preferable to spread the parameter budget across more types of weights rather than increasing the rank for fewer weights.</li>\n</ul>\n<h6 id=\"conclusion-and-strategy-for-applying-lora\">Conclusion and Strategy for Applying LoRA</h6>\n<ul>\n  <li>Based on these findings, when applying LoRA within a limited parameter budget, it is advisable to:\n    <ul>\n      <li><strong>Distribute Parameters Across Multiple Weights</strong>: Focus on adapting multiple types of attention weights (such as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-193-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1025\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1026\"><span class=\"msubsup\" id=\"MathJax-Span-1027\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1029\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-193\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-194-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1030\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1031\"><span class=\"msubsup\" id=\"MathJax-Span-1032\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1033\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-194\">W_v</script>) rather than a single type, as this approach leverages the synergistic effects of adapting multiple aspects of the attention mechanism.</li>\n      <li><strong>Use Lower Ranks for Multiple Weights</strong>: Opt for a lower rank when adapting multiple weights to ensure that the parameter budget is used efficiently without compromising the ability to capture significant adaptations.</li>\n    </ul>\n  </li>\n  <li>This strategy maximizes the impact of the available parameters by enhancing more dimensions of the self-attention mechanism, which is crucial for the model’s ability to understand and process input data effectively across different tasks.</li>\n</ul>\n<ul>\n      <li><strong>Distribute Parameters Across Multiple Weights</strong>: Focus on adapting multiple types of attention weights (such as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-193-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1025\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1026\"><span class=\"msubsup\" id=\"MathJax-Span-1027\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1029\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-193\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-194-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1030\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1031\"><span class=\"msubsup\" id=\"MathJax-Span-1032\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1033\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-194\">W_v</script>) rather than a single type, as this approach leverages the synergistic effects of adapting multiple aspects of the attention mechanism.</li>\n      <li><strong>Use Lower Ranks for Multiple Weights</strong>: Opt for a lower rank when adapting multiple weights to ensure that the parameter budget is used efficiently without compromising the ability to capture significant adaptations.</li>\n    </ul>\n<h5 id=\"is-there-a-relationship-between-setting-scaling-factor-and-rank-in-lora\">Is There a Relationship Between Setting Scaling Factor and Rank in LoRA?</h5>\n<ul>\n  <li>In the LoRA framework, the relationship between the scaling factor <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-195-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mi\" id=\"MathJax-Span-1037\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-195\">\\alpha</script> and the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-196-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mi\" id=\"MathJax-Span-1040\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-196\">r</script> of the adaptation matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-197-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1041\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1042\"><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-197\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-198-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1044\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mi\" id=\"MathJax-Span-1046\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-198\">B</script> is an important consideration for tuning the model’s performance and managing how adaptations are applied to the pre-trained weights. Both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-199-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1047\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1048\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-199\">\\alpha</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-200-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1050\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1051\"><span class=\"mi\" id=\"MathJax-Span-1052\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-200\">r</script> play significant roles in determining the impact of the low-rank updates on the model, and their settings can influence each other in terms of the overall effect on the model’s behavior.</li>\n</ul>\n<h6 id=\"understanding-alpha-and-r\">Understanding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-201-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1053\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1054\"><span class=\"mi\" id=\"MathJax-Span-1055\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-201\">\\alpha</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-202-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1056\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1057\"><span class=\"mi\" id=\"MathJax-Span-1058\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-202\">r</script></h6>\n<ul>\n  <li><strong>Scaling Factor <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-203-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1059\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1060\"><span class=\"mi\" id=\"MathJax-Span-1061\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-203\">\\alpha</script></strong>: This parameter scales the contribution of the low-rank updates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-204-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1062\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1063\"><span class=\"mi\" id=\"MathJax-Span-1064\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1066\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1067\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mi\" id=\"MathJax-Span-1068\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-204\">\\Delta W = BA</script> before they are applied to the original model weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-205-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1069\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1070\"><span class=\"mi\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-205\">W</script>. It controls the magnitude of changes introduced by the adaptation, effectively modulating how aggressive or subtle the updates are.</li>\n  <li><strong>Rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-206-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1072\" style=\"width: 0.467em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.364em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.36em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1073\"><span class=\"mi\" id=\"MathJax-Span-1074\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-206\">r</script></strong>: This determines the dimensionality of the low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-207-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1075\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1076\"><span class=\"mi\" id=\"MathJax-Span-1077\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-207\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-208-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1078\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1079\"><span class=\"mi\" id=\"MathJax-Span-1080\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-208\">B</script>. The rank controls the expressiveness of the low-rank updates, with higher ranks allowing for more complex adaptations but increasing computational costs and potentially the risk of overfitting.</li>\n</ul>\n<h6 id=\"relationship-and-interaction\">Relationship and Interaction</h6>\n<ul>\n  <li><strong>Balancing Impact</strong>: A higher rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-209-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1081\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1082\"><span class=\"mi\" id=\"MathJax-Span-1083\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-209\">r</script> allows the model to capture more complex relationships and nuances in the adaptations, potentially leading to more significant changes to the model’s behavior. In such cases, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-210-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1084\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1085\"><span class=\"mi\" id=\"MathJax-Span-1086\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-210\">\\alpha</script> might be adjusted downward to temper the overall impact, ensuring that the modifications do not destabilize the model’s pre-trained knowledge excessively.</li>\n  <li><strong>Adjusting for Subtlety</strong>: Conversely, if the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-211-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1087\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1088\"><span class=\"mi\" id=\"MathJax-Span-1089\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-211\">r</script> is set lower, which constrains the flexibility and range of the updates, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-212-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1090\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1091\"><span class=\"mi\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-212\">\\alpha</script> may need to be increased to make the limited updates more impactful. This can help ensure that the adaptations, though less complex, are sufficient to achieve the desired performance improvements.</li>\n  <li><strong>Experimental Tuning</strong>: The optimal settings for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-213-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1093\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1094\"><span class=\"mi\" id=\"MathJax-Span-1095\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-213\">\\alpha</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-214-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1096\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1097\"><span class=\"mi\" id=\"MathJax-Span-1098\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-214\">r</script> often depend on the specific task, the dataset, and the desired balance between adapting to new tasks and retaining generalizability. Experimentation and validation are typically necessary to find the best combination.</li>\n</ul>\n<h6 id=\"practical-considerations\">Practical Considerations</h6>\n<ul>\n  <li><strong>Overfitting vs. Underfitting</strong>: Higher ranks with aggressive scaling factors can lead to overfitting, especially when the model starts fitting too closely to nuances of the training data that do not generalize well. Conversely, too low a rank and/or too conservative an <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-215-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1099\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1100\"><span class=\"mi\" id=\"MathJax-Span-1101\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-215\">\\alpha</script> might lead to underfitting, where the model fails to adapt adequately to new tasks.</li>\n  <li><strong>Computational Efficiency</strong>: Higher ranks increase the number of parameters and computational costs. Balancing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-216-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1102\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1103\"><span class=\"mi\" id=\"MathJax-Span-1104\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-216\">\\alpha</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-217-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1105\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1106\"><span class=\"mi\" id=\"MathJax-Span-1107\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-217\">r</script> can help manage computational demands while still achieving meaningful model improvements.</li>\n</ul>\n<h6 id=\"conclusion\">Conclusion</h6>\n<ul>\n  <li>The relationship between <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-218-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1108\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1109\"><span class=\"mi\" id=\"MathJax-Span-1110\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-218\">\\alpha</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-219-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1111\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1112\"><span class=\"mi\" id=\"MathJax-Span-1113\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-219\">r</script> in LoRA involves a delicate balance. Adjusting one can necessitate compensatory changes to the other to maintain a desired level of adaptation effectiveness without sacrificing the model’s stability or performance. Understanding how these parameters interact can significantly enhance the strategic deployment of LoRA in practical machine learning tasks.</li>\n</ul>\n<h5 id=\"how-do-you-determine-the-optimal-rank-r-for-lora\">How Do You Determine the Optimal Rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-220-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1114\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1115\"><span class=\"mi\" id=\"MathJax-Span-1116\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-220\">r</script> for LoRA?</h5>\n<ul>\n  <li>The optimal rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-221-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1117\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1118\"><span class=\"mi\" id=\"MathJax-Span-1119\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-221\">r</script> for LoRA is influenced by the specific task and the type of weight adaptation. Based on the results reported in the paper from the experiments on the WikiSQL and MultiNLI datasets:\n    <ul>\n      <li><strong>For WikiSQL</strong>:\n        <ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-222-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1120\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1121\"><span class=\"msubsup\" id=\"MathJax-Span-1122\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-222\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-223-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1125\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1126\"><span class=\"mi\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-223\">r = 4</script>, with a validation accuracy of 70.5%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-224-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1130\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1131\"><span class=\"msubsup\" id=\"MathJax-Span-1132\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-224\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-225-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1135\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1136\"><span class=\"msubsup\" id=\"MathJax-Span-1137\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-225\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-226-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-226\">r = 8</script>, with a validation accuracy of 73.8%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-227-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"msubsup\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1150\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1151\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1155\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1156\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1158\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1159\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-227\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-228-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1162\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1163\"><span class=\"mi\" id=\"MathJax-Span-1164\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1166\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-228\">r = 4</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-229-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1167\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1168\"><span class=\"mi\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-229\">r = 8</script>, both achieving a validation accuracy of 74.0%.</li>\n        </ul>\n      </li>\n      <li><strong>For MultiNLI</strong>:\n        <ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-230-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1172\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1173\"><span class=\"msubsup\" id=\"MathJax-Span-1174\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-230\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-231-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1177\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1178\"><span class=\"mi\" id=\"MathJax-Span-1179\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1181\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-231\">r = 4</script>, with a validation accuracy of 91.1%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-232-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1182\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1183\"><span class=\"msubsup\" id=\"MathJax-Span-1184\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1185\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-232\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-233-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1187\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1188\"><span class=\"msubsup\" id=\"MathJax-Span-1189\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1190\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1191\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-233\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-234-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1192\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1193\"><span class=\"mi\" id=\"MathJax-Span-1194\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-234\">r = 8</script>, with a validation accuracy of 91.6%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-235-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1197\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1198\"><span class=\"msubsup\" id=\"MathJax-Span-1199\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1202\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1203\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1204\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1207\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1208\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1210\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1211\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1212\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-235\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-236-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1214\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1215\"><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-236\">r = 2</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-237-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1219\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1220\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-237\">r = 4</script>, both achieving a validation accuracy of 91.7%.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>The table below from the paper shows the validation accuracy on WikiSQL and MultiNLI with different rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-238-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1224\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1225\"><span class=\"mi\" id=\"MathJax-Span-1226\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-238\">r</script> by adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-239-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>{</mo><mrow><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub></mrow><mo>}</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1227\" style=\"width: 4.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.065em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1003.96em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1228\"><span class=\"mrow\" id=\"MathJax-Span-1229\"><span class=\"mo\" id=\"MathJax-Span-1230\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">{</span></span><span class=\"mrow\" id=\"MathJax-Span-1231\"><span class=\"msubsup\" id=\"MathJax-Span-1232\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1233\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1235\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1236\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1237\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1238\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1239\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">}</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>{</mo><mrow><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub></mrow><mo>}</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-239\">\\left\\{W_q, W_v\\right\\}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-240-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow><mo>{</mo><mrow><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>c</mi></msub></mrow><mo>}</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1240\" style=\"width: 8.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1007.35em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1241\"><span class=\"mrow\" id=\"MathJax-Span-1242\"><span class=\"mo\" id=\"MathJax-Span-1243\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">{</span></span><span class=\"mrow\" id=\"MathJax-Span-1244\"><span class=\"msubsup\" id=\"MathJax-Span-1245\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1247\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1248\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1249\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1250\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1252\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1253\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1254\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1255\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1256\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1257\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1258\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1260\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">}</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo>{</mo><mrow><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>c</mi></msub></mrow><mo>}</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-240\">\\left\\{W_q, W_k, W_v, W_c\\right\\}</script>, and just <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-241-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1261\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1262\"><span class=\"msubsup\" id=\"MathJax-Span-1263\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1265\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-241\">W_q</script> for a comparison.. To our surprise, a rank as small as one suffices for adapting both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-242-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1266\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1267\"><span class=\"msubsup\" id=\"MathJax-Span-1268\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-242\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-243-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1271\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1272\"><span class=\"msubsup\" id=\"MathJax-Span-1273\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1275\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-243\">W_v</script> on these datasets while training <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-244-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1276\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1277\"><span class=\"msubsup\" id=\"MathJax-Span-1278\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1279\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-244\">W_q</script> alone needs a larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-245-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1281\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1282\"><span class=\"mi\" id=\"MathJax-Span-1283\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-245\">r</script>.</li>\n</ul>\n<ul>\n      <li><strong>For WikiSQL</strong>:\n        <ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-222-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1120\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1121\"><span class=\"msubsup\" id=\"MathJax-Span-1122\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-222\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-223-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1125\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1126\"><span class=\"mi\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-223\">r = 4</script>, with a validation accuracy of 70.5%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-224-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1130\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1131\"><span class=\"msubsup\" id=\"MathJax-Span-1132\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-224\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-225-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1135\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1136\"><span class=\"msubsup\" id=\"MathJax-Span-1137\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-225\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-226-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-226\">r = 8</script>, with a validation accuracy of 73.8%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-227-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"msubsup\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1150\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1151\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1155\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1156\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1158\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1159\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-227\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-228-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1162\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1163\"><span class=\"mi\" id=\"MathJax-Span-1164\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1166\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-228\">r = 4</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-229-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1167\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1168\"><span class=\"mi\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-229\">r = 8</script>, both achieving a validation accuracy of 74.0%.</li>\n        </ul>\n      </li>\n      <li><strong>For MultiNLI</strong>:\n        <ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-230-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1172\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1173\"><span class=\"msubsup\" id=\"MathJax-Span-1174\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-230\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-231-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1177\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1178\"><span class=\"mi\" id=\"MathJax-Span-1179\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1181\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-231\">r = 4</script>, with a validation accuracy of 91.1%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-232-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1182\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1183\"><span class=\"msubsup\" id=\"MathJax-Span-1184\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1185\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-232\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-233-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1187\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1188\"><span class=\"msubsup\" id=\"MathJax-Span-1189\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1190\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1191\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-233\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-234-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1192\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1193\"><span class=\"mi\" id=\"MathJax-Span-1194\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-234\">r = 8</script>, with a validation accuracy of 91.6%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-235-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1197\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1198\"><span class=\"msubsup\" id=\"MathJax-Span-1199\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1202\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1203\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1204\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1207\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1208\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1210\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1211\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1212\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-235\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-236-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1214\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1215\"><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-236\">r = 2</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-237-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1219\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1220\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-237\">r = 4</script>, both achieving a validation accuracy of 91.7%.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-222-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1120\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1121\"><span class=\"msubsup\" id=\"MathJax-Span-1122\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-222\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-223-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1125\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1126\"><span class=\"mi\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-223\">r = 4</script>, with a validation accuracy of 70.5%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-224-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1130\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1131\"><span class=\"msubsup\" id=\"MathJax-Span-1132\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-224\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-225-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1135\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1136\"><span class=\"msubsup\" id=\"MathJax-Span-1137\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-225\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-226-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-226\">r = 8</script>, with a validation accuracy of 73.8%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-227-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"msubsup\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1150\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1151\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1155\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1156\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1158\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1159\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-227\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-228-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1162\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1163\"><span class=\"mi\" id=\"MathJax-Span-1164\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1166\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-228\">r = 4</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-229-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1167\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1168\"><span class=\"mi\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-229\">r = 8</script>, both achieving a validation accuracy of 74.0%.</li>\n        </ul>\n<ul>\n          <li>When adapting only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-230-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1172\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1173\"><span class=\"msubsup\" id=\"MathJax-Span-1174\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-230\">W_q</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-231-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1177\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1178\"><span class=\"mi\" id=\"MathJax-Span-1179\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1181\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-231\">r = 4</script>, with a validation accuracy of 91.1%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-232-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1182\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1183\"><span class=\"msubsup\" id=\"MathJax-Span-1184\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1185\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-232\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-233-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1187\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1188\"><span class=\"msubsup\" id=\"MathJax-Span-1189\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1190\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1191\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-233\">W_v</script>, the optimal rank is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-234-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1192\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1193\"><span class=\"mi\" id=\"MathJax-Span-1194\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-234\">r = 8</script>, with a validation accuracy of 91.6%.</li>\n          <li>When adapting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-235-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1197\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.36em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1198\"><span class=\"msubsup\" id=\"MathJax-Span-1199\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1202\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1203\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1204\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1207\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1208\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1210\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1211\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1212\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>k</mi></msub><mo>,</mo><msub><mi>W</mi><mi>v</mi></msub><mo>,</mo><msub><mi>W</mi><mi>o</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-235\">W_q, W_k, W_v, W_o</script>, the optimal ranks are <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-236-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1214\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1215\"><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-236\">r = 2</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-237-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1219\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1220\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-237\">r = 4</script>, both achieving a validation accuracy of 91.7%.</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/lora_optimal_rank.jpg\" alt=\"\"></p>\n<ul>\n  <li>In summary, while the optimal rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-246-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1284\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1285\"><span class=\"mi\" id=\"MathJax-Span-1286\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-246\">r</script> varies depending on the dataset and the type of weight adaptation, a rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-247-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1287\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1288\"><span class=\"mi\" id=\"MathJax-Span-1289\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1290\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1291\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-247\">r = 4</script> or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-248-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1292\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1293\"><span class=\"mi\" id=\"MathJax-Span-1294\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1295\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1296\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-248\">r = 8</script> generally yields the best performance. Specifically, a rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-249-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1297\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1298\"><span class=\"mi\" id=\"MathJax-Span-1299\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1300\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1301\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-249\">r = 4</script> is often sufficient for single weight types like <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-250-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1302\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1303\"><span class=\"msubsup\" id=\"MathJax-Span-1304\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1305\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1306\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-250\">W_q</script>, and a rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-251-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1307\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1308\"><span class=\"mi\" id=\"MathJax-Span-1309\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1310\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1311\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-251\">r = 8</script> is more effective for adapting multiple weight types such as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-252-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1312\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1313\"><span class=\"msubsup\" id=\"MathJax-Span-1314\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1315\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-252\">W_q</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-253-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1317\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1318\"><span class=\"msubsup\" id=\"MathJax-Span-1319\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1320\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1321\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-253\">W_v</script>.</li>\n  <li>However, a small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-254-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1322\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1323\"><span class=\"mi\" id=\"MathJax-Span-1324\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-254\">r</script> cannot be expected to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-255-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1325\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.86em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1326\"><span class=\"mi\" id=\"MathJax-Span-1327\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1328\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1329\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1330\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1331\"><span class=\"mrow\" id=\"MathJax-Span-1332\"><span class=\"mi\" id=\"MathJax-Span-1333\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span><span class=\"mi\" id=\"MathJax-Span-1334\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-1335\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1336\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1337\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-255\">r = d_{model}</script>) could certainly outperform LoRA with a small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-256-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1338\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1339\"><span class=\"mi\" id=\"MathJax-Span-1340\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-256\">r</script>.</li>\n  <li>In summary, selecting a rank that is too high can counteract the benefits of the low-rank adaptation by allowing the model to become overly complex and fit the training data too precisely. Conversely, choosing a rank that’s too low may limit the model’s ability to capture necessary information, leading to underfitting. Therefore, setting the rank in LoRA fine-tuning involves finding a balance: enough capacity to adapt to new data without overfitting.</li>\n</ul>\n<h5 id=\"how-do-lora-hyperparameters-interact-with-each-other-is-there-a-relationship-between-lora-hyperparameters\">How Do LoRA Hyperparameters Interact with Each Other? is There a Relationship Between LoRA Hyperparameters?</h5>\n<ul>\n  <li>\n    <p>There is a significant relationship among the hyperparameters in the Low-Rank Adaptation (LoRA) technique, particularly how they interact and influence each other to affect the adaptation and performance of the model. Understanding the interactions between these hyperparameters is crucial for effectively tuning the model to achieve desired behaviors and performance improvements. Here’s a detailed breakdown of the primary hyperparameters in LoRA and how they are interrelated:</p>\n  </li>\n  <li><strong>Rank and Scaling Factor</strong>:\n    <ul>\n      <li>Higher ranks allow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-257-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1341\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1342\"><span class=\"mi\" id=\"MathJax-Span-1343\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-257\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-258-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1344\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1345\"><span class=\"mi\" id=\"MathJax-Span-1346\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-258\">B</script> to capture more detailed and complex modifications. However, with increased rank, the potential for overfitting and destabilizing the original model’s behavior also rises. The scaling factor <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-259-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1347\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1348\"><span class=\"mi\" id=\"MathJax-Span-1349\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-259\">\\alpha</script> often needs to be adjusted in response to the rank; a higher rank might require a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-260-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1350\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1351\"><span class=\"mi\" id=\"MathJax-Span-1352\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-260\">\\alpha</script> to moderate the effect of these more complex updates.</li>\n    </ul>\n  </li>\n  <li><strong>Rank and Regularization</strong>:\n    <ul>\n      <li>As the rank increases, the number of parameters in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-261-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1353\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1354\"><span class=\"mi\" id=\"MathJax-Span-1355\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-261\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-262-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1356\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1357\"><span class=\"mi\" id=\"MathJax-Span-1358\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-262\">B</script> also increases, which can lead to overfitting. Regularization becomes more critical with higher ranks to ensure that the model generalizes well and does not just memorize the training data.</li>\n    </ul>\n  </li>\n  <li><strong>Learning Rate and Scaling Factor</strong>:\n    <ul>\n      <li>The learning rate for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-263-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1359\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1360\"><span class=\"mi\" id=\"MathJax-Span-1361\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-263\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-264-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1362\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1363\"><span class=\"mi\" id=\"MathJax-Span-1364\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-264\">B</script> can influence how quickly the model adapts the low-rank updates. If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-265-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1365\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1366\"><span class=\"mi\" id=\"MathJax-Span-1367\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-265\">\\alpha</script> is high, leading to stronger updates, a lower learning rate might be necessary to prevent training instability. Conversely, with a lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-266-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1368\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1369\"><span class=\"mi\" id=\"MathJax-Span-1370\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-266\">\\alpha</script>, a higher learning rate might be feasible to ensure that the updates are sufficiently impactful.</li>\n    </ul>\n  </li>\n  <li><strong>Regularization and Learning Rate</strong>:\n    <ul>\n      <li>Regularization settings might need adjustment based on the learning rate. A higher learning rate can cause larger updates, which might increase the risk of overfitting unless balanced by stronger regularization.</li>\n    </ul>\n  </li>\n</ul>\n<p>There is a significant relationship among the hyperparameters in the Low-Rank Adaptation (LoRA) technique, particularly how they interact and influence each other to affect the adaptation and performance of the model. Understanding the interactions between these hyperparameters is crucial for effectively tuning the model to achieve desired behaviors and performance improvements. Here’s a detailed breakdown of the primary hyperparameters in LoRA and how they are interrelated:</p>\n<ul>\n      <li>Higher ranks allow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-257-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1341\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1342\"><span class=\"mi\" id=\"MathJax-Span-1343\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-257\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-258-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1344\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1345\"><span class=\"mi\" id=\"MathJax-Span-1346\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-258\">B</script> to capture more detailed and complex modifications. However, with increased rank, the potential for overfitting and destabilizing the original model’s behavior also rises. The scaling factor <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-259-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1347\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1348\"><span class=\"mi\" id=\"MathJax-Span-1349\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-259\">\\alpha</script> often needs to be adjusted in response to the rank; a higher rank might require a smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-260-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1350\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1351\"><span class=\"mi\" id=\"MathJax-Span-1352\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-260\">\\alpha</script> to moderate the effect of these more complex updates.</li>\n    </ul>\n<ul>\n      <li>As the rank increases, the number of parameters in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-261-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1353\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1354\"><span class=\"mi\" id=\"MathJax-Span-1355\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-261\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-262-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1356\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1357\"><span class=\"mi\" id=\"MathJax-Span-1358\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-262\">B</script> also increases, which can lead to overfitting. Regularization becomes more critical with higher ranks to ensure that the model generalizes well and does not just memorize the training data.</li>\n    </ul>\n<ul>\n      <li>The learning rate for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-263-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1359\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1360\"><span class=\"mi\" id=\"MathJax-Span-1361\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-263\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-264-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1362\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1363\"><span class=\"mi\" id=\"MathJax-Span-1364\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-264\">B</script> can influence how quickly the model adapts the low-rank updates. If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-265-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1365\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1366\"><span class=\"mi\" id=\"MathJax-Span-1367\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-265\">\\alpha</script> is high, leading to stronger updates, a lower learning rate might be necessary to prevent training instability. Conversely, with a lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-266-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1368\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1369\"><span class=\"mi\" id=\"MathJax-Span-1370\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-266\">\\alpha</script>, a higher learning rate might be feasible to ensure that the updates are sufficiently impactful.</li>\n    </ul>\n<ul>\n      <li>Regularization settings might need adjustment based on the learning rate. A higher learning rate can cause larger updates, which might increase the risk of overfitting unless balanced by stronger regularization.</li>\n    </ul>\n<h6 id=\"practical-considerations-1\">Practical Considerations</h6>\n<ul>\n  <li><strong>Tuning Strategy</strong>:\n    <ul>\n      <li>Tuning these hyperparameters requires careful experimentation and validation. Often, changes to one parameter necessitate adjustments to others to maintain a balanced and effective training regime.</li>\n    </ul>\n  </li>\n  <li><strong>Trade-offs</strong>:\n    <ul>\n      <li>There are trade-offs between model flexibility, training stability, computational efficiency, and the risk of overfitting. Effective management of LoRA’s hyperparameters is key to navigating these trade-offs.</li>\n    </ul>\n  </li>\n  <li><strong>Application-Specific Adjustments</strong>:\n    <ul>\n      <li>Depending on the specific requirements of the task and characteristics of the data, the optimal settings for these hyperparameters can vary significantly. Task-specific performance metrics and validation are essential to guide these adjustments.</li>\n    </ul>\n  </li>\n  <li>In summary, understanding and managing the relationships between these LoRA hyperparameters enables practitioners to finely tune their models to specific tasks without extensive retraining while leveraging pre-trained model architectures efficiently.</li>\n</ul>\n<ul>\n      <li>Tuning these hyperparameters requires careful experimentation and validation. Often, changes to one parameter necessitate adjustments to others to maintain a balanced and effective training regime.</li>\n    </ul>\n<ul>\n      <li>There are trade-offs between model flexibility, training stability, computational efficiency, and the risk of overfitting. Effective management of LoRA’s hyperparameters is key to navigating these trade-offs.</li>\n    </ul>\n<ul>\n      <li>Depending on the specific requirements of the task and characteristics of the data, the optimal settings for these hyperparameters can vary significantly. Task-specific performance metrics and validation are essential to guide these adjustments.</li>\n    </ul>\n<h5 id=\"why-does-a-higher-rank-make-it-the-easier-to-overfit\">Why Does a Higher Rank Make It the Easier to Overfit?</h5>\n<ul>\n  <li>In LoRA-based fine-tuning, a higher rank can indeed lead to easier overfitting. To understand why, let’s break down the mechanics of LoRA and how rank affects model capacity and overfitting.</li>\n  <li>The <strong>rank</strong> in LoRA determines the dimensions of these additional matrices, effectively controlling their capacity to capture information:\n    <ul>\n      <li><strong>Low Rank</strong>: Small matrices that can represent only limited information.</li>\n      <li><strong>High Rank</strong>: Larger matrices with greater capacity to capture complex patterns.</li>\n    </ul>\n  </li>\n  <li>\n    <p>In mathematical terms, a higher rank means more degrees of freedom in the low-rank matrices, allowing them to approximate more complex relationships in the data.</p>\n  </li>\n  <li>Here’s why a higher rank increases overfitting in LoRA:\n    <ol>\n      <li>\n        <p><strong>Increased Capacity to Capture Training Noise</strong>: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.</p>\n      </li>\n      <li>\n        <p><strong>Less Regularization Effect</strong>: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.</p>\n      </li>\n      <li>\n        <p><strong>Reduced Ability to Generalize</strong>: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.</p>\n      </li>\n      <li>\n        <p><strong>Higher Variance in Learned Features</strong>: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.</p>\n      </li>\n    </ol>\n  </li>\n</ul>\n<ul>\n      <li><strong>Low Rank</strong>: Small matrices that can represent only limited information.</li>\n      <li><strong>High Rank</strong>: Larger matrices with greater capacity to capture complex patterns.</li>\n    </ul>\n<p>In mathematical terms, a higher rank means more degrees of freedom in the low-rank matrices, allowing them to approximate more complex relationships in the data.</p>\n<ol>\n      <li>\n        <p><strong>Increased Capacity to Capture Training Noise</strong>: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.</p>\n      </li>\n      <li>\n        <p><strong>Less Regularization Effect</strong>: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.</p>\n      </li>\n      <li>\n        <p><strong>Reduced Ability to Generalize</strong>: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.</p>\n      </li>\n      <li>\n        <p><strong>Higher Variance in Learned Features</strong>: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.</p>\n      </li>\n    </ol>\n<p><strong>Increased Capacity to Capture Training Noise</strong>: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.</p>\n<p><strong>Less Regularization Effect</strong>: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.</p>\n<p><strong>Reduced Ability to Generalize</strong>: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.</p>\n<p><strong>Higher Variance in Learned Features</strong>: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.</p>\n<h5 id=\"does-lora-adapt-weights-in-all-layers\">Does LoRA Adapt Weights in All Layers?</h5>\n<ul>\n  <li>\n    <p>LoRA does not typically adapt weights across all layers of a neural network; instead, it targets specific layers, often the <strong>attention layers</strong> in large LLMs. This selective adaptation is a design choice aimed at balancing the effectiveness of fine-tuning with computational efficiency and minimizing the risk of overfitting. By modifying only key layers, like attention layers, LoRA efficiently focuses on layers where task-specific information is most impactful while preserving the general-purpose features of the lower layers.</p>\n  </li>\n  <li>\n    <p><strong>Layers Typically Adapted in LoRA</strong>:</p>\n  </li>\n  <li>In the original <a href=\"https://arxiv.org/abs/2106.09685\">LoRA implementation</a>:\n    <ol>\n      <li><strong>Attention Layers</strong>: LoRA primarily targets attention layers (such as the query and value projections in transformers) because they play a critical role in capturing contextual information. By adapting only these layers, LoRA can achieve significant task-specific improvements without needing to modify the entire model.</li>\n      <li><strong>Few Additional Layers (if necessary)</strong>: Sometimes, LoRA may extend adaptation to a few other layers (like feed-forward layers in transformers) if the new task requires it. However, this is usually done with caution to avoid overfitting and to keep the parameter footprint low.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Why not all layers?</strong>:</p>\n\n    <ol>\n      <li><strong>Computational Efficiency</strong>: Adapting all layers would introduce a large number of low-rank matrices throughout the model, greatly increasing the memory and computation requirements, which LoRA is specifically designed to reduce.</li>\n      <li><strong>Risk of Overfitting</strong>: Adapting all layers, especially the lower (more general) layers, could lead the model to overfit to the fine-tuning dataset, particularly if the dataset is small. Lower layers tend to capture general features, and adapting them might make the model too specialized, losing generalization ability.</li>\n      <li><strong>Focus on Task-Specific Information</strong>: The upper (or top) layers of a model generally capture task-specific features, while lower layers handle more general, reusable features. LoRA’s selective adaptation focuses on adjusting only those layers where task-specific learning is most beneficial.</li>\n    </ol>\n  </li>\n</ul>\n<p>LoRA does not typically adapt weights across all layers of a neural network; instead, it targets specific layers, often the <strong>attention layers</strong> in large LLMs. This selective adaptation is a design choice aimed at balancing the effectiveness of fine-tuning with computational efficiency and minimizing the risk of overfitting. By modifying only key layers, like attention layers, LoRA efficiently focuses on layers where task-specific information is most impactful while preserving the general-purpose features of the lower layers.</p>\n<p><strong>Layers Typically Adapted in LoRA</strong>:</p>\n<ol>\n      <li><strong>Attention Layers</strong>: LoRA primarily targets attention layers (such as the query and value projections in transformers) because they play a critical role in capturing contextual information. By adapting only these layers, LoRA can achieve significant task-specific improvements without needing to modify the entire model.</li>\n      <li><strong>Few Additional Layers (if necessary)</strong>: Sometimes, LoRA may extend adaptation to a few other layers (like feed-forward layers in transformers) if the new task requires it. However, this is usually done with caution to avoid overfitting and to keep the parameter footprint low.</li>\n    </ol>\n<p><strong>Why not all layers?</strong>:</p>\n<ol>\n      <li><strong>Computational Efficiency</strong>: Adapting all layers would introduce a large number of low-rank matrices throughout the model, greatly increasing the memory and computation requirements, which LoRA is specifically designed to reduce.</li>\n      <li><strong>Risk of Overfitting</strong>: Adapting all layers, especially the lower (more general) layers, could lead the model to overfit to the fine-tuning dataset, particularly if the dataset is small. Lower layers tend to capture general features, and adapting them might make the model too specialized, losing generalization ability.</li>\n      <li><strong>Focus on Task-Specific Information</strong>: The upper (or top) layers of a model generally capture task-specific features, while lower layers handle more general, reusable features. LoRA’s selective adaptation focuses on adjusting only those layers where task-specific learning is most beneficial.</li>\n    </ol>\n<h6 id=\"does-lora-impact-lower-attention-layers-less-than-higher-attention-layers\">Does LoRA Impact Lower Attention Layers Less Than Higher Attention Layers?</h6>\n<ul>\n  <li>\n    <p>Yes, in practice, LoRA impacts higher attention layers more than lower ones. This is because LoRA selectively adapts layers, targeting the task-specific adaptability of higher layers while preserving the general-purpose features in lower layers. This design enables effective task adaptation with minimal overfitting, allowing the model to retain broad applicability.</p>\n  </li>\n  <li>\n    <p><strong>Why higher attention layers are more affected:</strong></p>\n\n    <ol>\n      <li>\n        <p><strong>Function of Higher Attention Layers</strong>: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.</p>\n      </li>\n      <li>\n        <p><strong>Less Impact on Lower Layers</strong>: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.</p>\n      </li>\n      <li>\n        <p><strong>LoRA’s Selective Impact</strong>: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.</p>\n      </li>\n      <li>\n        <p><strong>Regularization Effect in Lower Layers</strong>: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Practical Implications:</strong></p>\n  </li>\n  <li>\n    <p>In many cases, fine-tuning with LoRA results in:</p>\n    <ul>\n      <li><strong>Major adjustments</strong> to higher layers, allowing the model to learn specific features of the fine-tuning task.</li>\n      <li><strong>Minimal impact</strong> on lower layers, preserving general knowledge from pre-training and preventing overfitting.</li>\n    </ul>\n  </li>\n</ul>\n<p>Yes, in practice, LoRA impacts higher attention layers more than lower ones. This is because LoRA selectively adapts layers, targeting the task-specific adaptability of higher layers while preserving the general-purpose features in lower layers. This design enables effective task adaptation with minimal overfitting, allowing the model to retain broad applicability.</p>\n<p><strong>Why higher attention layers are more affected:</strong></p>\n<ol>\n      <li>\n        <p><strong>Function of Higher Attention Layers</strong>: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.</p>\n      </li>\n      <li>\n        <p><strong>Less Impact on Lower Layers</strong>: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.</p>\n      </li>\n      <li>\n        <p><strong>LoRA’s Selective Impact</strong>: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.</p>\n      </li>\n      <li>\n        <p><strong>Regularization Effect in Lower Layers</strong>: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.</p>\n      </li>\n    </ol>\n<p><strong>Function of Higher Attention Layers</strong>: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.</p>\n<p><strong>Less Impact on Lower Layers</strong>: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.</p>\n<p><strong>LoRA’s Selective Impact</strong>: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.</p>\n<p><strong>Regularization Effect in Lower Layers</strong>: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.</p>\n<p><strong>Practical Implications:</strong></p>\n<p>In many cases, fine-tuning with LoRA results in:</p>\n<ul>\n      <li><strong>Major adjustments</strong> to higher layers, allowing the model to learn specific features of the fine-tuning task.</li>\n      <li><strong>Minimal impact</strong> on lower layers, preserving general knowledge from pre-training and preventing overfitting.</li>\n    </ul>\n<h4 id=\"quantized-low-rank-adaptation-qlora\">Quantized Low-Rank Adaptation (QLoRA)</h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA: Efficient Finetuning of Quantized LLMs</a> by Dettmers et al. from the University of Washington.</li>\n  <li>QLoRA is an efficient finetuning method that enables training very large models—up to 65B parameters—on a single 48GB GPU, without loss of accuracy compared to full 16-bit finetuning. It does this by <strong>backpropagating through a frozen, 4-bit quantized pretrained model</strong> into <strong>LoRA Adapters</strong>.</li>\n  <li>In short, QLoRA combines LoRA’s parameter-efficient adaptation with two key quantization innovations—<strong>4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>)</strong> and <strong>Double Quantization (DQ)</strong>—plus <strong>paged optimizers</strong> to handle memory spikes. This combination greatly reduces GPU memory usage while preserving performance.</li>\n  <li>For a practical introduction to QLoRA, see this Hugging Face blog on <a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>.</li>\n</ul>\n<h5 id=\"high-level-process\">High-level Process</h5>\n<ol>\n  <li>Quantize the pretrained model weights to 4-bit and freeze them.</li>\n  <li>Attach small, trainable LoRA adapters throughout the transformer layers.</li>\n  <li>Fine-tune only these adapters while the quantized base model remains frozen.</li>\n</ol>\n<ul>\n  <li>The figure below from the paper shows different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.</li>\n</ul>\n<p><img src=\"../../../images/papers/QLoRA.jpg\" alt=\"\"></p>\n<ul>\n  <li>The figure below (<a href=\"https://www.linkedin.com/in/mary-newhauser/\">source</a>) compares full fine-tuning, LoRA, and QLoRA:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/FFT-LoRA-QLoRA.jpg\" alt=\"\"></p>\n<h5 id=\"key-components\">Key Components</h5>\n<ol>\n  <li>\n    <p><strong>Low-Rank Adaptation (LoRA)</strong>:</p>\n\n    <ul>\n      <li>Inserts small rank-decomposed trainable matrices into all of a base model’s layers. Put simply, the LoRA adapters are applied to  both the <strong>self-attention module</strong> (including query, key, value, and output projections)–as in vanilla LoRA–and the <strong>feed-forward network (MLP)</strong> layers inside each transformer block. This broader placement was found to be critical for matching the performance of full 16-bit finetuning.</li>\n      <li>Only these adapters are updated; the large frozen weight matrices are never modified.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>) Quantization</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">NF4</code> is an <strong>information-theoretically optimal quantization scheme</strong> for values drawn from a zero-centered normal distribution (common for pretrained LLM weights).</li>\n      <li>It is based on quantile quantization, ensuring each bin contains an equal probability mass, minimizing quantization error.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">NF4</code> avoids expensive per-tensor quantile estimation by exploiting the fact that all weights can be scaled to match a fixed <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-267-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1371\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.87em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1372\"><span class=\"mi\" id=\"MathJax-Span-1373\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1374\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-1375\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1376\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-1377\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-1378\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-267\">N(0,1)</script> distribution, allowing the use of precomputed quantization bin edges.</li>\n      <li>The quantization is <strong>asymmetric with an exact zero representation</strong>, which is crucial for padding/sparse elements, and means that only a <strong>scale value</strong> (no zero-point) needs to be stored for each block.</li>\n      <li>Compared to regular 4-bit floats (<code class=\"language-plaintext highlighter-rouge\">FP4</code>), <code class=\"language-plaintext highlighter-rouge\">NF4</code> yields lower perplexity and higher accuracy across benchmarks while keeping the same memory footprint.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Double Quantization (DQ)</strong>:</p>\n\n    <ul>\n      <li>In block-wise quantization, each block of weights has an associated <strong>quantization constant (scale)</strong> — but no separate zero-point, since <code class=\"language-plaintext highlighter-rouge\">NF4</code>’s asymmetric codebook already contains an exact zero representation. These scale values are the only per-block metadata stored alongside the quantized weights, and in QLoRA’s DQ scheme, they are themselves quantized to further reduce memory overhead. For small block sizes (e.g., 64), storing these constants can be a large relative memory cost.</li>\n      <li>DQ reduces this overhead by <strong>quantizing the quantization constants themselves</strong>.</li>\n      <li>The first quantization maps <code class=\"language-plaintext highlighter-rouge\">FP32</code> weights → <code class=\"language-plaintext highlighter-rouge\">NF4</code> values with per-block <code class=\"language-plaintext highlighter-rouge\">FP32</code> scales.</li>\n      <li>The second quantization maps these scales into <code class=\"language-plaintext highlighter-rouge\">FP8</code> (block size 256), plus a second set of <code class=\"language-plaintext highlighter-rouge\">FP32</code> scales for dequantization.</li>\n      <li>This reduces quantization constant storage from 0.5 bits per parameter to 0.127 bits per parameter—a 0.373 bit saving per parameter—without measurable performance loss.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Paged Optimizers</strong>:</p>\n\n    <ul>\n      <li>Uses NVIDIA Unified Memory to automatically swap optimizer states between CPU RAM and GPU memory when GPU memory is full during large sequence processing.</li>\n      <li>This avoids out-of-memory errors without slowing down training under typical sequence lengths.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Low-Rank Adaptation (LoRA)</strong>:</p>\n<ul>\n      <li>Inserts small rank-decomposed trainable matrices into all of a base model’s layers. Put simply, the LoRA adapters are applied to  both the <strong>self-attention module</strong> (including query, key, value, and output projections)–as in vanilla LoRA–and the <strong>feed-forward network (MLP)</strong> layers inside each transformer block. This broader placement was found to be critical for matching the performance of full 16-bit finetuning.</li>\n      <li>Only these adapters are updated; the large frozen weight matrices are never modified.</li>\n    </ul>\n<p><strong>4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>) Quantization</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">NF4</code> is an <strong>information-theoretically optimal quantization scheme</strong> for values drawn from a zero-centered normal distribution (common for pretrained LLM weights).</li>\n      <li>It is based on quantile quantization, ensuring each bin contains an equal probability mass, minimizing quantization error.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">NF4</code> avoids expensive per-tensor quantile estimation by exploiting the fact that all weights can be scaled to match a fixed <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-267-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1371\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.87em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1372\"><span class=\"mi\" id=\"MathJax-Span-1373\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1374\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-1375\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1376\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-1377\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-1378\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-267\">N(0,1)</script> distribution, allowing the use of precomputed quantization bin edges.</li>\n      <li>The quantization is <strong>asymmetric with an exact zero representation</strong>, which is crucial for padding/sparse elements, and means that only a <strong>scale value</strong> (no zero-point) needs to be stored for each block.</li>\n      <li>Compared to regular 4-bit floats (<code class=\"language-plaintext highlighter-rouge\">FP4</code>), <code class=\"language-plaintext highlighter-rouge\">NF4</code> yields lower perplexity and higher accuracy across benchmarks while keeping the same memory footprint.</li>\n    </ul>\n<p><strong>Double Quantization (DQ)</strong>:</p>\n<ul>\n      <li>In block-wise quantization, each block of weights has an associated <strong>quantization constant (scale)</strong> — but no separate zero-point, since <code class=\"language-plaintext highlighter-rouge\">NF4</code>’s asymmetric codebook already contains an exact zero representation. These scale values are the only per-block metadata stored alongside the quantized weights, and in QLoRA’s DQ scheme, they are themselves quantized to further reduce memory overhead. For small block sizes (e.g., 64), storing these constants can be a large relative memory cost.</li>\n      <li>DQ reduces this overhead by <strong>quantizing the quantization constants themselves</strong>.</li>\n      <li>The first quantization maps <code class=\"language-plaintext highlighter-rouge\">FP32</code> weights → <code class=\"language-plaintext highlighter-rouge\">NF4</code> values with per-block <code class=\"language-plaintext highlighter-rouge\">FP32</code> scales.</li>\n      <li>The second quantization maps these scales into <code class=\"language-plaintext highlighter-rouge\">FP8</code> (block size 256), plus a second set of <code class=\"language-plaintext highlighter-rouge\">FP32</code> scales for dequantization.</li>\n      <li>This reduces quantization constant storage from 0.5 bits per parameter to 0.127 bits per parameter—a 0.373 bit saving per parameter—without measurable performance loss.</li>\n    </ul>\n<p><strong>Paged Optimizers</strong>:</p>\n<ul>\n      <li>Uses NVIDIA Unified Memory to automatically swap optimizer states between CPU RAM and GPU memory when GPU memory is full during large sequence processing.</li>\n      <li>This avoids out-of-memory errors without slowing down training under typical sequence lengths.</li>\n    </ul>\n<h5 id=\"operation\">Operation</h5>\n<ul>\n  <li>\n    <p>In a single linear layer with LoRA in QLoRA:</p>\n\n    <ul>\n      <li>Stored weight format: <strong><code class=\"language-plaintext highlighter-rouge\">NF4</code> quantized base model weights</strong>, plus LoRA adapter weights in <code class=\"language-plaintext highlighter-rouge\">BF16</code>.</li>\n      <li>Computation:\n        <ul>\n          <li>Dequantize <code class=\"language-plaintext highlighter-rouge\">NF4</code> weights (double dequantization if DQ is used) into <code class=\"language-plaintext highlighter-rouge\">BF16</code> on the fly.</li>\n          <li>Perform matrix multiplications with the dequantized weights plus LoRA projections.</li>\n          <li>Backpropagate gradients <strong>only into LoRA parameters</strong>; no gradient storage for the base weights.</li>\n        </ul>\n      </li>\n    </ul>\n\n    <blockquote>\n      <p>In QLoRA, only the original model’s weights are quantized to <code class=\"language-plaintext highlighter-rouge\">NF4</code>. The LoRA adapter weights remain in higher precision (<code class=\"language-plaintext highlighter-rouge\">BF16</code>) and are the only parameters updated during finetuning.</p>\n    </blockquote>\n  </li>\n  <li>\n    <p>Formally:</p>\n\n    <ol>\n      <li>\n        <p><strong>Forward pass with double dequantization</strong>:</p>\n\n        <ul>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-268-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>NF4</mtext></mrow></msub><mover><mo>&amp;#x2192;</mo><mpadded width=&quot;+0.611em&quot; lspace=&quot;0.278em&quot; voffset=&quot;.15em&quot;><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1379\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.628em, 1006.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1380\"><span class=\"msubsup\" id=\"MathJax-Span-1381\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1383\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mtext\" id=\"MathJax-Span-1385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">NF4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1386\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.117em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1387\" style=\"\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: -0.049em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.784em;\">→<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.367em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1001.25em, 4.273em, -999.997em); top: -4.685em; left: 0.003em;\"><span class=\"mpadded\" id=\"MathJax-Span-1388\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.273em, -999.997em); top: -4.164em; left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-1389\"><span class=\"mtext\" id=\"MathJax-Span-1390\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">DQ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1391\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1392\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1393\"><span class=\"mrow\" id=\"MathJax-Span-1394\"><span class=\"mtext\" id=\"MathJax-Span-1395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>NF4</mtext></mrow></msub><mover><mo>→</mo><mpadded width=\"+0.611em\" lspace=\"0.278em\" voffset=\".15em\"><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-268\">W_{\\text{NF4}} \\xrightarrow{\\text{DQ}} W_{\\text{BF16}}</script>\n          </li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-269-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1396\" style=\"width: 17.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1014.43em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1397\"><span class=\"msubsup\" id=\"MathJax-Span-1398\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1400\"><span class=\"mrow\" id=\"MathJax-Span-1401\"><span class=\"mtext\" id=\"MathJax-Span-1402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1404\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1405\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1406\"><span class=\"mrow\" id=\"MathJax-Span-1407\"><span class=\"mtext\" id=\"MathJax-Span-1408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1409\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1410\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1411\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1412\"><span class=\"mrow\" id=\"MathJax-Span-1413\"><span class=\"mtext\" id=\"MathJax-Span-1414\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1415\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-1416\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1417\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1418\"><span class=\"mrow\" id=\"MathJax-Span-1419\"><span class=\"mtext\" id=\"MathJax-Span-1420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1421\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1423\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1424\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1426\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>⋅</mo><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-269\">Y_{\\text{BF16}} = X_{\\text{BF16}} \\cdot W_{\\text{BF16}} + X_{\\text{BF16}} L_1 L_2</script>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Backward pass</strong>:</p>\n\n        <ul>\n          <li>Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n</ul>\n<p>In a single linear layer with LoRA in QLoRA:</p>\n<ul>\n      <li>Stored weight format: <strong><code class=\"language-plaintext highlighter-rouge\">NF4</code> quantized base model weights</strong>, plus LoRA adapter weights in <code class=\"language-plaintext highlighter-rouge\">BF16</code>.</li>\n      <li>Computation:\n        <ul>\n          <li>Dequantize <code class=\"language-plaintext highlighter-rouge\">NF4</code> weights (double dequantization if DQ is used) into <code class=\"language-plaintext highlighter-rouge\">BF16</code> on the fly.</li>\n          <li>Perform matrix multiplications with the dequantized weights plus LoRA projections.</li>\n          <li>Backpropagate gradients <strong>only into LoRA parameters</strong>; no gradient storage for the base weights.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Dequantize <code class=\"language-plaintext highlighter-rouge\">NF4</code> weights (double dequantization if DQ is used) into <code class=\"language-plaintext highlighter-rouge\">BF16</code> on the fly.</li>\n          <li>Perform matrix multiplications with the dequantized weights plus LoRA projections.</li>\n          <li>Backpropagate gradients <strong>only into LoRA parameters</strong>; no gradient storage for the base weights.</li>\n        </ul>\n<blockquote>\n      <p>In QLoRA, only the original model’s weights are quantized to <code class=\"language-plaintext highlighter-rouge\">NF4</code>. The LoRA adapter weights remain in higher precision (<code class=\"language-plaintext highlighter-rouge\">BF16</code>) and are the only parameters updated during finetuning.</p>\n    </blockquote>\n<p>In QLoRA, only the original model’s weights are quantized to <code class=\"language-plaintext highlighter-rouge\">NF4</code>. The LoRA adapter weights remain in higher precision (<code class=\"language-plaintext highlighter-rouge\">BF16</code>) and are the only parameters updated during finetuning.</p>\n<p>Formally:</p>\n<ol>\n      <li>\n        <p><strong>Forward pass with double dequantization</strong>:</p>\n\n        <ul>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-268-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>NF4</mtext></mrow></msub><mover><mo>&amp;#x2192;</mo><mpadded width=&quot;+0.611em&quot; lspace=&quot;0.278em&quot; voffset=&quot;.15em&quot;><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1379\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.628em, 1006.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1380\"><span class=\"msubsup\" id=\"MathJax-Span-1381\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1383\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mtext\" id=\"MathJax-Span-1385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">NF4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1386\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.117em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1387\" style=\"\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: -0.049em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.784em;\">→<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.367em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1001.25em, 4.273em, -999.997em); top: -4.685em; left: 0.003em;\"><span class=\"mpadded\" id=\"MathJax-Span-1388\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.273em, -999.997em); top: -4.164em; left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-1389\"><span class=\"mtext\" id=\"MathJax-Span-1390\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">DQ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1391\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1392\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1393\"><span class=\"mrow\" id=\"MathJax-Span-1394\"><span class=\"mtext\" id=\"MathJax-Span-1395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>NF4</mtext></mrow></msub><mover><mo>→</mo><mpadded width=\"+0.611em\" lspace=\"0.278em\" voffset=\".15em\"><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-268\">W_{\\text{NF4}} \\xrightarrow{\\text{DQ}} W_{\\text{BF16}}</script>\n          </li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-269-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1396\" style=\"width: 17.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1014.43em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1397\"><span class=\"msubsup\" id=\"MathJax-Span-1398\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1400\"><span class=\"mrow\" id=\"MathJax-Span-1401\"><span class=\"mtext\" id=\"MathJax-Span-1402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1404\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1405\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1406\"><span class=\"mrow\" id=\"MathJax-Span-1407\"><span class=\"mtext\" id=\"MathJax-Span-1408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1409\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1410\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1411\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1412\"><span class=\"mrow\" id=\"MathJax-Span-1413\"><span class=\"mtext\" id=\"MathJax-Span-1414\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1415\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-1416\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1417\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1418\"><span class=\"mrow\" id=\"MathJax-Span-1419\"><span class=\"mtext\" id=\"MathJax-Span-1420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1421\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1423\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1424\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1426\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>⋅</mo><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-269\">Y_{\\text{BF16}} = X_{\\text{BF16}} \\cdot W_{\\text{BF16}} + X_{\\text{BF16}} L_1 L_2</script>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Backward pass</strong>:</p>\n\n        <ul>\n          <li>Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Forward pass with double dequantization</strong>:</p>\n<ul>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-268-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>NF4</mtext></mrow></msub><mover><mo>&amp;#x2192;</mo><mpadded width=&quot;+0.611em&quot; lspace=&quot;0.278em&quot; voffset=&quot;.15em&quot;><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1379\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.628em, 1006.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1380\"><span class=\"msubsup\" id=\"MathJax-Span-1381\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1383\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mtext\" id=\"MathJax-Span-1385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">NF4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1386\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.117em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1387\" style=\"\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: -0.049em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.784em;\">→<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.367em;\">−<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1001.25em, 4.273em, -999.997em); top: -4.685em; left: 0.003em;\"><span class=\"mpadded\" id=\"MathJax-Span-1388\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.273em, -999.997em); top: -4.164em; left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-1389\"><span class=\"mtext\" id=\"MathJax-Span-1390\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">DQ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1391\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1392\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1393\"><span class=\"mrow\" id=\"MathJax-Span-1394\"><span class=\"mtext\" id=\"MathJax-Span-1395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>NF4</mtext></mrow></msub><mover><mo>→</mo><mpadded width=\"+0.611em\" lspace=\"0.278em\" voffset=\".15em\"><mtext>DQ</mtext></mpadded></mover><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-268\">W_{\\text{NF4}} \\xrightarrow{\\text{DQ}} W_{\\text{BF16}}</script>\n          </li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-269-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1396\" style=\"width: 17.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1014.43em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1397\"><span class=\"msubsup\" id=\"MathJax-Span-1398\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1400\"><span class=\"mrow\" id=\"MathJax-Span-1401\"><span class=\"mtext\" id=\"MathJax-Span-1402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1404\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1405\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1406\"><span class=\"mrow\" id=\"MathJax-Span-1407\"><span class=\"mtext\" id=\"MathJax-Span-1408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1409\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1410\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1411\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1412\"><span class=\"mrow\" id=\"MathJax-Span-1413\"><span class=\"mtext\" id=\"MathJax-Span-1414\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1415\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-1416\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1417\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1418\"><span class=\"mrow\" id=\"MathJax-Span-1419\"><span class=\"mtext\" id=\"MathJax-Span-1420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">BF16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1421\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1423\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1424\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1426\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>⋅</mo><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>BF16</mtext></mrow></msub><msub><mi>L</mi><mn>1</mn></msub><msub><mi>L</mi><mn>2</mn></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-269\">Y_{\\text{BF16}} = X_{\\text{BF16}} \\cdot W_{\\text{BF16}} + X_{\\text{BF16}} L_1 L_2</script>\n          </li>\n        </ul>\n<p><strong>Backward pass</strong>:</p>\n<ul>\n          <li>Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.</li>\n        </ul>\n<h5 id=\"impact-and-results\">Impact and Results</h5>\n<ul>\n  <li>Guanaco, the best-performing QLoRA model family, achieves 99.3% of ChatGPT’s score on the Vicuna benchmark after 24 hours of finetuning on a single GPU.</li>\n  <li>QLoRA’s memory optimizations—<code class=\"language-plaintext highlighter-rouge\">NF4</code>, double quantization, and paged optimizers—enable training 33B parameter models on 24GB consumer GPUs and 65B models on a single 48GB GPU.</li>\n  <li>Across GLUE, Super-NaturalInstructions, and MMLU benchmarks, <code class=\"language-plaintext highlighter-rouge\">NF4</code> + DQ matches or exceeds full 16-bit LoRA and full finetuning results, outperforming <code class=\"language-plaintext highlighter-rouge\">FP4</code> and <code class=\"language-plaintext highlighter-rouge\">INT4</code> quantization.</li>\n  <li>This makes large-scale finetuning accessible for small teams and researchers without massive compute clusters.</li>\n</ul>\n<h4 id=\"quantization-aware-low-rank-adaptation-qa-lora\">Quantization-Aware Low-Rank Adaptation (QA-LoRA)</h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2309.14717\">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large LLMs</a>.</li>\n  <li>Xu et al. from Huawei propose Quantization-Aware Low-Rank Adaptation (QA-LoRA), which jointly performs parameter-efficient fine-tuning and low-bit quantization. Unlike prior methods, QA-LoRA ensures that the fine-tuned weights remain in low-bit form after training, eliminating the need for post-training quantization (PTQ) — a step that typically introduces accuracy degradation at very low bit widths such as <code class=\"language-plaintext highlighter-rouge\">INT3</code> or <code class=\"language-plaintext highlighter-rouge\">INT2</code>. In standard approaches like LoRA or QLoRA, fine-tuning ends with full-precision weights (e.g., <code class=\"language-plaintext highlighter-rouge\">FP16</code>). Deploying in low precision then requires PTQ, which compresses the model <strong>after</strong> it has been optimized in high precision, leading to significant quantization error and loss in accuracy for low-bit formats. QA-LoRA avoids this by training directly in the target low-bit format in a quantization-aware manner, so the model is deployment-ready without further quantization steps.</li>\n  <li>Put simply, QA-LoRA extends QLoRA’s idea but removes its main deployment bottleneck (FP16 fallback) while improving accuracy at lower bit widths, all with minimal implementation overhead. The merged weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-270-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo>+</mo><mi>s</mi><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1427\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.25em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1428\"><span class=\"msup\" id=\"MathJax-Span-1429\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1430\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.992em;\"><span class=\"mo\" id=\"MathJax-Span-1431\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1432\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"texatom\" id=\"MathJax-Span-1433\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1434\"><span class=\"munderover\" id=\"MathJax-Span-1435\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1437\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1438\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1439\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span class=\"mi\" id=\"MathJax-Span-1440\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1441\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>W</mi><mo>′</mo></msup><mo>=</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>+</mo><mi>s</mi><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-270\">W' = \\tilde{W} + sAB</script> remain quantized throughout fine-tuning, meaning inference can run natively on low-bit integer kernels without precision fallback.</li>\n  <li>The motivation comes from an <strong>imbalance between the Degrees of Freedom (DoF) of quantization and adaptation</strong>: traditional column-wise quantization assigns one scaling and zero factor per column (low DoF) but uses many LoRA parameters per column (high DoF). This can lead to large quantization errors and difficulty in merging LoRA and base weights in low precision. QA-LoRA solves this by using <strong>group-wise quantization and group-shared LoRA parameters</strong>, increasing quantization DoF and reducing adaptation DoF in a balanced way. By balancing quantization and adaptation degrees of freedom via group-wise design, it offers low-bit fine-tuning and efficient deployment for LLMs on both server and edge environments.</li>\n  <li>Code is available at <a href=\"https://github.com/yuhuixu1993/qa-lora\">GitHub</a>.</li>\n</ul>\n<h5 id=\"key-ideas-and-contributions\">Key Ideas and Contributions</h5>\n<ol>\n  <li><strong>Group-wise quantization:</strong> Partition each column of the pre-trained weight matrix into <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-271-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1442\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1443\"><span class=\"mi\" id=\"MathJax-Span-1444\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-271\">L</script> groups. Each group has its own scaling (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-272-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1445\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1446\"><span class=\"mi\" id=\"MathJax-Span-1447\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-272\">\\alpha</script>) and zero (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-273-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1448\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1449\"><span class=\"mi\" id=\"MathJax-Span-1450\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-273\">\\beta</script>) factors, enabling finer quantization granularity and reducing quantization error.</li>\n  <li><strong>Group-wise LoRA:</strong> Within each group, all rows of the LoRA <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-274-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1451\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1452\"><span class=\"mi\" id=\"MathJax-Span-1453\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-274\">A</script> matrix share the same values. This is implemented via a parameter-free summation (average pooling) over the input vector, reducing its dimension from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-275-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>n</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1454\" style=\"width: 1.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.36em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1455\"><span class=\"msubsup\" id=\"MathJax-Span-1456\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1457\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1458\"><span class=\"mrow\" id=\"MathJax-Span-1459\"><span class=\"mi\" id=\"MathJax-Span-1460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1461\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>n</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-275\">D_{in}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-276-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1462\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1463\"><span class=\"mi\" id=\"MathJax-Span-1464\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-276\">L</script> before applying LoRA.</li>\n  <li><strong>Mergeability in low-bit form:</strong> By aligning the grouping in quantization and LoRA, the merged weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-277-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo>+</mo><mi>s</mi><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1465\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.25em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1466\"><span class=\"msup\" id=\"MathJax-Span-1467\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1468\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.992em;\"><span class=\"mo\" id=\"MathJax-Span-1469\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1470\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"texatom\" id=\"MathJax-Span-1471\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1472\"><span class=\"munderover\" id=\"MathJax-Span-1473\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1474\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1475\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1476\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1477\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span class=\"mi\" id=\"MathJax-Span-1478\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1479\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>W</mi><mo>′</mo></msup><mo>=</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>+</mo><mi>s</mi><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-277\">W' = \\tilde{W} + sAB</script> can remain in <code class=\"language-plaintext highlighter-rouge\">INT4</code>/<code class=\"language-plaintext highlighter-rouge\">INT3</code>/<code class=\"language-plaintext highlighter-rouge\">INT2</code> format, enabling fast inference without <code class=\"language-plaintext highlighter-rouge\">FP16</code> fallback.</li>\n  <li><strong>Efficient operators:</strong> QA-LoRA uses standard integer formats (e.g., <code class=\"language-plaintext highlighter-rouge\">INT4</code>) with CUDA-optimized kernels, avoiding the lack of operator-level acceleration for <code class=\"language-plaintext highlighter-rouge\">NF4</code> (used in QLoRA).</li>\n</ol>\n<h5 id=\"implementation-details\">Implementation Details</h5>\n<ul>\n  <li>\n    <p><strong>Minimal code change:</strong> Implemented by inserting a few lines into LoRA’s forward pass. The pseudocode in the paper shows:</p>\n\n    <ul>\n      <li>Pre-quantization of weights with group-wise scaling and zero factors.</li>\n      <li>Pooling (QA) over input groups before applying LoRA <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-278-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1480\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1481\"><span class=\"mi\" id=\"MathJax-Span-1482\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-278\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-279-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1483\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1484\"><span class=\"mi\" id=\"MathJax-Span-1485\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-279\">B</script> matrices.</li>\n      <li>Adjusting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-280-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1486\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1487\"><span class=\"mi\" id=\"MathJax-Span-1488\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-280\">\\beta</script> factors in the merge step to incorporate LoRA updates without leaving the low-bit domain.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Quantization uses GPTQ with <code class=\"language-plaintext highlighter-rouge\">group_size = 32</code> by default, asymmetric quantization, <code class=\"language-plaintext highlighter-rouge\">act_order = false</code>, and <code class=\"language-plaintext highlighter-rouge\">true_sequential = true</code>.</p>\n  </li>\n  <li>\n    <p>The following figure illustrates QA-LoRA’s design. Compared to LoRA and QLoRA, QA-LoRA is efficient in both fine-tuning and inference, without accuracy loss from PTQ. While <code class=\"language-plaintext highlighter-rouge\">INT4</code> is shown, QA-LoRA generalizes to <code class=\"language-plaintext highlighter-rouge\">INT3</code> and <code class=\"language-plaintext highlighter-rouge\">INT2</code>.</p>\n  </li>\n</ul>\n<p><strong>Minimal code change:</strong> Implemented by inserting a few lines into LoRA’s forward pass. The pseudocode in the paper shows:</p>\n<ul>\n      <li>Pre-quantization of weights with group-wise scaling and zero factors.</li>\n      <li>Pooling (QA) over input groups before applying LoRA <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-278-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1480\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1481\"><span class=\"mi\" id=\"MathJax-Span-1482\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-278\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-279-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1483\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1484\"><span class=\"mi\" id=\"MathJax-Span-1485\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-279\">B</script> matrices.</li>\n      <li>Adjusting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-280-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1486\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1487\"><span class=\"mi\" id=\"MathJax-Span-1488\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-280\">\\beta</script> factors in the merge step to incorporate LoRA updates without leaving the low-bit domain.</li>\n    </ul>\n<p>Quantization uses GPTQ with <code class=\"language-plaintext highlighter-rouge\">group_size = 32</code> by default, asymmetric quantization, <code class=\"language-plaintext highlighter-rouge\">act_order = false</code>, and <code class=\"language-plaintext highlighter-rouge\">true_sequential = true</code>.</p>\n<p>The following figure illustrates QA-LoRA’s design. Compared to LoRA and QLoRA, QA-LoRA is efficient in both fine-tuning and inference, without accuracy loss from PTQ. While <code class=\"language-plaintext highlighter-rouge\">INT4</code> is shown, QA-LoRA generalizes to <code class=\"language-plaintext highlighter-rouge\">INT3</code> and <code class=\"language-plaintext highlighter-rouge\">INT2</code>.</p>\n<p><img src=\"../../../images/papers/QA-LoRA.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p><strong>Fine-tuning settings:</strong></p>\n\n    <ul>\n      <li><strong>Datasets:</strong> Alpaca (52K) and FLAN v2 subset (320K).</li>\n      <li><strong>Optimizer:</strong> Paged AdamW, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-281-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi><mo>=</mo><mn>2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1489\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1490\"><span class=\"mi\" id=\"MathJax-Span-1491\" style=\"font-family: STIXGeneral-Italic;\">η</span><span class=\"mo\" id=\"MathJax-Span-1492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1493\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-1494\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1495\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1496\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1497\"><span class=\"mrow\" id=\"MathJax-Span-1498\"><span class=\"mo\" id=\"MathJax-Span-1499\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1500\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi><mo>=</mo><mn>2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-281\">\\eta = 2 \\times 10^{-5}</script> (LLaMA-7B/13B) or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-282-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi><mo>=</mo><mn>1</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1501\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1502\"><span class=\"mi\" id=\"MathJax-Span-1503\" style=\"font-family: STIXGeneral-Italic;\">η</span><span class=\"mo\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1505\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-1506\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1507\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1508\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1509\"><span class=\"mrow\" id=\"MathJax-Span-1510\"><span class=\"mo\" id=\"MathJax-Span-1511\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1512\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi><mo>=</mo><mn>1</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-282\">\\eta = 1 \\times 10^{-5}</script> (LLaMA-33B/65B), <code class=\"language-plaintext highlighter-rouge\">max_grad_norm = 0.3</code>, <code class=\"language-plaintext highlighter-rouge\">batch_size = 16</code>.</li>\n      <li><strong>Steps:</strong> 10K (Alpaca), 20K (FLAN v2).</li>\n      <li><strong>Hardware:</strong> Tesla V100 GPUs (1 GPU for ≤33B models, 2 GPUs for 65B).</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Fine-tuning settings:</strong></p>\n<ul>\n      <li><strong>Datasets:</strong> Alpaca (52K) and FLAN v2 subset (320K).</li>\n      <li><strong>Optimizer:</strong> Paged AdamW, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-281-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi><mo>=</mo><mn>2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1489\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1490\"><span class=\"mi\" id=\"MathJax-Span-1491\" style=\"font-family: STIXGeneral-Italic;\">η</span><span class=\"mo\" id=\"MathJax-Span-1492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1493\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-1494\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1495\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1496\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1497\"><span class=\"mrow\" id=\"MathJax-Span-1498\"><span class=\"mo\" id=\"MathJax-Span-1499\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1500\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi><mo>=</mo><mn>2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-281\">\\eta = 2 \\times 10^{-5}</script> (LLaMA-7B/13B) or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-282-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi><mo>=</mo><mn>1</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1501\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1502\"><span class=\"mi\" id=\"MathJax-Span-1503\" style=\"font-family: STIXGeneral-Italic;\">η</span><span class=\"mo\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1505\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-1506\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1507\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1508\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1509\"><span class=\"mrow\" id=\"MathJax-Span-1510\"><span class=\"mo\" id=\"MathJax-Span-1511\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1512\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi><mo>=</mo><mn>1</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-282\">\\eta = 1 \\times 10^{-5}</script> (LLaMA-33B/65B), <code class=\"language-plaintext highlighter-rouge\">max_grad_norm = 0.3</code>, <code class=\"language-plaintext highlighter-rouge\">batch_size = 16</code>.</li>\n      <li><strong>Steps:</strong> 10K (Alpaca), 20K (FLAN v2).</li>\n      <li><strong>Hardware:</strong> Tesla V100 GPUs (1 GPU for ≤33B models, 2 GPUs for 65B).</li>\n    </ul>\n<h5 id=\"algorithm-steps\">Algorithm Steps</h5>\n<ol>\n  <li><strong>Group-wise quantization of base weights</strong> into <code class=\"language-plaintext highlighter-rouge\">INT4/3/2</code> at fine-tuning start.</li>\n  <li><strong>Group-wise LoRA pooling and adaptation</strong> — only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-283-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo>&amp;#x00D7;</mo><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>n</mi><mi>t</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1513\" style=\"width: 3.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.28em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1514\"><span class=\"mi\" id=\"MathJax-Span-1515\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1516\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1517\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1518\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1519\"><span class=\"mrow\" id=\"MathJax-Span-1520\"><span class=\"mi\" id=\"MathJax-Span-1521\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1522\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1523\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo>×</mo><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>n</mi><mi>t</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-283\">L \\times D_{int}</script> parameters for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-284-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1524\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1525\"><span class=\"mi\" id=\"MathJax-Span-1526\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-284\">A</script> instead of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-285-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>n</mi></mrow></msub><mo>&amp;#x00D7;</mo><msub><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>n</mi><mi>t</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1527\" style=\"width: 4.846em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.01em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1528\"><span class=\"msubsup\" id=\"MathJax-Span-1529\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1530\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1531\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1534\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1535\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-1536\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1538\"><span class=\"mrow\" id=\"MathJax-Span-1539\"><span class=\"mi\" id=\"MathJax-Span-1540\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>n</mi></mrow></msub><mo>×</mo><msub><mi>D</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>n</mi><mi>t</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-285\">D_{in} \\times D_{int}</script>.</li>\n  <li><strong>Fine-tuning</strong> LoRA parameters while keeping base weights quantized.</li>\n  <li><strong>Merging</strong> LoRA and base weights in the quantized domain by adjusting <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-286-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1543\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1544\"><span class=\"mi\" id=\"MathJax-Span-1545\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-286\">\\beta</script> factors directly.</li>\n</ol>\n<h5 id=\"advantages-over-prior-work\">Advantages Over Prior Work</h5>\n<ul>\n  <li><strong>Compared to LoRA:</strong> Lower fine-tuning memory and faster inference due to quantization.</li>\n  <li><strong>Compared to QLoRA:</strong>\n    <ul>\n      <li>Avoids <code class=\"language-plaintext highlighter-rouge\">FP16</code> fallback after merging.</li>\n      <li>Uses CUDA-optimized INT formats instead of <code class=\"language-plaintext highlighter-rouge\">NF4</code>, yielding &gt;50% faster inference.</li>\n      <li>Higher accuracy at low bit widths (especially <code class=\"language-plaintext highlighter-rouge\">INT2</code>/<code class=\"language-plaintext highlighter-rouge\">INT3</code>) due to quantization-aware adaptation.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Avoids <code class=\"language-plaintext highlighter-rouge\">FP16</code> fallback after merging.</li>\n      <li>Uses CUDA-optimized INT formats instead of <code class=\"language-plaintext highlighter-rouge\">NF4</code>, yielding &gt;50% faster inference.</li>\n      <li>Higher accuracy at low bit widths (especially <code class=\"language-plaintext highlighter-rouge\">INT2</code>/<code class=\"language-plaintext highlighter-rouge\">INT3</code>) due to quantization-aware adaptation.</li>\n    </ul>\n<h5 id=\"results\">Results</h5>\n<ul>\n  <li>On LLaMA and Llama 2 models, QA-LoRA matches or exceeds QLoRA’s accuracy at <code class=\"language-plaintext highlighter-rouge\">INT4</code> and significantly outperforms it at <code class=\"language-plaintext highlighter-rouge\">INT3</code> and <code class=\"language-plaintext highlighter-rouge\">INT2</code>.</li>\n  <li>Training time reduction: For LLaMA-13B, fine-tuning time drops from 73.1h (QLoRA) to 29.5h (QA-LoRA).</li>\n  <li>Commonsense QA tasks show consistent improvements, with QA-LoRA (2-bit) achieving +15% accuracy over QLoRA (2-bit) with PTQ.</li>\n</ul>\n<h5 id=\"comparison-of-lora-qlora-and-qa-lora\">Comparison of LoRA, QLoRA, and QA-LoRA</h5>\n<ul>\n  <li>The table below summarizes key differences in methodology, quantization formats, fine-tuning characteristics, inference properties, and practical considerations for deployment for LoRA, QLoRA, and QA-LoRA.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LoRA<br>(Hu et al., 2021)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QLoRA<br>(Dettmers et al., 2023)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>QA-LoRA<br>(Xu et al., 2023)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Goal</td>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient fine-tuning (reduce trainable parameters while preserving accuracy)</td>\n<td class=\"tg-tleft-valign-first\">Combine low-bit quantization and LoRA to reduce fine-tuning memory</td>\n<td class=\"tg-tleft-valign-second\">Joint low-bit quantization and LoRA with mergeability in quantized form for both fine-tuning and inference</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Base Weight Precision During Fine-Tuning</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code> or <code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>NF4</code> (NormalFloat4, 4-bit floating point)</td>\n<td class=\"tg-tleft-valign-second\"><code>INT4, INT3, or INT2</code> (integer quantization) with group-wise scaling</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LoRA Weight Precision During Fine-Tuning</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code>/<code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code></td>\n<td class=\"tg-tleft-valign-second\">Same precision as base weight (<code>INT4/3/2</code>) due to quantization-aware adaptation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Adaptation Method</td>\n<td class=\"tg-tleft-valign-first\">Low-rank matrices A and B added to frozen base weights</td>\n<td class=\"tg-tleft-valign-first\">Same as LoRA but applied to NF4-quantized base weights</td>\n<td class=\"tg-tleft-valign-second\">Group-wise LoRA (shared parameters within quantization groups) for mergeability in low-bit domain</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Granularity</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Column-wise <code>NF4</code> quantization of base weights</td>\n<td class=\"tg-tleft-valign-second\">Group-wise integer quantization (e.g., group size 32) with group-specific scale and zero</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Post-Fine-Tuning Merge Result</td>\n<td class=\"tg-tleft-valign-first\">Full-precision model (merging A and B into W yields <code>FP16</code>/<code>FP32</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full-precision model (merging requires <code>FP16</code> fallback) unless PTQ is applied (accuracy loss at low bits)</td>\n<td class=\"tg-tleft-valign-second\">Fully quantized merged model, no need for PTQ, no accuracy loss compared to full precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Precision</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code>/<code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code> (without PTQ) or <code>INT4</code> (with PTQ, accuracy drop)</td>\n<td class=\"tg-tleft-valign-second\"><code>INT4/3/2</code>, same as during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Efficiency</td>\n<td class=\"tg-tleft-valign-first\">Reduced training memory vs. full fine-tuning; no change in inference speed</td>\n<td class=\"tg-tleft-valign-first\">Reduced training memory; inference slower without PTQ due to <code>FP16</code> fallback; NF4 has no widespread hardware acceleration</td>\n<td class=\"tg-tleft-valign-second\">Reduced training memory; inference fast due to native integer kernels; compatible with existing INT quantization acceleration</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Accuracy at Low Bit Widths</td>\n<td class=\"tg-tleft-valign-first\">High (since no quantization)</td>\n<td class=\"tg-tleft-valign-first\">Good at 4-bit <code>NF4</code>, but drops significantly with <code>INT4</code> PTQ, and worse at 3-bit/2-bit</td>\n<td class=\"tg-tleft-valign-second\">Matches QLoRA at 4-bit <code>NF4</code>; outperforms significantly at INT3 and INT2</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Implementation Complexity</td>\n<td class=\"tg-tleft-valign-first\">Moderate; widely supported in libraries</td>\n<td class=\"tg-tleft-valign-first\">Moderate; requires NF4 quantization and mixed precision handling</td>\n<td class=\"tg-tleft-valign-second\">Low; a few extra lines on top of LoRA, uses standard INT formats and group pooling</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Representative Use Cases</td>\n<td class=\"tg-tleft-valign-first\">PEFT when quantization is not needed or when deployment can afford <code>FP16</code></td>\n<td class=\"tg-tleft-valign-first\">Fine-tuning large models under memory constraints, where deployment can use <code>FP16</code> or 4-bit <code>NF4</code></td>\n<td class=\"tg-tleft-valign-second\">Fine-tuning and deploying large models directly in <code>INT4/3/2</code> on resource-limited hardware</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LoRA<br>(Hu et al., 2021)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QLoRA<br>(Dettmers et al., 2023)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>QA-LoRA<br>(Xu et al., 2023)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Goal</td>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient fine-tuning (reduce trainable parameters while preserving accuracy)</td>\n<td class=\"tg-tleft-valign-first\">Combine low-bit quantization and LoRA to reduce fine-tuning memory</td>\n<td class=\"tg-tleft-valign-second\">Joint low-bit quantization and LoRA with mergeability in quantized form for both fine-tuning and inference</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Base Weight Precision During Fine-Tuning</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code> or <code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>NF4</code> (NormalFloat4, 4-bit floating point)</td>\n<td class=\"tg-tleft-valign-second\"><code>INT4, INT3, or INT2</code> (integer quantization) with group-wise scaling</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LoRA Weight Precision During Fine-Tuning</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code>/<code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code></td>\n<td class=\"tg-tleft-valign-second\">Same precision as base weight (<code>INT4/3/2</code>) due to quantization-aware adaptation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Adaptation Method</td>\n<td class=\"tg-tleft-valign-first\">Low-rank matrices A and B added to frozen base weights</td>\n<td class=\"tg-tleft-valign-first\">Same as LoRA but applied to NF4-quantized base weights</td>\n<td class=\"tg-tleft-valign-second\">Group-wise LoRA (shared parameters within quantization groups) for mergeability in low-bit domain</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Granularity</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Column-wise <code>NF4</code> quantization of base weights</td>\n<td class=\"tg-tleft-valign-second\">Group-wise integer quantization (e.g., group size 32) with group-specific scale and zero</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Post-Fine-Tuning Merge Result</td>\n<td class=\"tg-tleft-valign-first\">Full-precision model (merging A and B into W yields <code>FP16</code>/<code>FP32</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full-precision model (merging requires <code>FP16</code> fallback) unless PTQ is applied (accuracy loss at low bits)</td>\n<td class=\"tg-tleft-valign-second\">Fully quantized merged model, no need for PTQ, no accuracy loss compared to full precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Precision</td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code>/<code>FP32</code></td>\n<td class=\"tg-tleft-valign-first\"><code>FP16</code> (without PTQ) or <code>INT4</code> (with PTQ, accuracy drop)</td>\n<td class=\"tg-tleft-valign-second\"><code>INT4/3/2</code>, same as during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Efficiency</td>\n<td class=\"tg-tleft-valign-first\">Reduced training memory vs. full fine-tuning; no change in inference speed</td>\n<td class=\"tg-tleft-valign-first\">Reduced training memory; inference slower without PTQ due to <code>FP16</code> fallback; NF4 has no widespread hardware acceleration</td>\n<td class=\"tg-tleft-valign-second\">Reduced training memory; inference fast due to native integer kernels; compatible with existing INT quantization acceleration</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Accuracy at Low Bit Widths</td>\n<td class=\"tg-tleft-valign-first\">High (since no quantization)</td>\n<td class=\"tg-tleft-valign-first\">Good at 4-bit <code>NF4</code>, but drops significantly with <code>INT4</code> PTQ, and worse at 3-bit/2-bit</td>\n<td class=\"tg-tleft-valign-second\">Matches QLoRA at 4-bit <code>NF4</code>; outperforms significantly at INT3 and INT2</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Implementation Complexity</td>\n<td class=\"tg-tleft-valign-first\">Moderate; widely supported in libraries</td>\n<td class=\"tg-tleft-valign-first\">Moderate; requires NF4 quantization and mixed precision handling</td>\n<td class=\"tg-tleft-valign-second\">Low; a few extra lines on top of LoRA, uses standard INT formats and group pooling</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Representative Use Cases</td>\n<td class=\"tg-tleft-valign-first\">PEFT when quantization is not needed or when deployment can afford <code>FP16</code></td>\n<td class=\"tg-tleft-valign-first\">Fine-tuning large models under memory constraints, where deployment can use <code>FP16</code> or 4-bit <code>NF4</code></td>\n<td class=\"tg-tleft-valign-second\">Fine-tuning and deploying large models directly in <code>INT4/3/2</code> on resource-limited hardware</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"refined-low-rank-adaptation-relora\"><a href=\"https://arxiv.org/abs/2307.05695\">Refined Low-Rank Adaptation (ReLoRA)</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2307.05695\">Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</a> by Lialin et al. from UMass Lowell.</li>\n  <li>Refined Low-Rank Adaptation (ReLoRA) is a low-rank training technique as an alternative approach to training large neural networks. ReLoRA utilizes low-rank updates to train high-rank networks. Put simply, they explore whether LoRA can be used for pretraining (as opposed to finetuning) LLMs in a parameter-efficient manner.</li>\n  <li>They apply ReLoRA to pre-training transformer LLMs with up to 350M parameters and demonstrate comparable performance to regular neural network training.</li>\n  <li>Furthermore, they observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Their findings shed light on the potential of low-rank training techniques and their implications for scaling laws.</li>\n  <li>A caveat worth mentioning is that the researchers only pretrained models up to 350 M parameters for now (the smallest Llama 2 model is 7B parameters, for comparison).</li>\n  <li>The following figure <a href=\"https://www.linkedin.com/in/sebastianraschka/\">(source)</a> presents an overview of their results:</li>\n</ul>\n<p><img src=\"../../../images/papers/relora.webp\" alt=\"\"></p>\n<h4 id=\"s-lora-serving-thousands-of-concurrent-lora-adapters\"><a href=\"https://arxiv.org/abs/2311.03285\">S-LoRA: Serving Thousands of Concurrent LoRA Adapters</a></h4>\n<ul>\n  <li>This paper by Sheng et al. from UC Berkeley, Stanford, and Shanghai Jiao Tong focuses on the scalable serving of LoRA (Low-Rank Adaptation) adapters for large LLMs (LLMs).</li>\n  <li>The “pretrain-then-finetune” paradigm, widely adopted in deploying LLMs, leads to numerous fine-tuned variants, presenting significant opportunities for batched inference during serving. The paper introduces S-LoRA, a system designed for this purpose.</li>\n  <li>S-LoRA addresses memory management challenges by storing all adapters in main memory and fetching them to GPU memory as needed. The system employs Unified Paging, a unified memory pool managing dynamic adapter weights and KV cache tensors, to reduce memory fragmentation and I/O overhead.</li>\n  <li>The paper presents a novel tensor parallelism strategy and customized CUDA kernels for efficient heterogeneous batching of LoRA computations, enabling the serving of thousands of adapters on a single or multiple GPUs with minimal overhead.</li>\n  <li>The following image from the paper shows separated batched computation for the base model and LoRA computation. The batched computation of the base model is implemented by GEMM. The batched computation for LoRA adapters is implemented by custom CUDA kernels which support batching various sequence lengths and adapter ranks.</li>\n</ul>\n<p><img src=\"../../../images/papers/SLoRA_1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The following image from the paper shows an overview of memory allocation in S-LoRA. S-LoRA stores all adapters in the main memory and fetches the active adapters for the current batch to the GPU memory. The GPU memory is used to store the KV cache, adapter weights, base model weights, and other temporary tensors.</li>\n</ul>\n<p><img src=\"../../../images/papers/SLoRA_2.jpg\" alt=\"\"></p>\n<ul>\n  <li>S-LoRA’s performance is evaluated against state-of-the-art libraries like Weights PEFT and vLLM, showing up to 4 times higher throughput and the capability to serve significantly more adapters.</li>\n  <li>The system is effective in reducing the training and communication costs in Federated Learning, making it a promising approach for deploying large LLMs in resource-constrained environments.</li>\n  <li>This paper contributes significantly to the field of machine learning by presenting a novel and efficient method for serving a large number of LoRA adapters, a crucial aspect in the deployment of large-scale LLMs.</li>\n  <li><a href=\"https://github.com/S-LoRA/S-LoRA\">Code</a></li>\n</ul>\n<h5 id=\"predibase\"><a href=\"https://predibase.com/\">Predibase</a></h5>\n<ul>\n  <li>Similar to S-LoRA, <a href=\"https://predibase.com/\">Predibase</a>, a startup, offers a unique serving infrastructure – <a href=\"https://github.com/predibase/lorax\">LoRAX</a> – which lets you cost-effectively serve many fine-tuned adapters on a single GPU in dedicated deployments.</li>\n</ul>\n<h4 id=\"weight-decomposed-low-rank-adaptation-dora\"><a href=\"https://arxiv.org/abs/2402.09353\">Weight-Decomposed Low-Rank Adaptation (DoRA)</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2402.09353\">DoRA: Weight-Decomposed Low-Rank Adaptation</a> by Liu et al. from  NVIDIA and HKUST.</li>\n  <li>Weight-Decomposed Low-Rank Adaptation (DoRA) is a novel Parameter-Efficient Fine-Tuning (PEFT) method that surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs.</li>\n  <li>The authors’ weight decomposition analysis reveals fundamental differences between full fine-tuning and LoRA, showing that directional updates play a crucial role in learning capability. DoRA employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.</li>\n  <li>DoRA demonstrates superior performance across a range of tasks, including commonsense reasoning, visual instruction tuning, and image/video-text understanding, across models like LLaMA, LLaVA, and VL-BART. It achieves this by effectively managing the trade-off between the number of trainable parameters and learning capacity, without adding inference overhead.</li>\n  <li>The following figure from the paper illustrates an overview of DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-287-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo>&amp;#x22C5;</mo><msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1546\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.19em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1547\"><span class=\"mo\" id=\"MathJax-Span-1548\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-1549\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1550\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1551\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-1552\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo fence=\"false\" stretchy=\"false\">‖</mo><mo>⋅</mo><msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-287\">\\|\\cdot\\|_c</script> denotes the vector-wise norm of a matrix across each column vector.</li>\n</ul>\n<p><img src=\"../../../images/papers/DoRA.jpg\" alt=\"\"></p>\n<ul>\n  <li>Experiments show that DoRA not only outperforms LoRA but also matches or exceeds the performance of full fine-tuning across different tasks, with significant improvements in commonsense reasoning tasks and multimodal understanding, illustrating its effectiveness and efficiency.</li>\n  <li>The paper also explores DoRA’s compatibility with other LoRA variants, such as VeRA, and demonstrates its adaptability across different training sizes and rank settings, further establishing its utility as a versatile and powerful fine-tuning method.</li>\n  <li><a href=\"https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/\">Blog</a></li>\n</ul>\n<h4 id=\"summary-of-lora-techniques\">Summary of LoRA Techniques</h4>\n<ul>\n  <li>The following section is inspired from Cameron Woulfe’s <a href=\"https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/\">(source)</a> post.</li>\n  <li>Here’s an overview of some prevalent variants of LoRA techniques:\n    <ul>\n      <li><strong>LoRA</strong> models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.</li>\n      <li><strong>QLoRA</strong> is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.</li>\n      <li><strong>QA-LoRA</strong> is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).</li>\n      <li><strong>LoftQ</strong> studies a similar idea to QA-LoRA – applying quantization and LoRA finetuning on a pretrained model simultaneously.</li>\n      <li><strong>LongLoRA</strong> attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\n        <ul>\n          <li>Using sparse local attention instead of dense global attention (optional at inference time).</li>\n          <li>Using LoRA (authors find that this works well for context extension).</li>\n        </ul>\n      </li>\n      <li><strong>S-LoRA</strong> aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\n        <ul>\n          <li>Stores all LoRA modules in main memory.</li>\n          <li>Puts modules being used to run the current query into GPU memory.</li>\n          <li>Uses unified paging to allocate GPU memory and avoid fragmentation.</li>\n          <li>Proposes a new tensor parallelism strategy to batch LoRA computations.</li>\n        </ul>\n      </li>\n      <li><strong>**ReLoRA</strong> refines neural network training by iteratively applying low-rank updates to achieve high-rank performance, streamlining the process for large models.</li>\n      <li><strong>DoRA</strong> surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs. It employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.</li>\n      <li>Many other LoRA variants exist as well:\n        <ul>\n          <li><strong>LQ-LoRA:</strong> uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.</li>\n          <li><strong>MultiLoRA:</strong> extension of LoRA that better handles complex multi-task learning scenarios.</li>\n          <li><strong>LoRA-FA:</strong> freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.</li>\n          <li><strong>Tied-LoRA:</strong> leverages weight tying to further improve the parameter efficiency of LoRA.</li>\n          <li><strong>GLoRA:</strong> extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>LoRA</strong> models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.</li>\n      <li><strong>QLoRA</strong> is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.</li>\n      <li><strong>QA-LoRA</strong> is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).</li>\n      <li><strong>LoftQ</strong> studies a similar idea to QA-LoRA – applying quantization and LoRA finetuning on a pretrained model simultaneously.</li>\n      <li><strong>LongLoRA</strong> attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\n        <ul>\n          <li>Using sparse local attention instead of dense global attention (optional at inference time).</li>\n          <li>Using LoRA (authors find that this works well for context extension).</li>\n        </ul>\n      </li>\n      <li><strong>S-LoRA</strong> aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\n        <ul>\n          <li>Stores all LoRA modules in main memory.</li>\n          <li>Puts modules being used to run the current query into GPU memory.</li>\n          <li>Uses unified paging to allocate GPU memory and avoid fragmentation.</li>\n          <li>Proposes a new tensor parallelism strategy to batch LoRA computations.</li>\n        </ul>\n      </li>\n      <li><strong>**ReLoRA</strong> refines neural network training by iteratively applying low-rank updates to achieve high-rank performance, streamlining the process for large models.</li>\n      <li><strong>DoRA</strong> surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs. It employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.</li>\n      <li>Many other LoRA variants exist as well:\n        <ul>\n          <li><strong>LQ-LoRA:</strong> uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.</li>\n          <li><strong>MultiLoRA:</strong> extension of LoRA that better handles complex multi-task learning scenarios.</li>\n          <li><strong>LoRA-FA:</strong> freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.</li>\n          <li><strong>Tied-LoRA:</strong> leverages weight tying to further improve the parameter efficiency of LoRA.</li>\n          <li><strong>GLoRA:</strong> extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Using sparse local attention instead of dense global attention (optional at inference time).</li>\n          <li>Using LoRA (authors find that this works well for context extension).</li>\n        </ul>\n<ul>\n          <li>Stores all LoRA modules in main memory.</li>\n          <li>Puts modules being used to run the current query into GPU memory.</li>\n          <li>Uses unified paging to allocate GPU memory and avoid fragmentation.</li>\n          <li>Proposes a new tensor parallelism strategy to batch LoRA computations.</li>\n        </ul>\n<ul>\n          <li><strong>LQ-LoRA:</strong> uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.</li>\n          <li><strong>MultiLoRA:</strong> extension of LoRA that better handles complex multi-task learning scenarios.</li>\n          <li><strong>LoRA-FA:</strong> freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.</li>\n          <li><strong>Tied-LoRA:</strong> leverages weight tying to further improve the parameter efficiency of LoRA.</li>\n          <li><strong>GLoRA:</strong> extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/parameter-efficient-fine-tuning/LoRAoverview.jpeg\" alt=\"\"></p>\n<h4 id=\"low-rank-linear-subspace-reft-loreft\"><a href=\"https://arxiv.org/abs/2404.03592\">Low-rank Linear Subspace ReFT (LoReFT)</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2404.03592\">ReFT: Representation Finetuning for LLMs</a> by Wu et al. from Stanford and the Pr(Ai)2R Group.</li>\n  <li>Representation Finetuning (ReFT) is a suite of methods to modify the hidden representations of LLMs (LMs) for task-specific adaptation. Unlike traditional parameter-efficient finetuning (PEFT) methods that adapt by modifying weights, ReFT manipulates a small fraction of model representations, enhancing the interpretability and flexibility of the interventions.</li>\n  <li>A key variant within ReFT, named Low-rank Linear Subspace ReFT (LoReFT), leverages a low-rank projection matrix to edit representations in a linear subspace. This approach is demonstrated to be 10<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-288-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x00D7;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1553\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1554\"><span class=\"mo\" id=\"MathJax-Span-1555\" style=\"font-family: STIXGeneral-Regular;\">×</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>×</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-288\">\\times</script>–50<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-289-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x00D7;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1556\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1557\"><span class=\"mo\" id=\"MathJax-Span-1558\" style=\"font-family: STIXGeneral-Regular;\">×</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>×</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-289\">\\times</script> more parameter-efficient compared to existing state-of-the-art PEFTs like LoRA.</li>\n  <li>The ReFT methodology, specifically Low-rank Linear Subspace ReFT (LoReFT), operates by editing hidden representations in a linear subspace. LoReFT modifies these representations using a projection matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-290-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1559\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1560\"><span class=\"mi\" id=\"MathJax-Span-1561\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-290\">R</script>, which redefines them in a low-dimensional subspace for efficiency. The matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-291-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1562\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1563\"><span class=\"mi\" id=\"MathJax-Span-1564\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-291\">R</script> has orthonormal rows, which are crucial for maintaining the quality of the intervention without adding much complexity.</li>\n  <li>The core intervention of LoReFT, as per the distributed interchange intervention (DII) formula <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-292-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mi>I</mi><mi>I</mi><mo stretchy=&quot;false&quot;>(</mo><mi>b</mi><mo>,</mo><mi>s</mi><mo>,</mo><mi>R</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>b</mi><mo>+</mo><msup><mi>R</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo stretchy=&quot;false&quot;>(</mo><mi>R</mi><mi>s</mi><mo>&amp;#x2212;</mo><mi>R</mi><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1565\" style=\"width: 15.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1566\"><span class=\"mi\" id=\"MathJax-Span-1567\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mi\" id=\"MathJax-Span-1568\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1569\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1570\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1571\" style=\"font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-1572\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1573\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">s</span><span class=\"mo\" id=\"MathJax-Span-1574\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1575\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">R</span><span class=\"mo\" id=\"MathJax-Span-1576\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1577\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1578\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span><span class=\"mo\" id=\"MathJax-Span-1579\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-1580\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1581\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1582\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1583\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1584\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mi\" id=\"MathJax-Span-1585\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1586\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1587\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">R</span><span class=\"mi\" id=\"MathJax-Span-1588\" style=\"font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-1589\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi><mi>I</mi><mi>I</mi><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>s</mi><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>b</mi><mo>+</mo><msup><mi>R</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo stretchy=\"false\">(</mo><mi>R</mi><mi>s</mi><mo>−</mo><mi>R</mi><mi>b</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-292\">DII(b, s, R) = b + R^\\top(Rs - Rb)</script>, leverages the projection matrix to adjust the hidden states <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-293-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1590\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1591\"><span class=\"mi\" id=\"MathJax-Span-1592\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-293\">b</script> towards a target state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-294-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1593\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1594\"><span class=\"mi\" id=\"MathJax-Span-1595\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-294\">s</script> by the application of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-295-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1596\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1597\"><span class=\"mi\" id=\"MathJax-Span-1598\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-295\">R</script>. This intervention is designed to manipulate the model output towards desired behaviors or answers subtly and effectively.</li>\n  <li>LoReFT employs a linear transformation defined by the parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-296-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1599\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1600\"><span class=\"mi\" id=\"MathJax-Span-1601\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-296\">W</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-297-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1602\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1603\"><span class=\"mi\" id=\"MathJax-Span-1604\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-297\">b</script> (not to be confused with the bias term), which projects the representation into the subspace before it is edited. This transformation helps in aligning the representation more closely with the task-specific features that are crucial for performance.</li>\n  <li>Practically, LoReFT is implemented as a set of non-overlapping interventions across multiple layers of a Transformer-based model. These interventions are strategically placed to modify the behavior of the model without extensive retraining of the underlying parameters.</li>\n  <li>Each intervention is applied after the computation of layer <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-298-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1605\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1606\"><span class=\"mi\" id=\"MathJax-Span-1607\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-298\">L</script> representations, meaning it directly affects the computation of subsequent layers <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-299-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo>+</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1608\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1609\"><span class=\"mi\" id=\"MathJax-Span-1610\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1611\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-1612\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo>+</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-299\">L+1</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-300-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo>+</mo><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1613\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1614\"><span class=\"mi\" id=\"MathJax-Span-1615\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1616\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1617\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo>+</mo><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-300\">L+m</script>. This placement ensures that the interventions have a cascading effect, enhancing their impact on the final model output.</li>\n  <li>The hyperparameter tuning for LoReFT focuses on the number and placement of interventions across the layers, optimizing both the effectiveness of each intervention and the overall computational overhead. This involves selecting the appropriate number of prefix and suffix positions in the input where interventions are most beneficial, as well as deciding on the layers where these modifications will have the most impact.</li>\n  <li>The figure below from the paper shows an illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-301-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x03A6;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1618\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1619\"><span class=\"mi\" id=\"MathJax-Span-1620\" style=\"font-family: STIXGeneral-Regular;\">Φ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Φ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-301\">\\Phi</script> is applied to hidden representations at positions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-302-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1621\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1622\"><span class=\"mi\" id=\"MathJax-Span-1623\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-302\">P</script> in layer <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-303-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1624\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1625\"><span class=\"mi\" id=\"MathJax-Span-1626\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-303\">L</script>. (2) The right panel depicts the hyperparameters we tune when experimenting with LoReFT. Specifically, the figure depicts application of LoReFT at all layers with prefix length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-304-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1627\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1628\"><span class=\"mi\" id=\"MathJax-Span-1629\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-304\">p</script> = 2 and suffix length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-305-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1630\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1631\"><span class=\"mi\" id=\"MathJax-Span-1632\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-305\">s</script> = 2. When not tying layer weights, we train separate intervention parameters at each position and layer, resulting in 16 interventions with unique parameters in this example.</li>\n</ul>\n<p><img src=\"../../../images/papers/ReFT.jpg\" alt=\"\"></p>\n<ul>\n  <li>The authors evaluate LoReFT across multiple domains, including commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. It is shown that LoReFT achieves competitive or superior performance on all tasks, especially shining in commonsense reasoning benchmarks.</li>\n  <li>Implementation details reveal that LoReFT interventions are applied at selected layers and positions within the LM, optimizing both the number of interventions and their locations through hyperparameter tuning. This targeted approach allows for minimal additional computational overhead at inference.</li>\n  <li>LoReFT is implemented in a publicly available Python library, <code class=\"language-plaintext highlighter-rouge\">pyreft</code>, which facilitates the adoption of ReFT methods by providing tools to apply these interventions on any pretrained LM from the HuggingFace model hub.</li>\n  <li>The paper establishes the potential of representation-focused finetuning as a more effective alternative to weight-based methods, setting new standards for efficiency and performance in adapting large-scale LMs to diverse tasks.</li>\n</ul>\n<h4 id=\"stratified-progressive-adaptation-fine-tuning-spafit\"><a href=\"https://arxiv.org/abs/2405.00201\">Stratified Progressive Adaptation Fine-tuning (SPAFIT)</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2405.00201\">SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large LLMs</a> by Arora and Wang from Simon Fraser University, Stratified Progressive Adaptation Fine-tuning (SPAFIT) is a novel Parameter-Efficient Fine-Tuning (PEFT) method aimed at optimizing the fine-tuning process of Transformer-based large LLMs by localizing the fine-tuning to specific layers according to their linguistic knowledge importance. This addresses issues like catastrophic forgetting and computational inefficiency common in full fine-tuning methods.</li>\n  <li>SPAFIT organizes the model into three groups of layers, with increasing complexity of fine-tuning allowed as the layers progress from basic linguistic processing to more task-specific functions. Group 1 layers remain completely frozen, Group 2 layers undergo fine-tuning only on bias terms, and Group 3 layers are fine-tuned using both BitFit for simple parameters and Low-Rank Adaptation (LoRA) for more significant weight matrices.</li>\n  <li>The authors conducted experiments using the BERT-large-cased model across nine tasks from the GLUE benchmark. Their results demonstrate that SPAFIT can achieve or exceed the performance of full fine-tuning and other PEFT methods like Full BitFit and Full LoRA while fine-tuning significantly fewer parameters.</li>\n  <li>The figure below from the paper illustrates an example implementation of SPAFIT on BERT.</li>\n</ul>\n<p><img src=\"../../../images/papers/SPAFIT.jpg\" alt=\"\"></p>\n<ul>\n  <li>Notable results include SPAFIT models achieving the best performance on tasks involving sentence similarity, like MRPC and STS-B, and showing a substantial reduction in the number of parameters fine-tuned—highlighting SPAFIT’s efficiency.</li>\n  <li>The research suggests that different types of linguistic knowledge can indeed be localized to specific layers of an LLM, potentially leading to more targeted and efficient fine-tuning strategies.</li>\n  <li>The paper raises points for future investigation, including the application of SPAFIT to more complex tasks like summarization and to models that contain both encoder and decoder architectures. The study also acknowledges the need for further analysis on the optimal balance of parameter efficiency against task performance and the extent of adaptation necessary at different layers.</li>\n</ul>\n<h4 id=\"bitfit\"><a href=\"https://arxiv.org/abs/2106.10199\">BitFit</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2106.10199\">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a> by Ben-Zaken et al.  from Yoav Goldberg’s group at Bar Ilan University and the Allen Institute for Artificial Intelligence introduces BitFit, a fine-tuning method for pre-trained BERT models. \nBitFit focuses on updating only the bias-terms of the model, which are a minimal fraction of the model’s parameters, effectively reducing the memory footprint and computational demands typically associated with full model fine-tuning.</li>\n  <li>BitFit’s methodology leverages the observation that fine-tuning often doesn’t require extensive retraining of all parameters. Instead, fine-tuning only the bias terms achieves competitive results compared to full model fine-tuning, especially with small to medium-sized datasets. In scenarios permitting slight performance degradation, the method can be constrained to adjust only two specific types of bias terms, representing just 0.04% of the total model parameters.</li>\n  <li>Implementation details include freezing the transformer-encoder’s main weights and training only the bias terms along with a task-specific classification layer. This approach allows the model to handle multiple tasks efficiently in a streaming fashion without requiring simultaneous access to all task datasets.</li>\n  <li>Experimental results on the GLUE benchmark show that BitFit is comparable or superior to full fine-tuning in several NLP tasks. It also outperforms other parameter-efficient methods like Diff-Pruning and Adapters in terms of the number of parameters modified, showcasing its effectiveness in achieving high performance with significantly fewer trainable parameters.</li>\n  <li>The findings underscore the potential of focusing fine-tuning efforts on a small subset of parameters, specifically bias terms, to maintain or even enhance performance while minimizing computational costs. This approach also prompts further exploration of the role of bias terms in neural networks and their impact on model behavior and task transferability.</li>\n</ul>\n<h4 id=\"nola\"><a href=\"https://openreview.net/pdf?id=TjfXcDgvzk\">NOLA</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://openreview.net/pdf?id=TjfXcDgvzk\">NOLA: Compressing LoRA Using Linear Combination of Random Basis</a> by Koohpayegani et al. in ICLR 2024, NOLA is a novel method for compressing large LLMs (LLMs) that addresses the limitations of Low-Rank Adaptation (LoRA). NOLA reparameterizes the rank-decomposition matrices used in LoRA through linear combinations of randomly generated basis matrices, significantly reducing the parameter count by optimizing only the mixture coefficients.</li>\n  <li>NOLA decouples the number of trainable parameters from both the rank choice and network architecture, unlike LoRA, where parameters are inherently dependent on the matrix dimensions and rank, which must be an integer. This method not only preserves the adaptation quality but also allows for extreme compression, achieving up to 20 times fewer parameters than the most compressed LoRA models without loss of performance.</li>\n  <li>The method’s implementation includes using a pseudo-random number generator for creating basis matrices, where the generator’s seed and the linear coefficients are stored, greatly reducing storage requirements. Quantization of these coefficients further minimizes storage needs without impacting model performance.</li>\n  <li>The figure below from the paper shows the process that NOLA follows. After constraining the rank of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-306-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1633\" style=\"width: 2.086em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.72em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1634\"><span class=\"mi\" id=\"MathJax-Span-1635\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-1636\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-306\">\\Delta W</script> by decomposing it to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-307-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x00D7;</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1637\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1638\"><span class=\"mi\" id=\"MathJax-Span-1639\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1640\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-1641\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>×</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-307\">A \\times B</script>, we reparametrize A and B to be a linear combination of several random basis matrices. We freeze the basis and W and learn the combination coefficients. To reconstruct the model, we store the coefficients and the seed of the random generator which is a single scalar. NOLA results in more compression compared to LoRA and more importantly decouples the compression ratio from the rank and dimensions of W. One can reduce the number of parameters to 4 times smaller than rank=1 of LoRA which is not possible with LoRA due to rank being an integer number.</li>\n</ul>\n<p><img src=\"../../../images/papers/NOLA.jpg\" alt=\"\"></p>\n<ul>\n  <li>Detailed experimental evaluations across several tasks and models, including GPT-2 and LLaMA-2, showcase NOLA’s effectiveness. It maintains or exceeds benchmark metrics such as BLEU and ROUGE-L while using significantly fewer parameters compared to both LoRA and full model fine-tuning.</li>\n  <li>The approach’s versatility is demonstrated through its application not only in natural language processing tasks but also in adapting Vision Transformer (ViT) models for image classification, indicating its potential widespread applicability across different types of deep learning architectures.</li>\n  <li><a href=\"https://github.com/UCDvision/NOLA\">Code</a></li>\n</ul>\n<h4 id=\"matrix-of-rank-adaptation-mora\"><a href=\"https://arxiv.org/abs/2405.12130v1\">Matrix of Rank Adaptation (MoRA)</a></h4>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2405.12130v1\">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a> by Jiang et al. from Beihang University and Microsoft introduces a novel method, MoRA (Matrix of Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique for LLMs. The authors identify limitations in existing PEFT methods, particularly Low-Rank Adaptation (LoRA), which may restrict LLMs’ ability to learn and retain new knowledge. To address these issues, MoRA employs a high-rank updating mechanism using a square matrix to achieve greater flexibility and effectiveness without increasing the number of trainable parameters.</li>\n  <li>MoRA utilizes non-parameterized operators to adjust input and output dimensions, ensuring the weight can be integrated back into LLMs like LoRA. The method involves the following steps:\n    <ol>\n      <li><strong>Reduction of Input Dimension</strong>: Non-parameter operators reduce the input dimension for the square matrix.</li>\n      <li><strong>Increase of Output Dimension</strong>: Corresponding operators increase the output dimension, maintaining the number of trainable parameters while achieving high-rank updates.</li>\n    </ol>\n  </li>\n  <li>The figure below from the paper illustrates an overview of our method compared to LoRA under same number of trainable parameters. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-308-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1642\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1643\"><span class=\"mi\" id=\"MathJax-Span-1644\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-308\">W</script> is the frozen weight from model. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-309-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1645\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1646\"><span class=\"mi\" id=\"MathJax-Span-1647\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-309\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-310-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1648\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1649\"><span class=\"mi\" id=\"MathJax-Span-1650\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-310\">B</script> are trainable low-rank matrices in LoRA. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-311-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1651\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1652\"><span class=\"mi\" id=\"MathJax-Span-1653\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-311\">M</script> is the trainable matrix in our method. Gray parts are non-parameter operators to reducing the input dimension and increasing the output dimension. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-312-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1654\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1655\"><span class=\"mi\" id=\"MathJax-Span-1656\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-312\">r</script> represents the rank in two methods.</li>\n</ul>\n<ol>\n      <li><strong>Reduction of Input Dimension</strong>: Non-parameter operators reduce the input dimension for the square matrix.</li>\n      <li><strong>Increase of Output Dimension</strong>: Corresponding operators increase the output dimension, maintaining the number of trainable parameters while achieving high-rank updates.</li>\n    </ol>\n<p><img src=\"../../../images/papers/MoRA2.jpg\" alt=\"\"></p>\n<ul>\n  <li>The authors comprehensively evaluate MoRA across five tasks—instruction tuning, mathematical reasoning, continual pretraining, memory, and pretraining—demonstrating that MoRA outperforms LoRA in memory-intensive tasks and achieves comparable performance in other areas.</li>\n  <li><strong>Technical Details and Implementation:</strong>\n    <ul>\n      <li><strong>Low-Rank Limitation in LoRA</strong>: LoRA uses low-rank matrices to approximate full-rank updates, limiting its capacity to store new information, especially in memory-intensive tasks. The low-rank matrices A and B in LoRA struggle to fully capture the complexity needed for tasks requiring substantial knowledge enhancement.</li>\n      <li><strong>High-Rank Updating in MoRA</strong>: MoRA replaces the low-rank matrices with a square matrix, significantly increasing the rank and thus the capacity for updates. For example, LoRA with rank 8 employs matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-313-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4096</mn><mo>&amp;#x00D7;</mo><mn>8</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1657\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.95em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1658\"><span class=\"mi\" id=\"MathJax-Span-1659\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1660\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1661\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1662\"><span class=\"mrow\" id=\"MathJax-Span-1663\"><span class=\"mi\" id=\"MathJax-Span-1664\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1665\"><span class=\"mrow\" id=\"MathJax-Span-1666\"><span class=\"mn\" id=\"MathJax-Span-1667\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4096</span><span class=\"mo\" id=\"MathJax-Span-1668\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1669\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>4096</mn><mo>×</mo><mn>8</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-313\">A \\in \\mathbb{R}^{4096 \\times 8}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-314-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>8</mn><mo>&amp;#x00D7;</mo><mn>4096</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1670\" style=\"width: 5.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.9em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1671\"><span class=\"mi\" id=\"MathJax-Span-1672\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1673\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1674\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1675\"><span class=\"mrow\" id=\"MathJax-Span-1676\"><span class=\"mi\" id=\"MathJax-Span-1677\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1678\"><span class=\"mrow\" id=\"MathJax-Span-1679\"><span class=\"mn\" id=\"MathJax-Span-1680\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span><span class=\"mo\" id=\"MathJax-Span-1681\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1682\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4096</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>8</mn><mo>×</mo><mn>4096</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-314\">B \\in \\mathbb{R}^{8 \\times 4096}</script>, while MoRA uses a square matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-315-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>256</mn><mo>&amp;#x00D7;</mo><mn>256</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1683\" style=\"width: 6.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.52em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1684\"><span class=\"mi\" id=\"MathJax-Span-1685\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1686\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1687\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1688\"><span class=\"mrow\" id=\"MathJax-Span-1689\"><span class=\"mi\" id=\"MathJax-Span-1690\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1691\"><span class=\"mrow\" id=\"MathJax-Span-1692\"><span class=\"mn\" id=\"MathJax-Span-1693\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">256</span><span class=\"mo\" id=\"MathJax-Span-1694\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1695\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">256</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>256</mn><mo>×</mo><mn>256</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-315\">M \\in \\mathbb{R}^{256 \\times 256}</script>, achieving a higher rank with the same number of parameters.</li>\n      <li><strong>Compression and Decompression Functions</strong>: MoRA employs various methods to implement compression and decompression functions, including truncation, sharing rows/columns, reshaping, and rotation. These methods help reduce the input dimension and increase the output dimension effectively.</li>\n      <li><strong>Rotation Operators</strong>: Inspired by RoPE (Rotary Position Embedding), MoRA introduces rotation operators to differentiate inputs, enhancing the expressiveness of the square matrix.</li>\n    </ul>\n  </li>\n  <li><strong>Evaluation and Results:</strong>\n    <ul>\n      <li><strong>Memory Task</strong>: In memorizing UUID pairs, MoRA showed significant improvements over LoRA with the same number of trainable parameters. MoRA required fewer training steps to achieve high accuracy compared to LoRA, demonstrating its effectiveness in memory-intensive tasks.</li>\n      <li><strong>Fine-Tuning Tasks</strong>: MoRA was evaluated on instruction tuning (using Tülu v2 dataset), mathematical reasoning (using MetaMath, GSM8K, MATH), and continual pretraining (in biomedical and financial domains). It matched LoRA’s performance in instruction tuning and mathematical reasoning but outperformed LoRA in continual pretraining tasks, benefiting from high-rank updating.</li>\n      <li><strong>Pretraining</strong>: MoRA and a variant, ReMoRA (which merges updates back into the model during training), were evaluated on pretraining transformers from scratch on the C4 dataset. MoRA showed better pretraining loss and perplexity metrics compared to LoRA and ReLoRA, further validating the advantages of high-rank updating.</li>\n    </ul>\n  </li>\n  <li>MoRA addresses the limitations of low-rank updates in LoRA by employing high-rank matrices, significantly enhancing the model’s capacity to learn and memorize new knowledge. This method shows promise for improving parameter-efficient fine-tuning of LLMs, especially in memory-intensive and domain-specific tasks. The authors provide comprehensive implementation details and empirical evaluations, establishing MoRA as an effective advancement in the field of PEFT.</li>\n</ul>\n<ul>\n      <li><strong>Low-Rank Limitation in LoRA</strong>: LoRA uses low-rank matrices to approximate full-rank updates, limiting its capacity to store new information, especially in memory-intensive tasks. The low-rank matrices A and B in LoRA struggle to fully capture the complexity needed for tasks requiring substantial knowledge enhancement.</li>\n      <li><strong>High-Rank Updating in MoRA</strong>: MoRA replaces the low-rank matrices with a square matrix, significantly increasing the rank and thus the capacity for updates. For example, LoRA with rank 8 employs matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-313-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4096</mn><mo>&amp;#x00D7;</mo><mn>8</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1657\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.95em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1658\"><span class=\"mi\" id=\"MathJax-Span-1659\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1660\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1661\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1662\"><span class=\"mrow\" id=\"MathJax-Span-1663\"><span class=\"mi\" id=\"MathJax-Span-1664\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1665\"><span class=\"mrow\" id=\"MathJax-Span-1666\"><span class=\"mn\" id=\"MathJax-Span-1667\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4096</span><span class=\"mo\" id=\"MathJax-Span-1668\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1669\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>4096</mn><mo>×</mo><mn>8</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-313\">A \\in \\mathbb{R}^{4096 \\times 8}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-314-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>8</mn><mo>&amp;#x00D7;</mo><mn>4096</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1670\" style=\"width: 5.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.9em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1671\"><span class=\"mi\" id=\"MathJax-Span-1672\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1673\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1674\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1675\"><span class=\"mrow\" id=\"MathJax-Span-1676\"><span class=\"mi\" id=\"MathJax-Span-1677\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1678\"><span class=\"mrow\" id=\"MathJax-Span-1679\"><span class=\"mn\" id=\"MathJax-Span-1680\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span><span class=\"mo\" id=\"MathJax-Span-1681\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1682\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4096</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>8</mn><mo>×</mo><mn>4096</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-314\">B \\in \\mathbb{R}^{8 \\times 4096}</script>, while MoRA uses a square matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-315-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>256</mn><mo>&amp;#x00D7;</mo><mn>256</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1683\" style=\"width: 6.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.52em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1684\"><span class=\"mi\" id=\"MathJax-Span-1685\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1686\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1687\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1688\"><span class=\"mrow\" id=\"MathJax-Span-1689\"><span class=\"mi\" id=\"MathJax-Span-1690\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1691\"><span class=\"mrow\" id=\"MathJax-Span-1692\"><span class=\"mn\" id=\"MathJax-Span-1693\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">256</span><span class=\"mo\" id=\"MathJax-Span-1694\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-1695\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">256</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>256</mn><mo>×</mo><mn>256</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-315\">M \\in \\mathbb{R}^{256 \\times 256}</script>, achieving a higher rank with the same number of parameters.</li>\n      <li><strong>Compression and Decompression Functions</strong>: MoRA employs various methods to implement compression and decompression functions, including truncation, sharing rows/columns, reshaping, and rotation. These methods help reduce the input dimension and increase the output dimension effectively.</li>\n      <li><strong>Rotation Operators</strong>: Inspired by RoPE (Rotary Position Embedding), MoRA introduces rotation operators to differentiate inputs, enhancing the expressiveness of the square matrix.</li>\n    </ul>\n<ul>\n      <li><strong>Memory Task</strong>: In memorizing UUID pairs, MoRA showed significant improvements over LoRA with the same number of trainable parameters. MoRA required fewer training steps to achieve high accuracy compared to LoRA, demonstrating its effectiveness in memory-intensive tasks.</li>\n      <li><strong>Fine-Tuning Tasks</strong>: MoRA was evaluated on instruction tuning (using Tülu v2 dataset), mathematical reasoning (using MetaMath, GSM8K, MATH), and continual pretraining (in biomedical and financial domains). It matched LoRA’s performance in instruction tuning and mathematical reasoning but outperformed LoRA in continual pretraining tasks, benefiting from high-rank updating.</li>\n      <li><strong>Pretraining</strong>: MoRA and a variant, ReMoRA (which merges updates back into the model during training), were evaluated on pretraining transformers from scratch on the C4 dataset. MoRA showed better pretraining loss and perplexity metrics compared to LoRA and ReLoRA, further validating the advantages of high-rank updating.</li>\n    </ul>",
    "contentMarkdown": "#### Low-Rank Adaptation (LoRA)\n\n##### Background\n\n###### Rank of a Matrix\n\n*   The rank of a matrix is a measure of the number of linearly independent rows or columns in the matrix.\n*   If a matrix has rank 1, it means all rows or all columns can be represented as multiples of each other, so there’s essentially only one unique “direction” in the data.\n*   A full-rank matrix has rank equal to the smallest of its dimensions (number of rows or columns), meaning all rows and columns are independent.\n*   On a related note, a matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. For more, refer [Wikipedia: Rank](https://en.wikipedia.org/wiki/Rank_\\(linear_algebra\\)).\n    \n*   **Example**:\n    \n    *   Consider the following 3x3 matrix AAA:\n    \n    A\\=⎡⎣⎢⎢147258369⎤⎦⎥⎥A\\=\\[123456789\\]\n    \n    A = \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\\\\\ 7 & 8 & 9 \\\\end{bmatrix}\n*   **Step-by-Step to Determine the Rank**:\n    \n    1.  **Row Reduction**: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.\n        \n        After row-reducing AAA, we get:\n        \n        A\\=⎡⎣⎢⎢1002−303−60⎤⎦⎥⎥A\\=\\[1230−3−6000\\]\n        \n        A = \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 0 & -3 & -6 \\\\\\\\ 0 & 0 & 0 \\\\end{bmatrix}\n    2.  **Count Independent Rows**: Now we look at the rows with non-zero entries:\n        \n        *   The first row \\[1,2,3\\]\\[1,2,3\\]\\[1, 2, 3\\] is non-zero.\n        *   The second row \\[0,−3,−6\\]\\[0,−3,−6\\]\\[0, -3, -6\\] is also non-zero and independent of the first row.\n        *   The third row is all zeros, which does not contribute to the rank.\n        \n        Since there are two non-zero, independent rows in the row echelon form, the rank of AAA is 2.\n        \n*   **Explanation**:\n    \n    *   The rank of 2 indicates that only two rows or columns in AAA contain unique information, and the third row (or column) can be derived from a combination of the other two. Essentially, this matrix can be thought of as existing in a 2-dimensional space rather than a full 3-dimensional space, despite its 3x3 size.\n*   **In summary:**\n    *   The rank of matrix AAA is 2.\n    *   This rank tells us the matrix’s actual dimensionality in terms of its independent information.\n\nOn a related note, a matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. For more, refer [Wikipedia: Rank](https://en.wikipedia.org/wiki/Rank_\\(linear_algebra\\)).\n\n**Example**:\n\n*   Consider the following 3x3 matrix AAA:\n\n**Step-by-Step to Determine the Rank**:\n\n1.  **Row Reduction**: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.\n    \n    After row-reducing AAA, we get:\n    \n    A\\=⎡⎣⎢⎢1002−303−60⎤⎦⎥⎥A\\=\\[1230−3−6000\\]\n    \n    A = \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 0 & -3 & -6 \\\\\\\\ 0 & 0 & 0 \\\\end{bmatrix}\n2.  **Count Independent Rows**: Now we look at the rows with non-zero entries:\n    \n    *   The first row \\[1,2,3\\]\\[1,2,3\\]\\[1, 2, 3\\] is non-zero.\n    *   The second row \\[0,−3,−6\\]\\[0,−3,−6\\]\\[0, -3, -6\\] is also non-zero and independent of the first row.\n    *   The third row is all zeros, which does not contribute to the rank.\n    \n    Since there are two non-zero, independent rows in the row echelon form, the rank of AAA is 2.\n    \n\n**Row Reduction**: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.\n\nAfter row-reducing AAA, we get:\n\n**Count Independent Rows**: Now we look at the rows with non-zero entries:\n\n*   The first row \\[1,2,3\\]\\[1,2,3\\]\\[1, 2, 3\\] is non-zero.\n*   The second row \\[0,−3,−6\\]\\[0,−3,−6\\]\\[0, -3, -6\\] is also non-zero and independent of the first row.\n*   The third row is all zeros, which does not contribute to the rank.\n\nSince there are two non-zero, independent rows in the row echelon form, the rank of AAA is 2.\n\n**Explanation**:\n\n*   The rank of 2 indicates that only two rows or columns in AAA contain unique information, and the third row (or column) can be derived from a combination of the other two. Essentially, this matrix can be thought of as existing in a 2-dimensional space rather than a full 3-dimensional space, despite its 3x3 size.\n\n*   The rank of matrix AAA is 2.\n*   This rank tells us the matrix’s actual dimensionality in terms of its independent information.\n\n###### Related: Rank of a Tensor\n\n*   While LoRA injects trainable low-rank matrices, it is important to understand rank in the context of tensors as well.\n*   The rank of a tensor refers to the number of dimensions in the tensor. This is different from the rank of a matrix, which relates to the number of linearly independent rows or columns. For tensors, rank simply tells us how many dimensions or axes the tensor has.\n    \n*   **Explanation with Examples**:\n    \n    1.  **Scalar (Rank 0 Tensor)**:\n        *   A scalar is a single number with no dimensions.\n        *   Example: `5` or `3.14`\n        *   Shape: `()` (no dimensions)\n        *   **Rank**: 0\n    2.  **Vector (Rank 1 Tensor)**:\n        *   A vector is a one-dimensional array of numbers.\n        *   Example: `[3, 7, 2]`\n        *   Shape: `(3,)` (one dimension with 3 elements)\n        *   **Rank**: 1\n    3.  **Matrix (Rank 2 Tensor)**:\n        *   A matrix is a two-dimensional array of numbers, like a table.\n        *   Example: \\[142536\\]\\[123456\\]\\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}\n        *   Shape: `(2, 3)` (two dimensions: 2 rows, 3 columns)\n        *   **Rank**: 2\n    4.  **3D Tensor (Rank 3 Tensor)**:\n        *   A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.\n        *   Example: \\[\\[142536\\],\\[710811912\\]\\]\\[\\[123456\\],\\[789101112\\]\\]\\\\begin{bmatrix} \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}, \\\\begin{bmatrix} 7 & 8 & 9 \\\\\\\\ 10 & 11 & 12 \\\\end{bmatrix} \\\\end{bmatrix}\n        *   Shape: `(2, 2, 3)` (three dimensions: 2 matrices, each with 2 rows and 3 columns)\n        *   **Rank**: 3\n    5.  **4D Tensor (Rank 4 Tensor)**:\n        *   A 4D tensor might represent multiple “stacks” of 3D tensors.\n        *   Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions `[batch size, channels, height, width]`.\n        *   Shape: `(10, 3, 64, 64)` for a batch of 10 images, each with 3 color channels and a resolution of 64x64.\n        *   **Rank**: 4\n*   **General Rule**:\n    *   **Rank** = Number of dimensions (or axes) of the tensor.\n*   **Why Rank Matters**:\n    *   The rank of a tensor tells us about its structural complexity and the data it can represent. Higher-rank tensors can represent more complex data structures, which is essential in fields like deep learning, physics simulations, and data science for handling multi-dimensional data.\n\nThe rank of a tensor refers to the number of dimensions in the tensor. This is different from the rank of a matrix, which relates to the number of linearly independent rows or columns. For tensors, rank simply tells us how many dimensions or axes the tensor has.\n\n**Explanation with Examples**:\n\n1.  **Scalar (Rank 0 Tensor)**:\n    *   A scalar is a single number with no dimensions.\n    *   Example: `5` or `3.14`\n    *   Shape: `()` (no dimensions)\n    *   **Rank**: 0\n2.  **Vector (Rank 1 Tensor)**:\n    *   A vector is a one-dimensional array of numbers.\n    *   Example: `[3, 7, 2]`\n    *   Shape: `(3,)` (one dimension with 3 elements)\n    *   **Rank**: 1\n3.  **Matrix (Rank 2 Tensor)**:\n    *   A matrix is a two-dimensional array of numbers, like a table.\n    *   Example: \\[142536\\]\\[123456\\]\\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}\n    *   Shape: `(2, 3)` (two dimensions: 2 rows, 3 columns)\n    *   **Rank**: 2\n4.  **3D Tensor (Rank 3 Tensor)**:\n    *   A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.\n    *   Example: \\[\\[142536\\],\\[710811912\\]\\]\\[\\[123456\\],\\[789101112\\]\\]\\\\begin{bmatrix} \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}, \\\\begin{bmatrix} 7 & 8 & 9 \\\\\\\\ 10 & 11 & 12 \\\\end{bmatrix} \\\\end{bmatrix}\n    *   Shape: `(2, 2, 3)` (three dimensions: 2 matrices, each with 2 rows and 3 columns)\n    *   **Rank**: 3\n5.  **4D Tensor (Rank 4 Tensor)**:\n    *   A 4D tensor might represent multiple “stacks” of 3D tensors.\n    *   Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions `[batch size, channels, height, width]`.\n    *   Shape: `(10, 3, 64, 64)` for a batch of 10 images, each with 3 color channels and a resolution of 64x64.\n    *   **Rank**: 4\n\n*   A scalar is a single number with no dimensions.\n*   Example: `5` or `3.14`\n*   Shape: `()` (no dimensions)\n*   **Rank**: 0\n\n*   A vector is a one-dimensional array of numbers.\n*   Example: `[3, 7, 2]`\n*   Shape: `(3,)` (one dimension with 3 elements)\n*   **Rank**: 1\n\n*   A matrix is a two-dimensional array of numbers, like a table.\n*   Example: \\[142536\\]\\[123456\\]\\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}\n*   Shape: `(2, 3)` (two dimensions: 2 rows, 3 columns)\n*   **Rank**: 2\n\n*   A 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.\n*   Example: \\[\\[142536\\],\\[710811912\\]\\]\\[\\[123456\\],\\[789101112\\]\\]\\\\begin{bmatrix} \\\\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{bmatrix}, \\\\begin{bmatrix} 7 & 8 & 9 \\\\\\\\ 10 & 11 & 12 \\\\end{bmatrix} \\\\end{bmatrix}\n*   Shape: `(2, 2, 3)` (three dimensions: 2 matrices, each with 2 rows and 3 columns)\n*   **Rank**: 3\n\n*   A 4D tensor might represent multiple “stacks” of 3D tensors.\n*   Example: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions `[batch size, channels, height, width]`.\n*   Shape: `(10, 3, 64, 64)` for a batch of 10 images, each with 3 color channels and a resolution of 64x64.\n*   **Rank**: 4\n\n*   **Rank** = Number of dimensions (or axes) of the tensor.\n\n*   The rank of a tensor tells us about its structural complexity and the data it can represent. Higher-rank tensors can represent more complex data structures, which is essential in fields like deep learning, physics simulations, and data science for handling multi-dimensional data.\n\n##### Overview\n\n*   **Intrinsic Rank Hypothesis**:\n    \n    *   Low-Rank Adaptation (LoRA) is motivated by the hypothesis that the updates to a model’s weights during task-specific adaptation exhibit a low “intrinsic rank.” This suggests that the weight changes required for effective adaptation are inherently low-dimensional. Thus, LoRA constrains these updates by representing them through low-rank decomposition matrices, enabling efficient adaptation without fine-tuning all model parameters.\n    *   As illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, AAA and BBB, which capture the adaptation. AAA (the **down-projection matrix**) projects the input into a lower-dimensional subspace, while BBB (the **up-projection matrix**) maps it back to the original dimension. Per the [LoRA paper](https://arxiv.org/pdf/2106.09685) by Hu et al. (2021), the matrix AAA is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix BBB is initialized to zeros so that ΔW\\=BAΔW\\=BA\\\\Delta W = B A starts as the zero matrix. During training, the product of AAA and BBB forms a low-rank update matrix that is added to the original, pre-trained weight matrix WWW to produce the adapted model output hhh. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights WWW frozen.\n        \n        ![](/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg)\n        \n        *   This product, BABABA, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if BBB is a d×rd×rd \\\\times r matrix and AAA is an r×dr×dr \\\\times d matrix, where rrr is much smaller than ddd, the resulting product BABABA will have a maximum rank of rrr, regardless of the dimensions of ddd. This means the update to WWW is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.\n        *   For example, if d\\=1000d\\=1000d = 1000 and r\\=2r\\=2r = 2, the update matrix BABABA will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means BABABA is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights WWW frozen.\n*   **Process**:\n    *   LoRA fundamentally changes the approach to fine-tuning large neural networks by introducing a method to decompose high-dimensional weight matrices into lower-dimensional forms, preserving essential information while reducing computational load. Put simply, LoRA efficiently fine-tunes large-scale neural networks by introducing trainable low-rank matrices, simplifying the model’s complexity while retaining its robust learning capabilities.\n    *   LoRA is similar to methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), leveraging the observation that weight updates during fine-tuning are often rank-deficient. By imposing a low-rank constraint on these updates, LoRA enables a lightweight adaptation process that captures the most critical directions or “components” in the weight space. This means the model can retain essential knowledge from its pre-training phase while focusing on the specific nuances of the new task, resulting in efficient adaptation with a smaller memory and computational footprint.\n    *   During backpropagation, only the low-rank LoRA matrices (A and B) receive gradient updates; the original pretrained weights remain frozen. Gradients flow through the combined effective weight (W = W\\_0 + BA), but only the LoRA-specific parameters (A and B) have `requires_grad=True`, ensuring that only these low-rank components are optimized while the base model weights remain unchanged.\n*   **Application**:\n    *   LoRA’s primary application is in the efficient fine-tuning of large neural networks, particularly in transformer-based architectures. In practice, LoRA identifies key dimensions within the original weight matrix that are crucial for a specific task, significantly reducing the adaptation’s dimensionality.\n    *   Instead of fine-tuning all weights, which is computationally prohibitive for large models, LoRA introduces two trainable low-rank matrices, AAA and BBB, in specific layers of the transformer (e.g., in the query and value projections of the attention mechanism). These matrices are optimized during training while keeping the core model parameters frozen. This architecture adjustment means that only a fraction of the original parameters are actively updated, thus lowering memory usage and enabling faster computations.\n    *   By focusing only on the most relevant dimensions for each task, LoRA enables the deployment of large models on hardware with limited resources and facilitates task-switching by updating only the low-rank matrices without retraining the entire model.\n*   **Benefits**:\n    *   LoRA provides several advantages that make it particularly suitable for practical use in industry and research settings where resource constraints are a concern. The primary benefit is in memory and computational efficiency.\n    *   By keeping the majority of the model’s parameters frozen and only updating the low-rank matrices, LoRA significantly reduces the number of trainable parameters, leading to lower memory consumption and faster training times. This reduction in training complexity also means that fewer GPUs or lower-spec hardware can be used to fine-tune large models, broadening accessibility.\n    *   Additionally, LoRA avoids introducing any additional latency during inference because, once the low-rank matrices have been trained, they can be merged back into the pre-trained weights without altering the architecture.\n    *   This setup makes LoRA ideal for environments where models need to switch between tasks frequently, as only the task-specific low-rank weights need to be loaded, allowing the model to quickly adapt to new tasks or datasets with minimal overhead.\n*   **In Summary**:\n    \n    *   LoRA represents a highly efficient approach to fine-tuning, striking a balance between the depth of knowledge encapsulated in large pre-trained models and the need for targeted adaptation to new tasks or data.\n    *   By leveraging the low intrinsic rank of weight updates, LoRA retains the model’s robustness and generalization capability while allowing for quick, cost-effective adaptations. This approach redefines efficiency in the era of massive LLMs, making it feasible to use and adapt large-scale pre-trained models across various applications and domains without the extensive computational burden associated with traditional fine-tuning.\n    \n    ![](/primers/ai/assets/parameter-efficient-fine-tuning/lora2.png)\n    \n*   As a recap of traditional finetuning vs. LoRA [(source)](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html):\n\n**Intrinsic Rank Hypothesis**:\n\n*   Low-Rank Adaptation (LoRA) is motivated by the hypothesis that the updates to a model’s weights during task-specific adaptation exhibit a low “intrinsic rank.” This suggests that the weight changes required for effective adaptation are inherently low-dimensional. Thus, LoRA constrains these updates by representing them through low-rank decomposition matrices, enabling efficient adaptation without fine-tuning all model parameters.\n*   As illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, AAA and BBB, which capture the adaptation. AAA (the **down-projection matrix**) projects the input into a lower-dimensional subspace, while BBB (the **up-projection matrix**) maps it back to the original dimension. Per the [LoRA paper](https://arxiv.org/pdf/2106.09685) by Hu et al. (2021), the matrix AAA is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix BBB is initialized to zeros so that ΔW\\=BAΔW\\=BA\\\\Delta W = B A starts as the zero matrix. During training, the product of AAA and BBB forms a low-rank update matrix that is added to the original, pre-trained weight matrix WWW to produce the adapted model output hhh. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights WWW frozen.\n    \n    ![](/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg)\n    \n    *   This product, BABABA, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if BBB is a d×rd×rd \\\\times r matrix and AAA is an r×dr×dr \\\\times d matrix, where rrr is much smaller than ddd, the resulting product BABABA will have a maximum rank of rrr, regardless of the dimensions of ddd. This means the update to WWW is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.\n    *   For example, if d\\=1000d\\=1000d = 1000 and r\\=2r\\=2r = 2, the update matrix BABABA will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means BABABA is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights WWW frozen.\n\nAs illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, AAA and BBB, which capture the adaptation. AAA (the **down-projection matrix**) projects the input into a lower-dimensional subspace, while BBB (the **up-projection matrix**) maps it back to the original dimension. Per the [LoRA paper](https://arxiv.org/pdf/2106.09685) by Hu et al. (2021), the matrix AAA is initialized with random Gaussian noise (i.i.d. samples from a normal distribution), while the matrix BBB is initialized to zeros so that ΔW\\=BAΔW\\=BA\\\\Delta W = B A starts as the zero matrix. During training, the product of AAA and BBB forms a low-rank update matrix that is added to the original, pre-trained weight matrix WWW to produce the adapted model output hhh. This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights WWW frozen.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/lora.jpeg)\n\n*   This product, BABABA, is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if BBB is a d×rd×rd \\\\times r matrix and AAA is an r×dr×dr \\\\times d matrix, where rrr is much smaller than ddd, the resulting product BABABA will have a maximum rank of rrr, regardless of the dimensions of ddd. This means the update to WWW is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.\n*   For example, if d\\=1000d\\=1000d = 1000 and r\\=2r\\=2r = 2, the update matrix BABABA will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means BABABA is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights WWW frozen.\n\n*   LoRA fundamentally changes the approach to fine-tuning large neural networks by introducing a method to decompose high-dimensional weight matrices into lower-dimensional forms, preserving essential information while reducing computational load. Put simply, LoRA efficiently fine-tunes large-scale neural networks by introducing trainable low-rank matrices, simplifying the model’s complexity while retaining its robust learning capabilities.\n*   LoRA is similar to methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), leveraging the observation that weight updates during fine-tuning are often rank-deficient. By imposing a low-rank constraint on these updates, LoRA enables a lightweight adaptation process that captures the most critical directions or “components” in the weight space. This means the model can retain essential knowledge from its pre-training phase while focusing on the specific nuances of the new task, resulting in efficient adaptation with a smaller memory and computational footprint.\n*   During backpropagation, only the low-rank LoRA matrices (A and B) receive gradient updates; the original pretrained weights remain frozen. Gradients flow through the combined effective weight (W = W\\_0 + BA), but only the LoRA-specific parameters (A and B) have `requires_grad=True`, ensuring that only these low-rank components are optimized while the base model weights remain unchanged.\n\n*   LoRA’s primary application is in the efficient fine-tuning of large neural networks, particularly in transformer-based architectures. In practice, LoRA identifies key dimensions within the original weight matrix that are crucial for a specific task, significantly reducing the adaptation’s dimensionality.\n*   Instead of fine-tuning all weights, which is computationally prohibitive for large models, LoRA introduces two trainable low-rank matrices, AAA and BBB, in specific layers of the transformer (e.g., in the query and value projections of the attention mechanism). These matrices are optimized during training while keeping the core model parameters frozen. This architecture adjustment means that only a fraction of the original parameters are actively updated, thus lowering memory usage and enabling faster computations.\n*   By focusing only on the most relevant dimensions for each task, LoRA enables the deployment of large models on hardware with limited resources and facilitates task-switching by updating only the low-rank matrices without retraining the entire model.\n\n*   LoRA provides several advantages that make it particularly suitable for practical use in industry and research settings where resource constraints are a concern. The primary benefit is in memory and computational efficiency.\n*   By keeping the majority of the model’s parameters frozen and only updating the low-rank matrices, LoRA significantly reduces the number of trainable parameters, leading to lower memory consumption and faster training times. This reduction in training complexity also means that fewer GPUs or lower-spec hardware can be used to fine-tune large models, broadening accessibility.\n*   Additionally, LoRA avoids introducing any additional latency during inference because, once the low-rank matrices have been trained, they can be merged back into the pre-trained weights without altering the architecture.\n*   This setup makes LoRA ideal for environments where models need to switch between tasks frequently, as only the task-specific low-rank weights need to be loaded, allowing the model to quickly adapt to new tasks or datasets with minimal overhead.\n\n*   LoRA represents a highly efficient approach to fine-tuning, striking a balance between the depth of knowledge encapsulated in large pre-trained models and the need for targeted adaptation to new tasks or data.\n*   By leveraging the low intrinsic rank of weight updates, LoRA retains the model’s robustness and generalization capability while allowing for quick, cost-effective adaptations. This approach redefines efficiency in the era of massive LLMs, making it feasible to use and adapt large-scale pre-trained models across various applications and domains without the extensive computational burden associated with traditional fine-tuning.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/lora2.png)\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/1.png)\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/2.png)\n\n##### Advantages\n\n###### Parameter Efficiency\n\n*   Compared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the number of trainable parameters by 10,000 times. Specifically, this means that LoRA only fine-tunes approximately 0.01% of the parameters of the original model.\n*   The below table from the LoRA paper indicates that for GPT-3 with LoRA, we see that we only fine-tune 4.7175255×100\\=0.002%4.7175255×100\\=0.002%\\\\frac{4.7}{175255} \\\\times 100 = 0.002\\\\% and 38175255×100\\=0.02%38175255×100\\=0.02%\\\\frac{38}{175255} \\\\times 100 = 0.02\\\\% parameters.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/param-efficiency.jpg)\n\n###### GPU Memory (and Storage) Savings\n\n*   Compared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the GPU memory requirement by 3 times. Specifically, this means that LoRA fine-tunes the original model with 33% of the memory.\n*   For a large Transformer trained with Adam, LoRA reduces VRAM usage by up to two-thirds by avoiding the need to store optimizer states for the frozen parameters. On GPT-3 175B, VRAM consumption during training drops from 1.2TB to 350GB. When adapting only the query and value projection matrices with a rank r\\=4r\\=4r = 4, the checkpoint size decreases significantly from approximately 350GB to 35MB. This efficiency allows training with significantly fewer GPUs and avoids I/O bottlenecks.\n\n###### Efficient Task Switching\n\n*   Task switching is more cost-effective as only the LoRA weights need swapping, enabling the creation of numerous customized models that can be dynamically swapped on machines storing the pre-trained weights in VRAM.\n\n###### Faster Training Speed\n\n*   Training speed also improves by 25% compared to full fine-tuning, as the gradient calculation for the vast majority of the parameters is unnecessary.\n\n###### No Additional Inference Latency\n\n*   LoRA ensures no additional inference latency when deployed in production by allowing explicit computation and storage of the combined weight matrix W\\=W0+BAW\\=W0+BAW = W\\_0 + BA. During inference, this approach uses the pre-computed matrix WWW, which includes the original pre-trained weights W0W0W\\_0 and the low-rank adaptation matrices BBB and AAA. This method eliminates the need for dynamic computations during inference.\n*   When switching to another downstream task, the pre-trained weights W0W0W\\_0 can be quickly restored by subtracting the current low-rank product BABABA and adding the new task-specific low-rank product B′A′B′A′B' A'. This operation incurs minimal memory overhead and allows for efficient task switching without impacting inference speed. By merging the low-rank matrices with the pre-trained weights in advance, LoRA avoids the extra computational burden during real-time inference (unlike adapters), ensuring latency remains on par with that of fully fine-tuned models.\n\n##### Limitations\n\n*   While LoRA offers significant advantages in terms of parameter efficiency and memory savings, it also has some limitations. One notable limitation is the complexity involved in batching inputs for different tasks when using distinct low-rank matrices AAA and BBB. If the goal is to absorb AAA and BBB into the combined weight matrix WWW to avoid additional inference latency, it becomes challenging to batch inputs from different tasks in a single forward pass. This is because each task would require a different set of AAA and BBB matrices, complicating the batching process.\n*   Additionally, although it is possible to avoid merging the weights and dynamically select the appropriate LoRA modules for each sample in a batch, this approach is more feasible in scenarios where latency is not a critical concern. This workaround does not fully address the need for seamless integration when low-latency inference is required across multiple tasks.\n*   In summary, while LoRA provides a highly efficient adaptation method, the complexity in handling multiple tasks simultaneously and the need for careful management of low-rank matrices during batching are important considerations for its practical deployment.\n\n##### Hyperparameters\n\n*   LoRA-specific hyperparameters include rank (rrr) and alpha (αα\\\\alpha). Others, while still used for LoRA-based fine-tuning, such as learning rate (lr), dropout probability (ppp), and batch size (NNN), are more generic to deep learning-based model training/fine-tuning. Here’s a detailed explanation of each:\n\n###### Rank (rrr)\n\n*   **Description**: In LoRA, instead of fine-tuning the full weight matrix, the weight updates are modeled as a low-rank approximation. Specifically, the weight update matrix ΔWΔW\\\\Delta W is decomposed into two smaller matrices, A∈ℝd×rA∈Rd×rA \\\\in \\\\mathbb{R}^{d \\\\times r} and B∈ℝr×kB∈Rr×kB \\\\in \\\\mathbb{R}^{r \\\\times k}, where rrr is much smaller than ddd or kkk. The rank (rrr) of matrices AAA and BBB – one of the core hyperparameters in LoRA – represents the rank of the low-rank decomposition applied to the weight matrices. The new weight is then modeled as:\n    \n    W\\=W0+ΔW\\=W0+A⋅BW\\=W0+ΔW\\=W0+A⋅B\n    \n    W = W\\_0 + \\\\Delta W = W\\_0 + A \\\\cdot B\n*   **Role**: The rank controls the dimensionality of the low-rank matrices and hence the number of additional parameters introduced during fine-tuning.\n*   **Interpretation**: Lower values of rrr will impose stronger restrictions on how much the weight matrices can adapt, potentially limiting the model’s flexibility but greatly reducing the computational and memory footprint. Higher values of rrr allow for more expressive updates but increase the number of parameters and computation required.\n    \n*   **Equation**: In matrix form, for any original weight matrix W0∈ℝd×kW0∈Rd×kW\\_0 \\\\in \\\\mathbb{R}^{d \\\\times k}, the adapted weight update is expressed as:\n    \n    ΔW\\=A⋅BΔW\\=A⋅B\n    \n    \\\\Delta W = A \\\\cdot B\n    *   where, A∈ℝd×rA∈Rd×rA \\\\in \\\\mathbb{R}^{d \\\\times r} and B∈ℝr×kB∈Rr×kB \\\\in \\\\mathbb{R}^{r \\\\times k}, where r≪d,kr≪d,kr \\\\ll d, k.\n*   **Typical Values**: 2–16, depending on the size of the model and the complexity of the task. In most tasks, a small rank (e.g., 4 or 8) provides a good trade-off between performance and efficiency.\n    \n*   **Higher Values**: For more complex tasks, larger models, or cases where the pretrained model diverges significantly from the specialized task, higher rank values (e.g., 32, 64, or 128) may be used. Examples include:\n    \n    *   Adapting a general language model to legal contract review, where formal, domain-specific syntax and terminology dominate.\n    *   Fine-tuning for biomedical question answering or clinical note summarization, which involves specialized jargon not well represented in general corpora.\n    *   Tuning for code generation in a low-resource or proprietary programming language.\n    *   Adapting to historical or archaic language for cultural heritage and digitization tasks.\n        \n    *   These scenarios benefit from higher-rank LoRA modules due to the substantial gap between the pretraining data and the target domain, requiring more capacity to learn meaningful adaptations.\n\n**Description**: In LoRA, instead of fine-tuning the full weight matrix, the weight updates are modeled as a low-rank approximation. Specifically, the weight update matrix ΔWΔW\\\\Delta W is decomposed into two smaller matrices, A∈ℝd×rA∈Rd×rA \\\\in \\\\mathbb{R}^{d \\\\times r} and B∈ℝr×kB∈Rr×kB \\\\in \\\\mathbb{R}^{r \\\\times k}, where rrr is much smaller than ddd or kkk. The rank (rrr) of matrices AAA and BBB – one of the core hyperparameters in LoRA – represents the rank of the low-rank decomposition applied to the weight matrices. The new weight is then modeled as:\n\n**Interpretation**: Lower values of rrr will impose stronger restrictions on how much the weight matrices can adapt, potentially limiting the model’s flexibility but greatly reducing the computational and memory footprint. Higher values of rrr allow for more expressive updates but increase the number of parameters and computation required.\n\n**Equation**: In matrix form, for any original weight matrix W0∈ℝd×kW0∈Rd×kW\\_0 \\\\in \\\\mathbb{R}^{d \\\\times k}, the adapted weight update is expressed as:\n\n*   where, A∈ℝd×rA∈Rd×rA \\\\in \\\\mathbb{R}^{d \\\\times r} and B∈ℝr×kB∈Rr×kB \\\\in \\\\mathbb{R}^{r \\\\times k}, where r≪d,kr≪d,kr \\\\ll d, k.\n\n**Typical Values**: 2–16, depending on the size of the model and the complexity of the task. In most tasks, a small rank (e.g., 4 or 8) provides a good trade-off between performance and efficiency.\n\n**Higher Values**: For more complex tasks, larger models, or cases where the pretrained model diverges significantly from the specialized task, higher rank values (e.g., 32, 64, or 128) may be used. Examples include:\n\n*   Adapting a general language model to legal contract review, where formal, domain-specific syntax and terminology dominate.\n*   Fine-tuning for biomedical question answering or clinical note summarization, which involves specialized jargon not well represented in general corpora.\n*   Tuning for code generation in a low-resource or proprietary programming language.\n*   Adapting to historical or archaic language for cultural heritage and digitization tasks.\n    \n*   These scenarios benefit from higher-rank LoRA modules due to the substantial gap between the pretraining data and the target domain, requiring more capacity to learn meaningful adaptations.\n\nAdapting to historical or archaic language for cultural heritage and digitization tasks.\n\n###### Scaling Factor (αα\\\\alpha)\n\n*   **Description**: αα\\\\alpha is a scaling factor applied to the LoRA updates. Specifically, it scales the low-rank updates A⋅BA⋅BA \\\\cdot B before adding them to the base weight matrix W0W0W\\_0. The weight update rule becomes:\n    \n    W\\=W0+αr⋅(A⋅B)W\\=W0+αr⋅(A⋅B)\n    \n    W = W\\_0 + \\\\frac{\\\\alpha}{r} \\\\cdot (A \\\\cdot B)\n*   **Role**: The purpose of αα\\\\alpha is to control the magnitude of the low-rank updates to prevent the model from diverging too far from the pre-trained weights. By dividing αα\\\\alpha by the rank rrr, LoRA ensures that the update magnitude is normalized according to the size of the low-rank decomposition. This is crucial because a larger rank would introduce more freedom for updates, and the division by rrr keeps the updates in check.\n    \n*   **Interpretation**: A higher αα\\\\alpha means that the low-rank updates will have a larger impact on the final weight, while a smaller αα\\\\alpha means the low-rank updates will contribute less to the adapted model. The division by rrr helps keep the effect of the low-rank update consistent across different choices of rank.\n    \n*   **Equation**: The weight update is now written as:\n    \n    ΔW\\=αr⋅(A⋅B)ΔW\\=αr⋅(A⋅B)\n    \n    \\\\Delta W = \\\\frac{\\\\alpha}{r} \\\\cdot (A \\\\cdot B)\n*   **Typical Values**: Common values for αα\\\\alpha are in the range of 1–32. The typical recommendation is to set α\\=rbase rankα\\=rbase rank\\\\alpha = \\\\frac{r}{\\\\text{base rank}}, where base rankbase rank\\\\text{base rank} is a predetermined scale for the model.\n    \n\n**Description**: αα\\\\alpha is a scaling factor applied to the LoRA updates. Specifically, it scales the low-rank updates A⋅BA⋅BA \\\\cdot B before adding them to the base weight matrix W0W0W\\_0. The weight update rule becomes:\n\n**Role**: The purpose of αα\\\\alpha is to control the magnitude of the low-rank updates to prevent the model from diverging too far from the pre-trained weights. By dividing αα\\\\alpha by the rank rrr, LoRA ensures that the update magnitude is normalized according to the size of the low-rank decomposition. This is crucial because a larger rank would introduce more freedom for updates, and the division by rrr keeps the updates in check.\n\n**Interpretation**: A higher αα\\\\alpha means that the low-rank updates will have a larger impact on the final weight, while a smaller αα\\\\alpha means the low-rank updates will contribute less to the adapted model. The division by rrr helps keep the effect of the low-rank update consistent across different choices of rank.\n\n**Equation**: The weight update is now written as:\n\n**Typical Values**: Common values for αα\\\\alpha are in the range of 1–32. The typical recommendation is to set α\\=rbase rankα\\=rbase rank\\\\alpha = \\\\frac{r}{\\\\text{base rank}}, where base rankbase rank\\\\text{base rank} is a predetermined scale for the model.\n\n###### Dropout Probability (ppp)\n\n*   **Description**: [Dropout](../dropout) is a regularization technique used to prevent overfitting, and it is applied in the LoRA framework as well. The dropout probability (ppp) refers to the probability with which a particular element in the low-rank matrices AAA and BBB is randomly set to zero during training. Dropout is typically used to reduce overfitting by introducing noise during training.\n    \n    *   **Role**: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.\n        \n    *   **Interpretation**: Higher values of dropout probability ppp imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of ppp imply less regularization and could potentially lead to overfitting on small datasets.\n        \n    *   **Equation**: The dropout operation is typically represented as:\n        \n        Adropped\\=A⊙Bernoulli(1−p)Adropped\\=A⊙Bernoulli(1−p)\n        \n        A\\_{dropped} = A \\\\odot \\\\text{Bernoulli}(1-p)\n        *   where, ⊙⊙\\\\odot denotes element-wise multiplication, and Bernoulli(1−p)Bernoulli(1−p)\\\\text{Bernoulli}(1-p) is a binary mask where each element is independently drawn from a Bernoulli distribution with probability 1−p1−p1 - p.\n    *   **Typical Values**: Dropout probabilities ppp are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.\n        \n\n**Description**: [Dropout](../dropout) is a regularization technique used to prevent overfitting, and it is applied in the LoRA framework as well. The dropout probability (ppp) refers to the probability with which a particular element in the low-rank matrices AAA and BBB is randomly set to zero during training. Dropout is typically used to reduce overfitting by introducing noise during training.\n\n*   **Role**: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.\n    \n*   **Interpretation**: Higher values of dropout probability ppp imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of ppp imply less regularization and could potentially lead to overfitting on small datasets.\n    \n*   **Equation**: The dropout operation is typically represented as:\n    \n    Adropped\\=A⊙Bernoulli(1−p)Adropped\\=A⊙Bernoulli(1−p)\n    \n    A\\_{dropped} = A \\\\odot \\\\text{Bernoulli}(1-p)\n    *   where, ⊙⊙\\\\odot denotes element-wise multiplication, and Bernoulli(1−p)Bernoulli(1−p)\\\\text{Bernoulli}(1-p) is a binary mask where each element is independently drawn from a Bernoulli distribution with probability 1−p1−p1 - p.\n*   **Typical Values**: Dropout probabilities ppp are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.\n    \n\n**Role**: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.\n\n**Interpretation**: Higher values of dropout probability ppp imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of ppp imply less regularization and could potentially lead to overfitting on small datasets.\n\n**Equation**: The dropout operation is typically represented as:\n\n*   where, ⊙⊙\\\\odot denotes element-wise multiplication, and Bernoulli(1−p)Bernoulli(1−p)\\\\text{Bernoulli}(1-p) is a binary mask where each element is independently drawn from a Bernoulli distribution with probability 1−p1−p1 - p.\n\n**Typical Values**: Dropout probabilities ppp are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.\n\n###### Learning Rate (ηη\\\\eta)\n\n*   **Description**: The learning rate is a fundamental hyperparameter in any optimization process, and it determines the step size at which the model’s parameters are updated during training. In the context of LoRA, it controls the update of the low-rank matrices AAA and BBB rather than the full model weights.\n    \n    *   **Role**: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.\n        \n    *   **Interpretation**: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when αα\\\\alpha is large.\n        \n    *   **Equation**: The update to the low-rank parameters follows the standard gradient descent update rule:\n        \n        θt+1\\=θt−η⋅∇θLθt+1\\=θt−η⋅∇θL\n        \n        \\\\theta\\_{t+1} = \\\\theta\\_t - \\\\eta \\\\cdot \\\\nabla\\_{\\\\theta} L\n        \n        Where LLL is the loss function, ∇θL∇θL\\\\nabla\\_{\\\\theta} L is the gradient of the loss with respect to the low-rank parameters θθ\\\\theta, and ηη\\\\eta is the learning rate.\n        \n    *   **Typical Values**: Learning rates for LoRA typically range from 10−510−510^{-5} to 10−310−310^{-3}, depending on the model, the task, and the scale of adaptation needed.\n        \n\n**Description**: The learning rate is a fundamental hyperparameter in any optimization process, and it determines the step size at which the model’s parameters are updated during training. In the context of LoRA, it controls the update of the low-rank matrices AAA and BBB rather than the full model weights.\n\n*   **Role**: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.\n    \n*   **Interpretation**: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when αα\\\\alpha is large.\n    \n*   **Equation**: The update to the low-rank parameters follows the standard gradient descent update rule:\n    \n    θt+1\\=θt−η⋅∇θLθt+1\\=θt−η⋅∇θL\n    \n    \\\\theta\\_{t+1} = \\\\theta\\_t - \\\\eta \\\\cdot \\\\nabla\\_{\\\\theta} L\n    \n    Where LLL is the loss function, ∇θL∇θL\\\\nabla\\_{\\\\theta} L is the gradient of the loss with respect to the low-rank parameters θθ\\\\theta, and ηη\\\\eta is the learning rate.\n    \n*   **Typical Values**: Learning rates for LoRA typically range from 10−510−510^{-5} to 10−310−310^{-3}, depending on the model, the task, and the scale of adaptation needed.\n    \n\n**Role**: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.\n\n**Interpretation**: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when αα\\\\alpha is large.\n\n**Equation**: The update to the low-rank parameters follows the standard gradient descent update rule:\n\nWhere LLL is the loss function, ∇θL∇θL\\\\nabla\\_{\\\\theta} L is the gradient of the loss with respect to the low-rank parameters θθ\\\\theta, and ηη\\\\eta is the learning rate.\n\n**Typical Values**: Learning rates for LoRA typically range from 10−510−510^{-5} to 10−310−310^{-3}, depending on the model, the task, and the scale of adaptation needed.\n\n###### Batch Size (NNN)\n\n*   **Description**: The batch size is the number of examples that are passed through the model at one time before updating the weights. It is a crucial hyperparameter for stabilizing the training process.\n    \n    *   **Role**: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.\n        \n    *   **Interpretation**: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.\n        \n    *   **Equation**: The loss for a given batch of size NNN is averaged over the batch:\n        \n        Lbatch\\=1N∑i\\=1NLiLbatch\\=1N∑i\\=1NLi\n        \n        L\\_{\\\\text{batch}} = \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} L\\_i\n        *   where, LiLiL\\_i is the loss for the iii\\-th example in the batch.\n    *   **Typical Values**: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.\n        \n\n**Description**: The batch size is the number of examples that are passed through the model at one time before updating the weights. It is a crucial hyperparameter for stabilizing the training process.\n\n*   **Role**: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.\n    \n*   **Interpretation**: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.\n    \n*   **Equation**: The loss for a given batch of size NNN is averaged over the batch:\n    \n    Lbatch\\=1N∑i\\=1NLiLbatch\\=1N∑i\\=1NLi\n    \n    L\\_{\\\\text{batch}} = \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} L\\_i\n    *   where, LiLiL\\_i is the loss for the iii\\-th example in the batch.\n*   **Typical Values**: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.\n    \n\n**Role**: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.\n\n**Interpretation**: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.\n\n**Equation**: The loss for a given batch of size NNN is averaged over the batch:\n\n*   where, LiLiL\\_i is the loss for the iii\\-th example in the batch.\n\n**Typical Values**: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.\n\n###### Summary\n\n*   The main hyperparameters involved in LoRA—rank (rrr), alpha (αα\\\\alpha), dropout probability (ppp), learning rate (ηη\\\\eta), and batch size (NNN)—are crucial for controlling the behavior and effectiveness of LoRA. By adjusting these parameters, LoRA can offer an efficient way to fine-tune large pre-trained models with significantly reduced computational costs and memory usage while maintaining competitive performance. Each of these hyperparameters impacts the trade-off between model flexibility, computational efficiency, and training stability.\n*   These hyperparameters are interconnected, especially scaling factor and rank; changes in one can require adjustments in others; more on this in the section on [Is There a Relationship Between Setting Scaling Factor and Rank in LoRA?](#is-there-a-relationship-between-setting-scaling-factor-and-rank-in-lora). Effective tuning of these parameters is critical for leveraging LoRA’s capabilities to adapt large models without extensive retraining.\n\n##### How Does Having a Low-rank Matrix in LoRA Help the Fine-tuning Process?\n\n*   In LoRA, a low-rank matrix is a matrix with a rank significantly smaller than its full dimensionality, which enables efficient and focused adjustments to model parameters. This lightweight adaptation mechanism allows large LLMs to learn new tasks without overfitting by capturing only the most essential adjustments, thus optimizing both information representation and parameter efficiency.\n\n###### What is a Low-rank Matrix?\n\n*   A matrix is considered low-rank when its rank (the number of independent rows or columns) is much smaller than its dimensions. For example, a 1000x1000 matrix with rank 10 is low-rank because only 10 of its rows or columns contain unique information, and the others can be derived from these. This smaller rank indicates that the matrix contains a limited variety of independent patterns or directions, meaning it has a reduced capacity to capture complex relationships.\n\n###### Low-Rank in LoRA Context\n\n*   In LoRA, low-rank matrices are introduced to fine-tune large LLMs with fewer trainable parameters. Here’s how it works:\n    1.  **Adding Low-Rank Matrices**: LoRA adds small, low-rank matrices to the model’s layers (typically linear or attention layers). These matrices serve as “adaptation” layers that adjust the original layer’s output.\n    2.  **Freezing the Original Weights**: The original model weights remain frozen during fine-tuning. Only the low-rank matrices are trained, which reduces the number of parameters to update.\n*   By limiting the rank of these new matrices, LoRA effectively limits the number of patterns they can represent. For instance, a rank-5 matrix in a high-dimensional space can only capture 5 independent directions, which forces the model to learn only essential, low-dimensional adjustments without becoming too complex.\n\n1.  **Adding Low-Rank Matrices**: LoRA adds small, low-rank matrices to the model’s layers (typically linear or attention layers). These matrices serve as “adaptation” layers that adjust the original layer’s output.\n2.  **Freezing the Original Weights**: The original model weights remain frozen during fine-tuning. Only the low-rank matrices are trained, which reduces the number of parameters to update.\n\n###### Example\n\n*   Suppose we have a pre-trained model layer represented by a 512x512 matrix (common in large LLMs). Instead of fine-tuning this large matrix directly, LoRA adds two low-rank matrices, AAA and BBB, with dimensions 512x10 and 10x512, respectively. Here:\n    *   The product A×BA×BA \\\\times B has a rank of 10, much smaller than 512.\n    *   This product effectively adds a low-rank adaptation to the original layer, allowing it to adjust its output in just a few key directions (10 in this case), rather than making unrestricted adjustments.\n\n*   The product A×BA×BA \\\\times B has a rank of 10, much smaller than 512.\n*   This product effectively adds a low-rank adaptation to the original layer, allowing it to adjust its output in just a few key directions (10 in this case), rather than making unrestricted adjustments.\n\n###### Why Rank Matters\n\n*   The rank of the LoRA matrices directly affects the model’s ability to learn task-specific patterns:\n    *   **Lower Rank**: Imposes a strong constraint on the model, which helps it generalize better and reduces the risk of overfitting.\n    *   **Higher Rank**: Provides more flexibility but also increases the risk of overfitting, as the model can learn more complex adjustments that may fit the fine-tuning data too closely.\n\n*   **Lower Rank**: Imposes a strong constraint on the model, which helps it generalize better and reduces the risk of overfitting.\n*   **Higher Rank**: Provides more flexibility but also increases the risk of overfitting, as the model can learn more complex adjustments that may fit the fine-tuning data too closely.\n\n##### How Does Low-rank Constraint Introduced by LoRA Inherently Act As a Form of Regularization, Especially for the Lower Layers of the Model?\n\n*   In LoRA, the low-rank constraint serves as a built-in regularization mechanism by limiting the model’s flexibility during fine-tuning. This constraint especially impacts lower layers, which are designed to capture general, foundational features. By further restricting these layers, LoRA minimizes their adaptation to task-specific data, reducing the risk of overfitting. This regularization preserves the model’s foundational knowledge in the lower layers, while allowing the higher layers—where task-specific adjustments are most beneficial—to adapt more freely.\n\n###### Low-Rank Constraint As Regularization\n\n1.  **Low-Rank Matrices Limit Complexity**: By adding only low-rank matrices to the model’s layers, LoRA restricts the model’s capacity to represent highly complex, task-specific patterns. A low-rank matrix has fewer independent “directions” or dimensions in which it can vary. This means that the model, even when fine-tuned, can only make adjustments within a constrained range, learning broad, generalizable patterns rather than memorizing specific details of the training data. This limited capacity serves as a form of regularization, preventing the model from overfitting.\n    \n2.  **Reduced Sensitivity to Noisy Patterns**: Low-rank matrices inherently ignore minor or highly detailed variations in the training data, focusing only on dominant, overarching patterns. This makes LoRA less sensitive to the idiosyncrasies of the fine-tuning dataset, enhancing the model’s robustness and generalization ability.\n    \n\n**Low-Rank Matrices Limit Complexity**: By adding only low-rank matrices to the model’s layers, LoRA restricts the model’s capacity to represent highly complex, task-specific patterns. A low-rank matrix has fewer independent “directions” or dimensions in which it can vary. This means that the model, even when fine-tuned, can only make adjustments within a constrained range, learning broad, generalizable patterns rather than memorizing specific details of the training data. This limited capacity serves as a form of regularization, preventing the model from overfitting.\n\n**Reduced Sensitivity to Noisy Patterns**: Low-rank matrices inherently ignore minor or highly detailed variations in the training data, focusing only on dominant, overarching patterns. This makes LoRA less sensitive to the idiosyncrasies of the fine-tuning dataset, enhancing the model’s robustness and generalization ability.\n\n###### Effect on Lower Layers\n\n*   The **lower layers** of a neural network, especially in a transformer model, are primarily responsible for extracting general-purpose features from the input data. In LLMs, for example:\n    *   Lower layers capture basic syntactic relationships, such as sentence structure and word dependencies.\n    *   These layers learn representations that are widely applicable across tasks and domains.\n*   Because these lower layers are already optimized to represent broad, generalizable patterns from pre-training, they are naturally less flexible and more constrained in what they capture compared to higher layers, which focus on more task-specific details. Adding a low-rank constraint in LoRA further reinforces this effect:\n\n*   Lower layers capture basic syntactic relationships, such as sentence structure and word dependencies.\n*   These layers learn representations that are widely applicable across tasks and domains.\n\n1.  **Enhanced Regularization on Lower Layers**: Since lower layers are already constrained to capture only general patterns, the addition of a low-rank constraint essentially adds a second layer of regularization. This means that these layers become even less likely to adapt in ways that would compromise their general-purpose functionality. The low-rank constraint reinforces their role as foundational feature extractors, preserving their generalization capability while preventing overfitting on the specific details of the fine-tuning data.\n    \n2.  **Minimal Disruption of Pre-Trained Knowledge**: The low-rank adaptation in LoRA ensures that lower layers maintain the knowledge they acquired during pre-training. Because these layers are regularized by the low-rank constraint, they are less likely to overfit to new data patterns introduced during fine-tuning. This preservation of pre-trained knowledge is crucial for maintaining the model’s transferability to other tasks or domains, as lower layers retain their broad, foundational representations.\n    \n\n**Enhanced Regularization on Lower Layers**: Since lower layers are already constrained to capture only general patterns, the addition of a low-rank constraint essentially adds a second layer of regularization. This means that these layers become even less likely to adapt in ways that would compromise their general-purpose functionality. The low-rank constraint reinforces their role as foundational feature extractors, preserving their generalization capability while preventing overfitting on the specific details of the fine-tuning data.\n\n**Minimal Disruption of Pre-Trained Knowledge**: The low-rank adaptation in LoRA ensures that lower layers maintain the knowledge they acquired during pre-training. Because these layers are regularized by the low-rank constraint, they are less likely to overfit to new data patterns introduced during fine-tuning. This preservation of pre-trained knowledge is crucial for maintaining the model’s transferability to other tasks or domains, as lower layers retain their broad, foundational representations.\n\n###### Why This Matters for Generalization\n\n*   When fine-tuning with LoRA:\n    *   **Higher Layers Adapt More Easily**: Higher layers, being closer to the output, are more adaptable and can more readily accommodate task-specific changes introduced during fine-tuning.\n    *   **Lower Layers Remain Generalized**: Lower layers, reinforced by the low-rank constraint, retain their focus on general patterns. This balanced approach helps the model generalize well to unseen data because the lower layers still provide robust, general-purpose representations while the higher layers adapt to the new task.\n\n*   **Higher Layers Adapt More Easily**: Higher layers, being closer to the output, are more adaptable and can more readily accommodate task-specific changes introduced during fine-tuning.\n*   **Lower Layers Remain Generalized**: Lower layers, reinforced by the low-rank constraint, retain their focus on general patterns. This balanced approach helps the model generalize well to unseen data because the lower layers still provide robust, general-purpose representations while the higher layers adapt to the new task.\n\n##### How Does LoRA Help Avoid Catastrophic Forgetting?\n\n*   LoRA helps prevent catastrophic forgetting by fine-tuning large pre-trained models in a way that preserves their foundational knowledge while allowing for task-specific adaptations. Catastrophic forgetting occurs when fine-tuning neural networks, particularly large pre-trained models, causes them to overwrite or disrupt previously learned information, reducing performance on earlier tasks. LoRA mitigates this risk through a few key strategies:\n    \n    *   **Freezing Original Weights**: The core model parameters remain untouched, preserving the base knowledge and preventing interference.\n    *   **Introducing Low-Rank Matrices**: These matrices have limited capacity, focusing solely on task-specific adjustments, which allows the model to adapt to new tasks without losing general knowledge.\n    *   **Targeting Specific Layers**: LoRA typically modifies higher attention layers, avoiding disruption to fundamental representations in lower layers.\n    *   **Parameter-Efficient, Modular Adaptation**: LoRA’s modular design allows for reversible, task-specific adjustments, making it suitable for flexible multi-task and continual learning.\n*   Through this approach, LoRA enables large models to adapt efficiently to new tasks while retaining previously learned information, which is especially valuable for applications requiring retention of prior knowledge.\n    \n\nLoRA helps prevent catastrophic forgetting by fine-tuning large pre-trained models in a way that preserves their foundational knowledge while allowing for task-specific adaptations. Catastrophic forgetting occurs when fine-tuning neural networks, particularly large pre-trained models, causes them to overwrite or disrupt previously learned information, reducing performance on earlier tasks. LoRA mitigates this risk through a few key strategies:\n\n*   **Freezing Original Weights**: The core model parameters remain untouched, preserving the base knowledge and preventing interference.\n*   **Introducing Low-Rank Matrices**: These matrices have limited capacity, focusing solely on task-specific adjustments, which allows the model to adapt to new tasks without losing general knowledge.\n*   **Targeting Specific Layers**: LoRA typically modifies higher attention layers, avoiding disruption to fundamental representations in lower layers.\n*   **Parameter-Efficient, Modular Adaptation**: LoRA’s modular design allows for reversible, task-specific adjustments, making it suitable for flexible multi-task and continual learning.\n\nThrough this approach, LoRA enables large models to adapt efficiently to new tasks while retaining previously learned information, which is especially valuable for applications requiring retention of prior knowledge.\n\n###### Freezing the Original Weights\n\n*   One of the core aspects of LoRA is that it freezes the original model weights and adds new, low-rank matrices that handle the fine-tuning process:\n    *   The frozen original weights retain the model’s general knowledge from pre-training. This means that core information, patterns, and representations acquired from extensive pre-training on large datasets remain unaffected.\n    *   Since only the low-rank matrices are adjusted for the new task, there is no direct modification of the original weights. This minimizes the risk of overwriting or disrupting the knowledge captured in those weights.\n*   By keeping the original parameters intact, LoRA avoids catastrophic forgetting in a way that typical fine-tuning (where the original weights are updated) does not.\n\n*   The frozen original weights retain the model’s general knowledge from pre-training. This means that core information, patterns, and representations acquired from extensive pre-training on large datasets remain unaffected.\n*   Since only the low-rank matrices are adjusted for the new task, there is no direct modification of the original weights. This minimizes the risk of overwriting or disrupting the knowledge captured in those weights.\n\n###### Low-Rank Adaptation Layers for Task-Specific Adjustments\n\n*   LoRA introduces low-rank matrices as additional layers to the model, which have the following properties:\n    *   **Limited Capacity**: Low-rank matrices have a constrained capacity to represent new information, which forces them to focus only on essential, task-specific adaptations. This means they cannot significantly alter the underlying model’s behavior, preserving the broader general knowledge.\n    *   **Focused Adaptation**: By adding task-specific information via low-rank matrices rather than altering the model’s entire parameter space, LoRA ensures that the new task-specific changes are confined to these auxiliary matrices. This helps the model adapt to new tasks without losing its prior knowledge.\n\n*   **Limited Capacity**: Low-rank matrices have a constrained capacity to represent new information, which forces them to focus only on essential, task-specific adaptations. This means they cannot significantly alter the underlying model’s behavior, preserving the broader general knowledge.\n*   **Focused Adaptation**: By adding task-specific information via low-rank matrices rather than altering the model’s entire parameter space, LoRA ensures that the new task-specific changes are confined to these auxiliary matrices. This helps the model adapt to new tasks without losing its prior knowledge.\n\n###### Layer-Specific Impact\n\n*   LoRA often targets specific layers in the model, commonly the attention layers:\n    *   **Higher Attention Layers**: These layers (closer to the output) are responsible for more task-specific representations and are typically the ones modified by LoRA. This selective adaptation means that the deeper, more task-general features in lower layers are left intact, reducing the risk of catastrophic forgetting.\n    *   **Minimal Lower-Layer Impact**: Since lower layers (closer to the input) remain unchanged or minimally affected, the model retains the general-purpose, foundational features learned during pre-training, which are crucial for generalization.\n*   This selective impact allows LoRA to introduce new, task-specific representations while preserving fundamental information, balancing new task learning with knowledge retention.\n\n*   **Higher Attention Layers**: These layers (closer to the output) are responsible for more task-specific representations and are typically the ones modified by LoRA. This selective adaptation means that the deeper, more task-general features in lower layers are left intact, reducing the risk of catastrophic forgetting.\n*   **Minimal Lower-Layer Impact**: Since lower layers (closer to the input) remain unchanged or minimally affected, the model retains the general-purpose, foundational features learned during pre-training, which are crucial for generalization.\n\n###### Parameter-Efficient Fine-Tuning\n\n*   LoRA is designed for parameter-efficient fine-tuning, meaning it uses a fraction of the parameters that traditional fine-tuning would require:\n    *   LoRA adds only a small number of new parameters through the low-rank matrices. This efficiency keeps the model changes lightweight, making it less likely to interfere with the original model’s representations.\n    *   The low-rank constraint also regularizes the fine-tuning process, helping to prevent overfitting to the new task, which can indirectly support retention of general knowledge. Overfitting can cause catastrophic forgetting if the model becomes too specialized, as it loses flexibility in dealing with tasks beyond the fine-tuning data.\n\n*   LoRA adds only a small number of new parameters through the low-rank matrices. This efficiency keeps the model changes lightweight, making it less likely to interfere with the original model’s representations.\n*   The low-rank constraint also regularizes the fine-tuning process, helping to prevent overfitting to the new task, which can indirectly support retention of general knowledge. Overfitting can cause catastrophic forgetting if the model becomes too specialized, as it loses flexibility in dealing with tasks beyond the fine-tuning data.\n\n###### Easy Reversibility\n\n*   Since LoRA’s approach is to add new matrices rather than alter the original model’s weights, it makes it easy to revert the model to its original state or apply it to different tasks:\n    *   The low-rank matrices can be removed or swapped out without affecting the base model. This modularity allows for rapid switching between tasks or models, making it easy to adapt the model to different tasks while maintaining the pre-trained knowledge.\n    *   This adaptability is particularly useful for multi-task learning or continual learning, as it allows LoRA-enhanced models to apply distinct low-rank adaptations for different tasks without compromising the model’s underlying pre-trained knowledge.\n\n*   The low-rank matrices can be removed or swapped out without affecting the base model. This modularity allows for rapid switching between tasks or models, making it easy to adapt the model to different tasks while maintaining the pre-trained knowledge.\n*   This adaptability is particularly useful for multi-task learning or continual learning, as it allows LoRA-enhanced models to apply distinct low-rank adaptations for different tasks without compromising the model’s underlying pre-trained knowledge.\n\n###### Modular and Reusable Adapters\n\n*   With LoRA, fine-tuning for different tasks can be achieved by creating different low-rank matrices for each new task:\n    *   These modular, reusable matrices enable task-specific tuning without overwriting previous adaptations or the original model. This is especially valuable for applications where the model needs to perform multiple tasks or domains interchangeably.\n    *   By associating each task with its own set of low-rank matrices, LoRA enables the model to maintain knowledge across tasks without interference, effectively circumventing catastrophic forgetting.\n\n*   These modular, reusable matrices enable task-specific tuning without overwriting previous adaptations or the original model. This is especially valuable for applications where the model needs to perform multiple tasks or domains interchangeably.\n*   By associating each task with its own set of low-rank matrices, LoRA enables the model to maintain knowledge across tasks without interference, effectively circumventing catastrophic forgetting.\n\n##### How Does Multiplication of Two Low-rank Matrices in LoRA Lead to Lower Attention Layers Being Impacted Less Than Higher Attention Layers?\n\n*   In LoRA, the use of low-rank matrices enables efficient, controlled updates by selectively applying them to specific layers—primarily in the higher attention layers rather than the lower ones. This targeted approach allows the model to adjust effectively to task-specific nuances in these higher layers, which capture more complex patterns and contextual information, while preserving the general features encoded in the lower layers. By focusing fine-tuning efforts on the higher layers, LoRA minimizes overfitting and retains foundational knowledge from pre-training, making it an efficient and effective fine-tuning strategy.\n\n###### Role of Low-Rank Matrices in LoRA\n\n*   LoRA adds two low-rank matrices, AAA and BBB, to certain layers, typically in the form: Wnew\\=W+A×BWnew\\=W+A×BW\\_{\\\\text{new}} = W + A \\\\times B\n    *   where:\n        *   WWW is the original (frozen) weight matrix in the model layer.\n        *   AAA and BBB are low-rank matrices (with ranks much smaller than the original dimensionality of WWW), creating a low-rank adaptation.\n*   The product A×BA×BA \\\\times B has a limited rank and thus introduces only a restricted adjustment to WWW. This adjustment constrains the layer to learn only a few independent patterns rather than a full set of complex, task-specific transformations.\n\n*   where:\n    *   WWW is the original (frozen) weight matrix in the model layer.\n    *   AAA and BBB are low-rank matrices (with ranks much smaller than the original dimensionality of WWW), creating a low-rank adaptation.\n\n*   WWW is the original (frozen) weight matrix in the model layer.\n*   AAA and BBB are low-rank matrices (with ranks much smaller than the original dimensionality of WWW), creating a low-rank adaptation.\n\n###### Higher Attention Layers: Task-Specific Focus\n\n*   In large models, higher attention layers (closer to the output) tend to capture task-specific, abstract features, while lower attention layers (closer to the input) capture general, reusable patterns. By applying LoRA-based fine-tuning primarily to higher attention layers:\n*   The model’s low-rank adaptation focuses on high-level, task-specific adjustments rather than modifying general representations.\n*   Higher layers, which already deal with more specific information, are more sensitive to the small adjustments made by A×BA×BA \\\\times B since they directly influence task-related outputs.\n*   In practice, LoRA-based fine-tuning modifies these higher layers more significantly because these layers are more directly responsible for adapting the model to new tasks. Lower layers, in contrast, require less task-specific adjustment and retain their general-purpose features.\n\n###### Limited Capacity of Low-Rank Matrices and Layer Impact\n\n*   The low-rank matrices AAA and BBB have limited expressive power (due to their low rank), meaning they can only introduce a small number of directional adjustments in the weight space. This limited capacity aligns well with higher layers because:\n*   Higher layers don’t need drastic changes but rather subtle adjustments to fine-tune the model to specific tasks.\n*   The constraint imposed by low-rank matrices helps avoid overfitting by restricting the number of learned patterns, which is ideal for the high-level, abstract representations in higher layers.\n*   For lower layers, which capture broad, general-purpose features, such limited adjustments don’t significantly impact the model. Lower layers still operate with the general features learned during pre-training, while higher layers adapt to task-specific details.\n\n###### Why Lower Layers are Less Affected\n\n*   Lower layers in the attention stack are less impacted by LoRA’s low-rank updates because:\n*   They are often not fine-tuned at all in LoRA-based setups, preserving the general features learned during pre-training.\n*   Even when fine-tuned with low-rank matrices, the limited capacity of A×BA×BA \\\\times B is not sufficient to drastically alter their broader, foundational representations.\n\n##### In LoRA, Why is AAA Initialized Using a Gaussian and BBB Set to 0?\n\n*   In LoRA, the initialization strategy where matrix AAA is initialized with a Gaussian distribution and matrix BBB is set to zero is crucial for ensuring a smooth integration of the adaptation with minimal initial disruption to the pre-trained model. This approach is designed with specific goals in mind:\n\n###### Preserving Initial Model Behavior\n\n*   **Rationale**: By setting BBB to zero, the product ΔW\\=BAΔW\\=BA\\\\Delta W = BA initially equals zero. This means that the adapted weights do not alter the original pre-trained weights at the beginning of the training process.\n*   **Impact**: This preserves the behavior of the original model at the start of fine-tuning, allowing the model to maintain its pre-trained performance and stability. The model begins adaptation from a known good state, reducing the risk of drastic initial performance drops.\n\n###### Gradual Learning and Adaptation\n\n*   **Rationale**: Starting with ΔW\\=0ΔW\\=0\\\\Delta W = 0 allows the model to gradually adapt through the updates to BBB during training. This gradual adjustment is less likely to destabilize the model than a sudden, large change would.\n*   **Impact**: As BBB starts updating from zero, any changes in the model’s behavior are introduced slowly. This controlled adaptation is beneficial for training dynamics, as it allows the model to incrementally learn how to incorporate new information effectively without losing valuable prior knowledge.\n\n###### Ensuring Controlled Updates\n\n*   **Rationale**: Gaussian initialization of AAA provides a set of initial values that, while random, are statistically regularized by the properties of the Gaussian distribution (such as having a mean of zero and a defined variance). This regularity helps in providing a balanced and predictable set of initial conditions for the adaptation process.\n*   **Impact**: The Gaussian distribution helps ensure that the values in AAA are neither too large nor too biased in any direction, which could lead to disproportionate influence on the updates when BBB begins to change. This helps in maintaining a stable and effective learning process.\n\n###### Focused Adaptation\n\n*   **Rationale**: The low-rank matrices AAA and BBB are intended to capture the most essential aspects of the new data or tasks relative to the model’s existing capabilities. By starting with B\\=0B\\=0B = 0 and AAA initialized randomly, the learning focuses on identifying and optimizing only those aspects that truly need adaptation, as opposed to re-learning aspects that the model already performs well.\n*   **Impact**: This focus helps optimize training efficiency by directing computational resources and learning efforts towards making meaningful updates that enhance the model’s capabilities in specific new areas.\n    \n*   This initialization strategy supports the overall goal of LoRA: to adapt large, pre-trained models efficiently with minimal resource expenditure and without compromising the foundational strengths of the original model. This approach ensures that any new learning builds on and complements the existing pre-trained model structure.\n\n**Impact**: This focus helps optimize training efficiency by directing computational resources and learning efforts towards making meaningful updates that enhance the model’s capabilities in specific new areas.\n\n##### For a Given Task, How Do We Determine Whether to Fine-tune the Attention Layers or Feed-forward Layers?\n\n*   Deciding whether to fine-tune the attention layers or the feed-forward (MLP) layers in a model adapted using LoRA involves several considerations. These include the nature of the task, the model architecture, and the distribution of parameters between attention and feed-forward layers.\n*   Note that the LoRA paper originally only adapted the attention weights for downstream tasks and froze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency. Thus, the number of attention weights relative to feed-forward weights can impact the choice of .\n*   Here are some key factors to guide this decision:\n\n###### Nature of the Task\n\n*   **Task Requirements**: Attention mechanisms are particularly effective for tasks that benefit from modeling relationships between different parts of the input, such as sequence-to-sequence tasks or tasks requiring contextual understanding. If the task demands strong relational reasoning or context sensitivity, fine-tuning attention layers might be more beneficial.\n*   **Feed-Forward Layer Role**: MLPs generally focus on transforming the representation at individual positions without considering other positions. They are effective for tasks requiring more substantial non-linear transformation of features. If the task demands significant feature transformation at individual positions, MLPs may need adaptation.\n\n###### Model Architecture\n\n*   **Proportion of Parameters**: In transformer architectures, MLPs typically contain a larger number of parameters compared to attention mechanisms (of the order of 2x to 5x). For example, in standard configurations like those seen in BERT or GPT models, the MLPs can contain around three times more parameters than the attention layers.\n*   **Impact on Efficiency**: Because MLPs are parameter-heavy, fine-tuning them can significantly increase the number of trainable parameters, impacting training efficiency and computational requirements. If parameter efficiency is a priority, you might opt to adapt only the attention layers, as originally done in the LoRA approach.\n\n###### Computational Constraints\n\n*   **Resource Availability**: The decision can also be influenced by available computational resources. Adapting attention layers only can save computational resources and training time, making it a preferable option when resources are limited.\n*   **Balance of Adaptation and Performance**: If computational resources allow, experimenting with both components can be useful to understand which contributes more to performance improvements on specific tasks.\n\n###### Empirical Testing\n\n*   **A/B Testing**: One effective way to determine the optimal strategy for a specific model and task is to conduct empirical tests where you fine-tune the attention layers alone, the MLP layers alone, and both together in different experiments to compare the performance impacts.\n*   **Performance Metrics**: Monitoring key performance metrics specific to the task during these tests will guide which components are more critical to fine-tune.\n\n###### Task-Specific Research and Insights\n\n*   **Literature and Benchmarks**: Insights from research papers and benchmarks on similar tasks can provide guidelines on what has worked well historically in similar scenarios. For example, tasks that require nuanced understanding of input relationships (like question answering or summarization) might benefit more from tuning attention mechanisms.\n    \n*   In summary, the choice between tuning attention or MLP layers depends on the specific demands of the task, the model’s architecture, the balance of parameters, and empirical results. Considering these aspects can help in making a decision that optimizes both performance and efficiency.\n    \n\n**Literature and Benchmarks**: Insights from research papers and benchmarks on similar tasks can provide guidelines on what has worked well historically in similar scenarios. For example, tasks that require nuanced understanding of input relationships (like question answering or summarization) might benefit more from tuning attention mechanisms.\n\nIn summary, the choice between tuning attention or MLP layers depends on the specific demands of the task, the model’s architecture, the balance of parameters, and empirical results. Considering these aspects can help in making a decision that optimizes both performance and efficiency.\n\n##### Assuming We’re Fine-tuning Attention Weights, Which Specific Attention Weight Matrices Should We Apply LoRA To?\n\n*   The question of which attention weight matrices in the transformer architecture should be adapted using LoRA to optimize performance on downstream tasks is central to maximizing the effectiveness of parameter usage, especially when dealing with large models like GPT-3. Based on the findings reported in the LoRA paper and the specific experiment mentioned, here’s a detailed explanation and recommendation:\n\n###### Context and Setup\n\n*   The LoRA paper explores the adaptation of various weight matrices within the self-attention module of GPT-3 under a limited parameter budget. With a constraint of 18 million trainable parameters, the authors tested different configurations of adapting the weights associated with the query (WqWqW\\_q), key (WkWkW\\_k), value (WvWvW\\_v), and output (WoWoW\\_o) matrices. This setup allows for a comparison of the effectiveness of adapting different combinations of weights at varying ranks.\n\n###### Experimental Findings\n\n*   **Parameter Allocation**: The experiment considered adapting individual weight types at a rank of 8 and combinations of weights at lower ranks (4 and 2) due to the fixed parameter budget. This arrangement allowed assessing whether it is more beneficial to distribute the available parameters across multiple weight types or concentrate them on fewer weights at a higher rank.\n*   **Performance Metrics**: The validation accuracies on the WikiSQL and MultiNLI datasets served as the primary performance indicators. The results show varying degrees of success depending on which weights were adapted and how the ranks were distributed. The table below from the LoRA paper shows validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both WqWqW\\_q and WvWvW\\_v gives the best performance overall. They find the standard deviation across random seeds to be consistent for a given dataset, which they report in the first column.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/lora_which_weights.jpg)\n\n###### Key Results and Recommendations\n\n*   **Single vs. Multiple Weight Adaptation**: Adapting single weight matrices (WqWqW\\_q, WkWkW\\_k, WvWvW\\_v, or WoWoW\\_o individually) at a higher rank generally resulted in lower performance compared to adapting combinations of weights at a reduced rank. Specifically, putting all parameters in ∆WqWqW\\_q or ∆WkWkW\\_k alone did not yield optimal results.\n*   **Optimal Combination**: The combination of adapting both WqWqW\\_q and WvWvW\\_v at a rank of 4 emerged as the most effective strategy, achieving the highest validation accuracies on both datasets. This suggests a balanced approach to distributing the parameter budget across multiple types of attention weights, rather than focusing on a single type, leads to better performance.\n*   **Effectiveness of Rank Distribution**: The result indicates that a lower rank (such as 4) is sufficient to capture essential adaptations in the weights, making it preferable to spread the parameter budget across more types of weights rather than increasing the rank for fewer weights.\n\n###### Conclusion and Strategy for Applying LoRA\n\n*   Based on these findings, when applying LoRA within a limited parameter budget, it is advisable to:\n    *   **Distribute Parameters Across Multiple Weights**: Focus on adapting multiple types of attention weights (such as WqWqW\\_q and WvWvW\\_v) rather than a single type, as this approach leverages the synergistic effects of adapting multiple aspects of the attention mechanism.\n    *   **Use Lower Ranks for Multiple Weights**: Opt for a lower rank when adapting multiple weights to ensure that the parameter budget is used efficiently without compromising the ability to capture significant adaptations.\n*   This strategy maximizes the impact of the available parameters by enhancing more dimensions of the self-attention mechanism, which is crucial for the model’s ability to understand and process input data effectively across different tasks.\n\n*   **Distribute Parameters Across Multiple Weights**: Focus on adapting multiple types of attention weights (such as WqWqW\\_q and WvWvW\\_v) rather than a single type, as this approach leverages the synergistic effects of adapting multiple aspects of the attention mechanism.\n*   **Use Lower Ranks for Multiple Weights**: Opt for a lower rank when adapting multiple weights to ensure that the parameter budget is used efficiently without compromising the ability to capture significant adaptations.\n\n##### Is There a Relationship Between Setting Scaling Factor and Rank in LoRA?\n\n*   In the LoRA framework, the relationship between the scaling factor αα\\\\alpha and the rank rrr of the adaptation matrices AAA and BBB is an important consideration for tuning the model’s performance and managing how adaptations are applied to the pre-trained weights. Both αα\\\\alpha and rrr play significant roles in determining the impact of the low-rank updates on the model, and their settings can influence each other in terms of the overall effect on the model’s behavior.\n\n###### Understanding αα\\\\alpha and rrr\n\n*   **Scaling Factor αα\\\\alpha**: This parameter scales the contribution of the low-rank updates ΔW\\=BAΔW\\=BA\\\\Delta W = BA before they are applied to the original model weights WWW. It controls the magnitude of changes introduced by the adaptation, effectively modulating how aggressive or subtle the updates are.\n*   **Rank rrr**: This determines the dimensionality of the low-rank matrices AAA and BBB. The rank controls the expressiveness of the low-rank updates, with higher ranks allowing for more complex adaptations but increasing computational costs and potentially the risk of overfitting.\n\n###### Relationship and Interaction\n\n*   **Balancing Impact**: A higher rank rrr allows the model to capture more complex relationships and nuances in the adaptations, potentially leading to more significant changes to the model’s behavior. In such cases, αα\\\\alpha might be adjusted downward to temper the overall impact, ensuring that the modifications do not destabilize the model’s pre-trained knowledge excessively.\n*   **Adjusting for Subtlety**: Conversely, if the rank rrr is set lower, which constrains the flexibility and range of the updates, αα\\\\alpha may need to be increased to make the limited updates more impactful. This can help ensure that the adaptations, though less complex, are sufficient to achieve the desired performance improvements.\n*   **Experimental Tuning**: The optimal settings for αα\\\\alpha and rrr often depend on the specific task, the dataset, and the desired balance between adapting to new tasks and retaining generalizability. Experimentation and validation are typically necessary to find the best combination.\n\n###### Practical Considerations\n\n*   **Overfitting vs. Underfitting**: Higher ranks with aggressive scaling factors can lead to overfitting, especially when the model starts fitting too closely to nuances of the training data that do not generalize well. Conversely, too low a rank and/or too conservative an αα\\\\alpha might lead to underfitting, where the model fails to adapt adequately to new tasks.\n*   **Computational Efficiency**: Higher ranks increase the number of parameters and computational costs. Balancing αα\\\\alpha and rrr can help manage computational demands while still achieving meaningful model improvements.\n\n###### Conclusion\n\n*   The relationship between αα\\\\alpha and rrr in LoRA involves a delicate balance. Adjusting one can necessitate compensatory changes to the other to maintain a desired level of adaptation effectiveness without sacrificing the model’s stability or performance. Understanding how these parameters interact can significantly enhance the strategic deployment of LoRA in practical machine learning tasks.\n\n##### How Do You Determine the Optimal Rank rrr for LoRA?\n\n*   The optimal rank rrr for LoRA is influenced by the specific task and the type of weight adaptation. Based on the results reported in the paper from the experiments on the WikiSQL and MultiNLI datasets:\n    *   **For WikiSQL**:\n        *   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 70.5%.\n        *   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 73.8%.\n        *   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=4r\\=4r = 4 and r\\=8r\\=8r = 8, both achieving a validation accuracy of 74.0%.\n    *   **For MultiNLI**:\n        *   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 91.1%.\n        *   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 91.6%.\n        *   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=2r\\=2r = 2 and r\\=4r\\=4r = 4, both achieving a validation accuracy of 91.7%.\n*   The table below from the paper shows the validation accuracy on WikiSQL and MultiNLI with different rank rrr by adapting {Wq,Wv}{Wq,Wv}\\\\left\\\\{W\\_q, W\\_v\\\\right\\\\}, {Wq,Wk,Wv,Wc}{Wq,Wk,Wv,Wc}\\\\left\\\\{W\\_q, W\\_k, W\\_v, W\\_c\\\\right\\\\}, and just WqWqW\\_q for a comparison.. To our surprise, a rank as small as one suffices for adapting both WqWqW\\_q and WvWvW\\_v on these datasets while training WqWqW\\_q alone needs a larger rrr.\n\n*   **For WikiSQL**:\n    *   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 70.5%.\n    *   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 73.8%.\n    *   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=4r\\=4r = 4 and r\\=8r\\=8r = 8, both achieving a validation accuracy of 74.0%.\n*   **For MultiNLI**:\n    *   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 91.1%.\n    *   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 91.6%.\n    *   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=2r\\=2r = 2 and r\\=4r\\=4r = 4, both achieving a validation accuracy of 91.7%.\n\n*   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 70.5%.\n*   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 73.8%.\n*   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=4r\\=4r = 4 and r\\=8r\\=8r = 8, both achieving a validation accuracy of 74.0%.\n\n*   When adapting only WqWqW\\_q, the optimal rank is r\\=4r\\=4r = 4, with a validation accuracy of 91.1%.\n*   When adapting WqWqW\\_q and WvWvW\\_v, the optimal rank is r\\=8r\\=8r = 8, with a validation accuracy of 91.6%.\n*   When adapting Wq,Wk,Wv,WoWq,Wk,Wv,WoW\\_q, W\\_k, W\\_v, W\\_o, the optimal ranks are r\\=2r\\=2r = 2 and r\\=4r\\=4r = 4, both achieving a validation accuracy of 91.7%.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/lora_optimal_rank.jpg)\n\n*   In summary, while the optimal rank rrr varies depending on the dataset and the type of weight adaptation, a rank of r\\=4r\\=4r = 4 or r\\=8r\\=8r = 8 generally yields the best performance. Specifically, a rank of r\\=4r\\=4r = 4 is often sufficient for single weight types like WqWqW\\_q, and a rank of r\\=8r\\=8r = 8 is more effective for adapting multiple weight types such as WqWqW\\_q and WvWvW\\_v.\n*   However, a small rrr cannot be expected to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with r\\=dmodelr\\=dmodelr = d\\_{model}) could certainly outperform LoRA with a small rrr.\n*   In summary, selecting a rank that is too high can counteract the benefits of the low-rank adaptation by allowing the model to become overly complex and fit the training data too precisely. Conversely, choosing a rank that’s too low may limit the model’s ability to capture necessary information, leading to underfitting. Therefore, setting the rank in LoRA fine-tuning involves finding a balance: enough capacity to adapt to new data without overfitting.\n\n##### How Do LoRA Hyperparameters Interact with Each Other? is There a Relationship Between LoRA Hyperparameters?\n\n*   There is a significant relationship among the hyperparameters in the Low-Rank Adaptation (LoRA) technique, particularly how they interact and influence each other to affect the adaptation and performance of the model. Understanding the interactions between these hyperparameters is crucial for effectively tuning the model to achieve desired behaviors and performance improvements. Here’s a detailed breakdown of the primary hyperparameters in LoRA and how they are interrelated:\n    \n*   **Rank and Scaling Factor**:\n    *   Higher ranks allow AAA and BBB to capture more detailed and complex modifications. However, with increased rank, the potential for overfitting and destabilizing the original model’s behavior also rises. The scaling factor αα\\\\alpha often needs to be adjusted in response to the rank; a higher rank might require a smaller αα\\\\alpha to moderate the effect of these more complex updates.\n*   **Rank and Regularization**:\n    *   As the rank increases, the number of parameters in AAA and BBB also increases, which can lead to overfitting. Regularization becomes more critical with higher ranks to ensure that the model generalizes well and does not just memorize the training data.\n*   **Learning Rate and Scaling Factor**:\n    *   The learning rate for AAA and BBB can influence how quickly the model adapts the low-rank updates. If αα\\\\alpha is high, leading to stronger updates, a lower learning rate might be necessary to prevent training instability. Conversely, with a lower αα\\\\alpha, a higher learning rate might be feasible to ensure that the updates are sufficiently impactful.\n*   **Regularization and Learning Rate**:\n    *   Regularization settings might need adjustment based on the learning rate. A higher learning rate can cause larger updates, which might increase the risk of overfitting unless balanced by stronger regularization.\n\nThere is a significant relationship among the hyperparameters in the Low-Rank Adaptation (LoRA) technique, particularly how they interact and influence each other to affect the adaptation and performance of the model. Understanding the interactions between these hyperparameters is crucial for effectively tuning the model to achieve desired behaviors and performance improvements. Here’s a detailed breakdown of the primary hyperparameters in LoRA and how they are interrelated:\n\n*   Higher ranks allow AAA and BBB to capture more detailed and complex modifications. However, with increased rank, the potential for overfitting and destabilizing the original model’s behavior also rises. The scaling factor αα\\\\alpha often needs to be adjusted in response to the rank; a higher rank might require a smaller αα\\\\alpha to moderate the effect of these more complex updates.\n\n*   As the rank increases, the number of parameters in AAA and BBB also increases, which can lead to overfitting. Regularization becomes more critical with higher ranks to ensure that the model generalizes well and does not just memorize the training data.\n\n*   The learning rate for AAA and BBB can influence how quickly the model adapts the low-rank updates. If αα\\\\alpha is high, leading to stronger updates, a lower learning rate might be necessary to prevent training instability. Conversely, with a lower αα\\\\alpha, a higher learning rate might be feasible to ensure that the updates are sufficiently impactful.\n\n*   Regularization settings might need adjustment based on the learning rate. A higher learning rate can cause larger updates, which might increase the risk of overfitting unless balanced by stronger regularization.\n\n###### Practical Considerations\n\n*   **Tuning Strategy**:\n    *   Tuning these hyperparameters requires careful experimentation and validation. Often, changes to one parameter necessitate adjustments to others to maintain a balanced and effective training regime.\n*   **Trade-offs**:\n    *   There are trade-offs between model flexibility, training stability, computational efficiency, and the risk of overfitting. Effective management of LoRA’s hyperparameters is key to navigating these trade-offs.\n*   **Application-Specific Adjustments**:\n    *   Depending on the specific requirements of the task and characteristics of the data, the optimal settings for these hyperparameters can vary significantly. Task-specific performance metrics and validation are essential to guide these adjustments.\n*   In summary, understanding and managing the relationships between these LoRA hyperparameters enables practitioners to finely tune their models to specific tasks without extensive retraining while leveraging pre-trained model architectures efficiently.\n\n*   Tuning these hyperparameters requires careful experimentation and validation. Often, changes to one parameter necessitate adjustments to others to maintain a balanced and effective training regime.\n\n*   There are trade-offs between model flexibility, training stability, computational efficiency, and the risk of overfitting. Effective management of LoRA’s hyperparameters is key to navigating these trade-offs.\n\n*   Depending on the specific requirements of the task and characteristics of the data, the optimal settings for these hyperparameters can vary significantly. Task-specific performance metrics and validation are essential to guide these adjustments.\n\n##### Why Does a Higher Rank Make It the Easier to Overfit?\n\n*   In LoRA-based fine-tuning, a higher rank can indeed lead to easier overfitting. To understand why, let’s break down the mechanics of LoRA and how rank affects model capacity and overfitting.\n*   The **rank** in LoRA determines the dimensions of these additional matrices, effectively controlling their capacity to capture information:\n    *   **Low Rank**: Small matrices that can represent only limited information.\n    *   **High Rank**: Larger matrices with greater capacity to capture complex patterns.\n*   In mathematical terms, a higher rank means more degrees of freedom in the low-rank matrices, allowing them to approximate more complex relationships in the data.\n    \n*   Here’s why a higher rank increases overfitting in LoRA:\n    1.  **Increased Capacity to Capture Training Noise**: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.\n        \n    2.  **Less Regularization Effect**: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.\n        \n    3.  **Reduced Ability to Generalize**: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.\n        \n    4.  **Higher Variance in Learned Features**: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.\n        \n\n*   **Low Rank**: Small matrices that can represent only limited information.\n*   **High Rank**: Larger matrices with greater capacity to capture complex patterns.\n\nIn mathematical terms, a higher rank means more degrees of freedom in the low-rank matrices, allowing them to approximate more complex relationships in the data.\n\n1.  **Increased Capacity to Capture Training Noise**: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.\n    \n2.  **Less Regularization Effect**: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.\n    \n3.  **Reduced Ability to Generalize**: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.\n    \n4.  **Higher Variance in Learned Features**: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.\n    \n\n**Increased Capacity to Capture Training Noise**: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.\n\n**Less Regularization Effect**: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.\n\n**Reduced Ability to Generalize**: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.\n\n**Higher Variance in Learned Features**: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.\n\n##### Does LoRA Adapt Weights in All Layers?\n\n*   LoRA does not typically adapt weights across all layers of a neural network; instead, it targets specific layers, often the **attention layers** in large LLMs. This selective adaptation is a design choice aimed at balancing the effectiveness of fine-tuning with computational efficiency and minimizing the risk of overfitting. By modifying only key layers, like attention layers, LoRA efficiently focuses on layers where task-specific information is most impactful while preserving the general-purpose features of the lower layers.\n    \n*   **Layers Typically Adapted in LoRA**:\n    \n*   In the original [LoRA implementation](https://arxiv.org/abs/2106.09685):\n    1.  **Attention Layers**: LoRA primarily targets attention layers (such as the query and value projections in transformers) because they play a critical role in capturing contextual information. By adapting only these layers, LoRA can achieve significant task-specific improvements without needing to modify the entire model.\n    2.  **Few Additional Layers (if necessary)**: Sometimes, LoRA may extend adaptation to a few other layers (like feed-forward layers in transformers) if the new task requires it. However, this is usually done with caution to avoid overfitting and to keep the parameter footprint low.\n*   **Why not all layers?**:\n    \n    1.  **Computational Efficiency**: Adapting all layers would introduce a large number of low-rank matrices throughout the model, greatly increasing the memory and computation requirements, which LoRA is specifically designed to reduce.\n    2.  **Risk of Overfitting**: Adapting all layers, especially the lower (more general) layers, could lead the model to overfit to the fine-tuning dataset, particularly if the dataset is small. Lower layers tend to capture general features, and adapting them might make the model too specialized, losing generalization ability.\n    3.  **Focus on Task-Specific Information**: The upper (or top) layers of a model generally capture task-specific features, while lower layers handle more general, reusable features. LoRA’s selective adaptation focuses on adjusting only those layers where task-specific learning is most beneficial.\n\nLoRA does not typically adapt weights across all layers of a neural network; instead, it targets specific layers, often the **attention layers** in large LLMs. This selective adaptation is a design choice aimed at balancing the effectiveness of fine-tuning with computational efficiency and minimizing the risk of overfitting. By modifying only key layers, like attention layers, LoRA efficiently focuses on layers where task-specific information is most impactful while preserving the general-purpose features of the lower layers.\n\n**Layers Typically Adapted in LoRA**:\n\n1.  **Attention Layers**: LoRA primarily targets attention layers (such as the query and value projections in transformers) because they play a critical role in capturing contextual information. By adapting only these layers, LoRA can achieve significant task-specific improvements without needing to modify the entire model.\n2.  **Few Additional Layers (if necessary)**: Sometimes, LoRA may extend adaptation to a few other layers (like feed-forward layers in transformers) if the new task requires it. However, this is usually done with caution to avoid overfitting and to keep the parameter footprint low.\n\n**Why not all layers?**:\n\n1.  **Computational Efficiency**: Adapting all layers would introduce a large number of low-rank matrices throughout the model, greatly increasing the memory and computation requirements, which LoRA is specifically designed to reduce.\n2.  **Risk of Overfitting**: Adapting all layers, especially the lower (more general) layers, could lead the model to overfit to the fine-tuning dataset, particularly if the dataset is small. Lower layers tend to capture general features, and adapting them might make the model too specialized, losing generalization ability.\n3.  **Focus on Task-Specific Information**: The upper (or top) layers of a model generally capture task-specific features, while lower layers handle more general, reusable features. LoRA’s selective adaptation focuses on adjusting only those layers where task-specific learning is most beneficial.\n\n###### Does LoRA Impact Lower Attention Layers Less Than Higher Attention Layers?\n\n*   Yes, in practice, LoRA impacts higher attention layers more than lower ones. This is because LoRA selectively adapts layers, targeting the task-specific adaptability of higher layers while preserving the general-purpose features in lower layers. This design enables effective task adaptation with minimal overfitting, allowing the model to retain broad applicability.\n    \n*   **Why higher attention layers are more affected:**\n    \n    1.  **Function of Higher Attention Layers**: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.\n        \n    2.  **Less Impact on Lower Layers**: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.\n        \n    3.  **LoRA’s Selective Impact**: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.\n        \n    4.  **Regularization Effect in Lower Layers**: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.\n        \n*   **Practical Implications:**\n    \n*   In many cases, fine-tuning with LoRA results in:\n    \n    *   **Major adjustments** to higher layers, allowing the model to learn specific features of the fine-tuning task.\n    *   **Minimal impact** on lower layers, preserving general knowledge from pre-training and preventing overfitting.\n\nYes, in practice, LoRA impacts higher attention layers more than lower ones. This is because LoRA selectively adapts layers, targeting the task-specific adaptability of higher layers while preserving the general-purpose features in lower layers. This design enables effective task adaptation with minimal overfitting, allowing the model to retain broad applicability.\n\n**Why higher attention layers are more affected:**\n\n1.  **Function of Higher Attention Layers**: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.\n    \n2.  **Less Impact on Lower Layers**: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.\n    \n3.  **LoRA’s Selective Impact**: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.\n    \n4.  **Regularization Effect in Lower Layers**: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.\n    \n\n**Function of Higher Attention Layers**: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.\n\n**Less Impact on Lower Layers**: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in LLMs, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.\n\n**LoRA’s Selective Impact**: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.\n\n**Regularization Effect in Lower Layers**: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.\n\n**Practical Implications:**\n\nIn many cases, fine-tuning with LoRA results in:\n\n*   **Major adjustments** to higher layers, allowing the model to learn specific features of the fine-tuning task.\n*   **Minimal impact** on lower layers, preserving general knowledge from pre-training and preventing overfitting.\n\n#### Quantized Low-Rank Adaptation (QLoRA)\n\n*   Proposed in [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) by Dettmers et al. from the University of Washington.\n*   QLoRA is an efficient finetuning method that enables training very large models—up to 65B parameters—on a single 48GB GPU, without loss of accuracy compared to full 16-bit finetuning. It does this by **backpropagating through a frozen, 4-bit quantized pretrained model** into **LoRA Adapters**.\n*   In short, QLoRA combines LoRA’s parameter-efficient adaptation with two key quantization innovations—**4-bit NormalFloat (`NF4`)** and **Double Quantization (DQ)**—plus **paged optimizers** to handle memory spikes. This combination greatly reduces GPU memory usage while preserving performance.\n*   For a practical introduction to QLoRA, see this Hugging Face blog on [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes).\n\n##### High-level Process\n\n1.  Quantize the pretrained model weights to 4-bit and freeze them.\n2.  Attach small, trainable LoRA adapters throughout the transformer layers.\n3.  Fine-tune only these adapters while the quantized base model remains frozen.\n\n*   The figure below from the paper shows different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\n\n![](../../../images/papers/QLoRA.jpg)\n\n*   The figure below ([source](https://www.linkedin.com/in/mary-newhauser/)) compares full fine-tuning, LoRA, and QLoRA:\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/FFT-LoRA-QLoRA.jpg)\n\n##### Key Components\n\n1.  **Low-Rank Adaptation (LoRA)**:\n    \n    *   Inserts small rank-decomposed trainable matrices into all of a base model’s layers. Put simply, the LoRA adapters are applied to both the **self-attention module** (including query, key, value, and output projections)–as in vanilla LoRA–and the **feed-forward network (MLP)** layers inside each transformer block. This broader placement was found to be critical for matching the performance of full 16-bit finetuning.\n    *   Only these adapters are updated; the large frozen weight matrices are never modified.\n2.  **4-bit NormalFloat (`NF4`) Quantization**:\n    \n    *   `NF4` is an **information-theoretically optimal quantization scheme** for values drawn from a zero-centered normal distribution (common for pretrained LLM weights).\n    *   It is based on quantile quantization, ensuring each bin contains an equal probability mass, minimizing quantization error.\n    *   `NF4` avoids expensive per-tensor quantile estimation by exploiting the fact that all weights can be scaled to match a fixed N(0,1)N(0,1)N(0,1) distribution, allowing the use of precomputed quantization bin edges.\n    *   The quantization is **asymmetric with an exact zero representation**, which is crucial for padding/sparse elements, and means that only a **scale value** (no zero-point) needs to be stored for each block.\n    *   Compared to regular 4-bit floats (`FP4`), `NF4` yields lower perplexity and higher accuracy across benchmarks while keeping the same memory footprint.\n3.  **Double Quantization (DQ)**:\n    \n    *   In block-wise quantization, each block of weights has an associated **quantization constant (scale)** — but no separate zero-point, since `NF4`’s asymmetric codebook already contains an exact zero representation. These scale values are the only per-block metadata stored alongside the quantized weights, and in QLoRA’s DQ scheme, they are themselves quantized to further reduce memory overhead. For small block sizes (e.g., 64), storing these constants can be a large relative memory cost.\n    *   DQ reduces this overhead by **quantizing the quantization constants themselves**.\n    *   The first quantization maps `FP32` weights → `NF4` values with per-block `FP32` scales.\n    *   The second quantization maps these scales into `FP8` (block size 256), plus a second set of `FP32` scales for dequantization.\n    *   This reduces quantization constant storage from 0.5 bits per parameter to 0.127 bits per parameter—a 0.373 bit saving per parameter—without measurable performance loss.\n4.  **Paged Optimizers**:\n    \n    *   Uses NVIDIA Unified Memory to automatically swap optimizer states between CPU RAM and GPU memory when GPU memory is full during large sequence processing.\n    *   This avoids out-of-memory errors without slowing down training under typical sequence lengths.\n\n**Low-Rank Adaptation (LoRA)**:\n\n*   Inserts small rank-decomposed trainable matrices into all of a base model’s layers. Put simply, the LoRA adapters are applied to both the **self-attention module** (including query, key, value, and output projections)–as in vanilla LoRA–and the **feed-forward network (MLP)** layers inside each transformer block. This broader placement was found to be critical for matching the performance of full 16-bit finetuning.\n*   Only these adapters are updated; the large frozen weight matrices are never modified.\n\n**4-bit NormalFloat (`NF4`) Quantization**:\n\n*   `NF4` is an **information-theoretically optimal quantization scheme** for values drawn from a zero-centered normal distribution (common for pretrained LLM weights).\n*   It is based on quantile quantization, ensuring each bin contains an equal probability mass, minimizing quantization error.\n*   `NF4` avoids expensive per-tensor quantile estimation by exploiting the fact that all weights can be scaled to match a fixed N(0,1)N(0,1)N(0,1) distribution, allowing the use of precomputed quantization bin edges.\n*   The quantization is **asymmetric with an exact zero representation**, which is crucial for padding/sparse elements, and means that only a **scale value** (no zero-point) needs to be stored for each block.\n*   Compared to regular 4-bit floats (`FP4`), `NF4` yields lower perplexity and higher accuracy across benchmarks while keeping the same memory footprint.\n\n**Double Quantization (DQ)**:\n\n*   In block-wise quantization, each block of weights has an associated **quantization constant (scale)** — but no separate zero-point, since `NF4`’s asymmetric codebook already contains an exact zero representation. These scale values are the only per-block metadata stored alongside the quantized weights, and in QLoRA’s DQ scheme, they are themselves quantized to further reduce memory overhead. For small block sizes (e.g., 64), storing these constants can be a large relative memory cost.\n*   DQ reduces this overhead by **quantizing the quantization constants themselves**.\n*   The first quantization maps `FP32` weights → `NF4` values with per-block `FP32` scales.\n*   The second quantization maps these scales into `FP8` (block size 256), plus a second set of `FP32` scales for dequantization.\n*   This reduces quantization constant storage from 0.5 bits per parameter to 0.127 bits per parameter—a 0.373 bit saving per parameter—without measurable performance loss.\n\n**Paged Optimizers**:\n\n*   Uses NVIDIA Unified Memory to automatically swap optimizer states between CPU RAM and GPU memory when GPU memory is full during large sequence processing.\n*   This avoids out-of-memory errors without slowing down training under typical sequence lengths.\n\n##### Operation\n\n*   In a single linear layer with LoRA in QLoRA:\n    \n    *   Stored weight format: **`NF4` quantized base model weights**, plus LoRA adapter weights in `BF16`.\n    *   Computation:\n        *   Dequantize `NF4` weights (double dequantization if DQ is used) into `BF16` on the fly.\n        *   Perform matrix multiplications with the dequantized weights plus LoRA projections.\n        *   Backpropagate gradients **only into LoRA parameters**; no gradient storage for the base weights.\n    \n    > In QLoRA, only the original model’s weights are quantized to `NF4`. The LoRA adapter weights remain in higher precision (`BF16`) and are the only parameters updated during finetuning.\n    \n*   Formally:\n    \n    1.  **Forward pass with double dequantization**:\n        \n        *   WNF4−→−DQWBF16WNF4→DQWBF16\n            \n            W\\_{\\\\text{NF4}} \\\\xrightarrow{\\\\text{DQ}} W\\_{\\\\text{BF16}}\n        *   YBF16\\=XBF16⋅WBF16+XBF16L1L2YBF16\\=XBF16⋅WBF16+XBF16L1L2\n            \n            Y\\_{\\\\text{BF16}} = X\\_{\\\\text{BF16}} \\\\cdot W\\_{\\\\text{BF16}} + X\\_{\\\\text{BF16}} L\\_1 L\\_2\n    2.  **Backward pass**:\n        \n        *   Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.\n\nIn a single linear layer with LoRA in QLoRA:\n\n*   Stored weight format: **`NF4` quantized base model weights**, plus LoRA adapter weights in `BF16`.\n*   Computation:\n    *   Dequantize `NF4` weights (double dequantization if DQ is used) into `BF16` on the fly.\n    *   Perform matrix multiplications with the dequantized weights plus LoRA projections.\n    *   Backpropagate gradients **only into LoRA parameters**; no gradient storage for the base weights.\n\n*   Dequantize `NF4` weights (double dequantization if DQ is used) into `BF16` on the fly.\n*   Perform matrix multiplications with the dequantized weights plus LoRA projections.\n*   Backpropagate gradients **only into LoRA parameters**; no gradient storage for the base weights.\n\n> In QLoRA, only the original model’s weights are quantized to `NF4`. The LoRA adapter weights remain in higher precision (`BF16`) and are the only parameters updated during finetuning.\n\nIn QLoRA, only the original model’s weights are quantized to `NF4`. The LoRA adapter weights remain in higher precision (`BF16`) and are the only parameters updated during finetuning.\n\nFormally:\n\n1.  **Forward pass with double dequantization**:\n    \n    *   WNF4−→−DQWBF16WNF4→DQWBF16\n        \n        W\\_{\\\\text{NF4}} \\\\xrightarrow{\\\\text{DQ}} W\\_{\\\\text{BF16}}\n    *   YBF16\\=XBF16⋅WBF16+XBF16L1L2YBF16\\=XBF16⋅WBF16+XBF16L1L2\n        \n        Y\\_{\\\\text{BF16}} = X\\_{\\\\text{BF16}} \\\\cdot W\\_{\\\\text{BF16}} + X\\_{\\\\text{BF16}} L\\_1 L\\_2\n2.  **Backward pass**:\n    \n    *   Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.\n\n**Forward pass with double dequantization**:\n\n*   WNF4−→−DQWBF16WNF4→DQWBF16\n    \n    W\\_{\\\\text{NF4}} \\\\xrightarrow{\\\\text{DQ}} W\\_{\\\\text{BF16}}\n*   YBF16\\=XBF16⋅WBF16+XBF16L1L2YBF16\\=XBF16⋅WBF16+XBF16L1L2\n    \n    Y\\_{\\\\text{BF16}} = X\\_{\\\\text{BF16}} \\\\cdot W\\_{\\\\text{BF16}} + X\\_{\\\\text{BF16}} L\\_1 L\\_2\n\n**Backward pass**:\n\n*   Compute derivatives w.r.t. LoRA parameters only, while still dequantizing base weights for intermediate computations.\n\n##### Impact and Results\n\n*   Guanaco, the best-performing QLoRA model family, achieves 99.3% of ChatGPT’s score on the Vicuna benchmark after 24 hours of finetuning on a single GPU.\n*   QLoRA’s memory optimizations—`NF4`, double quantization, and paged optimizers—enable training 33B parameter models on 24GB consumer GPUs and 65B models on a single 48GB GPU.\n*   Across GLUE, Super-NaturalInstructions, and MMLU benchmarks, `NF4` + DQ matches or exceeds full 16-bit LoRA and full finetuning results, outperforming `FP4` and `INT4` quantization.\n*   This makes large-scale finetuning accessible for small teams and researchers without massive compute clusters.\n\n#### Quantization-Aware Low-Rank Adaptation (QA-LoRA)\n\n*   Proposed in [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large LLMs](https://arxiv.org/abs/2309.14717).\n*   Xu et al. from Huawei propose Quantization-Aware Low-Rank Adaptation (QA-LoRA), which jointly performs parameter-efficient fine-tuning and low-bit quantization. Unlike prior methods, QA-LoRA ensures that the fine-tuned weights remain in low-bit form after training, eliminating the need for post-training quantization (PTQ) — a step that typically introduces accuracy degradation at very low bit widths such as `INT3` or `INT2`. In standard approaches like LoRA or QLoRA, fine-tuning ends with full-precision weights (e.g., `FP16`). Deploying in low precision then requires PTQ, which compresses the model **after** it has been optimized in high precision, leading to significant quantization error and loss in accuracy for low-bit formats. QA-LoRA avoids this by training directly in the target low-bit format in a quantization-aware manner, so the model is deployment-ready without further quantization steps.\n*   Put simply, QA-LoRA extends QLoRA’s idea but removes its main deployment bottleneck (FP16 fallback) while improving accuracy at lower bit widths, all with minimal implementation overhead. The merged weights W′\\=W̃ +sABW′\\=W~+sABW' = \\\\tilde{W} + sAB remain quantized throughout fine-tuning, meaning inference can run natively on low-bit integer kernels without precision fallback.\n*   The motivation comes from an **imbalance between the Degrees of Freedom (DoF) of quantization and adaptation**: traditional column-wise quantization assigns one scaling and zero factor per column (low DoF) but uses many LoRA parameters per column (high DoF). This can lead to large quantization errors and difficulty in merging LoRA and base weights in low precision. QA-LoRA solves this by using **group-wise quantization and group-shared LoRA parameters**, increasing quantization DoF and reducing adaptation DoF in a balanced way. By balancing quantization and adaptation degrees of freedom via group-wise design, it offers low-bit fine-tuning and efficient deployment for LLMs on both server and edge environments.\n*   Code is available at [GitHub](https://github.com/yuhuixu1993/qa-lora).\n\n##### Key Ideas and Contributions\n\n1.  **Group-wise quantization:** Partition each column of the pre-trained weight matrix into LLL groups. Each group has its own scaling (αα\\\\alpha) and zero (ββ\\\\beta) factors, enabling finer quantization granularity and reducing quantization error.\n2.  **Group-wise LoRA:** Within each group, all rows of the LoRA AAA matrix share the same values. This is implemented via a parameter-free summation (average pooling) over the input vector, reducing its dimension from DinDinD\\_{in} to LLL before applying LoRA.\n3.  **Mergeability in low-bit form:** By aligning the grouping in quantization and LoRA, the merged weights W′\\=W̃ +sABW′\\=W~+sABW' = \\\\tilde{W} + sAB can remain in `INT4`/`INT3`/`INT2` format, enabling fast inference without `FP16` fallback.\n4.  **Efficient operators:** QA-LoRA uses standard integer formats (e.g., `INT4`) with CUDA-optimized kernels, avoiding the lack of operator-level acceleration for `NF4` (used in QLoRA).\n\n##### Implementation Details\n\n*   **Minimal code change:** Implemented by inserting a few lines into LoRA’s forward pass. The pseudocode in the paper shows:\n    \n    *   Pre-quantization of weights with group-wise scaling and zero factors.\n    *   Pooling (QA) over input groups before applying LoRA AAA and BBB matrices.\n    *   Adjusting ββ\\\\beta factors in the merge step to incorporate LoRA updates without leaving the low-bit domain.\n*   Quantization uses GPTQ with `group_size = 32` by default, asymmetric quantization, `act_order = false`, and `true_sequential = true`.\n    \n*   The following figure illustrates QA-LoRA’s design. Compared to LoRA and QLoRA, QA-LoRA is efficient in both fine-tuning and inference, without accuracy loss from PTQ. While `INT4` is shown, QA-LoRA generalizes to `INT3` and `INT2`.\n    \n\n**Minimal code change:** Implemented by inserting a few lines into LoRA’s forward pass. The pseudocode in the paper shows:\n\n*   Pre-quantization of weights with group-wise scaling and zero factors.\n*   Pooling (QA) over input groups before applying LoRA AAA and BBB matrices.\n*   Adjusting ββ\\\\beta factors in the merge step to incorporate LoRA updates without leaving the low-bit domain.\n\nQuantization uses GPTQ with `group_size = 32` by default, asymmetric quantization, `act_order = false`, and `true_sequential = true`.\n\nThe following figure illustrates QA-LoRA’s design. Compared to LoRA and QLoRA, QA-LoRA is efficient in both fine-tuning and inference, without accuracy loss from PTQ. While `INT4` is shown, QA-LoRA generalizes to `INT3` and `INT2`.\n\n![](../../../images/papers/QA-LoRA.jpg)\n\n*   **Fine-tuning settings:**\n    \n    *   **Datasets:** Alpaca (52K) and FLAN v2 subset (320K).\n    *   **Optimizer:** Paged AdamW, η\\=2×10−5η\\=2×10−5\\\\eta = 2 \\\\times 10^{-5} (LLaMA-7B/13B) or η\\=1×10−5η\\=1×10−5\\\\eta = 1 \\\\times 10^{-5} (LLaMA-33B/65B), `max_grad_norm = 0.3`, `batch_size = 16`.\n    *   **Steps:** 10K (Alpaca), 20K (FLAN v2).\n    *   **Hardware:** Tesla V100 GPUs (1 GPU for ≤33B models, 2 GPUs for 65B).\n\n**Fine-tuning settings:**\n\n*   **Datasets:** Alpaca (52K) and FLAN v2 subset (320K).\n*   **Optimizer:** Paged AdamW, η\\=2×10−5η\\=2×10−5\\\\eta = 2 \\\\times 10^{-5} (LLaMA-7B/13B) or η\\=1×10−5η\\=1×10−5\\\\eta = 1 \\\\times 10^{-5} (LLaMA-33B/65B), `max_grad_norm = 0.3`, `batch_size = 16`.\n*   **Steps:** 10K (Alpaca), 20K (FLAN v2).\n*   **Hardware:** Tesla V100 GPUs (1 GPU for ≤33B models, 2 GPUs for 65B).\n\n##### Algorithm Steps\n\n1.  **Group-wise quantization of base weights** into `INT4/3/2` at fine-tuning start.\n2.  **Group-wise LoRA pooling and adaptation** — only L×DintL×DintL \\\\times D\\_{int} parameters for AAA instead of Din×DintDin×DintD\\_{in} \\\\times D\\_{int}.\n3.  **Fine-tuning** LoRA parameters while keeping base weights quantized.\n4.  **Merging** LoRA and base weights in the quantized domain by adjusting ββ\\\\beta factors directly.\n\n##### Advantages Over Prior Work\n\n*   **Compared to LoRA:** Lower fine-tuning memory and faster inference due to quantization.\n*   **Compared to QLoRA:**\n    *   Avoids `FP16` fallback after merging.\n    *   Uses CUDA-optimized INT formats instead of `NF4`, yielding >50% faster inference.\n    *   Higher accuracy at low bit widths (especially `INT2`/`INT3`) due to quantization-aware adaptation.\n\n*   Avoids `FP16` fallback after merging.\n*   Uses CUDA-optimized INT formats instead of `NF4`, yielding >50% faster inference.\n*   Higher accuracy at low bit widths (especially `INT2`/`INT3`) due to quantization-aware adaptation.\n\n##### Results\n\n*   On LLaMA and Llama 2 models, QA-LoRA matches or exceeds QLoRA’s accuracy at `INT4` and significantly outperforms it at `INT3` and `INT2`.\n*   Training time reduction: For LLaMA-13B, fine-tuning time drops from 73.1h (QLoRA) to 29.5h (QA-LoRA).\n*   Commonsense QA tasks show consistent improvements, with QA-LoRA (2-bit) achieving +15% accuracy over QLoRA (2-bit) with PTQ.\n\n##### Comparison of LoRA, QLoRA, and QA-LoRA\n\n*   The table below summarizes key differences in methodology, quantization formats, fine-tuning characteristics, inference properties, and practical considerations for deployment for LoRA, QLoRA, and QA-LoRA.\n\n**Aspect**\n\n**LoRA  \n(Hu et al., 2021)**\n\n**QLoRA  \n(Dettmers et al., 2023)**\n\n**QA-LoRA  \n(Xu et al., 2023)**\n\nPrimary Goal\n\nParameter-efficient fine-tuning (reduce trainable parameters while preserving accuracy)\n\nCombine low-bit quantization and LoRA to reduce fine-tuning memory\n\nJoint low-bit quantization and LoRA with mergeability in quantized form for both fine-tuning and inference\n\nBase Weight Precision During Fine-Tuning\n\n`FP16` or `FP32`\n\n`NF4` (NormalFloat4, 4-bit floating point)\n\n`INT4, INT3, or INT2` (integer quantization) with group-wise scaling\n\nLoRA Weight Precision During Fine-Tuning\n\n`FP16`/`FP32`\n\n`FP16`\n\nSame precision as base weight (`INT4/3/2`) due to quantization-aware adaptation\n\nAdaptation Method\n\nLow-rank matrices A and B added to frozen base weights\n\nSame as LoRA but applied to NF4-quantized base weights\n\nGroup-wise LoRA (shared parameters within quantization groups) for mergeability in low-bit domain\n\nQuantization Granularity\n\nN/A\n\nColumn-wise `NF4` quantization of base weights\n\nGroup-wise integer quantization (e.g., group size 32) with group-specific scale and zero\n\nPost-Fine-Tuning Merge Result\n\nFull-precision model (merging A and B into W yields `FP16`/`FP32`)\n\nFull-precision model (merging requires `FP16` fallback) unless PTQ is applied (accuracy loss at low bits)\n\nFully quantized merged model, no need for PTQ, no accuracy loss compared to full precision\n\nInference Precision\n\n`FP16`/`FP32`\n\n`FP16` (without PTQ) or `INT4` (with PTQ, accuracy drop)\n\n`INT4/3/2`, same as during training\n\nHardware Efficiency\n\nReduced training memory vs. full fine-tuning; no change in inference speed\n\nReduced training memory; inference slower without PTQ due to `FP16` fallback; NF4 has no widespread hardware acceleration\n\nReduced training memory; inference fast due to native integer kernels; compatible with existing INT quantization acceleration\n\nAccuracy at Low Bit Widths\n\nHigh (since no quantization)\n\nGood at 4-bit `NF4`, but drops significantly with `INT4` PTQ, and worse at 3-bit/2-bit\n\nMatches QLoRA at 4-bit `NF4`; outperforms significantly at INT3 and INT2\n\nImplementation Complexity\n\nModerate; widely supported in libraries\n\nModerate; requires NF4 quantization and mixed precision handling\n\nLow; a few extra lines on top of LoRA, uses standard INT formats and group pooling\n\nRepresentative Use Cases\n\nPEFT when quantization is not needed or when deployment can afford `FP16`\n\nFine-tuning large models under memory constraints, where deployment can use `FP16` or 4-bit `NF4`\n\nFine-tuning and deploying large models directly in `INT4/3/2` on resource-limited hardware\n\n**Aspect**\n\n**LoRA  \n(Hu et al., 2021)**\n\n**QLoRA  \n(Dettmers et al., 2023)**\n\n**QA-LoRA  \n(Xu et al., 2023)**\n\nPrimary Goal\n\nParameter-efficient fine-tuning (reduce trainable parameters while preserving accuracy)\n\nCombine low-bit quantization and LoRA to reduce fine-tuning memory\n\nJoint low-bit quantization and LoRA with mergeability in quantized form for both fine-tuning and inference\n\nBase Weight Precision During Fine-Tuning\n\n`FP16` or `FP32`\n\n`NF4` (NormalFloat4, 4-bit floating point)\n\n`INT4, INT3, or INT2` (integer quantization) with group-wise scaling\n\nLoRA Weight Precision During Fine-Tuning\n\n`FP16`/`FP32`\n\n`FP16`\n\nSame precision as base weight (`INT4/3/2`) due to quantization-aware adaptation\n\nAdaptation Method\n\nLow-rank matrices A and B added to frozen base weights\n\nSame as LoRA but applied to NF4-quantized base weights\n\nGroup-wise LoRA (shared parameters within quantization groups) for mergeability in low-bit domain\n\nQuantization Granularity\n\nN/A\n\nColumn-wise `NF4` quantization of base weights\n\nGroup-wise integer quantization (e.g., group size 32) with group-specific scale and zero\n\nPost-Fine-Tuning Merge Result\n\nFull-precision model (merging A and B into W yields `FP16`/`FP32`)\n\nFull-precision model (merging requires `FP16` fallback) unless PTQ is applied (accuracy loss at low bits)\n\nFully quantized merged model, no need for PTQ, no accuracy loss compared to full precision\n\nInference Precision\n\n`FP16`/`FP32`\n\n`FP16` (without PTQ) or `INT4` (with PTQ, accuracy drop)\n\n`INT4/3/2`, same as during training\n\nHardware Efficiency\n\nReduced training memory vs. full fine-tuning; no change in inference speed\n\nReduced training memory; inference slower without PTQ due to `FP16` fallback; NF4 has no widespread hardware acceleration\n\nReduced training memory; inference fast due to native integer kernels; compatible with existing INT quantization acceleration\n\nAccuracy at Low Bit Widths\n\nHigh (since no quantization)\n\nGood at 4-bit `NF4`, but drops significantly with `INT4` PTQ, and worse at 3-bit/2-bit\n\nMatches QLoRA at 4-bit `NF4`; outperforms significantly at INT3 and INT2\n\nImplementation Complexity\n\nModerate; widely supported in libraries\n\nModerate; requires NF4 quantization and mixed precision handling\n\nLow; a few extra lines on top of LoRA, uses standard INT formats and group pooling\n\nRepresentative Use Cases\n\nPEFT when quantization is not needed or when deployment can afford `FP16`\n\nFine-tuning large models under memory constraints, where deployment can use `FP16` or 4-bit `NF4`\n\nFine-tuning and deploying large models directly in `INT4/3/2` on resource-limited hardware\n\n#### [Refined Low-Rank Adaptation (ReLoRA)](https://arxiv.org/abs/2307.05695)\n\n*   Proposed in [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates](https://arxiv.org/abs/2307.05695) by Lialin et al. from UMass Lowell.\n*   Refined Low-Rank Adaptation (ReLoRA) is a low-rank training technique as an alternative approach to training large neural networks. ReLoRA utilizes low-rank updates to train high-rank networks. Put simply, they explore whether LoRA can be used for pretraining (as opposed to finetuning) LLMs in a parameter-efficient manner.\n*   They apply ReLoRA to pre-training transformer LLMs with up to 350M parameters and demonstrate comparable performance to regular neural network training.\n*   Furthermore, they observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Their findings shed light on the potential of low-rank training techniques and their implications for scaling laws.\n*   A caveat worth mentioning is that the researchers only pretrained models up to 350 M parameters for now (the smallest Llama 2 model is 7B parameters, for comparison).\n*   The following figure [(source)](https://www.linkedin.com/in/sebastianraschka/) presents an overview of their results:\n\n![](../../../images/papers/relora.webp)\n\n#### [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285)\n\n*   This paper by Sheng et al. from UC Berkeley, Stanford, and Shanghai Jiao Tong focuses on the scalable serving of LoRA (Low-Rank Adaptation) adapters for large LLMs (LLMs).\n*   The “pretrain-then-finetune” paradigm, widely adopted in deploying LLMs, leads to numerous fine-tuned variants, presenting significant opportunities for batched inference during serving. The paper introduces S-LoRA, a system designed for this purpose.\n*   S-LoRA addresses memory management challenges by storing all adapters in main memory and fetching them to GPU memory as needed. The system employs Unified Paging, a unified memory pool managing dynamic adapter weights and KV cache tensors, to reduce memory fragmentation and I/O overhead.\n*   The paper presents a novel tensor parallelism strategy and customized CUDA kernels for efficient heterogeneous batching of LoRA computations, enabling the serving of thousands of adapters on a single or multiple GPUs with minimal overhead.\n*   The following image from the paper shows separated batched computation for the base model and LoRA computation. The batched computation of the base model is implemented by GEMM. The batched computation for LoRA adapters is implemented by custom CUDA kernels which support batching various sequence lengths and adapter ranks.\n\n![](../../../images/papers/SLoRA_1.jpg)\n\n*   The following image from the paper shows an overview of memory allocation in S-LoRA. S-LoRA stores all adapters in the main memory and fetches the active adapters for the current batch to the GPU memory. The GPU memory is used to store the KV cache, adapter weights, base model weights, and other temporary tensors.\n\n![](../../../images/papers/SLoRA_2.jpg)\n\n*   S-LoRA’s performance is evaluated against state-of-the-art libraries like Weights PEFT and vLLM, showing up to 4 times higher throughput and the capability to serve significantly more adapters.\n*   The system is effective in reducing the training and communication costs in Federated Learning, making it a promising approach for deploying large LLMs in resource-constrained environments.\n*   This paper contributes significantly to the field of machine learning by presenting a novel and efficient method for serving a large number of LoRA adapters, a crucial aspect in the deployment of large-scale LLMs.\n*   [Code](https://github.com/S-LoRA/S-LoRA)\n\n##### [Predibase](https://predibase.com/)\n\n*   Similar to S-LoRA, [Predibase](https://predibase.com/), a startup, offers a unique serving infrastructure – [LoRAX](https://github.com/predibase/lorax) – which lets you cost-effectively serve many fine-tuned adapters on a single GPU in dedicated deployments.\n\n#### [Weight-Decomposed Low-Rank Adaptation (DoRA)](https://arxiv.org/abs/2402.09353)\n\n*   Proposed in [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) by Liu et al. from NVIDIA and HKUST.\n*   Weight-Decomposed Low-Rank Adaptation (DoRA) is a novel Parameter-Efficient Fine-Tuning (PEFT) method that surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs.\n*   The authors’ weight decomposition analysis reveals fundamental differences between full fine-tuning and LoRA, showing that directional updates play a crucial role in learning capability. DoRA employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.\n*   DoRA demonstrates superior performance across a range of tasks, including commonsense reasoning, visual instruction tuning, and image/video-text understanding, across models like LLaMA, LLaVA, and VL-BART. It achieves this by effectively managing the trade-off between the number of trainable parameters and learning capacity, without adding inference overhead.\n*   The following figure from the paper illustrates an overview of DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that ‖⋅‖c‖⋅‖c\\\\|\\\\cdot\\\\|\\_c denotes the vector-wise norm of a matrix across each column vector.\n\n![](../../../images/papers/DoRA.jpg)\n\n*   Experiments show that DoRA not only outperforms LoRA but also matches or exceeds the performance of full fine-tuning across different tasks, with significant improvements in commonsense reasoning tasks and multimodal understanding, illustrating its effectiveness and efficiency.\n*   The paper also explores DoRA’s compatibility with other LoRA variants, such as VeRA, and demonstrates its adaptability across different training sizes and rank settings, further establishing its utility as a versatile and powerful fine-tuning method.\n*   [Blog](https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/)\n\n#### Summary of LoRA Techniques\n\n*   The following section is inspired from Cameron Woulfe’s [(source)](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/) post.\n*   Here’s an overview of some prevalent variants of LoRA techniques:\n    *   **LoRA** models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.\n    *   **QLoRA** is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.\n    *   **QA-LoRA** is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).\n    *   **LoftQ** studies a similar idea to QA-LoRA – applying quantization and LoRA finetuning on a pretrained model simultaneously.\n    *   **LongLoRA** attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\n        *   Using sparse local attention instead of dense global attention (optional at inference time).\n        *   Using LoRA (authors find that this works well for context extension).\n    *   **S-LoRA** aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\n        *   Stores all LoRA modules in main memory.\n        *   Puts modules being used to run the current query into GPU memory.\n        *   Uses unified paging to allocate GPU memory and avoid fragmentation.\n        *   Proposes a new tensor parallelism strategy to batch LoRA computations.\n    *   **\\*\\*ReLoRA** refines neural network training by iteratively applying low-rank updates to achieve high-rank performance, streamlining the process for large models.\n    *   **DoRA** surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs. It employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.\n    *   Many other LoRA variants exist as well:\n        *   **LQ-LoRA:** uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.\n        *   **MultiLoRA:** extension of LoRA that better handles complex multi-task learning scenarios.\n        *   **LoRA-FA:** freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.\n        *   **Tied-LoRA:** leverages weight tying to further improve the parameter efficiency of LoRA.\n        *   **GLoRA:** extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.\n\n*   **LoRA** models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.\n*   **QLoRA** is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.\n*   **QA-LoRA** is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).\n*   **LoftQ** studies a similar idea to QA-LoRA – applying quantization and LoRA finetuning on a pretrained model simultaneously.\n*   **LongLoRA** attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\n    *   Using sparse local attention instead of dense global attention (optional at inference time).\n    *   Using LoRA (authors find that this works well for context extension).\n*   **S-LoRA** aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\n    *   Stores all LoRA modules in main memory.\n    *   Puts modules being used to run the current query into GPU memory.\n    *   Uses unified paging to allocate GPU memory and avoid fragmentation.\n    *   Proposes a new tensor parallelism strategy to batch LoRA computations.\n*   **\\*\\*ReLoRA** refines neural network training by iteratively applying low-rank updates to achieve high-rank performance, streamlining the process for large models.\n*   **DoRA** surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs. It employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.\n*   Many other LoRA variants exist as well:\n    *   **LQ-LoRA:** uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.\n    *   **MultiLoRA:** extension of LoRA that better handles complex multi-task learning scenarios.\n    *   **LoRA-FA:** freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.\n    *   **Tied-LoRA:** leverages weight tying to further improve the parameter efficiency of LoRA.\n    *   **GLoRA:** extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.\n\n*   Using sparse local attention instead of dense global attention (optional at inference time).\n*   Using LoRA (authors find that this works well for context extension).\n\n*   Stores all LoRA modules in main memory.\n*   Puts modules being used to run the current query into GPU memory.\n*   Uses unified paging to allocate GPU memory and avoid fragmentation.\n*   Proposes a new tensor parallelism strategy to batch LoRA computations.\n\n*   **LQ-LoRA:** uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.\n*   **MultiLoRA:** extension of LoRA that better handles complex multi-task learning scenarios.\n*   **LoRA-FA:** freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.\n*   **Tied-LoRA:** leverages weight tying to further improve the parameter efficiency of LoRA.\n*   **GLoRA:** extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.\n\n![](/primers/ai/assets/parameter-efficient-fine-tuning/LoRAoverview.jpeg)\n\n#### [Low-rank Linear Subspace ReFT (LoReFT)](https://arxiv.org/abs/2404.03592)\n\n*   Proposed in [ReFT: Representation Finetuning for LLMs](https://arxiv.org/abs/2404.03592) by Wu et al. from Stanford and the Pr(Ai)2R Group.\n*   Representation Finetuning (ReFT) is a suite of methods to modify the hidden representations of LLMs (LMs) for task-specific adaptation. Unlike traditional parameter-efficient finetuning (PEFT) methods that adapt by modifying weights, ReFT manipulates a small fraction of model representations, enhancing the interpretability and flexibility of the interventions.\n*   A key variant within ReFT, named Low-rank Linear Subspace ReFT (LoReFT), leverages a low-rank projection matrix to edit representations in a linear subspace. This approach is demonstrated to be 10××\\\\times–50××\\\\times more parameter-efficient compared to existing state-of-the-art PEFTs like LoRA.\n*   The ReFT methodology, specifically Low-rank Linear Subspace ReFT (LoReFT), operates by editing hidden representations in a linear subspace. LoReFT modifies these representations using a projection matrix RRR, which redefines them in a low-dimensional subspace for efficiency. The matrix RRR has orthonormal rows, which are crucial for maintaining the quality of the intervention without adding much complexity.\n*   The core intervention of LoReFT, as per the distributed interchange intervention (DII) formula DII(b,s,R)\\=b+R⊤(Rs−Rb)DII(b,s,R)\\=b+R⊤(Rs−Rb)DII(b, s, R) = b + R^\\\\top(Rs - Rb), leverages the projection matrix to adjust the hidden states bbb towards a target state sss by the application of RRR. This intervention is designed to manipulate the model output towards desired behaviors or answers subtly and effectively.\n*   LoReFT employs a linear transformation defined by the parameters WWW and bbb (not to be confused with the bias term), which projects the representation into the subspace before it is edited. This transformation helps in aligning the representation more closely with the task-specific features that are crucial for performance.\n*   Practically, LoReFT is implemented as a set of non-overlapping interventions across multiple layers of a Transformer-based model. These interventions are strategically placed to modify the behavior of the model without extensive retraining of the underlying parameters.\n*   Each intervention is applied after the computation of layer LLL representations, meaning it directly affects the computation of subsequent layers L+1L+1L+1 to L+mL+mL+m. This placement ensures that the interventions have a cascading effect, enhancing their impact on the final model output.\n*   The hyperparameter tuning for LoReFT focuses on the number and placement of interventions across the layers, optimizing both the effectiveness of each intervention and the overall computational overhead. This involves selecting the appropriate number of prefix and suffix positions in the input where interventions are most beneficial, as well as deciding on the layers where these modifications will have the most impact.\n*   The figure below from the paper shows an illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function ΦΦ\\\\Phi is applied to hidden representations at positions PPP in layer LLL. (2) The right panel depicts the hyperparameters we tune when experimenting with LoReFT. Specifically, the figure depicts application of LoReFT at all layers with prefix length ppp = 2 and suffix length sss = 2. When not tying layer weights, we train separate intervention parameters at each position and layer, resulting in 16 interventions with unique parameters in this example.\n\n![](../../../images/papers/ReFT.jpg)\n\n*   The authors evaluate LoReFT across multiple domains, including commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. It is shown that LoReFT achieves competitive or superior performance on all tasks, especially shining in commonsense reasoning benchmarks.\n*   Implementation details reveal that LoReFT interventions are applied at selected layers and positions within the LM, optimizing both the number of interventions and their locations through hyperparameter tuning. This targeted approach allows for minimal additional computational overhead at inference.\n*   LoReFT is implemented in a publicly available Python library, `pyreft`, which facilitates the adoption of ReFT methods by providing tools to apply these interventions on any pretrained LM from the HuggingFace model hub.\n*   The paper establishes the potential of representation-focused finetuning as a more effective alternative to weight-based methods, setting new standards for efficiency and performance in adapting large-scale LMs to diverse tasks.\n\n#### [Stratified Progressive Adaptation Fine-tuning (SPAFIT)](https://arxiv.org/abs/2405.00201)\n\n*   Proposed in [SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large LLMs](https://arxiv.org/abs/2405.00201) by Arora and Wang from Simon Fraser University, Stratified Progressive Adaptation Fine-tuning (SPAFIT) is a novel Parameter-Efficient Fine-Tuning (PEFT) method aimed at optimizing the fine-tuning process of Transformer-based large LLMs by localizing the fine-tuning to specific layers according to their linguistic knowledge importance. This addresses issues like catastrophic forgetting and computational inefficiency common in full fine-tuning methods.\n*   SPAFIT organizes the model into three groups of layers, with increasing complexity of fine-tuning allowed as the layers progress from basic linguistic processing to more task-specific functions. Group 1 layers remain completely frozen, Group 2 layers undergo fine-tuning only on bias terms, and Group 3 layers are fine-tuned using both BitFit for simple parameters and Low-Rank Adaptation (LoRA) for more significant weight matrices.\n*   The authors conducted experiments using the BERT-large-cased model across nine tasks from the GLUE benchmark. Their results demonstrate that SPAFIT can achieve or exceed the performance of full fine-tuning and other PEFT methods like Full BitFit and Full LoRA while fine-tuning significantly fewer parameters.\n*   The figure below from the paper illustrates an example implementation of SPAFIT on BERT.\n\n![](../../../images/papers/SPAFIT.jpg)\n\n*   Notable results include SPAFIT models achieving the best performance on tasks involving sentence similarity, like MRPC and STS-B, and showing a substantial reduction in the number of parameters fine-tuned—highlighting SPAFIT’s efficiency.\n*   The research suggests that different types of linguistic knowledge can indeed be localized to specific layers of an LLM, potentially leading to more targeted and efficient fine-tuning strategies.\n*   The paper raises points for future investigation, including the application of SPAFIT to more complex tasks like summarization and to models that contain both encoder and decoder architectures. The study also acknowledges the need for further analysis on the optimal balance of parameter efficiency against task performance and the extent of adaptation necessary at different layers.\n\n#### [BitFit](https://arxiv.org/abs/2106.10199)\n\n*   Proposed in [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199) by Ben-Zaken et al. from Yoav Goldberg’s group at Bar Ilan University and the Allen Institute for Artificial Intelligence introduces BitFit, a fine-tuning method for pre-trained BERT models. BitFit focuses on updating only the bias-terms of the model, which are a minimal fraction of the model’s parameters, effectively reducing the memory footprint and computational demands typically associated with full model fine-tuning.\n*   BitFit’s methodology leverages the observation that fine-tuning often doesn’t require extensive retraining of all parameters. Instead, fine-tuning only the bias terms achieves competitive results compared to full model fine-tuning, especially with small to medium-sized datasets. In scenarios permitting slight performance degradation, the method can be constrained to adjust only two specific types of bias terms, representing just 0.04% of the total model parameters.\n*   Implementation details include freezing the transformer-encoder’s main weights and training only the bias terms along with a task-specific classification layer. This approach allows the model to handle multiple tasks efficiently in a streaming fashion without requiring simultaneous access to all task datasets.\n*   Experimental results on the GLUE benchmark show that BitFit is comparable or superior to full fine-tuning in several NLP tasks. It also outperforms other parameter-efficient methods like Diff-Pruning and Adapters in terms of the number of parameters modified, showcasing its effectiveness in achieving high performance with significantly fewer trainable parameters.\n*   The findings underscore the potential of focusing fine-tuning efforts on a small subset of parameters, specifically bias terms, to maintain or even enhance performance while minimizing computational costs. This approach also prompts further exploration of the role of bias terms in neural networks and their impact on model behavior and task transferability.\n\n#### [NOLA](https://openreview.net/pdf?id=TjfXcDgvzk)\n\n*   Proposed in [NOLA: Compressing LoRA Using Linear Combination of Random Basis](https://openreview.net/pdf?id=TjfXcDgvzk) by Koohpayegani et al. in ICLR 2024, NOLA is a novel method for compressing large LLMs (LLMs) that addresses the limitations of Low-Rank Adaptation (LoRA). NOLA reparameterizes the rank-decomposition matrices used in LoRA through linear combinations of randomly generated basis matrices, significantly reducing the parameter count by optimizing only the mixture coefficients.\n*   NOLA decouples the number of trainable parameters from both the rank choice and network architecture, unlike LoRA, where parameters are inherently dependent on the matrix dimensions and rank, which must be an integer. This method not only preserves the adaptation quality but also allows for extreme compression, achieving up to 20 times fewer parameters than the most compressed LoRA models without loss of performance.\n*   The method’s implementation includes using a pseudo-random number generator for creating basis matrices, where the generator’s seed and the linear coefficients are stored, greatly reducing storage requirements. Quantization of these coefficients further minimizes storage needs without impacting model performance.\n*   The figure below from the paper shows the process that NOLA follows. After constraining the rank of ΔWΔW\\\\Delta W by decomposing it to A×BA×BA \\\\times B, we reparametrize A and B to be a linear combination of several random basis matrices. We freeze the basis and W and learn the combination coefficients. To reconstruct the model, we store the coefficients and the seed of the random generator which is a single scalar. NOLA results in more compression compared to LoRA and more importantly decouples the compression ratio from the rank and dimensions of W. One can reduce the number of parameters to 4 times smaller than rank=1 of LoRA which is not possible with LoRA due to rank being an integer number.\n\n![](../../../images/papers/NOLA.jpg)\n\n*   Detailed experimental evaluations across several tasks and models, including GPT-2 and LLaMA-2, showcase NOLA’s effectiveness. It maintains or exceeds benchmark metrics such as BLEU and ROUGE-L while using significantly fewer parameters compared to both LoRA and full model fine-tuning.\n*   The approach’s versatility is demonstrated through its application not only in natural language processing tasks but also in adapting Vision Transformer (ViT) models for image classification, indicating its potential widespread applicability across different types of deep learning architectures.\n*   [Code](https://github.com/UCDvision/NOLA)\n\n#### [Matrix of Rank Adaptation (MoRA)](https://arxiv.org/abs/2405.12130v1)\n\n*   Proposed in [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130v1) by Jiang et al. from Beihang University and Microsoft introduces a novel method, MoRA (Matrix of Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique for LLMs. The authors identify limitations in existing PEFT methods, particularly Low-Rank Adaptation (LoRA), which may restrict LLMs’ ability to learn and retain new knowledge. To address these issues, MoRA employs a high-rank updating mechanism using a square matrix to achieve greater flexibility and effectiveness without increasing the number of trainable parameters.\n*   MoRA utilizes non-parameterized operators to adjust input and output dimensions, ensuring the weight can be integrated back into LLMs like LoRA. The method involves the following steps:\n    1.  **Reduction of Input Dimension**: Non-parameter operators reduce the input dimension for the square matrix.\n    2.  **Increase of Output Dimension**: Corresponding operators increase the output dimension, maintaining the number of trainable parameters while achieving high-rank updates.\n*   The figure below from the paper illustrates an overview of our method compared to LoRA under same number of trainable parameters. WWW is the frozen weight from model. AAA and BBB are trainable low-rank matrices in LoRA. MMM is the trainable matrix in our method. Gray parts are non-parameter operators to reducing the input dimension and increasing the output dimension. rrr represents the rank in two methods.\n\n1.  **Reduction of Input Dimension**: Non-parameter operators reduce the input dimension for the square matrix.\n2.  **Increase of Output Dimension**: Corresponding operators increase the output dimension, maintaining the number of trainable parameters while achieving high-rank updates.\n\n![](../../../images/papers/MoRA2.jpg)\n\n*   The authors comprehensively evaluate MoRA across five tasks—instruction tuning, mathematical reasoning, continual pretraining, memory, and pretraining—demonstrating that MoRA outperforms LoRA in memory-intensive tasks and achieves comparable performance in other areas.\n*   **Technical Details and Implementation:**\n    *   **Low-Rank Limitation in LoRA**: LoRA uses low-rank matrices to approximate full-rank updates, limiting its capacity to store new information, especially in memory-intensive tasks. The low-rank matrices A and B in LoRA struggle to fully capture the complexity needed for tasks requiring substantial knowledge enhancement.\n    *   **High-Rank Updating in MoRA**: MoRA replaces the low-rank matrices with a square matrix, significantly increasing the rank and thus the capacity for updates. For example, LoRA with rank 8 employs matrices A∈ℝ4096×8A∈R4096×8A \\\\in \\\\mathbb{R}^{4096 \\\\times 8} and B∈ℝ8×4096B∈R8×4096B \\\\in \\\\mathbb{R}^{8 \\\\times 4096}, while MoRA uses a square matrix M∈ℝ256×256M∈R256×256M \\\\in \\\\mathbb{R}^{256 \\\\times 256}, achieving a higher rank with the same number of parameters.\n    *   **Compression and Decompression Functions**: MoRA employs various methods to implement compression and decompression functions, including truncation, sharing rows/columns, reshaping, and rotation. These methods help reduce the input dimension and increase the output dimension effectively.\n    *   **Rotation Operators**: Inspired by RoPE (Rotary Position Embedding), MoRA introduces rotation operators to differentiate inputs, enhancing the expressiveness of the square matrix.\n*   **Evaluation and Results:**\n    *   **Memory Task**: In memorizing UUID pairs, MoRA showed significant improvements over LoRA with the same number of trainable parameters. MoRA required fewer training steps to achieve high accuracy compared to LoRA, demonstrating its effectiveness in memory-intensive tasks.\n    *   **Fine-Tuning Tasks**: MoRA was evaluated on instruction tuning (using Tülu v2 dataset), mathematical reasoning (using MetaMath, GSM8K, MATH), and continual pretraining (in biomedical and financial domains). It matched LoRA’s performance in instruction tuning and mathematical reasoning but outperformed LoRA in continual pretraining tasks, benefiting from high-rank updating.\n    *   **Pretraining**: MoRA and a variant, ReMoRA (which merges updates back into the model during training), were evaluated on pretraining transformers from scratch on the C4 dataset. MoRA showed better pretraining loss and perplexity metrics compared to LoRA and ReLoRA, further validating the advantages of high-rank updating.\n*   MoRA addresses the limitations of low-rank updates in LoRA by employing high-rank matrices, significantly enhancing the model’s capacity to learn and memorize new knowledge. This method shows promise for improving parameter-efficient fine-tuning of LLMs, especially in memory-intensive and domain-specific tasks. The authors provide comprehensive implementation details and empirical evaluations, establishing MoRA as an effective advancement in the field of PEFT.\n\n*   **Low-Rank Limitation in LoRA**: LoRA uses low-rank matrices to approximate full-rank updates, limiting its capacity to store new information, especially in memory-intensive tasks. The low-rank matrices A and B in LoRA struggle to fully capture the complexity needed for tasks requiring substantial knowledge enhancement.\n*   **High-Rank Updating in MoRA**: MoRA replaces the low-rank matrices with a square matrix, significantly increasing the rank and thus the capacity for updates. For example, LoRA with rank 8 employs matrices A∈ℝ4096×8A∈R4096×8A \\\\in \\\\mathbb{R}^{4096 \\\\times 8} and B∈ℝ8×4096B∈R8×4096B \\\\in \\\\mathbb{R}^{8 \\\\times 4096}, while MoRA uses a square matrix M∈ℝ256×256M∈R256×256M \\\\in \\\\mathbb{R}^{256 \\\\times 256}, achieving a higher rank with the same number of parameters.\n*   **Compression and Decompression Functions**: MoRA employs various methods to implement compression and decompression functions, including truncation, sharing rows/columns, reshaping, and rotation. These methods help reduce the input dimension and increase the output dimension effectively.\n*   **Rotation Operators**: Inspired by RoPE (Rotary Position Embedding), MoRA introduces rotation operators to differentiate inputs, enhancing the expressiveness of the square matrix.\n\n*   **Memory Task**: In memorizing UUID pairs, MoRA showed significant improvements over LoRA with the same number of trainable parameters. MoRA required fewer training steps to achieve high accuracy compared to LoRA, demonstrating its effectiveness in memory-intensive tasks.\n*   **Fine-Tuning Tasks**: MoRA was evaluated on instruction tuning (using Tülu v2 dataset), mathematical reasoning (using MetaMath, GSM8K, MATH), and continual pretraining (in biomedical and financial domains). It matched LoRA’s performance in instruction tuning and mathematical reasoning but outperformed LoRA in continual pretraining tasks, benefiting from high-rank updating.\n*   **Pretraining**: MoRA and a variant, ReMoRA (which merges updates back into the model during training), were evaluated on pretraining transformers from scratch on the C4 dataset. MoRA showed better pretraining loss and perplexity metrics compared to LoRA and ReLoRA, further validating the advantages of high-rank updating.",
    "order": 4,
    "orderInChapter": 3,
    "difficulty": 5,
    "estimatedMinutes": 125,
    "tags": [
      "datatraining",
      "neural network",
      "deep learning",
      "machine learning",
      "transformer",
      "attention",
      "embedding",
      "bert"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 24956,
      "contentLength": 1119543
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#reparameterization",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-soft-prompt-tuning-5",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "Soft Prompt Tuning",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> Soft Prompt tuning involves adding a small trainable prefix to the input of the pre-trained LLM during fine-tuning, which modifies the representation learned by the pre-trained model to better suit the downstream task.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts.</p>\n  </li>\n</ul>\n<p><strong>What:</strong> Soft Prompt tuning involves adding a small trainable prefix to the input of the pre-trained LLM during fine-tuning, which modifies the representation learned by the pre-trained model to better suit the downstream task.</p>\n<p><strong>When to use:</strong> Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts.</p>",
    "contentMarkdown": "*   **What:** Soft Prompt tuning involves adding a small trainable prefix to the input of the pre-trained LLM during fine-tuning, which modifies the representation learned by the pre-trained model to better suit the downstream task.\n    \n*   **When to use:** Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts.\n    \n\n**What:** Soft Prompt tuning involves adding a small trainable prefix to the input of the pre-trained LLM during fine-tuning, which modifies the representation learned by the pre-trained model to better suit the downstream task.\n\n**When to use:** Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts.",
    "order": 5,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 172,
      "contentLength": 1218
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#soft-prompt-tuning",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-prefix-tuning-6",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "Prefix Tuning",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> Prefix Tuning involves learning a set of trainable parameters that modify the pre-trained LLM’s hidden states in response to task-specific prompts during inference, effectively fine-tuning the model at inference time.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.</p>\n  </li>\n</ul>\n<p><strong>What:</strong> Prefix Tuning involves learning a set of trainable parameters that modify the pre-trained LLM’s hidden states in response to task-specific prompts during inference, effectively fine-tuning the model at inference time.</p>\n<p><strong>When to use:</strong> When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.</p>",
    "contentMarkdown": "*   **What:** Prefix Tuning involves learning a set of trainable parameters that modify the pre-trained LLM’s hidden states in response to task-specific prompts during inference, effectively fine-tuning the model at inference time.\n    \n*   **When to use:** When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.\n    \n\n**What:** Prefix Tuning involves learning a set of trainable parameters that modify the pre-trained LLM’s hidden states in response to task-specific prompts during inference, effectively fine-tuning the model at inference time.\n\n**When to use:** When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.",
    "order": 6,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 138,
      "contentLength": 1050
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#prefix-tuning",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-adapters-7",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "Adapters",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> Adapters are tiny modules that are added to pre-trained LLMs, typically between the pre-trained layers, to adapt the model to new downstream tasks. During fine-tuning, only the weights of the adapter are learned, while the pre-trained model’s parameters remain fixed.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> When you need to fine-tune multiple downstream tasks on the same pre-trained model. Additionally, Adapters are flexible and can be quickly and easily plugged into different parts of the pre-trained model without requiring major modifications.</p>\n  </li>\n</ul>\n<p><strong>What:</strong> Adapters are tiny modules that are added to pre-trained LLMs, typically between the pre-trained layers, to adapt the model to new downstream tasks. During fine-tuning, only the weights of the adapter are learned, while the pre-trained model’s parameters remain fixed.</p>\n<p><strong>When to use:</strong> When you need to fine-tune multiple downstream tasks on the same pre-trained model. Additionally, Adapters are flexible and can be quickly and easily plugged into different parts of the pre-trained model without requiring major modifications.</p>",
    "contentMarkdown": "*   **What:** Adapters are tiny modules that are added to pre-trained LLMs, typically between the pre-trained layers, to adapt the model to new downstream tasks. During fine-tuning, only the weights of the adapter are learned, while the pre-trained model’s parameters remain fixed.\n    \n*   **When to use:** When you need to fine-tune multiple downstream tasks on the same pre-trained model. Additionally, Adapters are flexible and can be quickly and easily plugged into different parts of the pre-trained model without requiring major modifications.\n    \n\n**What:** Adapters are tiny modules that are added to pre-trained LLMs, typically between the pre-trained layers, to adapt the model to new downstream tasks. During fine-tuning, only the weights of the adapter are learned, while the pre-trained model’s parameters remain fixed.\n\n**When to use:** When you need to fine-tune multiple downstream tasks on the same pre-trained model. Additionally, Adapters are flexible and can be quickly and easily plugged into different parts of the pre-trained model without requiring major modifications.",
    "order": 7,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 160,
      "contentLength": 1204
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#adapters",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-bitfit-8",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "BitFit",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> BitFit simplifies the fine-tuning process by only updating the bias terms of the model, reducing the number of parameters that need to be modified.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> BitFit is an excellent choice when computational resources are a constraint or when working with smaller datasets. It’s especially suited for tasks where slight performance compromises are acceptable in exchange for greater efficiency.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Bias-Only Training:</strong> By focusing on updating only the bias terms, BitFit significantly lowers the computational demands and memory usage.</li>\n      <li><strong>Efficient Adaptability:</strong> This method achieves comparable results to more extensive fine-tuning methods with far fewer parameter updates, making it ideal for rapid deployment and iterative development.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Freezing Main Weights:</strong> The main weights of the Transformer encoder are frozen, preserving the pre-trained knowledge.</li>\n      <li><strong>Bias Term Training:</strong> Only the bias terms are fine-tuned along with a task-specific classification layer, providing an efficient way to adapt the model to new tasks.</li>\n      <li><strong>Evaluation Across Tasks:</strong> BitFit’s efficacy is tested on various NLP tasks, showing its capability to maintain high performance with minimal parameter adjustments.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> BitFit simplifies the fine-tuning process by only updating the bias terms of the model, reducing the number of parameters that need to be modified.</p>\n<p><strong>When to use:</strong> BitFit is an excellent choice when computational resources are a constraint or when working with smaller datasets. It’s especially suited for tasks where slight performance compromises are acceptable in exchange for greater efficiency.</p>\n<ul>\n      <li><strong>Bias-Only Training:</strong> By focusing on updating only the bias terms, BitFit significantly lowers the computational demands and memory usage.</li>\n      <li><strong>Efficient Adaptability:</strong> This method achieves comparable results to more extensive fine-tuning methods with far fewer parameter updates, making it ideal for rapid deployment and iterative development.</li>\n    </ul>\n<ol>\n      <li><strong>Freezing Main Weights:</strong> The main weights of the Transformer encoder are frozen, preserving the pre-trained knowledge.</li>\n      <li><strong>Bias Term Training:</strong> Only the bias terms are fine-tuned along with a task-specific classification layer, providing an efficient way to adapt the model to new tasks.</li>\n      <li><strong>Evaluation Across Tasks:</strong> BitFit’s efficacy is tested on various NLP tasks, showing its capability to maintain high performance with minimal parameter adjustments.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** BitFit simplifies the fine-tuning process by only updating the bias terms of the model, reducing the number of parameters that need to be modified.\n    \n*   **When to use:** BitFit is an excellent choice when computational resources are a constraint or when working with smaller datasets. It’s especially suited for tasks where slight performance compromises are acceptable in exchange for greater efficiency.\n    \n*   **Key Features:**\n    *   **Bias-Only Training:** By focusing on updating only the bias terms, BitFit significantly lowers the computational demands and memory usage.\n    *   **Efficient Adaptability:** This method achieves comparable results to more extensive fine-tuning methods with far fewer parameter updates, making it ideal for rapid deployment and iterative development.\n*   **Process:**\n    1.  **Freezing Main Weights:** The main weights of the Transformer encoder are frozen, preserving the pre-trained knowledge.\n    2.  **Bias Term Training:** Only the bias terms are fine-tuned along with a task-specific classification layer, providing an efficient way to adapt the model to new tasks.\n    3.  **Evaluation Across Tasks:** BitFit’s efficacy is tested on various NLP tasks, showing its capability to maintain high performance with minimal parameter adjustments.\n\n**What:** BitFit simplifies the fine-tuning process by only updating the bias terms of the model, reducing the number of parameters that need to be modified.\n\n**When to use:** BitFit is an excellent choice when computational resources are a constraint or when working with smaller datasets. It’s especially suited for tasks where slight performance compromises are acceptable in exchange for greater efficiency.\n\n*   **Bias-Only Training:** By focusing on updating only the bias terms, BitFit significantly lowers the computational demands and memory usage.\n*   **Efficient Adaptability:** This method achieves comparable results to more extensive fine-tuning methods with far fewer parameter updates, making it ideal for rapid deployment and iterative development.\n\n1.  **Freezing Main Weights:** The main weights of the Transformer encoder are frozen, preserving the pre-trained knowledge.\n2.  **Bias Term Training:** Only the bias terms are fine-tuned along with a task-specific classification layer, providing an efficient way to adapt the model to new tasks.\n3.  **Evaluation Across Tasks:** BitFit’s efficacy is tested on various NLP tasks, showing its capability to maintain high performance with minimal parameter adjustments.",
    "order": 8,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "transformer",
      "nlp",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 357,
      "contentLength": 2985
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#bitfit",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-lora-9",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "LoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> LoRA (Low-Rank Adaptation) is a technique that modifies the pre-trained LLM’s attention mechanism during fine-tuning by introducing a low-rank matrix factorization that learns task-specific attention patterns.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:</p>\n    <ul>\n      <li>Memory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you’re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.</li>\n      <li>Real-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.</li>\n      <li>Task-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>What:</strong> LoRA (Low-Rank Adaptation) is a technique that modifies the pre-trained LLM’s attention mechanism during fine-tuning by introducing a low-rank matrix factorization that learns task-specific attention patterns.</p>\n<p><strong>When to use:</strong> LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:</p>\n<ul>\n      <li>Memory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you’re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.</li>\n      <li>Real-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.</li>\n      <li>Task-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.</li>\n    </ul>",
    "contentMarkdown": "*   **What:** LoRA (Low-Rank Adaptation) is a technique that modifies the pre-trained LLM’s attention mechanism during fine-tuning by introducing a low-rank matrix factorization that learns task-specific attention patterns.\n    \n*   **When to use:** LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:\n    \n    *   Memory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you’re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.\n    *   Real-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.\n    *   Task-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.\n\n**What:** LoRA (Low-Rank Adaptation) is a technique that modifies the pre-trained LLM’s attention mechanism during fine-tuning by introducing a low-rank matrix factorization that learns task-specific attention patterns.\n\n**When to use:** LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:\n\n*   Memory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you’re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.\n*   Real-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.\n*   Task-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.",
    "order": 9,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "attention",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 314,
      "contentLength": 2410
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#lora",
    "scrapedAt": "2025-12-28T11:49:57.383Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-qlora-10",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "QLoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> QLoRA (Quantized Low-Rank Adaptation) is an advanced fine-tuning technique that integrates quantization with low-rank adaptation, allowing for efficient fine-tuning of large LLMs with significantly reduced memory usage.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> QLoRA is ideal for scenarios where memory and computational efficiency are paramount, particularly when fine-tuning very large models on limited hardware. It is especially useful when working with low-bit model environments or when full 16-bit fine-tuning would be prohibitively expensive.</p>\n    <ul>\n      <li><strong>Key Features:</strong>\n        <ul>\n          <li><strong>4-bit Quantization:</strong> QLoRA uses a novel 4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>) quantization, optimized for normally distributed weights, to reduce the memory footprint.</li>\n          <li><strong>Double Quantization:</strong> This technique further reduces memory usage by quantizing the quantization constants.</li>\n          <li><strong>Paged Optimizers:</strong> These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.</li>\n        </ul>\n      </li>\n      <li><strong>Process:</strong>\n        <ol>\n          <li><strong>Model Quantization:</strong> The pre-trained model is quantized to 4-bit precision using <code class=\"language-plaintext highlighter-rouge\">NF4</code>.</li>\n          <li><strong>Adding LoRA Weights:</strong> LoRA weights are integrated into the quantized model.</li>\n          <li><strong>Fine-Tuning:</strong> The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.</li>\n          <li><strong>Double Quantization:</strong> Quantization constants are further quantized to minimize memory usage.</li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>What:</strong> QLoRA (Quantized Low-Rank Adaptation) is an advanced fine-tuning technique that integrates quantization with low-rank adaptation, allowing for efficient fine-tuning of large LLMs with significantly reduced memory usage.</p>\n<p><strong>When to use:</strong> QLoRA is ideal for scenarios where memory and computational efficiency are paramount, particularly when fine-tuning very large models on limited hardware. It is especially useful when working with low-bit model environments or when full 16-bit fine-tuning would be prohibitively expensive.</p>\n<ul>\n      <li><strong>Key Features:</strong>\n        <ul>\n          <li><strong>4-bit Quantization:</strong> QLoRA uses a novel 4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>) quantization, optimized for normally distributed weights, to reduce the memory footprint.</li>\n          <li><strong>Double Quantization:</strong> This technique further reduces memory usage by quantizing the quantization constants.</li>\n          <li><strong>Paged Optimizers:</strong> These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.</li>\n        </ul>\n      </li>\n      <li><strong>Process:</strong>\n        <ol>\n          <li><strong>Model Quantization:</strong> The pre-trained model is quantized to 4-bit precision using <code class=\"language-plaintext highlighter-rouge\">NF4</code>.</li>\n          <li><strong>Adding LoRA Weights:</strong> LoRA weights are integrated into the quantized model.</li>\n          <li><strong>Fine-Tuning:</strong> The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.</li>\n          <li><strong>Double Quantization:</strong> Quantization constants are further quantized to minimize memory usage.</li>\n        </ol>\n      </li>\n    </ul>\n<ul>\n          <li><strong>4-bit Quantization:</strong> QLoRA uses a novel 4-bit NormalFloat (<code class=\"language-plaintext highlighter-rouge\">NF4</code>) quantization, optimized for normally distributed weights, to reduce the memory footprint.</li>\n          <li><strong>Double Quantization:</strong> This technique further reduces memory usage by quantizing the quantization constants.</li>\n          <li><strong>Paged Optimizers:</strong> These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.</li>\n        </ul>\n<ol>\n          <li><strong>Model Quantization:</strong> The pre-trained model is quantized to 4-bit precision using <code class=\"language-plaintext highlighter-rouge\">NF4</code>.</li>\n          <li><strong>Adding LoRA Weights:</strong> LoRA weights are integrated into the quantized model.</li>\n          <li><strong>Fine-Tuning:</strong> The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.</li>\n          <li><strong>Double Quantization:</strong> Quantization constants are further quantized to minimize memory usage.</li>\n        </ol>",
    "contentMarkdown": "*   **What:** QLoRA (Quantized Low-Rank Adaptation) is an advanced fine-tuning technique that integrates quantization with low-rank adaptation, allowing for efficient fine-tuning of large LLMs with significantly reduced memory usage.\n    \n*   **When to use:** QLoRA is ideal for scenarios where memory and computational efficiency are paramount, particularly when fine-tuning very large models on limited hardware. It is especially useful when working with low-bit model environments or when full 16-bit fine-tuning would be prohibitively expensive.\n    \n    *   **Key Features:**\n        *   **4-bit Quantization:** QLoRA uses a novel 4-bit NormalFloat (`NF4`) quantization, optimized for normally distributed weights, to reduce the memory footprint.\n        *   **Double Quantization:** This technique further reduces memory usage by quantizing the quantization constants.\n        *   **Paged Optimizers:** These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.\n    *   **Process:**\n        1.  **Model Quantization:** The pre-trained model is quantized to 4-bit precision using `NF4`.\n        2.  **Adding LoRA Weights:** LoRA weights are integrated into the quantized model.\n        3.  **Fine-Tuning:** The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.\n        4.  **Double Quantization:** Quantization constants are further quantized to minimize memory usage.\n\n**What:** QLoRA (Quantized Low-Rank Adaptation) is an advanced fine-tuning technique that integrates quantization with low-rank adaptation, allowing for efficient fine-tuning of large LLMs with significantly reduced memory usage.\n\n**When to use:** QLoRA is ideal for scenarios where memory and computational efficiency are paramount, particularly when fine-tuning very large models on limited hardware. It is especially useful when working with low-bit model environments or when full 16-bit fine-tuning would be prohibitively expensive.\n\n*   **Key Features:**\n    *   **4-bit Quantization:** QLoRA uses a novel 4-bit NormalFloat (`NF4`) quantization, optimized for normally distributed weights, to reduce the memory footprint.\n    *   **Double Quantization:** This technique further reduces memory usage by quantizing the quantization constants.\n    *   **Paged Optimizers:** These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.\n*   **Process:**\n    1.  **Model Quantization:** The pre-trained model is quantized to 4-bit precision using `NF4`.\n    2.  **Adding LoRA Weights:** LoRA weights are integrated into the quantized model.\n    3.  **Fine-Tuning:** The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.\n    4.  **Double Quantization:** Quantization constants are further quantized to minimize memory usage.\n\n*   **4-bit Quantization:** QLoRA uses a novel 4-bit NormalFloat (`NF4`) quantization, optimized for normally distributed weights, to reduce the memory footprint.\n*   **Double Quantization:** This technique further reduces memory usage by quantizing the quantization constants.\n*   **Paged Optimizers:** These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.\n\n1.  **Model Quantization:** The pre-trained model is quantized to 4-bit precision using `NF4`.\n2.  **Adding LoRA Weights:** LoRA weights are integrated into the quantized model.\n3.  **Fine-Tuning:** The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.\n4.  **Double Quantization:** Quantization constants are further quantized to minimize memory usage.",
    "order": 10,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 466,
      "contentLength": 4923
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#qlora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-qa-lora-11",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "QA-LoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> QA-LoRA is a specialized technique for fine-tuning low-bit diffusion models. It integrates quantization-aware strategies with Low-Rank Adaptation (LoRA) principles, providing an efficient way to handle low-bit model environments.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> Ideal for scenarios where the primary goal is to optimize memory usage and computational efficiency in low-bit settings. This method is particularly effective when traditional fine-tuning approaches fall short due to the constraints of low-bit environments.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Quantization-Aware Approach:</strong> QA-LoRA uniquely combines LoRA weights with full-precision model weights, then jointly quantizes them, enhancing memory and computational efficiency during inference.</li>\n      <li><strong>Efficient for Low-Bit Models:</strong> Tailored for low-bit diffusion models, it addresses the specific challenges posed by these environments, making it a standout choice in such contexts.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Adding LoRA Weights:</strong> QA-LoRA begins by integrating LoRA weights into the pre-trained model.</li>\n      <li><strong>Fine-Tuning LoRA Weights:</strong> These weights are then fine-tuned, focusing solely on the LoRA weights while keeping the original model weights unchanged.</li>\n      <li><strong>Merging Weights:</strong> Post-fine-tuning, the LoRA and original model weights are merged.</li>\n      <li><strong>Quantization:</strong> The merged weights undergo quantization to a lower-bit format, crucial for reducing memory and computational costs.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> QA-LoRA is a specialized technique for fine-tuning low-bit diffusion models. It integrates quantization-aware strategies with Low-Rank Adaptation (LoRA) principles, providing an efficient way to handle low-bit model environments.</p>\n<p><strong>When to use:</strong> Ideal for scenarios where the primary goal is to optimize memory usage and computational efficiency in low-bit settings. This method is particularly effective when traditional fine-tuning approaches fall short due to the constraints of low-bit environments.</p>\n<ul>\n      <li><strong>Quantization-Aware Approach:</strong> QA-LoRA uniquely combines LoRA weights with full-precision model weights, then jointly quantizes them, enhancing memory and computational efficiency during inference.</li>\n      <li><strong>Efficient for Low-Bit Models:</strong> Tailored for low-bit diffusion models, it addresses the specific challenges posed by these environments, making it a standout choice in such contexts.</li>\n    </ul>\n<ol>\n      <li><strong>Adding LoRA Weights:</strong> QA-LoRA begins by integrating LoRA weights into the pre-trained model.</li>\n      <li><strong>Fine-Tuning LoRA Weights:</strong> These weights are then fine-tuned, focusing solely on the LoRA weights while keeping the original model weights unchanged.</li>\n      <li><strong>Merging Weights:</strong> Post-fine-tuning, the LoRA and original model weights are merged.</li>\n      <li><strong>Quantization:</strong> The merged weights undergo quantization to a lower-bit format, crucial for reducing memory and computational costs.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** QA-LoRA is a specialized technique for fine-tuning low-bit diffusion models. It integrates quantization-aware strategies with Low-Rank Adaptation (LoRA) principles, providing an efficient way to handle low-bit model environments.\n    \n*   **When to use:** Ideal for scenarios where the primary goal is to optimize memory usage and computational efficiency in low-bit settings. This method is particularly effective when traditional fine-tuning approaches fall short due to the constraints of low-bit environments.\n    \n*   **Key Features:**\n    *   **Quantization-Aware Approach:** QA-LoRA uniquely combines LoRA weights with full-precision model weights, then jointly quantizes them, enhancing memory and computational efficiency during inference.\n    *   **Efficient for Low-Bit Models:** Tailored for low-bit diffusion models, it addresses the specific challenges posed by these environments, making it a standout choice in such contexts.\n*   **Process:**\n    1.  **Adding LoRA Weights:** QA-LoRA begins by integrating LoRA weights into the pre-trained model.\n    2.  **Fine-Tuning LoRA Weights:** These weights are then fine-tuned, focusing solely on the LoRA weights while keeping the original model weights unchanged.\n    3.  **Merging Weights:** Post-fine-tuning, the LoRA and original model weights are merged.\n    4.  **Quantization:** The merged weights undergo quantization to a lower-bit format, crucial for reducing memory and computational costs.\n\n**What:** QA-LoRA is a specialized technique for fine-tuning low-bit diffusion models. It integrates quantization-aware strategies with Low-Rank Adaptation (LoRA) principles, providing an efficient way to handle low-bit model environments.\n\n**When to use:** Ideal for scenarios where the primary goal is to optimize memory usage and computational efficiency in low-bit settings. This method is particularly effective when traditional fine-tuning approaches fall short due to the constraints of low-bit environments.\n\n*   **Quantization-Aware Approach:** QA-LoRA uniquely combines LoRA weights with full-precision model weights, then jointly quantizes them, enhancing memory and computational efficiency during inference.\n*   **Efficient for Low-Bit Models:** Tailored for low-bit diffusion models, it addresses the specific challenges posed by these environments, making it a standout choice in such contexts.\n\n1.  **Adding LoRA Weights:** QA-LoRA begins by integrating LoRA weights into the pre-trained model.\n2.  **Fine-Tuning LoRA Weights:** These weights are then fine-tuned, focusing solely on the LoRA weights while keeping the original model weights unchanged.\n3.  **Merging Weights:** Post-fine-tuning, the LoRA and original model weights are merged.\n4.  **Quantization:** The merged weights undergo quantization to a lower-bit format, crucial for reducing memory and computational costs.",
    "order": 11,
    "orderInChapter": 7,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 375,
      "contentLength": 3357
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#qa-lora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-relora-12",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "ReLoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> ReLoRA is an innovative approach for training high-rank networks efficiently. It revises the Low-Rank Adaptation method by iteratively applying low-rank updates to gradually increase the model’s effective rank.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> Best suited for training large-scale models, particularly when the objective is to achieve high-rank training outcomes with less computational expenditure. ReLoRA is especially valuable for large transformer LLMs where resource efficiency is critical.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Iterative Low-Rank Updates:</strong> Unlike traditional low-rank methods, ReLoRA applies updates in an iterative manner, each time incrementally enhancing the model’s rank, leading to more efficient high-rank network training.</li>\n      <li><strong>Resource Efficiency:</strong> Allows for training of large, high-performing models while significantly reducing computational demands.</li>\n    </ul>\n  </li>\n  <li><strong>Differentiation from Other Techniques:</strong>\n    <ul>\n      <li>ReLoRA stands out from previous techniques like standard LoRA by its unique iterative process. This method incrementally increases the rank of the model through successive low-rank updates, enabling more dynamic and refined training for large-scale models.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>What:</strong> ReLoRA is an innovative approach for training high-rank networks efficiently. It revises the Low-Rank Adaptation method by iteratively applying low-rank updates to gradually increase the model’s effective rank.</p>\n<p><strong>When to use:</strong> Best suited for training large-scale models, particularly when the objective is to achieve high-rank training outcomes with less computational expenditure. ReLoRA is especially valuable for large transformer LLMs where resource efficiency is critical.</p>\n<ul>\n      <li><strong>Iterative Low-Rank Updates:</strong> Unlike traditional low-rank methods, ReLoRA applies updates in an iterative manner, each time incrementally enhancing the model’s rank, leading to more efficient high-rank network training.</li>\n      <li><strong>Resource Efficiency:</strong> Allows for training of large, high-performing models while significantly reducing computational demands.</li>\n    </ul>\n<ul>\n      <li>ReLoRA stands out from previous techniques like standard LoRA by its unique iterative process. This method incrementally increases the rank of the model through successive low-rank updates, enabling more dynamic and refined training for large-scale models.</li>\n    </ul>",
    "contentMarkdown": "*   **What:** ReLoRA is an innovative approach for training high-rank networks efficiently. It revises the Low-Rank Adaptation method by iteratively applying low-rank updates to gradually increase the model’s effective rank.\n    \n*   **When to use:** Best suited for training large-scale models, particularly when the objective is to achieve high-rank training outcomes with less computational expenditure. ReLoRA is especially valuable for large transformer LLMs where resource efficiency is critical.\n    \n*   **Key Features:**\n    *   **Iterative Low-Rank Updates:** Unlike traditional low-rank methods, ReLoRA applies updates in an iterative manner, each time incrementally enhancing the model’s rank, leading to more efficient high-rank network training.\n    *   **Resource Efficiency:** Allows for training of large, high-performing models while significantly reducing computational demands.\n*   **Differentiation from Other Techniques:**\n    *   ReLoRA stands out from previous techniques like standard LoRA by its unique iterative process. This method incrementally increases the rank of the model through successive low-rank updates, enabling more dynamic and refined training for large-scale models.\n\n**What:** ReLoRA is an innovative approach for training high-rank networks efficiently. It revises the Low-Rank Adaptation method by iteratively applying low-rank updates to gradually increase the model’s effective rank.\n\n**When to use:** Best suited for training large-scale models, particularly when the objective is to achieve high-rank training outcomes with less computational expenditure. ReLoRA is especially valuable for large transformer LLMs where resource efficiency is critical.\n\n*   **Iterative Low-Rank Updates:** Unlike traditional low-rank methods, ReLoRA applies updates in an iterative manner, each time incrementally enhancing the model’s rank, leading to more efficient high-rank network training.\n*   **Resource Efficiency:** Allows for training of large, high-performing models while significantly reducing computational demands.\n\n*   ReLoRA stands out from previous techniques like standard LoRA by its unique iterative process. This method incrementally increases the rank of the model through successive low-rank updates, enabling more dynamic and refined training for large-scale models.",
    "order": 12,
    "orderInChapter": 8,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "transformer",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 302,
      "contentLength": 2651
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#relora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-s-lora-13",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "S-LoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> S-LoRA is a scalable system for serving multiple LoRA (Low-Rank Adaptation) adapters concurrently in large LLMs (LLMs). It manages memory efficiently by storing all adapters in main memory and dynamically fetching them to GPU memory. The system uses customized CUDA kernels for batch processing, optimizing both memory usage and computational efficiency.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> S-LoRA is ideal for scenarios where many fine-tuned variants of LLMs need to be served simultaneously with high throughput. It significantly reduces memory fragmentation and I/O overhead, making it suitable for large-scale deployments in resource-constrained environments.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Efficient Memory Management:</strong> Utilizes a unified memory pool to manage adapter weights dynamically, reducing memory fragmentation.</li>\n      <li><strong>High Throughput Serving:</strong> Custom CUDA kernels enable efficient heterogeneous batching of LoRA computations, allowing the serving of thousands of adapters with minimal overhead.</li>\n      <li><strong>Reduced Training and Communication Costs:</strong> Offers an effective solution in federated learning scenarios by lowering the costs associated with training and data communication.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Storage of Adapters:</strong> All adapters are stored in the main memory, ready for dynamic retrieval.</li>\n      <li><strong>Dynamic Fetching:</strong> Adapters required for current computations are fetched into GPU memory as needed.</li>\n      <li><strong>Batch Processing:</strong> Customized CUDA kernels facilitate batch processing, ensuring efficient computation across various sequence lengths and adapter ranks.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> S-LoRA is a scalable system for serving multiple LoRA (Low-Rank Adaptation) adapters concurrently in large LLMs (LLMs). It manages memory efficiently by storing all adapters in main memory and dynamically fetching them to GPU memory. The system uses customized CUDA kernels for batch processing, optimizing both memory usage and computational efficiency.</p>\n<p><strong>When to use:</strong> S-LoRA is ideal for scenarios where many fine-tuned variants of LLMs need to be served simultaneously with high throughput. It significantly reduces memory fragmentation and I/O overhead, making it suitable for large-scale deployments in resource-constrained environments.</p>\n<ul>\n      <li><strong>Efficient Memory Management:</strong> Utilizes a unified memory pool to manage adapter weights dynamically, reducing memory fragmentation.</li>\n      <li><strong>High Throughput Serving:</strong> Custom CUDA kernels enable efficient heterogeneous batching of LoRA computations, allowing the serving of thousands of adapters with minimal overhead.</li>\n      <li><strong>Reduced Training and Communication Costs:</strong> Offers an effective solution in federated learning scenarios by lowering the costs associated with training and data communication.</li>\n    </ul>\n<ol>\n      <li><strong>Storage of Adapters:</strong> All adapters are stored in the main memory, ready for dynamic retrieval.</li>\n      <li><strong>Dynamic Fetching:</strong> Adapters required for current computations are fetched into GPU memory as needed.</li>\n      <li><strong>Batch Processing:</strong> Customized CUDA kernels facilitate batch processing, ensuring efficient computation across various sequence lengths and adapter ranks.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** S-LoRA is a scalable system for serving multiple LoRA (Low-Rank Adaptation) adapters concurrently in large LLMs (LLMs). It manages memory efficiently by storing all adapters in main memory and dynamically fetching them to GPU memory. The system uses customized CUDA kernels for batch processing, optimizing both memory usage and computational efficiency.\n    \n*   **When to use:** S-LoRA is ideal for scenarios where many fine-tuned variants of LLMs need to be served simultaneously with high throughput. It significantly reduces memory fragmentation and I/O overhead, making it suitable for large-scale deployments in resource-constrained environments.\n    \n*   **Key Features:**\n    *   **Efficient Memory Management:** Utilizes a unified memory pool to manage adapter weights dynamically, reducing memory fragmentation.\n    *   **High Throughput Serving:** Custom CUDA kernels enable efficient heterogeneous batching of LoRA computations, allowing the serving of thousands of adapters with minimal overhead.\n    *   **Reduced Training and Communication Costs:** Offers an effective solution in federated learning scenarios by lowering the costs associated with training and data communication.\n*   **Process:**\n    1.  **Storage of Adapters:** All adapters are stored in the main memory, ready for dynamic retrieval.\n    2.  **Dynamic Fetching:** Adapters required for current computations are fetched into GPU memory as needed.\n    3.  **Batch Processing:** Customized CUDA kernels facilitate batch processing, ensuring efficient computation across various sequence lengths and adapter ranks.\n\n**What:** S-LoRA is a scalable system for serving multiple LoRA (Low-Rank Adaptation) adapters concurrently in large LLMs (LLMs). It manages memory efficiently by storing all adapters in main memory and dynamically fetching them to GPU memory. The system uses customized CUDA kernels for batch processing, optimizing both memory usage and computational efficiency.\n\n**When to use:** S-LoRA is ideal for scenarios where many fine-tuned variants of LLMs need to be served simultaneously with high throughput. It significantly reduces memory fragmentation and I/O overhead, making it suitable for large-scale deployments in resource-constrained environments.\n\n*   **Efficient Memory Management:** Utilizes a unified memory pool to manage adapter weights dynamically, reducing memory fragmentation.\n*   **High Throughput Serving:** Custom CUDA kernels enable efficient heterogeneous batching of LoRA computations, allowing the serving of thousands of adapters with minimal overhead.\n*   **Reduced Training and Communication Costs:** Offers an effective solution in federated learning scenarios by lowering the costs associated with training and data communication.\n\n1.  **Storage of Adapters:** All adapters are stored in the main memory, ready for dynamic retrieval.\n2.  **Dynamic Fetching:** Adapters required for current computations are fetched into GPU memory as needed.\n3.  **Batch Processing:** Customized CUDA kernels facilitate batch processing, ensuring efficient computation across various sequence lengths and adapter ranks.",
    "order": 13,
    "orderInChapter": 9,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 419,
      "contentLength": 3629
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#s-lora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-dora-14",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "DoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> DoRA is an advanced fine-tuning method that decomposes pre-trained model weights into magnitude and directional components. This decomposition facilitates efficient fine-tuning by employing LoRA for directional updates and introducing trainable magnitude components to enhance the learning capacity and stability.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> DoRA is particularly effective when there is a need to bridge the performance gap between LoRA-based methods and full fine-tuning without increasing inference costs. It’s suitable for tasks that require high performance, such as commonsense reasoning, visual instruction tuning, and multimodal understanding.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Weight Decomposition:</strong> Separates weights into magnitude and direction, allowing for targeted updates that enhance learning capability without additional inference overhead.</li>\n      <li><strong>Enhanced Learning Capacity:</strong> Integrates trainable magnitude components with directional updates, providing a balanced approach to fine-tuning that improves both stability and learning capacity.</li>\n      <li><strong>Versatility Across Tasks:</strong> Demonstrates superior performance across various tasks and models, proving its adaptability and effectiveness in different settings.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Decomposition of Weights:</strong> Begins with the decomposition of pre-trained model weights into their magnitude and directional components.</li>\n      <li><strong>Directional Updates Using LoRA:</strong> Employs LoRA specifically for updating directional components during fine-tuning.</li>\n      <li><strong>Training of Magnitude Components:</strong> Trainable magnitude components are fine-tuned separately, enhancing the overall learning capacity of the model.</li>\n      <li><strong>Performance Evaluation:</strong> The effectiveness of DoRA is validated across multiple tasks, showcasing significant performance improvements compared to other fine-tuning methods.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> DoRA is an advanced fine-tuning method that decomposes pre-trained model weights into magnitude and directional components. This decomposition facilitates efficient fine-tuning by employing LoRA for directional updates and introducing trainable magnitude components to enhance the learning capacity and stability.</p>\n<p><strong>When to use:</strong> DoRA is particularly effective when there is a need to bridge the performance gap between LoRA-based methods and full fine-tuning without increasing inference costs. It’s suitable for tasks that require high performance, such as commonsense reasoning, visual instruction tuning, and multimodal understanding.</p>\n<ul>\n      <li><strong>Weight Decomposition:</strong> Separates weights into magnitude and direction, allowing for targeted updates that enhance learning capability without additional inference overhead.</li>\n      <li><strong>Enhanced Learning Capacity:</strong> Integrates trainable magnitude components with directional updates, providing a balanced approach to fine-tuning that improves both stability and learning capacity.</li>\n      <li><strong>Versatility Across Tasks:</strong> Demonstrates superior performance across various tasks and models, proving its adaptability and effectiveness in different settings.</li>\n    </ul>\n<ol>\n      <li><strong>Decomposition of Weights:</strong> Begins with the decomposition of pre-trained model weights into their magnitude and directional components.</li>\n      <li><strong>Directional Updates Using LoRA:</strong> Employs LoRA specifically for updating directional components during fine-tuning.</li>\n      <li><strong>Training of Magnitude Components:</strong> Trainable magnitude components are fine-tuned separately, enhancing the overall learning capacity of the model.</li>\n      <li><strong>Performance Evaluation:</strong> The effectiveness of DoRA is validated across multiple tasks, showcasing significant performance improvements compared to other fine-tuning methods.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** DoRA is an advanced fine-tuning method that decomposes pre-trained model weights into magnitude and directional components. This decomposition facilitates efficient fine-tuning by employing LoRA for directional updates and introducing trainable magnitude components to enhance the learning capacity and stability.\n    \n*   **When to use:** DoRA is particularly effective when there is a need to bridge the performance gap between LoRA-based methods and full fine-tuning without increasing inference costs. It’s suitable for tasks that require high performance, such as commonsense reasoning, visual instruction tuning, and multimodal understanding.\n    \n*   **Key Features:**\n    *   **Weight Decomposition:** Separates weights into magnitude and direction, allowing for targeted updates that enhance learning capability without additional inference overhead.\n    *   **Enhanced Learning Capacity:** Integrates trainable magnitude components with directional updates, providing a balanced approach to fine-tuning that improves both stability and learning capacity.\n    *   **Versatility Across Tasks:** Demonstrates superior performance across various tasks and models, proving its adaptability and effectiveness in different settings.\n*   **Process:**\n    1.  **Decomposition of Weights:** Begins with the decomposition of pre-trained model weights into their magnitude and directional components.\n    2.  **Directional Updates Using LoRA:** Employs LoRA specifically for updating directional components during fine-tuning.\n    3.  **Training of Magnitude Components:** Trainable magnitude components are fine-tuned separately, enhancing the overall learning capacity of the model.\n    4.  **Performance Evaluation:** The effectiveness of DoRA is validated across multiple tasks, showcasing significant performance improvements compared to other fine-tuning methods.\n\n**What:** DoRA is an advanced fine-tuning method that decomposes pre-trained model weights into magnitude and directional components. This decomposition facilitates efficient fine-tuning by employing LoRA for directional updates and introducing trainable magnitude components to enhance the learning capacity and stability.\n\n**When to use:** DoRA is particularly effective when there is a need to bridge the performance gap between LoRA-based methods and full fine-tuning without increasing inference costs. It’s suitable for tasks that require high performance, such as commonsense reasoning, visual instruction tuning, and multimodal understanding.\n\n*   **Weight Decomposition:** Separates weights into magnitude and direction, allowing for targeted updates that enhance learning capability without additional inference overhead.\n*   **Enhanced Learning Capacity:** Integrates trainable magnitude components with directional updates, providing a balanced approach to fine-tuning that improves both stability and learning capacity.\n*   **Versatility Across Tasks:** Demonstrates superior performance across various tasks and models, proving its adaptability and effectiveness in different settings.\n\n1.  **Decomposition of Weights:** Begins with the decomposition of pre-trained model weights into their magnitude and directional components.\n2.  **Directional Updates Using LoRA:** Employs LoRA specifically for updating directional components during fine-tuning.\n3.  **Training of Magnitude Components:** Trainable magnitude components are fine-tuned separately, enhancing the overall learning capacity of the model.\n4.  **Performance Evaluation:** The effectiveness of DoRA is validated across multiple tasks, showcasing significant performance improvements compared to other fine-tuning methods.",
    "order": 14,
    "orderInChapter": 10,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 451,
      "contentLength": 4211
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#dora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-spafit-15",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "SPAFIT",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> SPAFIT (Stratified Progressive Adaptation Fine-tuning) is a Parameter-Efficient Fine-Tuning (PEFT) method that targets specific layers of a Transformer-based large language model according to their contribution to linguistic knowledge.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> SPAFIT is effective when you want to avoid the pitfalls of catastrophic forgetting and computational inefficiency typical in full model fine-tuning. It’s particularly useful for tasks that require different levels of linguistic processing, allowing for tailored adaptation.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Layer-Specific Fine-Tuning:</strong> SPAFIT divides the model into three groups, allowing each group of layers to be fine-tuned to varying extents based on their importance to task performance.</li>\n      <li><strong>Efficiency and Performance:</strong> By fine-tuning fewer parameters, SPAFIT achieves competitive or superior results compared to full fine-tuning, particularly on tasks involving sentence similarity.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Layer Grouping:</strong> Model layers are categorized into three groups based on their function and linguistic contribution.</li>\n      <li><strong>Adaptive Fine-Tuning:</strong> Group 1 layers remain frozen, Group 2 layers are fine-tuned only on bias terms, and Group 3 layers undergo a more comprehensive fine-tuning using BitFit and LoRA for different components.</li>\n      <li><strong>Performance Evaluation:</strong> SPAFIT’s effectiveness is validated across multiple NLP tasks, showing strong results with fewer fine-tuned parameters.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> SPAFIT (Stratified Progressive Adaptation Fine-tuning) is a Parameter-Efficient Fine-Tuning (PEFT) method that targets specific layers of a Transformer-based large language model according to their contribution to linguistic knowledge.</p>\n<p><strong>When to use:</strong> SPAFIT is effective when you want to avoid the pitfalls of catastrophic forgetting and computational inefficiency typical in full model fine-tuning. It’s particularly useful for tasks that require different levels of linguistic processing, allowing for tailored adaptation.</p>\n<ul>\n      <li><strong>Layer-Specific Fine-Tuning:</strong> SPAFIT divides the model into three groups, allowing each group of layers to be fine-tuned to varying extents based on their importance to task performance.</li>\n      <li><strong>Efficiency and Performance:</strong> By fine-tuning fewer parameters, SPAFIT achieves competitive or superior results compared to full fine-tuning, particularly on tasks involving sentence similarity.</li>\n    </ul>\n<ol>\n      <li><strong>Layer Grouping:</strong> Model layers are categorized into three groups based on their function and linguistic contribution.</li>\n      <li><strong>Adaptive Fine-Tuning:</strong> Group 1 layers remain frozen, Group 2 layers are fine-tuned only on bias terms, and Group 3 layers undergo a more comprehensive fine-tuning using BitFit and LoRA for different components.</li>\n      <li><strong>Performance Evaluation:</strong> SPAFIT’s effectiveness is validated across multiple NLP tasks, showing strong results with fewer fine-tuned parameters.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** SPAFIT (Stratified Progressive Adaptation Fine-tuning) is a Parameter-Efficient Fine-Tuning (PEFT) method that targets specific layers of a Transformer-based large language model according to their contribution to linguistic knowledge.\n    \n*   **When to use:** SPAFIT is effective when you want to avoid the pitfalls of catastrophic forgetting and computational inefficiency typical in full model fine-tuning. It’s particularly useful for tasks that require different levels of linguistic processing, allowing for tailored adaptation.\n    \n*   **Key Features:**\n    *   **Layer-Specific Fine-Tuning:** SPAFIT divides the model into three groups, allowing each group of layers to be fine-tuned to varying extents based on their importance to task performance.\n    *   **Efficiency and Performance:** By fine-tuning fewer parameters, SPAFIT achieves competitive or superior results compared to full fine-tuning, particularly on tasks involving sentence similarity.\n*   **Process:**\n    1.  **Layer Grouping:** Model layers are categorized into three groups based on their function and linguistic contribution.\n    2.  **Adaptive Fine-Tuning:** Group 1 layers remain frozen, Group 2 layers are fine-tuned only on bias terms, and Group 3 layers undergo a more comprehensive fine-tuning using BitFit and LoRA for different components.\n    3.  **Performance Evaluation:** SPAFIT’s effectiveness is validated across multiple NLP tasks, showing strong results with fewer fine-tuned parameters.\n\n**What:** SPAFIT (Stratified Progressive Adaptation Fine-tuning) is a Parameter-Efficient Fine-Tuning (PEFT) method that targets specific layers of a Transformer-based large language model according to their contribution to linguistic knowledge.\n\n**When to use:** SPAFIT is effective when you want to avoid the pitfalls of catastrophic forgetting and computational inefficiency typical in full model fine-tuning. It’s particularly useful for tasks that require different levels of linguistic processing, allowing for tailored adaptation.\n\n*   **Layer-Specific Fine-Tuning:** SPAFIT divides the model into three groups, allowing each group of layers to be fine-tuned to varying extents based on their importance to task performance.\n*   **Efficiency and Performance:** By fine-tuning fewer parameters, SPAFIT achieves competitive or superior results compared to full fine-tuning, particularly on tasks involving sentence similarity.\n\n1.  **Layer Grouping:** Model layers are categorized into three groups based on their function and linguistic contribution.\n2.  **Adaptive Fine-Tuning:** Group 1 layers remain frozen, Group 2 layers are fine-tuned only on bias terms, and Group 3 layers undergo a more comprehensive fine-tuning using BitFit and LoRA for different components.\n3.  **Performance Evaluation:** SPAFIT’s effectiveness is validated across multiple NLP tasks, showing strong results with fewer fine-tuned parameters.",
    "order": 15,
    "orderInChapter": 11,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "transformer",
      "nlp",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 385,
      "contentLength": 3369
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#spafit",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-nola-16",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "NOLA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> NOLA is a novel method for compressing large LLMs that reparameterizes the matrices used in Low-Rank Adaptation (LoRA) through linear combinations of randomly generated basis matrices, drastically reducing the parameter count.</p>\n  </li>\n  <li>\n    <p><strong>When to use:</strong> Ideal for situations where extreme model compression is necessary without sacrificing performance, making it suitable for deployment in resource-constrained environments or when model storage costs need to be minimized.</p>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>Parameter Compression:</strong> Achieves up to 20 times fewer parameters than the most compressed LoRA models.</li>\n      <li><strong>Decoupling Parameter Count:</strong> Separates the number of trainable parameters from the rank choice and network architecture, allowing for more flexible and efficient model compression.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Matrix Reparameterization:</strong> Decomposes weight changes into two matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-316-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1696\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1697\"><span class=\"mi\" id=\"MathJax-Span-1698\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-316\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-317-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1699\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1700\"><span class=\"mi\" id=\"MathJax-Span-1701\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-317\">B</script>, which are then reparameterized using a linear combination of random basis matrices.</li>\n      <li><strong>Learning Combination Coefficients:</strong> Focuses on optimizing the mixture coefficients for these basis matrices while keeping the original matrices frozen.</li>\n      <li><strong>Storage Optimization:</strong> Stores only the coefficients and the seed of the random number generator used for creating the basis matrices, significantly reducing storage requirements.</li>\n      <li><strong>Evaluation on Multiple Tasks:</strong> Demonstrates effectiveness across various tasks and models, maintaining or exceeding benchmark metrics while significantly reducing the parameter count.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> NOLA is a novel method for compressing large LLMs that reparameterizes the matrices used in Low-Rank Adaptation (LoRA) through linear combinations of randomly generated basis matrices, drastically reducing the parameter count.</p>\n<p><strong>When to use:</strong> Ideal for situations where extreme model compression is necessary without sacrificing performance, making it suitable for deployment in resource-constrained environments or when model storage costs need to be minimized.</p>\n<ul>\n      <li><strong>Parameter Compression:</strong> Achieves up to 20 times fewer parameters than the most compressed LoRA models.</li>\n      <li><strong>Decoupling Parameter Count:</strong> Separates the number of trainable parameters from the rank choice and network architecture, allowing for more flexible and efficient model compression.</li>\n    </ul>\n<ol>\n      <li><strong>Matrix Reparameterization:</strong> Decomposes weight changes into two matrices, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-316-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1696\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1697\"><span class=\"mi\" id=\"MathJax-Span-1698\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-316\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-317-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1699\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1700\"><span class=\"mi\" id=\"MathJax-Span-1701\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-317\">B</script>, which are then reparameterized using a linear combination of random basis matrices.</li>\n      <li><strong>Learning Combination Coefficients:</strong> Focuses on optimizing the mixture coefficients for these basis matrices while keeping the original matrices frozen.</li>\n      <li><strong>Storage Optimization:</strong> Stores only the coefficients and the seed of the random number generator used for creating the basis matrices, significantly reducing storage requirements.</li>\n      <li><strong>Evaluation on Multiple Tasks:</strong> Demonstrates effectiveness across various tasks and models, maintaining or exceeding benchmark metrics while significantly reducing the parameter count.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** NOLA is a novel method for compressing large LLMs that reparameterizes the matrices used in Low-Rank Adaptation (LoRA) through linear combinations of randomly generated basis matrices, drastically reducing the parameter count.\n    \n*   **When to use:** Ideal for situations where extreme model compression is necessary without sacrificing performance, making it suitable for deployment in resource-constrained environments or when model storage costs need to be minimized.\n    \n*   **Key Features:**\n    *   **Parameter Compression:** Achieves up to 20 times fewer parameters than the most compressed LoRA models.\n    *   **Decoupling Parameter Count:** Separates the number of trainable parameters from the rank choice and network architecture, allowing for more flexible and efficient model compression.\n*   **Process:**\n    1.  **Matrix Reparameterization:** Decomposes weight changes into two matrices, AAA and BBB, which are then reparameterized using a linear combination of random basis matrices.\n    2.  **Learning Combination Coefficients:** Focuses on optimizing the mixture coefficients for these basis matrices while keeping the original matrices frozen.\n    3.  **Storage Optimization:** Stores only the coefficients and the seed of the random number generator used for creating the basis matrices, significantly reducing storage requirements.\n    4.  **Evaluation on Multiple Tasks:** Demonstrates effectiveness across various tasks and models, maintaining or exceeding benchmark metrics while significantly reducing the parameter count.\n\n**What:** NOLA is a novel method for compressing large LLMs that reparameterizes the matrices used in Low-Rank Adaptation (LoRA) through linear combinations of randomly generated basis matrices, drastically reducing the parameter count.\n\n**When to use:** Ideal for situations where extreme model compression is necessary without sacrificing performance, making it suitable for deployment in resource-constrained environments or when model storage costs need to be minimized.\n\n*   **Parameter Compression:** Achieves up to 20 times fewer parameters than the most compressed LoRA models.\n*   **Decoupling Parameter Count:** Separates the number of trainable parameters from the rank choice and network architecture, allowing for more flexible and efficient model compression.\n\n1.  **Matrix Reparameterization:** Decomposes weight changes into two matrices, AAA and BBB, which are then reparameterized using a linear combination of random basis matrices.\n2.  **Learning Combination Coefficients:** Focuses on optimizing the mixture coefficients for these basis matrices while keeping the original matrices frozen.\n3.  **Storage Optimization:** Stores only the coefficients and the seed of the random number generator used for creating the basis matrices, significantly reducing storage requirements.\n4.  **Evaluation on Multiple Tasks:** Demonstrates effectiveness across various tasks and models, maintaining or exceeding benchmark metrics while significantly reducing the parameter count.",
    "order": 16,
    "orderInChapter": 12,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "llm",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 401,
      "contentLength": 8261
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#nola",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-mora-17",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Which PEFT Technique to Choose: a Mental Model",
    "title": "MoRA",
    "subtitle": "Which PEFT Technique to Choose: a Mental Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>What:</strong> MoRA (Matrix of Rank Adaptation) is an advanced fine-tuning technique designed to enhance the capacity of large LLMs (LLMs) to learn and retain new knowledge. It replaces the low-rank matrices used in LoRA with a high-rank square matrix, significantly increasing the model’s update capacity without increasing the number of trainable parameters. This method introduces non-parameterized operators to adjust the input and output dimensions, ensuring efficient integration with existing LLMs.</p>\n  </li>\n  <li><strong>When to use:</strong> MoRA is particularly effective for tasks that require substantial knowledge enhancement and memory capacity. It is well-suited for scenarios where:\n    <ul>\n      <li><strong>Memory-Intensive Tasks:</strong> The task demands significant memorization and the retention of new knowledge, such as continual pretraining and memory tasks.</li>\n      <li><strong>Limited Resources:</strong> You need to maximize performance while maintaining low computational and memory overheads.</li>\n      <li><strong>Performance Matching or Exceeding LoRA:</strong> The method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, making it a versatile choice across various applications.</li>\n    </ul>\n  </li>\n  <li><strong>Key Features:</strong>\n    <ul>\n      <li><strong>High-Rank Updates:</strong> Utilizes a square matrix to achieve high-rank updates, significantly increasing the model’s capacity to learn and retain new information.</li>\n      <li><strong>Efficient Parameter Use:</strong> Maintains the same number of trainable parameters as LoRA by employing non-parameterized operators for input and output dimension adjustments.</li>\n      <li><strong>Versatility Across Tasks:</strong> Demonstrates superior performance in memory-intensive tasks and matches performance in other fine-tuning scenarios, proving its effectiveness across diverse applications.</li>\n    </ul>\n  </li>\n  <li><strong>Process:</strong>\n    <ol>\n      <li><strong>Input Dimension Reduction:</strong> Non-parameterized operators reduce the input dimension for the high-rank square matrix.</li>\n      <li><strong>Output Dimension Increase:</strong> Corresponding operators increase the output dimension, maintaining parameter efficiency.</li>\n      <li><strong>Integration with LLMs:</strong> The high-rank matrix and operators can be integrated back into the LLM, similar to LoRA, ensuring seamless deployment.</li>\n      <li><strong>Empirical Evaluation:</strong> Comprehensive evaluation across multiple tasks, including instruction tuning, mathematical reasoning, and continual pretraining, demonstrating significant improvements in memory-intensive tasks and comparable performance in others.</li>\n    </ol>\n  </li>\n</ul>\n<p><strong>What:</strong> MoRA (Matrix of Rank Adaptation) is an advanced fine-tuning technique designed to enhance the capacity of large LLMs (LLMs) to learn and retain new knowledge. It replaces the low-rank matrices used in LoRA with a high-rank square matrix, significantly increasing the model’s update capacity without increasing the number of trainable parameters. This method introduces non-parameterized operators to adjust the input and output dimensions, ensuring efficient integration with existing LLMs.</p>\n<ul>\n      <li><strong>Memory-Intensive Tasks:</strong> The task demands significant memorization and the retention of new knowledge, such as continual pretraining and memory tasks.</li>\n      <li><strong>Limited Resources:</strong> You need to maximize performance while maintaining low computational and memory overheads.</li>\n      <li><strong>Performance Matching or Exceeding LoRA:</strong> The method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, making it a versatile choice across various applications.</li>\n    </ul>\n<ul>\n      <li><strong>High-Rank Updates:</strong> Utilizes a square matrix to achieve high-rank updates, significantly increasing the model’s capacity to learn and retain new information.</li>\n      <li><strong>Efficient Parameter Use:</strong> Maintains the same number of trainable parameters as LoRA by employing non-parameterized operators for input and output dimension adjustments.</li>\n      <li><strong>Versatility Across Tasks:</strong> Demonstrates superior performance in memory-intensive tasks and matches performance in other fine-tuning scenarios, proving its effectiveness across diverse applications.</li>\n    </ul>\n<ol>\n      <li><strong>Input Dimension Reduction:</strong> Non-parameterized operators reduce the input dimension for the high-rank square matrix.</li>\n      <li><strong>Output Dimension Increase:</strong> Corresponding operators increase the output dimension, maintaining parameter efficiency.</li>\n      <li><strong>Integration with LLMs:</strong> The high-rank matrix and operators can be integrated back into the LLM, similar to LoRA, ensuring seamless deployment.</li>\n      <li><strong>Empirical Evaluation:</strong> Comprehensive evaluation across multiple tasks, including instruction tuning, mathematical reasoning, and continual pretraining, demonstrating significant improvements in memory-intensive tasks and comparable performance in others.</li>\n    </ol>",
    "contentMarkdown": "*   **What:** MoRA (Matrix of Rank Adaptation) is an advanced fine-tuning technique designed to enhance the capacity of large LLMs (LLMs) to learn and retain new knowledge. It replaces the low-rank matrices used in LoRA with a high-rank square matrix, significantly increasing the model’s update capacity without increasing the number of trainable parameters. This method introduces non-parameterized operators to adjust the input and output dimensions, ensuring efficient integration with existing LLMs.\n    \n*   **When to use:** MoRA is particularly effective for tasks that require substantial knowledge enhancement and memory capacity. It is well-suited for scenarios where:\n    *   **Memory-Intensive Tasks:** The task demands significant memorization and the retention of new knowledge, such as continual pretraining and memory tasks.\n    *   **Limited Resources:** You need to maximize performance while maintaining low computational and memory overheads.\n    *   **Performance Matching or Exceeding LoRA:** The method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, making it a versatile choice across various applications.\n*   **Key Features:**\n    *   **High-Rank Updates:** Utilizes a square matrix to achieve high-rank updates, significantly increasing the model’s capacity to learn and retain new information.\n    *   **Efficient Parameter Use:** Maintains the same number of trainable parameters as LoRA by employing non-parameterized operators for input and output dimension adjustments.\n    *   **Versatility Across Tasks:** Demonstrates superior performance in memory-intensive tasks and matches performance in other fine-tuning scenarios, proving its effectiveness across diverse applications.\n*   **Process:**\n    1.  **Input Dimension Reduction:** Non-parameterized operators reduce the input dimension for the high-rank square matrix.\n    2.  **Output Dimension Increase:** Corresponding operators increase the output dimension, maintaining parameter efficiency.\n    3.  **Integration with LLMs:** The high-rank matrix and operators can be integrated back into the LLM, similar to LoRA, ensuring seamless deployment.\n    4.  **Empirical Evaluation:** Comprehensive evaluation across multiple tasks, including instruction tuning, mathematical reasoning, and continual pretraining, demonstrating significant improvements in memory-intensive tasks and comparable performance in others.\n\n**What:** MoRA (Matrix of Rank Adaptation) is an advanced fine-tuning technique designed to enhance the capacity of large LLMs (LLMs) to learn and retain new knowledge. It replaces the low-rank matrices used in LoRA with a high-rank square matrix, significantly increasing the model’s update capacity without increasing the number of trainable parameters. This method introduces non-parameterized operators to adjust the input and output dimensions, ensuring efficient integration with existing LLMs.\n\n*   **Memory-Intensive Tasks:** The task demands significant memorization and the retention of new knowledge, such as continual pretraining and memory tasks.\n*   **Limited Resources:** You need to maximize performance while maintaining low computational and memory overheads.\n*   **Performance Matching or Exceeding LoRA:** The method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, making it a versatile choice across various applications.\n\n*   **High-Rank Updates:** Utilizes a square matrix to achieve high-rank updates, significantly increasing the model’s capacity to learn and retain new information.\n*   **Efficient Parameter Use:** Maintains the same number of trainable parameters as LoRA by employing non-parameterized operators for input and output dimension adjustments.\n*   **Versatility Across Tasks:** Demonstrates superior performance in memory-intensive tasks and matches performance in other fine-tuning scenarios, proving its effectiveness across diverse applications.\n\n1.  **Input Dimension Reduction:** Non-parameterized operators reduce the input dimension for the high-rank square matrix.\n2.  **Output Dimension Increase:** Corresponding operators increase the output dimension, maintaining parameter efficiency.\n3.  **Integration with LLMs:** The high-rank matrix and operators can be integrated back into the LLM, similar to LoRA, ensuring seamless deployment.\n4.  **Empirical Evaluation:** Comprehensive evaluation across multiple tasks, including instruction tuning, mathematical reasoning, and continual pretraining, demonstrating significant improvements in memory-intensive tasks and comparable performance in others.",
    "order": 17,
    "orderInChapter": 13,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 588,
      "contentLength": 5316
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#mora",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  },
  {
    "id": "ai-parameter-efficient-fine-tuning-lora-vs-qlora-experimentation-by-sebastian-raschka-18",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Parameter Efficient Fine-Tuning",
    "articleSlug": "parameter-efficient-fine-tuning",
    "chapter": "Related: Surgical Fine-tuning",
    "title": "LoRA vs. QLoRA Experimentation by Sebastian Raschka",
    "subtitle": "Related: Surgical Fine-tuning",
    "contentHtml": "<ul>\n  <li>This section is taken from <a href=\"https://www.linkedin.com/posts/sebastianraschka_llms-genai-deeplearning-activity-7118583338696671233-2_kY?utm_source=share&amp;utm_medium=member_desktop\">Sebastian Raschka’s</a> post on LoRA &amp; QLoRA experiments to finetune open-source LLMs, and presents his learnings:\n    <ol>\n      <li>Despite embracing the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.</li>\n      <li>QLoRA presents a trade-off that might be worthwhile if you’re constrained by GPU memory. It offers 33% memory savings at the cost of a 33% increase in runtime.</li>\n      <li>When finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While SGD on its own is suboptimal, there’s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.</li>\n      <li>While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.</li>\n      <li>For static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting.</li>\n      <li>If you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices, to maximize model performance.</li>\n      <li>Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value.</li>\n      <li>7B models can be finetuned efficiently within a few hours on a single GPU possessing 14 Gb of RAM.</li>\n    </ol>\n  </li>\n  <li>With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.</li>\n</ul>\n<ol>\n      <li>Despite embracing the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.</li>\n      <li>QLoRA presents a trade-off that might be worthwhile if you’re constrained by GPU memory. It offers 33% memory savings at the cost of a 33% increase in runtime.</li>\n      <li>When finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While SGD on its own is suboptimal, there’s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.</li>\n      <li>While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.</li>\n      <li>For static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting.</li>\n      <li>If you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices, to maximize model performance.</li>\n      <li>Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value.</li>\n      <li>7B models can be finetuned efficiently within a few hours on a single GPU possessing 14 Gb of RAM.</li>\n    </ol>",
    "contentMarkdown": "*   This section is taken from [Sebastian Raschka’s](https://www.linkedin.com/posts/sebastianraschka_llms-genai-deeplearning-activity-7118583338696671233-2_kY?utm_source=share&utm_medium=member_desktop) post on LoRA & QLoRA experiments to finetune open-source LLMs, and presents his learnings:\n    1.  Despite embracing the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.\n    2.  QLoRA presents a trade-off that might be worthwhile if you’re constrained by GPU memory. It offers 33% memory savings at the cost of a 33% increase in runtime.\n    3.  When finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While SGD on its own is suboptimal, there’s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.\n    4.  While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.\n    5.  For static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting.\n    6.  If you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices, to maximize model performance.\n    7.  Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value.\n    8.  7B models can be finetuned efficiently within a few hours on a single GPU possessing 14 Gb of RAM.\n*   With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.\n\n1.  Despite embracing the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.\n2.  QLoRA presents a trade-off that might be worthwhile if you’re constrained by GPU memory. It offers 33% memory savings at the cost of a 33% increase in runtime.\n3.  When finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While SGD on its own is suboptimal, there’s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.\n4.  While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.\n5.  For static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting.\n6.  If you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices, to maximize model performance.\n7.  Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value.\n8.  7B models can be finetuned efficiently within a few hours on a single GPU possessing 14 Gb of RAM.",
    "order": 18,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "datatraining",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 527,
      "contentLength": 3658
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/#lora-vs.-qlora-experimentation-by-sebastian-raschka",
    "scrapedAt": "2025-12-28T11:49:57.384Z"
  }
]