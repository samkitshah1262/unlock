[
  {
    "id": "ai-ml-runtimes-common-architectural-layers-1",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Architecture Overview of On-Device ML Runtimes",
    "title": "Common Architectural Layers",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Most on-device ML runtimes follow a layered architecture consisting of the following components:</p>\n\n    <ul>\n      <li>\n        <p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n      </li>\n      <li>\n        <p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n      </li>\n      <li>\n        <p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n      </li>\n      <li>\n        <p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n      </li>\n      <li>\n        <p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Most on-device ML runtimes follow a layered architecture consisting of the following components:</p>\n<ul>\n      <li>\n        <p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n      </li>\n      <li>\n        <p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n      </li>\n      <li>\n        <p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n      </li>\n      <li>\n        <p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n      </li>\n      <li>\n        <p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>\n      </li>\n    </ul>\n<p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n<p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n<p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n<p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n<p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n<p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>",
    "contentMarkdown": "*   Most on-device ML runtimes follow a layered architecture consisting of the following components:\n    \n    *   **Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n        \n    *   **Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n        \n    *   **Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n        \n    *   **Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n        \n    *   **Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n        \n    *   **Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.\n        \n\nMost on-device ML runtimes follow a layered architecture consisting of the following components:\n\n*   **Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n    \n*   **Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n    \n*   **Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n    \n*   **Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n    \n*   **Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n    \n*   **Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.\n    \n\n**Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n\n**Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n\n**Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n\n**Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n\n**Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n\n**Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.",
    "contentLength": 5903,
    "wordCount": 546,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#common-architectural-layers"
  },
  {
    "id": "ai-ml-runtimes-architecture-by-runtime-2",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Architecture Overview of On-Device ML Runtimes",
    "title": "Architecture by Runtime",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<h4 id=\"tensorrt\">TensorRT</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.plan</code> (TensorRT Engine)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Accepts models in ONNX, TensorFlow, or Caffe formats</li>\n      <li>Optimizes and compiles model into a serialized CUDA engine (<code class=\"language-plaintext highlighter-rouge\">.plan</code>)</li>\n      <li>Engine executes directly via CUDA on supported NVIDIA GPUs</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: NVIDIA GPUs (desktop, embedded, server)</li>\n  <li><strong>Backend Design</strong>: Layer fusion, kernel autotuning, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, Tensor Cores</li>\n  <li><strong>Strengths</strong>: Extreme inference speed on NVIDIA hardware, minimal latency, quantization support</li>\n  <li><strong>Weaknesses</strong>: GPU-only, requires CUDA, less flexible for model updates at runtime</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Accepts models in ONNX, TensorFlow, or Caffe formats</li>\n      <li>Optimizes and compiles model into a serialized CUDA engine (<code class=\"language-plaintext highlighter-rouge\">.plan</code>)</li>\n      <li>Engine executes directly via CUDA on supported NVIDIA GPUs</li>\n    </ul>\n<h4 id=\"core-ml\">Core ML</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, optionally converted from other formats using <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Model is compiled into a Core ML model package (<code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>)</li>\n      <li>Uses internal execution graph</li>\n      <li>Runtime determines target hardware (CPU, GPU, or ANE) dynamically</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, GPU, Apple Neural Engine (ANE)</li>\n  <li><strong>Backend Design</strong>: Proprietary graph engine, no direct user-accessible IR</li>\n  <li><strong>Strengths</strong>: Seamless Apple integration, high-level API, automatic hardware optimization</li>\n  <li><strong>Weaknesses</strong>: Apple-platform only, opaque architecture, limited transparency for debugging</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Model is compiled into a Core ML model package (<code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>)</li>\n      <li>Uses internal execution graph</li>\n      <li>Runtime determines target hardware (CPU, GPU, or ANE) dynamically</li>\n    </ul>\n<h4 id=\"mlx-apple-mlx\">MLX (Apple MLX)</h4>\n<ul>\n  <li><strong>Model Format</strong>: Python-based tensor operations with PyTorch-like syntax</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Eager mode and graph execution both supported</li>\n      <li>Uses Metal Performance Shaders and ANE backend where possible</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: Primarily Apple Silicon (M-series CPU, GPU, ANE)</li>\n  <li><strong>Backend Design</strong>: Dynamic execution engine; uses MLX backend API</li>\n  <li><strong>Strengths</strong>: Developer flexibility, research-oriented, direct tensor ops</li>\n  <li><strong>Weaknesses</strong>: Early-stage, Apple-only, smaller community, fewer pre-built models</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Eager mode and graph execution both supported</li>\n      <li>Uses Metal Performance Shaders and ANE backend where possible</li>\n    </ul>\n<h4 id=\"onnx-runtime\">ONNX Runtime</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.onnx</code></li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Loads ONNX graph and converts to optimized IR</li>\n      <li>Graph optimization passes applied (e.g., constant folding, fusion)</li>\n      <li>Execution providers (EPs) handle hardware-specific execution</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, GPU (CUDA, ROCm), NNAPI, DirectML, ARM, OpenVINO</li>\n  <li><strong>Backend Design</strong>: Pluggable EP system, modular kernel dispatch</li>\n  <li><strong>Strengths</strong>: Cross-platform, flexible, highly optimized</li>\n  <li><strong>Weaknesses</strong>: Model conversion may be lossy or complex, mobile-specific tuning needed</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Loads ONNX graph and converts to optimized IR</li>\n      <li>Graph optimization passes applied (e.g., constant folding, fusion)</li>\n      <li>Execution providers (EPs) handle hardware-specific execution</li>\n    </ul>\n<h4 id=\"executorch\">ExecuTorch</h4>\n<ul>\n  <li><strong>Model Format</strong>: PyTorch Lite models, <code class=\"language-plaintext highlighter-rouge\">ptc</code> compiled bytecode</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>TorchScript traced models compiled using Ahead-of-Time (AOT) compiler</li>\n      <li>Produces a minimal runtime with only needed ops</li>\n      <li>Bytecode is executed on microcontroller or mobile device</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, MCU, potentially DSP/NPU</li>\n  <li><strong>Backend Design</strong>: AOT compiler, custom micro runtime, graph executor</li>\n  <li><strong>Strengths</strong>: Lightweight, optimized for resource-constrained environments</li>\n  <li><strong>Weaknesses</strong>: Limited model format support, newer toolchain</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>TorchScript traced models compiled using Ahead-of-Time (AOT) compiler</li>\n      <li>Produces a minimal runtime with only needed ops</li>\n      <li>Bytecode is executed on microcontroller or mobile device</li>\n    </ul>\n<h4 id=\"lidartlm\">LidarTLM</h4>\n<ul>\n  <li><strong>Model Format</strong>: Custom or converted models for lidar data processing</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Ingests sparse point cloud or voxel data</li>\n      <li>Uses spatial and temporal inference pipelines</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: ARM CPUs, embedded GPU, or AI co-processors</li>\n  <li><strong>Backend Design</strong>: Spatially-aware computation graph; sensor-fusion modules</li>\n  <li><strong>Strengths</strong>: Specialized for lidar, supports sensor fusion</li>\n  <li><strong>Weaknesses</strong>: Niche use case, limited community and documentation</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Ingests sparse point cloud or voxel data</li>\n      <li>Uses spatial and temporal inference pipelines</li>\n    </ul>\n<h4 id=\"llamacpp\"><code class=\"language-plaintext Highlighter-rouge\">llama.cpp</code></h4>\n<ul>\n  <li><strong>Model Format</strong>: Quantized LLM formats (GGUF, etc.)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Loads quantized model into memory</li>\n      <li>Performs batched matmul-based transformer inference</li>\n      <li>Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, optionally GPU</li>\n  <li><strong>Backend Design</strong>: Minimalist tensor framework, custom linear algebra, no IR</li>\n  <li><strong>Strengths</strong>: Extremely portable, optimized for low-RAM devices, self-contained</li>\n  <li><strong>Weaknesses</strong>: Focused only on LLMs, lower-level interface</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Loads quantized model into memory</li>\n      <li>Performs batched matmul-based transformer inference</li>\n      <li>Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)</li>\n    </ul>\n<h4 id=\"tensorflow-lite--serving\">TensorFlow Lite / Serving</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.tflite</code> (Lite), <code class=\"language-plaintext highlighter-rouge\">.pb</code> or SavedModel (Serving)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>TFLite: uses FlatBuffer model, loads and interprets ops</li>\n      <li>Serving: REST/gRPC server for remote model inference</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Support</strong>:</p>\n\n    <ul>\n      <li>TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP</li>\n      <li>Serving: Primarily server-side; not for on-device use</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Backend Design</strong>:</p>\n\n    <ul>\n      <li>TFLite: statically compiled interpreters with kernel registry</li>\n      <li>TFLite delegates for hardware acceleration</li>\n    </ul>\n  </li>\n  <li><strong>Strengths</strong>: Broad compatibility, active ecosystem, stable</li>\n  <li><strong>Weaknesses</strong>: Delegate configuration can be tricky, Serving not suitable for offline use</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>TFLite: uses FlatBuffer model, loads and interprets ops</li>\n      <li>Serving: REST/gRPC server for remote model inference</li>\n    </ul>\n<p><strong>Hardware Support</strong>:</p>\n<ul>\n      <li>TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP</li>\n      <li>Serving: Primarily server-side; not for on-device use</li>\n    </ul>\n<p><strong>Backend Design</strong>:</p>\n<ul>\n      <li>TFLite: statically compiled interpreters with kernel registry</li>\n      <li>TFLite delegates for hardware acceleration</li>\n    </ul>",
    "contentMarkdown": "#### TensorRT\n\n*   **Model Format**: `.plan` (TensorRT Engine)\n*   **Execution Flow**:\n    \n    *   Accepts models in ONNX, TensorFlow, or Caffe formats\n    *   Optimizes and compiles model into a serialized CUDA engine (`.plan`)\n    *   Engine executes directly via CUDA on supported NVIDIA GPUs\n*   **Hardware Support**: NVIDIA GPUs (desktop, embedded, server)\n*   **Backend Design**: Layer fusion, kernel autotuning, `int8`/`float16` quantization, Tensor Cores\n*   **Strengths**: Extreme inference speed on NVIDIA hardware, minimal latency, quantization support\n*   **Weaknesses**: GPU-only, requires CUDA, less flexible for model updates at runtime\n\n**Execution Flow**:\n\n*   Accepts models in ONNX, TensorFlow, or Caffe formats\n*   Optimizes and compiles model into a serialized CUDA engine (`.plan`)\n*   Engine executes directly via CUDA on supported NVIDIA GPUs\n\n#### Core ML\n\n*   **Model Format**: `.mlmodel`, optionally converted from other formats using `coremltools`\n*   **Execution Flow**:\n    \n    *   Model is compiled into a Core ML model package (`.mlmodelc`)\n    *   Uses internal execution graph\n    *   Runtime determines target hardware (CPU, GPU, or ANE) dynamically\n*   **Hardware Support**: CPU, GPU, Apple Neural Engine (ANE)\n*   **Backend Design**: Proprietary graph engine, no direct user-accessible IR\n*   **Strengths**: Seamless Apple integration, high-level API, automatic hardware optimization\n*   **Weaknesses**: Apple-platform only, opaque architecture, limited transparency for debugging\n\n**Execution Flow**:\n\n*   Model is compiled into a Core ML model package (`.mlmodelc`)\n*   Uses internal execution graph\n*   Runtime determines target hardware (CPU, GPU, or ANE) dynamically\n\n#### MLX (Apple MLX)\n\n*   **Model Format**: Python-based tensor operations with PyTorch-like syntax\n*   **Execution Flow**:\n    \n    *   Eager mode and graph execution both supported\n    *   Uses Metal Performance Shaders and ANE backend where possible\n*   **Hardware Support**: Primarily Apple Silicon (M-series CPU, GPU, ANE)\n*   **Backend Design**: Dynamic execution engine; uses MLX backend API\n*   **Strengths**: Developer flexibility, research-oriented, direct tensor ops\n*   **Weaknesses**: Early-stage, Apple-only, smaller community, fewer pre-built models\n\n**Execution Flow**:\n\n*   Eager mode and graph execution both supported\n*   Uses Metal Performance Shaders and ANE backend where possible\n\n#### ONNX Runtime\n\n*   **Model Format**: `.onnx`\n*   **Execution Flow**:\n    \n    *   Loads ONNX graph and converts to optimized IR\n    *   Graph optimization passes applied (e.g., constant folding, fusion)\n    *   Execution providers (EPs) handle hardware-specific execution\n*   **Hardware Support**: CPU, GPU (CUDA, ROCm), NNAPI, DirectML, ARM, OpenVINO\n*   **Backend Design**: Pluggable EP system, modular kernel dispatch\n*   **Strengths**: Cross-platform, flexible, highly optimized\n*   **Weaknesses**: Model conversion may be lossy or complex, mobile-specific tuning needed\n\n**Execution Flow**:\n\n*   Loads ONNX graph and converts to optimized IR\n*   Graph optimization passes applied (e.g., constant folding, fusion)\n*   Execution providers (EPs) handle hardware-specific execution\n\n#### ExecuTorch\n\n*   **Model Format**: PyTorch Lite models, `ptc` compiled bytecode\n*   **Execution Flow**:\n    \n    *   TorchScript traced models compiled using Ahead-of-Time (AOT) compiler\n    *   Produces a minimal runtime with only needed ops\n    *   Bytecode is executed on microcontroller or mobile device\n*   **Hardware Support**: CPU, MCU, potentially DSP/NPU\n*   **Backend Design**: AOT compiler, custom micro runtime, graph executor\n*   **Strengths**: Lightweight, optimized for resource-constrained environments\n*   **Weaknesses**: Limited model format support, newer toolchain\n\n**Execution Flow**:\n\n*   TorchScript traced models compiled using Ahead-of-Time (AOT) compiler\n*   Produces a minimal runtime with only needed ops\n*   Bytecode is executed on microcontroller or mobile device\n\n#### LidarTLM\n\n*   **Model Format**: Custom or converted models for lidar data processing\n*   **Execution Flow**:\n    \n    *   Ingests sparse point cloud or voxel data\n    *   Uses spatial and temporal inference pipelines\n*   **Hardware Support**: ARM CPUs, embedded GPU, or AI co-processors\n*   **Backend Design**: Spatially-aware computation graph; sensor-fusion modules\n*   **Strengths**: Specialized for lidar, supports sensor fusion\n*   **Weaknesses**: Niche use case, limited community and documentation\n\n**Execution Flow**:\n\n*   Ingests sparse point cloud or voxel data\n*   Uses spatial and temporal inference pipelines\n\n#### `llama.cpp`\n\n*   **Model Format**: Quantized LLM formats (GGUF, etc.)\n*   **Execution Flow**:\n    \n    *   Loads quantized model into memory\n    *   Performs batched matmul-based transformer inference\n    *   Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)\n*   **Hardware Support**: CPU, optionally GPU\n*   **Backend Design**: Minimalist tensor framework, custom linear algebra, no IR\n*   **Strengths**: Extremely portable, optimized for low-RAM devices, self-contained\n*   **Weaknesses**: Focused only on LLMs, lower-level interface\n\n**Execution Flow**:\n\n*   Loads quantized model into memory\n*   Performs batched matmul-based transformer inference\n*   Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)\n\n#### TensorFlow Lite / Serving\n\n*   **Model Format**: `.tflite` (Lite), `.pb` or SavedModel (Serving)\n*   **Execution Flow**:\n    \n    *   TFLite: uses FlatBuffer model, loads and interprets ops\n    *   Serving: REST/gRPC server for remote model inference\n*   **Hardware Support**:\n    \n    *   TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP\n    *   Serving: Primarily server-side; not for on-device use\n*   **Backend Design**:\n    \n    *   TFLite: statically compiled interpreters with kernel registry\n    *   TFLite delegates for hardware acceleration\n*   **Strengths**: Broad compatibility, active ecosystem, stable\n*   **Weaknesses**: Delegate configuration can be tricky, Serving not suitable for offline use\n\n**Execution Flow**:\n\n*   TFLite: uses FlatBuffer model, loads and interprets ops\n*   Serving: REST/gRPC server for remote model inference\n\n**Hardware Support**:\n\n*   TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP\n*   Serving: Primarily server-side; not for on-device use\n\n**Backend Design**:\n\n*   TFLite: statically compiled interpreters with kernel registry\n*   TFLite delegates for hardware acceleration",
    "contentLength": 9578,
    "wordCount": 840,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture-by-runtime"
  },
  {
    "id": "ai-ml-runtimes-overview-3",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Overview",
    "order": 3,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Engineers deploying deep learning models on NVIDIA hardware</li>\n  <li><strong>Use Cases</strong>: Vision inference, robotics, autonomous vehicles, embedded AI with Jetson, high-throughput servers</li>\n  <li><strong>Model Format</strong>: ONNX, Caffe, TensorFlow (converted to <code class=\"language-plaintext highlighter-rouge\">.plan</code> engine)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, TensorRT Python/C++ APIs</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: Engineers deploying deep learning models on NVIDIA hardware\n*   **Use Cases**: Vision inference, robotics, autonomous vehicles, embedded AI with Jetson, high-throughput servers\n*   **Model Format**: ONNX, Caffe, TensorFlow (converted to `.plan` engine)\n*   **Conversion Tools**: `trtexec`, TensorRT Python/C++ APIs",
    "contentLength": 543,
    "wordCount": 42,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-4",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Architecture",
    "order": 4,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>TensorRT transforms trained models into an optimized engine using multiple optimization passes:</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Import</strong>: Loads model (typically ONNX) using TensorRT parser</li>\n      <li>\n        <p><strong>Optimization</strong>:</p>\n\n        <ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Engine Building</strong>:</p>\n\n        <ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Builder</strong>: Optimizes and generates runtime engine</li>\n      <li><strong>Runtime</strong>: Loads and executes serialized engine</li>\n      <li><strong>Execution Context</strong>: Holds all buffers and workspace</li>\n      <li><strong>Calibrator</strong>: Generates <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization scale factors using sample data</li>\n    </ul>\n  </li>\n</ul>\n<p>TensorRT transforms trained models into an optimized engine using multiple optimization passes:</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Import</strong>: Loads model (typically ONNX) using TensorRT parser</li>\n      <li>\n        <p><strong>Optimization</strong>:</p>\n\n        <ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Engine Building</strong>:</p>\n\n        <ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Optimization</strong>:</p>\n<ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n<p><strong>Engine Building</strong>:</p>\n<ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n<p><strong>Inference Execution</strong>:</p>\n<ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Builder</strong>: Optimizes and generates runtime engine</li>\n      <li><strong>Runtime</strong>: Loads and executes serialized engine</li>\n      <li><strong>Execution Context</strong>: Holds all buffers and workspace</li>\n      <li><strong>Calibrator</strong>: Generates <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization scale factors using sample data</li>\n    </ul>",
    "contentMarkdown": "*   TensorRT transforms trained models into an optimized engine using multiple optimization passes:\n    \n*   **Execution Flow**:\n    \n    1.  **Model Import**: Loads model (typically ONNX) using TensorRT parser\n    2.  **Optimization**:\n        \n        *   Layer fusion\n        *   Precision calibration (`float16`, `int8`)\n        *   Kernel selection and scheduling\n    3.  **Engine Building**:\n        \n        *   Generates a `.plan` file (serialized CUDA engine)\n        *   This engine can be reused for fast deployment\n    4.  **Inference Execution**:\n        \n        *   Input data fed through pre-allocated CUDA buffers\n        *   Execution is entirely GPU-bound using CUDA streams\n*   **Key Components**:\n    \n    *   **Builder**: Optimizes and generates runtime engine\n    *   **Runtime**: Loads and executes serialized engine\n    *   **Execution Context**: Holds all buffers and workspace\n    *   **Calibrator**: Generates `int8` quantization scale factors using sample data\n\nTensorRT transforms trained models into an optimized engine using multiple optimization passes:\n\n**Execution Flow**:\n\n1.  **Model Import**: Loads model (typically ONNX) using TensorRT parser\n2.  **Optimization**:\n    \n    *   Layer fusion\n    *   Precision calibration (`float16`, `int8`)\n    *   Kernel selection and scheduling\n3.  **Engine Building**:\n    \n    *   Generates a `.plan` file (serialized CUDA engine)\n    *   This engine can be reused for fast deployment\n4.  **Inference Execution**:\n    \n    *   Input data fed through pre-allocated CUDA buffers\n    *   Execution is entirely GPU-bound using CUDA streams\n\n**Optimization**:\n\n*   Layer fusion\n*   Precision calibration (`float16`, `int8`)\n*   Kernel selection and scheduling\n\n**Engine Building**:\n\n*   Generates a `.plan` file (serialized CUDA engine)\n*   This engine can be reused for fast deployment\n\n**Inference Execution**:\n\n*   Input data fed through pre-allocated CUDA buffers\n*   Execution is entirely GPU-bound using CUDA streams\n\n**Key Components**:\n\n*   **Builder**: Optimizes and generates runtime engine\n*   **Runtime**: Loads and executes serialized engine\n*   **Execution Context**: Holds all buffers and workspace\n*   **Calibrator**: Generates `int8` quantization scale factors using sample data",
    "contentLength": 4041,
    "wordCount": 278,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-5",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Implementation Details",
    "order": 5,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Quantization Support</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">float32</code>, <code class=\"language-plaintext highlighter-rouge\">float16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code> precision modes</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> requires calibration dataset (representative samples)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Layer Fusion</strong>:</p>\n\n    <ul>\n      <li>Combines ops like conv + bias + activation into a single kernel</li>\n      <li>Reduces memory overhead and execution latency</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Dynamic Shapes</strong>:</p>\n\n    <ul>\n      <li>Supports engines that accept varying input sizes with shape profiles</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment</strong>:</p>\n\n    <ul>\n      <li>Supports inference from Python or C++</li>\n      <li>Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Quantization Support</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">float32</code>, <code class=\"language-plaintext highlighter-rouge\">float16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code> precision modes</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> requires calibration dataset (representative samples)</li>\n    </ul>\n<p><strong>Layer Fusion</strong>:</p>\n<ul>\n      <li>Combines ops like conv + bias + activation into a single kernel</li>\n      <li>Reduces memory overhead and execution latency</li>\n    </ul>\n<p><strong>Dynamic Shapes</strong>:</p>\n<ul>\n      <li>Supports engines that accept varying input sizes with shape profiles</li>\n    </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n      <li>Supports inference from Python or C++</li>\n      <li>Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms</li>\n    </ul>",
    "contentMarkdown": "*   **Quantization Support**:\n    \n    *   `float32`, `float16`, and `int8` precision modes\n    *   `int8` requires calibration dataset (representative samples)\n*   **Layer Fusion**:\n    \n    *   Combines ops like conv + bias + activation into a single kernel\n    *   Reduces memory overhead and execution latency\n*   **Dynamic Shapes**:\n    \n    *   Supports engines that accept varying input sizes with shape profiles\n*   **Deployment**:\n    \n    *   Supports inference from Python or C++\n    *   Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms\n\n**Quantization Support**:\n\n*   `float32`, `float16`, and `int8` precision modes\n*   `int8` requires calibration dataset (representative samples)\n\n**Layer Fusion**:\n\n*   Combines ops like conv + bias + activation into a single kernel\n*   Reduces memory overhead and execution latency\n\n**Dynamic Shapes**:\n\n*   Supports engines that accept varying input sizes with shape profiles\n\n**Deployment**:\n\n*   Supports inference from Python or C++\n*   Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms",
    "contentLength": 1992,
    "wordCount": 140,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-6",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Pros and Cons",
    "order": 6,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Best-in-class GPU inference performance</li>\n      <li>Optimized for Tensor Cores (Ampere, Hopper, etc.)</li>\n      <li>Rich tooling (e.g., <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, calibration tools)</li>\n      <li>Integration with Jetson for embedded AI</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Requires NVIDIA GPU and CUDA runtime</li>\n      <li>Not suitable for CPU or cross-platform apps</li>\n      <li>Build/optimization pipeline adds complexity</li>\n      <li>Engine regeneration needed if input shape or model changes significantly</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Best-in-class GPU inference performance</li>\n      <li>Optimized for Tensor Cores (Ampere, Hopper, etc.)</li>\n      <li>Rich tooling (e.g., <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, calibration tools)</li>\n      <li>Integration with Jetson for embedded AI</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Requires NVIDIA GPU and CUDA runtime</li>\n      <li>Not suitable for CPU or cross-platform apps</li>\n      <li>Build/optimization pipeline adds complexity</li>\n      <li>Engine regeneration needed if input shape or model changes significantly</li>\n    </ul>",
    "contentMarkdown": "*   **Pros**:\n    \n    *   Best-in-class GPU inference performance\n    *   Optimized for Tensor Cores (Ampere, Hopper, etc.)\n    *   Rich tooling (e.g., `trtexec`, calibration tools)\n    *   Integration with Jetson for embedded AI\n*   **Cons**:\n    \n    *   Requires NVIDIA GPU and CUDA runtime\n    *   Not suitable for CPU or cross-platform apps\n    *   Build/optimization pipeline adds complexity\n    *   Engine regeneration needed if input shape or model changes significantly\n\n**Pros**:\n\n*   Best-in-class GPU inference performance\n*   Optimized for Tensor Cores (Ampere, Hopper, etc.)\n*   Rich tooling (e.g., `trtexec`, calibration tools)\n*   Integration with Jetson for embedded AI\n\n**Cons**:\n\n*   Requires NVIDIA GPU and CUDA runtime\n*   Not suitable for CPU or cross-platform apps\n*   Build/optimization pipeline adds complexity\n*   Engine regeneration needed if input shape or model changes significantly",
    "contentLength": 1346,
    "wordCount": 122,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-workflow-7",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Example Workflow",
    "order": 7,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li><strong>Model Conversion (ONNX <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mo\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">\\rightarrow</script> Engine):</strong></li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">trtexec <span class=\"nt\">--onnx</span><span class=\"o\">=</span>model.onnx <span class=\"nt\">--saveEngine</span><span class=\"o\">=</span>model.plan <span class=\"nt\">--</span><span class=\"sb\">`</span>float16<span class=\"sb\">`</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">trtexec <span class=\"nt\">--onnx</span><span class=\"o\">=</span>model.onnx <span class=\"nt\">--saveEngine</span><span class=\"o\">=</span>model.plan <span class=\"nt\">--</span><span class=\"sb\">`</span>float16<span class=\"sb\">`</span>\n</code></pre>\n<ul>\n  <li><strong>C++ Inference:</strong></li>\n</ul>\n<div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">IRuntime</span><span class=\"o\">*</span> <span class=\"n\">runtime</span> <span class=\"o\">=</span> <span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">createInferRuntime</span><span class=\"p\">(</span><span class=\"n\">logger</span><span class=\"p\">);</span>\n<span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ifstream</span> <span class=\"nf\">engineFile</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ios</span><span class=\"o\">::</span><span class=\"n\">binary</span><span class=\"p\">);</span>\n<span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">ICudaEngine</span><span class=\"o\">*</span> <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">runtime</span><span class=\"o\">-&gt;</span><span class=\"n\">deserializeCudaEngine</span><span class=\"p\">(...);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">IRuntime</span><span class=\"o\">*</span> <span class=\"n\">runtime</span> <span class=\"o\">=</span> <span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">createInferRuntime</span><span class=\"p\">(</span><span class=\"n\">logger</span><span class=\"p\">);</span>\n<span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ifstream</span> <span class=\"nf\">engineFile</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ios</span><span class=\"o\">::</span><span class=\"n\">binary</span><span class=\"p\">);</span>\n<span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">ICudaEngine</span><span class=\"o\">*</span> <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">runtime</span><span class=\"o\">-&gt;</span><span class=\"n\">deserializeCudaEngine</span><span class=\"p\">(...);</span>\n</code></pre>\n<ul>\n  <li><strong>Python Inference:</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">tensorrt</span> <span class=\"k\">as</span> <span class=\"n\">trt</span>\n<span class=\"n\">TRT_LOGGER</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Logger</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"s\">\"rb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Runtime</span><span class=\"p\">(</span><span class=\"n\">TRT_LOGGER</span><span class=\"p\">).</span><span class=\"n\">deserialize_cuda_engine</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">tensorrt</span> <span class=\"k\">as</span> <span class=\"n\">trt</span>\n<span class=\"n\">TRT_LOGGER</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Logger</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"s\">\"rb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Runtime</span><span class=\"p\">(</span><span class=\"n\">TRT_LOGGER</span><span class=\"p\">).</span><span class=\"n\">deserialize_cuda_engine</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n</code></pre>",
    "contentMarkdown": "*   **Model Conversion (ONNX →→\\\\rightarrow Engine):**\n\n![](https://aman.ai/images/copy.png)\n\n`` trtexec --onnx=model.onnx --saveEngine=model.plan --`float16` ``\n\n![](https://aman.ai/images/copy.png)\n\n`` trtexec --onnx=model.onnx --saveEngine=model.plan --`float16` ``\n\n*   **C++ Inference:**\n\n![](https://aman.ai/images/copy.png)\n\n`nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger); std::ifstream engineFile(\"model.plan\", std::ios::binary); nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(...);`\n\n![](https://aman.ai/images/copy.png)\n\n`nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger); std::ifstream engineFile(\"model.plan\", std::ios::binary); nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(...);`\n\n*   **Python Inference:**\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorrt as trt TRT_LOGGER = trt.Logger() with open(\"model.plan\", \"rb\") as f:     engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorrt as trt TRT_LOGGER = trt.Logger() with open(\"model.plan\", \"rb\") as f:     engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())`",
    "contentLength": 7728,
    "wordCount": 82,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-workflow"
  },
  {
    "id": "ai-ml-runtimes-suitable-applications-8",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorRT Deep Dive",
    "title": "Suitable Applications",
    "order": 8,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Real-time object detection on Jetson Nano/Xavier</li>\n  <li>Batch inference in ML inference servers</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">int8</code>-quantized NLP models for chatbots</li>\n  <li>\n    <p>High-throughput video analytics (via DeepStream)</p>\n  </li>\n  <li>TensorRT excels in performance-critical scenarios where latency, batch throughput, or GPU utilization is a bottleneck. It’s a specialized, production-grade runtime for teams fully committed to NVIDIA’s platform.</li>\n</ul>\n<p>High-throughput video analytics (via DeepStream)</p>",
    "contentMarkdown": "*   Real-time object detection on Jetson Nano/Xavier\n*   Batch inference in ML inference servers\n*   `int8`\\-quantized NLP models for chatbots\n*   High-throughput video analytics (via DeepStream)\n    \n*   TensorRT excels in performance-critical scenarios where latency, batch throughput, or GPU utilization is a bottleneck. It’s a specialized, production-grade runtime for teams fully committed to NVIDIA’s platform.\n\nHigh-throughput video analytics (via DeepStream)",
    "contentLength": 583,
    "wordCount": 59,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications"
  },
  {
    "id": "ai-ml-runtimes-overview-9",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Overview",
    "order": 9,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: iOS/macOS developers</li>\n  <li><strong>Use Cases</strong>: Image recognition, natural language processing, AR/VR, real-time gesture and object detection</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> (converted to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> at compile time)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">coremltools</code>, Apple Create ML, ONNX to Core ML converters</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: iOS/macOS developers\n*   **Use Cases**: Image recognition, natural language processing, AR/VR, real-time gesture and object detection\n*   **Model Format**: `.mlmodel` (converted to `.mlmodelc` at compile time)\n*   **Conversion Tools**: `coremltools`, Apple Create ML, ONNX to Core ML converters",
    "contentLength": 579,
    "wordCount": 41,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-10",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Architecture",
    "order": 10,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Compiler</strong>: Converts <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.</p>\n  </li>\n  <li>\n    <p><strong>Execution Pipeline</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: App loads the <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> file at runtime using the <code class=\"language-plaintext highlighter-rouge\">MLModel</code> API.</li>\n      <li><strong>Prediction API</strong>: Developer calls <code class=\"language-plaintext highlighter-rouge\">prediction(input:)</code>, which triggers the internal compute graph.</li>\n      <li><strong>Backend Selection</strong>: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.</li>\n      <li><strong>Execution Engine</strong>: Executes the optimized graph using Apple’s proprietary kernel implementations.</li>\n      <li><strong>Output</strong>: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><em>MLModel Interface</em>: Main interaction point for inference</li>\n      <li><em>MLMultiArray</em>: N-dimensional tensor abstraction</li>\n      <li><em>MLFeatureValue / MLFeatureProvider</em>: Input-output containers</li>\n      <li><em>NeuralNetwork.proto</em>: Defines underlying graph schema for neural network layers</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Compiler</strong>: Converts <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.</p>\n<p><strong>Execution Pipeline</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: App loads the <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> file at runtime using the <code class=\"language-plaintext highlighter-rouge\">MLModel</code> API.</li>\n      <li><strong>Prediction API</strong>: Developer calls <code class=\"language-plaintext highlighter-rouge\">prediction(input:)</code>, which triggers the internal compute graph.</li>\n      <li><strong>Backend Selection</strong>: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.</li>\n      <li><strong>Execution Engine</strong>: Executes the optimized graph using Apple’s proprietary kernel implementations.</li>\n      <li><strong>Output</strong>: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><em>MLModel Interface</em>: Main interaction point for inference</li>\n      <li><em>MLMultiArray</em>: N-dimensional tensor abstraction</li>\n      <li><em>MLFeatureValue / MLFeatureProvider</em>: Input-output containers</li>\n      <li><em>NeuralNetwork.proto</em>: Defines underlying graph schema for neural network layers</li>\n    </ul>",
    "contentMarkdown": "*   **Model Compiler**: Converts `.mlmodel` to `.mlmodelc`, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.\n    \n*   **Execution Pipeline**:\n    \n    1.  **Model Load**: App loads the `.mlmodelc` file at runtime using the `MLModel` API.\n    2.  **Prediction API**: Developer calls `prediction(input:)`, which triggers the internal compute graph.\n    3.  **Backend Selection**: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.\n    4.  **Execution Engine**: Executes the optimized graph using Apple’s proprietary kernel implementations.\n    5.  **Output**: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.\n*   **Key Components**:\n    \n    *   _MLModel Interface_: Main interaction point for inference\n    *   _MLMultiArray_: N-dimensional tensor abstraction\n    *   _MLFeatureValue / MLFeatureProvider_: Input-output containers\n    *   _NeuralNetwork.proto_: Defines underlying graph schema for neural network layers\n\n**Model Compiler**: Converts `.mlmodel` to `.mlmodelc`, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.\n\n**Execution Pipeline**:\n\n1.  **Model Load**: App loads the `.mlmodelc` file at runtime using the `MLModel` API.\n2.  **Prediction API**: Developer calls `prediction(input:)`, which triggers the internal compute graph.\n3.  **Backend Selection**: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.\n4.  **Execution Engine**: Executes the optimized graph using Apple’s proprietary kernel implementations.\n5.  **Output**: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.\n\n**Key Components**:\n\n*   _MLModel Interface_: Main interaction point for inference\n*   _MLMultiArray_: N-dimensional tensor abstraction\n*   _MLFeatureValue / MLFeatureProvider_: Input-output containers\n*   _NeuralNetwork.proto_: Defines underlying graph schema for neural network layers",
    "contentLength": 3279,
    "wordCount": 263,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-supported-model-types-11",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Supported Model Types",
    "order": 11,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Neural Networks (CNNs, RNNs, Transformers)</li>\n  <li>Decision Trees and Ensembles (from XGBoost, scikit-learn)</li>\n  <li>Natural Language models (tokenizers, embeddings)</li>\n  <li>Audio signal processing</li>\n  <li>Custom models using Core ML’s custom layers</li>\n</ul>",
    "contentMarkdown": "*   Neural Networks (CNNs, RNNs, Transformers)\n*   Decision Trees and Ensembles (from XGBoost, scikit-learn)\n*   Natural Language models (tokenizers, embeddings)\n*   Audio signal processing\n*   Custom models using Core ML’s custom layers",
    "contentLength": 283,
    "wordCount": 32,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#supported-model-types"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-12",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Implementation Details",
    "order": 12,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Conversion Process</strong>:</p>\n\n    <ul>\n      <li>Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> maps ops to Core ML equivalents and produces <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code></li>\n      <li>Optional model quantization (e.g., 16-bit float) can be applied to reduce size</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Utilization</strong>:</p>\n\n    <ul>\n      <li>Automatically uses ANE if available (iPhone 8 and later)</li>\n      <li>Fallback to Metal GPU or CPU if ANE doesn’t support all ops</li>\n      <li>Internal heuristics determine fallback patterns and op partitioning</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Custom Layers</strong>:</p>\n\n    <ul>\n      <li>Developers can define <code class=\"language-plaintext highlighter-rouge\">MLCustomModel</code> classes</li>\n      <li>Useful when Core ML lacks certain ops</li>\n      <li>Requires manual tensor handling and native Swift/Obj-C implementation</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Conversion Process</strong>:</p>\n<ul>\n      <li>Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> maps ops to Core ML equivalents and produces <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code></li>\n      <li>Optional model quantization (e.g., 16-bit float) can be applied to reduce size</li>\n    </ul>\n<p><strong>Hardware Utilization</strong>:</p>\n<ul>\n      <li>Automatically uses ANE if available (iPhone 8 and later)</li>\n      <li>Fallback to Metal GPU or CPU if ANE doesn’t support all ops</li>\n      <li>Internal heuristics determine fallback patterns and op partitioning</li>\n    </ul>\n<p><strong>Custom Layers</strong>:</p>\n<ul>\n      <li>Developers can define <code class=\"language-plaintext highlighter-rouge\">MLCustomModel</code> classes</li>\n      <li>Useful when Core ML lacks certain ops</li>\n      <li>Requires manual tensor handling and native Swift/Obj-C implementation</li>\n    </ul>",
    "contentMarkdown": "*   **Conversion Process**:\n    \n    *   Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format\n    *   `coremltools.convert()` maps ops to Core ML equivalents and produces `.mlmodel`\n    *   Optional model quantization (e.g., 16-bit float) can be applied to reduce size\n*   **Hardware Utilization**:\n    \n    *   Automatically uses ANE if available (iPhone 8 and later)\n    *   Fallback to Metal GPU or CPU if ANE doesn’t support all ops\n    *   Internal heuristics determine fallback patterns and op partitioning\n*   **Custom Layers**:\n    \n    *   Developers can define `MLCustomModel` classes\n    *   Useful when Core ML lacks certain ops\n    *   Requires manual tensor handling and native Swift/Obj-C implementation\n\n**Conversion Process**:\n\n*   Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format\n*   `coremltools.convert()` maps ops to Core ML equivalents and produces `.mlmodel`\n*   Optional model quantization (e.g., 16-bit float) can be applied to reduce size\n\n**Hardware Utilization**:\n\n*   Automatically uses ANE if available (iPhone 8 and later)\n*   Fallback to Metal GPU or CPU if ANE doesn’t support all ops\n*   Internal heuristics determine fallback patterns and op partitioning\n\n**Custom Layers**:\n\n*   Developers can define `MLCustomModel` classes\n*   Useful when Core ML lacks certain ops\n*   Requires manual tensor handling and native Swift/Obj-C implementation",
    "contentLength": 2248,
    "wordCount": 207,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-13",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Pros and Cons",
    "order": 13,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Deep Apple integration (Vision, AVFoundation, ARKit, etc.)</li>\n      <li>Seamless use of hardware accelerators</li>\n      <li>High-level Swift API for rapid development</li>\n      <li>Secure and privacy-focused (no data leaves device)</li>\n      <li>Optimized runtime with minimal latency</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Apple-only ecosystem</li>\n      <li>Conversion limitations (unsupported ops in some models)</li>\n      <li>Limited visibility into runtime internals</li>\n      <li>Custom layer interface can be verbose and inflexible</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Deep Apple integration (Vision, AVFoundation, ARKit, etc.)</li>\n      <li>Seamless use of hardware accelerators</li>\n      <li>High-level Swift API for rapid development</li>\n      <li>Secure and privacy-focused (no data leaves device)</li>\n      <li>Optimized runtime with minimal latency</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Apple-only ecosystem</li>\n      <li>Conversion limitations (unsupported ops in some models)</li>\n      <li>Limited visibility into runtime internals</li>\n      <li>Custom layer interface can be verbose and inflexible</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Deep Apple integration (Vision, AVFoundation, ARKit, etc.)\n    *   Seamless use of hardware accelerators\n    *   High-level Swift API for rapid development\n    *   Secure and privacy-focused (no data leaves device)\n    *   Optimized runtime with minimal latency\n*   **Cons:**\n    \n    *   Apple-only ecosystem\n    *   Conversion limitations (unsupported ops in some models)\n    *   Limited visibility into runtime internals\n    *   Custom layer interface can be verbose and inflexible\n\n**Pros:**\n\n*   Deep Apple integration (Vision, AVFoundation, ARKit, etc.)\n*   Seamless use of hardware accelerators\n*   High-level Swift API for rapid development\n*   Secure and privacy-focused (no data leaves device)\n*   Optimized runtime with minimal latency\n\n**Cons:**\n\n*   Apple-only ecosystem\n*   Conversion limitations (unsupported ops in some models)\n*   Limited visibility into runtime internals\n*   Custom layer interface can be verbose and inflexible",
    "contentLength": 1312,
    "wordCount": 128,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-code-snippet-14",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Core ML Deep Dive",
    "title": "Example Code Snippet",
    "order": 14,
    "orderInChapter": 6,
    "contentHtml": "<div class=\"language-swift highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"k\">guard</span> <span class=\"k\">let</span> <span class=\"nv\">model</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MyImageClassifier</span><span class=\"p\">(</span><span class=\"nv\">configuration</span><span class=\"p\">:</span> <span class=\"kt\">MLModelConfiguration</span><span class=\"p\">())</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n    <span class=\"nf\">fatalError</span><span class=\"p\">(</span><span class=\"s\">\"Model failed to load\"</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">input</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MLMultiArray</span><span class=\"p\">(</span><span class=\"nv\">shape</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">],</span> <span class=\"nv\">dataType</span><span class=\"p\">:</span> <span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"c1\">// Fill input array with pixel data</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">output</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"nf\">prediction</span><span class=\"p\">(</span><span class=\"nv\">input</span><span class=\"p\">:</span> <span class=\"n\">input</span><span class=\"o\">!</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">?</span><span class=\"o\">.</span><span class=\"n\">classLabel</span> <span class=\"p\">??</span> <span class=\"s\">\"Prediction failed\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"k\">guard</span> <span class=\"k\">let</span> <span class=\"nv\">model</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MyImageClassifier</span><span class=\"p\">(</span><span class=\"nv\">configuration</span><span class=\"p\">:</span> <span class=\"kt\">MLModelConfiguration</span><span class=\"p\">())</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n    <span class=\"nf\">fatalError</span><span class=\"p\">(</span><span class=\"s\">\"Model failed to load\"</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">input</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MLMultiArray</span><span class=\"p\">(</span><span class=\"nv\">shape</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">],</span> <span class=\"nv\">dataType</span><span class=\"p\">:</span> <span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"c1\">// Fill input array with pixel data</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">output</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"nf\">prediction</span><span class=\"p\">(</span><span class=\"nv\">input</span><span class=\"p\">:</span> <span class=\"n\">input</span><span class=\"o\">!</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">?</span><span class=\"o\">.</span><span class=\"n\">classLabel</span> <span class=\"p\">??</span> <span class=\"s\">\"Prediction failed\"</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`guard let model = try? MyImageClassifier(configuration: MLModelConfiguration()) else {     fatalError(\"Model failed to load\") }  let input = try? MLMultiArray(shape: [1, 3, 224, 224], dataType: .float32) // Fill input array with pixel data  let output = try? model.prediction(input: input!) print(output?.classLabel ?? \"Prediction failed\")`\n\n![](https://aman.ai/images/copy.png)\n\n`guard let model = try? MyImageClassifier(configuration: MLModelConfiguration()) else {     fatalError(\"Model failed to load\") }  let input = try? MLMultiArray(shape: [1, 3, 224, 224], dataType: .float32) // Fill input array with pixel data  let output = try? model.prediction(input: input!) print(output?.classLabel ?? \"Prediction failed\")`",
    "contentLength": 4406,
    "wordCount": 86,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet"
  },
  {
    "id": "ai-ml-runtimes-overview-15",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "MLX Deep Dive",
    "title": "Overview",
    "order": 15,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: ML researchers and developers using Apple Silicon</li>\n  <li><strong>Use Cases</strong>: Research, fine-tuning models on-device, LLM inference, Apple-optimized ML pipelines</li>\n  <li><strong>Model Format</strong>: No proprietary serialized model format; models are expressed in Python source code using <code class=\"language-plaintext highlighter-rouge\">mlx.nn</code> layers</li>\n  <li><strong>Conversion Tools</strong>: Emerging support for PyTorch model import via <code class=\"language-plaintext highlighter-rouge\">mlx-trace</code> and ONNX conversion</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: ML researchers and developers using Apple Silicon\n*   **Use Cases**: Research, fine-tuning models on-device, LLM inference, Apple-optimized ML pipelines\n*   **Model Format**: No proprietary serialized model format; models are expressed in Python source code using `mlx.nn` layers\n*   **Conversion Tools**: Emerging support for PyTorch model import via `mlx-trace` and ONNX conversion",
    "contentLength": 612,
    "wordCount": 54,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-16",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "MLX Deep Dive",
    "title": "Architecture",
    "order": 16,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.</p>\n  </li>\n  <li>\n    <p><strong>Execution Modes</strong>:</p>\n\n    <ul>\n      <li><strong>Eager Execution</strong>: Immediate computation for prototyping/debugging</li>\n      <li><strong>Compiled Graph</strong>: Via <code class=\"language-plaintext highlighter-rouge\">mlx.compile()</code> for performance-critical inference</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Core Components</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.core</code>: Tensor definitions and low-level math operations</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.nn</code>: High-level neural network module abstraction (analogous to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">nn.Module</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.optimizers</code>: Gradient-based optimizers for training</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.transforms</code>: Preprocessing utilities (e.g., normalization, resizing)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Abstraction</strong>:</p>\n\n    <ul>\n      <li>Primarily targets the GPU via MPS</li>\n      <li>MLX compiler performs static analysis to optimize kernel dispatch and memory usage</li>\n      <li>ANE support is still evolving and model-dependent</li>\n    </ul>\n  </li>\n</ul>\n<p>MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.</p>\n<p><strong>Execution Modes</strong>:</p>\n<ul>\n      <li><strong>Eager Execution</strong>: Immediate computation for prototyping/debugging</li>\n      <li><strong>Compiled Graph</strong>: Via <code class=\"language-plaintext highlighter-rouge\">mlx.compile()</code> for performance-critical inference</li>\n    </ul>\n<p><strong>Core Components</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.core</code>: Tensor definitions and low-level math operations</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.nn</code>: High-level neural network module abstraction (analogous to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">nn.Module</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.optimizers</code>: Gradient-based optimizers for training</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.transforms</code>: Preprocessing utilities (e.g., normalization, resizing)</li>\n    </ul>\n<p><strong>Hardware Abstraction</strong>:</p>\n<ul>\n      <li>Primarily targets the GPU via MPS</li>\n      <li>MLX compiler performs static analysis to optimize kernel dispatch and memory usage</li>\n      <li>ANE support is still evolving and model-dependent</li>\n    </ul>",
    "contentMarkdown": "*   MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.\n    \n*   **Execution Modes**:\n    \n    *   **Eager Execution**: Immediate computation for prototyping/debugging\n    *   **Compiled Graph**: Via `mlx.compile()` for performance-critical inference\n*   **Core Components**:\n    \n    *   `mlx.core`: Tensor definitions and low-level math operations\n    *   `mlx.nn`: High-level neural network module abstraction (analogous to PyTorch’s `nn.Module`)\n    *   `mlx.optimizers`: Gradient-based optimizers for training\n    *   `mlx.transforms`: Preprocessing utilities (e.g., normalization, resizing)\n*   **Hardware Abstraction**:\n    \n    *   Primarily targets the GPU via MPS\n    *   MLX compiler performs static analysis to optimize kernel dispatch and memory usage\n    *   ANE support is still evolving and model-dependent\n\nMLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.\n\n**Execution Modes**:\n\n*   **Eager Execution**: Immediate computation for prototyping/debugging\n*   **Compiled Graph**: Via `mlx.compile()` for performance-critical inference\n\n**Core Components**:\n\n*   `mlx.core`: Tensor definitions and low-level math operations\n*   `mlx.nn`: High-level neural network module abstraction (analogous to PyTorch’s `nn.Module`)\n*   `mlx.optimizers`: Gradient-based optimizers for training\n*   `mlx.transforms`: Preprocessing utilities (e.g., normalization, resizing)\n\n**Hardware Abstraction**:\n\n*   Primarily targets the GPU via MPS\n*   MLX compiler performs static analysis to optimize kernel dispatch and memory usage\n*   ANE support is still evolving and model-dependent",
    "contentLength": 2983,
    "wordCount": 216,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-17",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "MLX Deep Dive",
    "title": "Implementation Details",
    "order": 17,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Tensor Memory Model</strong>:</p>\n\n    <ul>\n      <li>MLX tensors are immutable</li>\n      <li>Operations generate new tensors rather than mutating in-place</li>\n      <li>Enables functional purity and easier graph compilation</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>JIT Compilation</strong>:</p>\n\n    <ul>\n      <li>While code is typically run in Python, MLX allows functions to be decorated with <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> to trace and compile computation graphs</li>\n      <li>Reduces memory allocations and kernel overhead</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Custom Modules</strong>:</p>\n\n    <ul>\n      <li>Developers can create custom layers by subclassing <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li>Supports standard layers like <code class=\"language-plaintext highlighter-rouge\">Linear</code>, <code class=\"language-plaintext highlighter-rouge\">Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">LayerNorm</code>, etc.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Interoperability</strong>:</p>\n\n    <ul>\n      <li>MLX includes tools to convert PyTorch models using tracing (WIP)</li>\n      <li>No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Tensor Memory Model</strong>:</p>\n<ul>\n      <li>MLX tensors are immutable</li>\n      <li>Operations generate new tensors rather than mutating in-place</li>\n      <li>Enables functional purity and easier graph compilation</li>\n    </ul>\n<p><strong>JIT Compilation</strong>:</p>\n<ul>\n      <li>While code is typically run in Python, MLX allows functions to be decorated with <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> to trace and compile computation graphs</li>\n      <li>Reduces memory allocations and kernel overhead</li>\n    </ul>\n<p><strong>Custom Modules</strong>:</p>\n<ul>\n      <li>Developers can create custom layers by subclassing <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li>Supports standard layers like <code class=\"language-plaintext highlighter-rouge\">Linear</code>, <code class=\"language-plaintext highlighter-rouge\">Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">LayerNorm</code>, etc.</li>\n    </ul>\n<p><strong>Interoperability</strong>:</p>\n<ul>\n      <li>MLX includes tools to convert PyTorch models using tracing (WIP)</li>\n      <li>No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing</li>\n    </ul>",
    "contentMarkdown": "*   **Tensor Memory Model**:\n    \n    *   MLX tensors are immutable\n    *   Operations generate new tensors rather than mutating in-place\n    *   Enables functional purity and easier graph compilation\n*   **JIT Compilation**:\n    \n    *   While code is typically run in Python, MLX allows functions to be decorated with `@mlx.compile` to trace and compile computation graphs\n    *   Reduces memory allocations and kernel overhead\n*   **Custom Modules**:\n    \n    *   Developers can create custom layers by subclassing `mlx.nn.Module`\n    *   Supports standard layers like `Linear`, `Conv2d`, `LayerNorm`, etc.\n*   **Interoperability**:\n    \n    *   MLX includes tools to convert PyTorch models using tracing (WIP)\n    *   No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing\n\n**Tensor Memory Model**:\n\n*   MLX tensors are immutable\n*   Operations generate new tensors rather than mutating in-place\n*   Enables functional purity and easier graph compilation\n\n**JIT Compilation**:\n\n*   While code is typically run in Python, MLX allows functions to be decorated with `@mlx.compile` to trace and compile computation graphs\n*   Reduces memory allocations and kernel overhead\n\n**Custom Modules**:\n\n*   Developers can create custom layers by subclassing `mlx.nn.Module`\n*   Supports standard layers like `Linear`, `Conv2d`, `LayerNorm`, etc.\n\n**Interoperability**:\n\n*   MLX includes tools to convert PyTorch models using tracing (WIP)\n*   No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing",
    "contentLength": 2604,
    "wordCount": 206,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-18",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "MLX Deep Dive",
    "title": "Pros and Cons",
    "order": 18,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Highly optimized for Apple Silicon (especially M1/M2)</li>\n      <li>Lightweight and minimalist API with functional programming style</li>\n      <li>Supports training and inference on-device</li>\n      <li>Fast experimentation with eager mode and compilation toggle</li>\n      <li>Tensor API is intuitive for PyTorch users</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)</li>\n      <li>Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)</li>\n      <li>No official deployment format—source code is the model</li>\n      <li>Interop with other frameworks is under active development but not production-ready</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Highly optimized for Apple Silicon (especially M1/M2)</li>\n      <li>Lightweight and minimalist API with functional programming style</li>\n      <li>Supports training and inference on-device</li>\n      <li>Fast experimentation with eager mode and compilation toggle</li>\n      <li>Tensor API is intuitive for PyTorch users</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)</li>\n      <li>Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)</li>\n      <li>No official deployment format—source code is the model</li>\n      <li>Interop with other frameworks is under active development but not production-ready</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Highly optimized for Apple Silicon (especially M1/M2)\n    *   Lightweight and minimalist API with functional programming style\n    *   Supports training and inference on-device\n    *   Fast experimentation with eager mode and compilation toggle\n    *   Tensor API is intuitive for PyTorch users\n*   **Cons:**\n    \n    *   Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)\n    *   Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)\n    *   No official deployment format—source code is the model\n    *   Interop with other frameworks is under active development but not production-ready\n\n**Pros:**\n\n*   Highly optimized for Apple Silicon (especially M1/M2)\n*   Lightweight and minimalist API with functional programming style\n*   Supports training and inference on-device\n*   Fast experimentation with eager mode and compilation toggle\n*   Tensor API is intuitive for PyTorch users\n\n**Cons:**\n\n*   Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)\n*   Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)\n*   No official deployment format—source code is the model\n*   Interop with other frameworks is under active development but not production-ready",
    "contentLength": 1602,
    "wordCount": 172,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-code-snippet-19",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "MLX Deep Dive",
    "title": "Example Code Snippet",
    "order": 19,
    "orderInChapter": 5,
    "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">import</span> <span class=\"nn\">mlx.core</span> <span class=\"k\">as</span> <span class=\"n\">mx</span>\n<span class=\"kn\">import</span> <span class=\"nn\">mlx.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SimpleMLP</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SimpleMLP</span><span class=\"p\">()</span>\n<span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">import</span> <span class=\"nn\">mlx.core</span> <span class=\"k\">as</span> <span class=\"n\">mx</span>\n<span class=\"kn\">import</span> <span class=\"nn\">mlx.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SimpleMLP</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SimpleMLP</span><span class=\"p\">()</span>\n<span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>For accelerated inference:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">compiled_fn</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">compiled_fn</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">compiled_fn</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">compiled_fn</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import mlx.core as mx import mlx.nn as nn  class SimpleMLP(nn.Module):     def __init__(self):         super().__init__()         self.linear1 = nn.Linear(784, 256)         self.relu = nn.ReLU()         self.linear2 = nn.Linear(256, 10)      def __call__(self, x):         x = self.linear1(x)         x = self.relu(x)         return self.linear2(x)  model = SimpleMLP() input = mx.random.normal((1, 784)) output = model(input)  print(\"Prediction:\", output)`\n\n![](https://aman.ai/images/copy.png)\n\n`import mlx.core as mx import mlx.nn as nn  class SimpleMLP(nn.Module):     def __init__(self):         super().__init__()         self.linear1 = nn.Linear(784, 256)         self.relu = nn.ReLU()         self.linear2 = nn.Linear(256, 10)      def __call__(self, x):         x = self.linear1(x)         x = self.relu(x)         return self.linear2(x)  model = SimpleMLP() input = mx.random.normal((1, 784)) output = model(input)  print(\"Prediction:\", output)`\n\n*   For accelerated inference:\n\n![](https://aman.ai/images/copy.png)\n\n`compiled_fn = mx.compile(model) output = compiled_fn(input)`\n\n![](https://aman.ai/images/copy.png)\n\n`compiled_fn = mx.compile(model) output = compiled_fn(input)`",
    "contentLength": 8334,
    "wordCount": 114,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet"
  },
  {
    "id": "ai-ml-runtimes-overview-20",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Overview",
    "order": 20,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Application developers, MLOps teams, platform architects</li>\n  <li><strong>Use Cases</strong>: Cross-framework inference, model portability, production deployments (cloud + edge), hardware acceleration</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.onnx</code> (Open Neural Network Exchange format)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">torch.onnx.export</code>, <code class=\"language-plaintext highlighter-rouge\">tf2onnx</code>, <code class=\"language-plaintext highlighter-rouge\">skl2onnx</code>, and many others</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: Application developers, MLOps teams, platform architects\n*   **Use Cases**: Cross-framework inference, model portability, production deployments (cloud + edge), hardware acceleration\n*   **Model Format**: `.onnx` (Open Neural Network Exchange format)\n*   **Conversion Tools**: `torch.onnx.export`, `tf2onnx`, `skl2onnx`, and many others",
    "contentLength": 677,
    "wordCount": 41,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-21",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Architecture",
    "order": 21,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: Parses the <code class=\"language-plaintext highlighter-rouge\">.onnx</code> model file into an internal graph representation.</li>\n      <li><strong>Graph Optimization</strong>: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.</li>\n      <li><strong>Execution Provider Selection</strong>: Based on available hardware and EP priorities, operators are assigned to execution backends.</li>\n      <li><strong>Execution</strong>: ORT schedules and dispatches kernel calls for each partition of the graph.</li>\n      <li><strong>Output Handling</strong>: Results are returned in native types or via C/C++/Python APIs.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Session</strong>: <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> is the main object for loading and running models.</li>\n      <li>\n        <p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n\n        <ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>\n      </li>\n      <li><strong>Graph Transformer</strong>: Rewrites and optimizes the computation graph</li>\n      <li><strong>Kernel Registry</strong>: Maps ONNX ops to optimized implementations</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: Parses the <code class=\"language-plaintext highlighter-rouge\">.onnx</code> model file into an internal graph representation.</li>\n      <li><strong>Graph Optimization</strong>: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.</li>\n      <li><strong>Execution Provider Selection</strong>: Based on available hardware and EP priorities, operators are assigned to execution backends.</li>\n      <li><strong>Execution</strong>: ORT schedules and dispatches kernel calls for each partition of the graph.</li>\n      <li><strong>Output Handling</strong>: Results are returned in native types or via C/C++/Python APIs.</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Session</strong>: <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> is the main object for loading and running models.</li>\n      <li>\n        <p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n\n        <ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>\n      </li>\n      <li><strong>Graph Transformer</strong>: Rewrites and optimizes the computation graph</li>\n      <li><strong>Kernel Registry</strong>: Maps ONNX ops to optimized implementations</li>\n    </ul>\n<p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n<ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>",
    "contentMarkdown": "*   ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).\n    \n*   **Execution Flow**:\n    \n    1.  **Model Load**: Parses the `.onnx` model file into an internal graph representation.\n    2.  **Graph Optimization**: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.\n    3.  **Execution Provider Selection**: Based on available hardware and EP priorities, operators are assigned to execution backends.\n    4.  **Execution**: ORT schedules and dispatches kernel calls for each partition of the graph.\n    5.  **Output Handling**: Results are returned in native types or via C/C++/Python APIs.\n*   **Key Components**:\n    \n    *   **Session**: `InferenceSession` is the main object for loading and running models.\n    *   **Execution Providers (EPs)**: Modular backend plugins such as:\n        \n        *   CPU (default)\n        *   CUDA (NVIDIA GPUs)\n        *   DirectML (Windows GPU)\n        *   OpenVINO (Intel accelerators)\n        *   NNAPI (Android)\n        *   CoreML (iOS/macOS)\n        *   TensorRT\n        *   QNN (Qualcomm AI Engine)\n    *   **Graph Transformer**: Rewrites and optimizes the computation graph\n    *   **Kernel Registry**: Maps ONNX ops to optimized implementations\n\nONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).\n\n**Execution Flow**:\n\n1.  **Model Load**: Parses the `.onnx` model file into an internal graph representation.\n2.  **Graph Optimization**: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.\n3.  **Execution Provider Selection**: Based on available hardware and EP priorities, operators are assigned to execution backends.\n4.  **Execution**: ORT schedules and dispatches kernel calls for each partition of the graph.\n5.  **Output Handling**: Results are returned in native types or via C/C++/Python APIs.\n\n**Key Components**:\n\n*   **Session**: `InferenceSession` is the main object for loading and running models.\n*   **Execution Providers (EPs)**: Modular backend plugins such as:\n    \n    *   CPU (default)\n    *   CUDA (NVIDIA GPUs)\n    *   DirectML (Windows GPU)\n    *   OpenVINO (Intel accelerators)\n    *   NNAPI (Android)\n    *   CoreML (iOS/macOS)\n    *   TensorRT\n    *   QNN (Qualcomm AI Engine)\n*   **Graph Transformer**: Rewrites and optimizes the computation graph\n*   **Kernel Registry**: Maps ONNX ops to optimized implementations\n\n**Execution Providers (EPs)**: Modular backend plugins such as:\n\n*   CPU (default)\n*   CUDA (NVIDIA GPUs)\n*   DirectML (Windows GPU)\n*   OpenVINO (Intel accelerators)\n*   NNAPI (Android)\n*   CoreML (iOS/macOS)\n*   TensorRT\n*   QNN (Qualcomm AI Engine)",
    "contentLength": 4271,
    "wordCount": 405,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-22",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Implementation Details",
    "order": 22,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Format</strong>:</p>\n\n    <ul>\n      <li>ONNX models are stored in protobuf format</li>\n      <li>Static computation graph with explicit type and shape information</li>\n      <li>Supports operator versioning to ensure backward compatibility</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Customization</strong>:</p>\n\n    <ul>\n      <li>Developers can register custom ops and execution providers</li>\n      <li>Optional use of external initializers and custom inference contexts</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Execution Optimization</strong>:</p>\n\n    <ul>\n      <li>Graph transformation level can be controlled (basic, extended, all)</li>\n      <li>EPs can share execution (e.g., some layers on CPU, others on GPU)</li>\n      <li>Quantization and sparsity-aware execution supported via tools like <code class=\"language-plaintext highlighter-rouge\">onnxruntime-tools</code></li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Mobile Support</strong>:</p>\n\n    <ul>\n      <li>ONNX Runtime Mobile: A statically linked, size-reduced runtime</li>\n      <li>Works with Android and iOS, using NNAPI, Core ML, or CPU fallback</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Format</strong>:</p>\n<ul>\n      <li>ONNX models are stored in protobuf format</li>\n      <li>Static computation graph with explicit type and shape information</li>\n      <li>Supports operator versioning to ensure backward compatibility</li>\n    </ul>\n<p><strong>Customization</strong>:</p>\n<ul>\n      <li>Developers can register custom ops and execution providers</li>\n      <li>Optional use of external initializers and custom inference contexts</li>\n    </ul>\n<p><strong>Execution Optimization</strong>:</p>\n<ul>\n      <li>Graph transformation level can be controlled (basic, extended, all)</li>\n      <li>EPs can share execution (e.g., some layers on CPU, others on GPU)</li>\n      <li>Quantization and sparsity-aware execution supported via tools like <code class=\"language-plaintext highlighter-rouge\">onnxruntime-tools</code></li>\n    </ul>\n<p><strong>Mobile Support</strong>:</p>\n<ul>\n      <li>ONNX Runtime Mobile: A statically linked, size-reduced runtime</li>\n      <li>Works with Android and iOS, using NNAPI, Core ML, or CPU fallback</li>\n    </ul>",
    "contentMarkdown": "*   **Model Format**:\n    \n    *   ONNX models are stored in protobuf format\n    *   Static computation graph with explicit type and shape information\n    *   Supports operator versioning to ensure backward compatibility\n*   **Customization**:\n    \n    *   Developers can register custom ops and execution providers\n    *   Optional use of external initializers and custom inference contexts\n*   **Execution Optimization**:\n    \n    *   Graph transformation level can be controlled (basic, extended, all)\n    *   EPs can share execution (e.g., some layers on CPU, others on GPU)\n    *   Quantization and sparsity-aware execution supported via tools like `onnxruntime-tools`\n*   **Mobile Support**:\n    \n    *   ONNX Runtime Mobile: A statically linked, size-reduced runtime\n    *   Works with Android and iOS, using NNAPI, Core ML, or CPU fallback\n\n**Model Format**:\n\n*   ONNX models are stored in protobuf format\n*   Static computation graph with explicit type and shape information\n*   Supports operator versioning to ensure backward compatibility\n\n**Customization**:\n\n*   Developers can register custom ops and execution providers\n*   Optional use of external initializers and custom inference contexts\n\n**Execution Optimization**:\n\n*   Graph transformation level can be controlled (basic, extended, all)\n*   EPs can share execution (e.g., some layers on CPU, others on GPU)\n*   Quantization and sparsity-aware execution supported via tools like `onnxruntime-tools`\n\n**Mobile Support**:\n\n*   ONNX Runtime Mobile: A statically linked, size-reduced runtime\n*   Works with Android and iOS, using NNAPI, Core ML, or CPU fallback",
    "contentLength": 2262,
    "wordCount": 218,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-23",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Pros and Cons",
    "order": 23,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Framework agnostic and highly interoperable</li>\n      <li>Broad hardware support via modular execution providers</li>\n      <li>Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)</li>\n      <li>Mobile support with optimized builds and quantized execution</li>\n      <li>Extensive language bindings (Python, C++, C#, Java)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Debugging can be complex across EPs</li>\n      <li>Conversion process from other frameworks may require custom scripts</li>\n      <li>ONNX opset compatibility issues can arise across versions</li>\n      <li>Mobile optimization (size, latency) requires manual tuning</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Framework agnostic and highly interoperable</li>\n      <li>Broad hardware support via modular execution providers</li>\n      <li>Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)</li>\n      <li>Mobile support with optimized builds and quantized execution</li>\n      <li>Extensive language bindings (Python, C++, C#, Java)</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Debugging can be complex across EPs</li>\n      <li>Conversion process from other frameworks may require custom scripts</li>\n      <li>ONNX opset compatibility issues can arise across versions</li>\n      <li>Mobile optimization (size, latency) requires manual tuning</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Framework agnostic and highly interoperable\n    *   Broad hardware support via modular execution providers\n    *   Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)\n    *   Mobile support with optimized builds and quantized execution\n    *   Extensive language bindings (Python, C++, C#, Java)\n*   **Cons:**\n    \n    *   Debugging can be complex across EPs\n    *   Conversion process from other frameworks may require custom scripts\n    *   ONNX opset compatibility issues can arise across versions\n    *   Mobile optimization (size, latency) requires manual tuning\n\n**Pros:**\n\n*   Framework agnostic and highly interoperable\n*   Broad hardware support via modular execution providers\n*   Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)\n*   Mobile support with optimized builds and quantized execution\n*   Extensive language bindings (Python, C++, C#, Java)\n\n**Cons:**\n\n*   Debugging can be complex across EPs\n*   Conversion process from other frameworks may require custom scripts\n*   ONNX opset compatibility issues can arise across versions\n*   Mobile optimization (size, latency) requires manual tuning",
    "contentLength": 1516,
    "wordCount": 156,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-code-snippet-python-24",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Example Code Snippet (Python)",
    "order": 24,
    "orderInChapter": 5,
    "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">onnxruntime</span> <span class=\"k\">as</span> <span class=\"n\">ort</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load ONNX model\n</span><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_name</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">get_inputs</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">name</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">input_name</span><span class=\"p\">:</span> <span class=\"n\">input_data</span><span class=\"p\">})</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction shape:\"</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">onnxruntime</span> <span class=\"k\">as</span> <span class=\"n\">ort</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load ONNX model\n</span><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_name</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">get_inputs</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">name</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">input_name</span><span class=\"p\">:</span> <span class=\"n\">input_data</span><span class=\"p\">})</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction shape:\"</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</code></pre>\n<p><strong>Using CUDA Execution Provider</strong>:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">providers</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'CUDAExecutionProvider'</span><span class=\"p\">])</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">providers</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'CUDAExecutionProvider'</span><span class=\"p\">])</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import onnxruntime as ort import numpy as np  # Load ONNX model session = ort.InferenceSession(\"resnet50.onnx\")  # Prepare input input_name = session.get_inputs()[0].name input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)  # Run inference outputs = session.run(None, {input_name: input_data})  print(\"Prediction shape:\", outputs[0].shape)`\n\n![](https://aman.ai/images/copy.png)\n\n`import onnxruntime as ort import numpy as np  # Load ONNX model session = ort.InferenceSession(\"resnet50.onnx\")  # Prepare input input_name = session.get_inputs()[0].name input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)  # Run inference outputs = session.run(None, {input_name: input_data})  print(\"Prediction shape:\", outputs[0].shape)`\n\n**Using CUDA Execution Provider**:\n\n![](https://aman.ai/images/copy.png)\n\n`session = ort.InferenceSession(\"resnet50.onnx\", providers=['CUDAExecutionProvider'])`\n\n![](https://aman.ai/images/copy.png)\n\n`session = ort.InferenceSession(\"resnet50.onnx\", providers=['CUDAExecutionProvider'])`",
    "contentLength": 6222,
    "wordCount": 92,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet-(python)"
  },
  {
    "id": "ai-ml-runtimes-use-in-edge-on-device-scenarios-25",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ONNX Runtime Deep Dive",
    "title": "Use in Edge / On-Device Scenarios",
    "order": 25,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:</p>\n\n    <ul>\n      <li>Stripped-down build (~1–2 MB)</li>\n      <li>FlatBuffer format support in preview</li>\n      <li>Android NNAPI and iOS Core ML integration</li>\n      <li>Prebuilt minimal runtime packages for specific models</li>\n    </ul>\n  </li>\n  <li>\n    <p>ONNX Runtime is best suited for applications where:</p>\n\n    <ul>\n      <li>Portability across hardware is essential</li>\n      <li>Mixed execution (CPU + accelerator) is beneficial</li>\n      <li>The model pipeline involves multiple frameworks</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:</p>\n<ul>\n      <li>Stripped-down build (~1–2 MB)</li>\n      <li>FlatBuffer format support in preview</li>\n      <li>Android NNAPI and iOS Core ML integration</li>\n      <li>Prebuilt minimal runtime packages for specific models</li>\n    </ul>\n<p>ONNX Runtime is best suited for applications where:</p>\n<ul>\n      <li>Portability across hardware is essential</li>\n      <li>Mixed execution (CPU + accelerator) is beneficial</li>\n      <li>The model pipeline involves multiple frameworks</li>\n    </ul>",
    "contentMarkdown": "*   ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:\n    \n    *   Stripped-down build (~1–2 MB)\n    *   FlatBuffer format support in preview\n    *   Android NNAPI and iOS Core ML integration\n    *   Prebuilt minimal runtime packages for specific models\n*   ONNX Runtime is best suited for applications where:\n    \n    *   Portability across hardware is essential\n    *   Mixed execution (CPU + accelerator) is beneficial\n    *   The model pipeline involves multiple frameworks\n\nONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:\n\n*   Stripped-down build (~1–2 MB)\n*   FlatBuffer format support in preview\n*   Android NNAPI and iOS Core ML integration\n*   Prebuilt minimal runtime packages for specific models\n\nONNX Runtime is best suited for applications where:\n\n*   Portability across hardware is essential\n*   Mixed execution (CPU + accelerator) is beneficial\n*   The model pipeline involves multiple frameworks",
    "contentLength": 1262,
    "wordCount": 142,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#use-in-edge-/-on-device-scenarios"
  },
  {
    "id": "ai-ml-runtimes-overview-26",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Overview",
    "order": 26,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Embedded ML engineers, mobile and edge system developers</li>\n  <li><strong>Use Cases</strong>: Sensor fusion, vision at the edge, voice command detection, ultra-low-power AI applications</li>\n  <li><strong>Model Format</strong>: Compiled TorchScript bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>)</li>\n  <li><strong>Conversion Tools</strong>: PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mo\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\rightarrow</script> TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\rightarrow</script> ExecuTorch via AOT pipeline</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: Embedded ML engineers, mobile and edge system developers\n*   **Use Cases**: Sensor fusion, vision at the edge, voice command detection, ultra-low-power AI applications\n*   **Model Format**: Compiled TorchScript bytecode (`.ptc`)\n*   **Conversion Tools**: PyTorch →→\\\\rightarrow TorchScript →→\\\\rightarrow ExecuTorch via AOT pipeline",
    "contentLength": 2949,
    "wordCount": 44,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-27",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Architecture",
    "order": 27,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Model Export</strong>:</p>\n\n        <ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Runtime Embedding</strong>:</p>\n\n        <ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Deployment</strong>:</p>\n\n        <ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Bytecode Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.ptc</code> files contain compiled operators and control flow</li>\n      <li><strong>VM Runtime</strong>: A minimal interpreter that reads and executes bytecode</li>\n      <li><strong>Dispatcher</strong>: Routes ops to backend implementations</li>\n      <li><strong>Memory Arena</strong>: Static memory model, optionally no dynamic allocation</li>\n    </ul>\n  </li>\n</ul>\n<p>ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Model Export</strong>:</p>\n\n        <ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Runtime Embedding</strong>:</p>\n\n        <ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Deployment</strong>:</p>\n\n        <ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Model Export</strong>:</p>\n<ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n<p><strong>Runtime Embedding</strong>:</p>\n<ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Bytecode Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.ptc</code> files contain compiled operators and control flow</li>\n      <li><strong>VM Runtime</strong>: A minimal interpreter that reads and executes bytecode</li>\n      <li><strong>Dispatcher</strong>: Routes ops to backend implementations</li>\n      <li><strong>Memory Arena</strong>: Static memory model, optionally no dynamic allocation</li>\n    </ul>",
    "contentMarkdown": "*   ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Export**:\n        \n        *   Model defined in PyTorch and traced/scripted via TorchScript.\n        *   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n    2.  **Runtime Embedding**:\n        \n        *   The bytecode and necessary ops are compiled with the target runtime.\n        *   Optional op pruning removes unused operations.\n    3.  **Deployment**:\n        \n        *   Model and runtime are flashed onto the device.\n        *   Inference is run via a lightweight VM interpreter.\n*   **Key Components**:\n    \n    *   **Bytecode Format**: `.ptc` files contain compiled operators and control flow\n    *   **VM Runtime**: A minimal interpreter that reads and executes bytecode\n    *   **Dispatcher**: Routes ops to backend implementations\n    *   **Memory Arena**: Static memory model, optionally no dynamic allocation\n\nExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.\n\n**Execution Flow**:\n\n1.  **Model Export**:\n    \n    *   Model defined in PyTorch and traced/scripted via TorchScript.\n    *   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n2.  **Runtime Embedding**:\n    \n    *   The bytecode and necessary ops are compiled with the target runtime.\n    *   Optional op pruning removes unused operations.\n3.  **Deployment**:\n    \n    *   Model and runtime are flashed onto the device.\n    *   Inference is run via a lightweight VM interpreter.\n\n**Model Export**:\n\n*   Model defined in PyTorch and traced/scripted via TorchScript.\n*   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n\n**Runtime Embedding**:\n\n*   The bytecode and necessary ops are compiled with the target runtime.\n*   Optional op pruning removes unused operations.\n\n**Deployment**:\n\n*   Model and runtime are flashed onto the device.\n*   Inference is run via a lightweight VM interpreter.\n\n**Key Components**:\n\n*   **Bytecode Format**: `.ptc` files contain compiled operators and control flow\n*   **VM Runtime**: A minimal interpreter that reads and executes bytecode\n*   **Dispatcher**: Routes ops to backend implementations\n*   **Memory Arena**: Static memory model, optionally no dynamic allocation",
    "contentLength": 3827,
    "wordCount": 341,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-28",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Implementation Details",
    "order": 28,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>AOT Compiler</strong>:</p>\n\n    <ul>\n      <li>Converts scripted TorchScript models into bytecode and op kernels</li>\n      <li>Includes a model linker that statically binds required ops</li>\n      <li>Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Operator Handling</strong>:</p>\n\n    <ul>\n      <li>Customizable op kernels allow device-specific optimization</li>\n      <li>Optional kernel fusion via compiler passes for performance</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Runtime Constraints</strong>:</p>\n\n    <ul>\n      <li>Code size: Can be &lt;500 KB with aggressive pruning</li>\n      <li>No reliance on dynamic memory allocation (static buffer planning)</li>\n      <li>Designed for devices with as little as 256 KB RAM</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Integration</strong>:</p>\n\n    <ul>\n      <li>Written in C++</li>\n      <li>Can integrate with sensor pipelines, real-time OS, or MCU firmware</li>\n      <li>Open-sourced with tooling for building and flashing models to hardware</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>AOT Compiler</strong>:</p>\n<ul>\n      <li>Converts scripted TorchScript models into bytecode and op kernels</li>\n      <li>Includes a model linker that statically binds required ops</li>\n      <li>Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)</li>\n    </ul>\n<p><strong>Operator Handling</strong>:</p>\n<ul>\n      <li>Customizable op kernels allow device-specific optimization</li>\n      <li>Optional kernel fusion via compiler passes for performance</li>\n    </ul>\n<p><strong>Runtime Constraints</strong>:</p>\n<ul>\n      <li>Code size: Can be &lt;500 KB with aggressive pruning</li>\n      <li>No reliance on dynamic memory allocation (static buffer planning)</li>\n      <li>Designed for devices with as little as 256 KB RAM</li>\n    </ul>\n<p><strong>Integration</strong>:</p>\n<ul>\n      <li>Written in C++</li>\n      <li>Can integrate with sensor pipelines, real-time OS, or MCU firmware</li>\n      <li>Open-sourced with tooling for building and flashing models to hardware</li>\n    </ul>",
    "contentMarkdown": "*   **AOT Compiler**:\n    \n    *   Converts scripted TorchScript models into bytecode and op kernels\n    *   Includes a model linker that statically binds required ops\n    *   Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)\n*   **Operator Handling**:\n    \n    *   Customizable op kernels allow device-specific optimization\n    *   Optional kernel fusion via compiler passes for performance\n*   **Runtime Constraints**:\n    \n    *   Code size: Can be <500 KB with aggressive pruning\n    *   No reliance on dynamic memory allocation (static buffer planning)\n    *   Designed for devices with as little as 256 KB RAM\n*   **Integration**:\n    \n    *   Written in C++\n    *   Can integrate with sensor pipelines, real-time OS, or MCU firmware\n    *   Open-sourced with tooling for building and flashing models to hardware\n\n**AOT Compiler**:\n\n*   Converts scripted TorchScript models into bytecode and op kernels\n*   Includes a model linker that statically binds required ops\n*   Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)\n\n**Operator Handling**:\n\n*   Customizable op kernels allow device-specific optimization\n*   Optional kernel fusion via compiler passes for performance\n\n**Runtime Constraints**:\n\n*   Code size: Can be <500 KB with aggressive pruning\n*   No reliance on dynamic memory allocation (static buffer planning)\n*   Designed for devices with as little as 256 KB RAM\n\n**Integration**:\n\n*   Written in C++\n*   Can integrate with sensor pipelines, real-time OS, or MCU firmware\n*   Open-sourced with tooling for building and flashing models to hardware",
    "contentLength": 2142,
    "wordCount": 222,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-29",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Pros and Cons",
    "order": 29,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Extremely lightweight, MCU-ready</li>\n      <li>AOT compilation reduces runtime overhead</li>\n      <li>Deterministic memory usage (good for real-time applications)</li>\n      <li>Modular and open-source with low-level control</li>\n      <li>PyTorch-compatible workflow for training and export</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Requires model to be written in a static subset of PyTorch</li>\n      <li>Limited dynamic control flow (must be scriptable)</li>\n      <li>Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite</li>\n      <li>Focused on inference only; no training support on-device</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Extremely lightweight, MCU-ready</li>\n      <li>AOT compilation reduces runtime overhead</li>\n      <li>Deterministic memory usage (good for real-time applications)</li>\n      <li>Modular and open-source with low-level control</li>\n      <li>PyTorch-compatible workflow for training and export</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Requires model to be written in a static subset of PyTorch</li>\n      <li>Limited dynamic control flow (must be scriptable)</li>\n      <li>Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite</li>\n      <li>Focused on inference only; no training support on-device</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Extremely lightweight, MCU-ready\n    *   AOT compilation reduces runtime overhead\n    *   Deterministic memory usage (good for real-time applications)\n    *   Modular and open-source with low-level control\n    *   PyTorch-compatible workflow for training and export\n*   **Cons:**\n    \n    *   Requires model to be written in a static subset of PyTorch\n    *   Limited dynamic control flow (must be scriptable)\n    *   Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite\n    *   Focused on inference only; no training support on-device\n\n**Pros:**\n\n*   Extremely lightweight, MCU-ready\n*   AOT compilation reduces runtime overhead\n*   Deterministic memory usage (good for real-time applications)\n*   Modular and open-source with low-level control\n*   PyTorch-compatible workflow for training and export\n\n**Cons:**\n\n*   Requires model to be written in a static subset of PyTorch\n*   Limited dynamic control flow (must be scriptable)\n*   Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite\n*   Focused on inference only; no training support on-device",
    "contentLength": 1462,
    "wordCount": 152,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-workflow-30",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Example Workflow",
    "order": 30,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li><strong>Model Export (Python):</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TinyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TinyModel</span><span class=\"p\">()</span>\n<span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model.pt\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TinyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TinyModel</span><span class=\"p\">()</span>\n<span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model.pt\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>ExecuTorch AOT Compilation (CLI or CMake):</strong></li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">executorchc compile <span class=\"nt\">--model</span> model.pt <span class=\"nt\">--output</span> model.ptc <span class=\"nt\">--target</span> cortex-m\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">executorchc compile <span class=\"nt\">--model</span> model.pt <span class=\"nt\">--output</span> model.ptc <span class=\"nt\">--target</span> cortex-m\n</code></pre>\n<ul>\n  <li><strong>Embedded Runtime Integration (C++):</strong></li>\n</ul>\n<div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"cp\">#include</span> <span class=\"cpf\">\"executorch/runtime/runtime.h\"</span><span class=\"cp\">\n</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"model.ptc\"</span><span class=\"p\">);</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">run_model</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"cp\">#include</span> <span class=\"cpf\">\"executorch/runtime/runtime.h\"</span><span class=\"cp\">\n</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"model.ptc\"</span><span class=\"p\">);</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">run_model</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre>",
    "contentMarkdown": "*   **Model Export (Python):**\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn  class TinyModel(nn.Module):     def __init__(self):         super().__init__()         self.fc = nn.Linear(4, 2)      def forward(self, x):         return self.fc(x)  model = TinyModel() scripted = torch.jit.script(model) scripted.save(\"model.pt\")`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn  class TinyModel(nn.Module):     def __init__(self):         super().__init__()         self.fc = nn.Linear(4, 2)      def forward(self, x):         return self.fc(x)  model = TinyModel() scripted = torch.jit.script(model) scripted.save(\"model.pt\")`\n\n*   **ExecuTorch AOT Compilation (CLI or CMake):**\n\n![](https://aman.ai/images/copy.png)\n\n`executorchc compile --model model.pt --output model.ptc --target cortex-m`\n\n![](https://aman.ai/images/copy.png)\n\n`executorchc compile --model model.pt --output model.ptc --target cortex-m`\n\n*   **Embedded Runtime Integration (C++):**\n\n![](https://aman.ai/images/copy.png)\n\n`#include \"executorch/runtime/runtime.h\" executorch::load_model(\"model.ptc\"); executorch::run_model(input_tensor, output_tensor);`\n\n![](https://aman.ai/images/copy.png)\n\n`#include \"executorch/runtime/runtime.h\" executorch::load_model(\"model.ptc\"); executorch::run_model(input_tensor, output_tensor);`",
    "contentLength": 7251,
    "wordCount": 102,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-workflow"
  },
  {
    "id": "ai-ml-runtimes-suitable-applications-31",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "ExecuTorch Deep Dive",
    "title": "Suitable Applications",
    "order": 31,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Wake-word detection on MCUs</li>\n  <li>Gesture recognition using MEMS sensors</li>\n  <li>Smart agriculture (tiny vision models)</li>\n  <li>\n    <p>Battery-powered health monitoring devices</p>\n  </li>\n  <li>ExecuTorch fills a critical niche for deploying PyTorch-trained models on hardware where traditional runtimes like TensorFlow Lite or ONNX Runtime are too heavy.</li>\n</ul>\n<p>Battery-powered health monitoring devices</p>",
    "contentMarkdown": "*   Wake-word detection on MCUs\n*   Gesture recognition using MEMS sensors\n*   Smart agriculture (tiny vision models)\n*   Battery-powered health monitoring devices\n    \n*   ExecuTorch fills a critical niche for deploying PyTorch-trained models on hardware where traditional runtimes like TensorFlow Lite or ONNX Runtime are too heavy.\n\nBattery-powered health monitoring devices",
    "contentLength": 439,
    "wordCount": 50,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications"
  },
  {
    "id": "ai-ml-runtimes-overview-32",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Overview",
    "order": 32,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Robotics, ADAS, and autonomous system engineers</li>\n  <li><strong>Use Cases</strong>: Real-time 3D object detection, SLAM (Simultaneous Localization and Mapping), point cloud segmentation, obstacle avoidance</li>\n  <li><strong>Model Format</strong>: Often custom or adapted from PyTorch/ONNX; serialized as tensors or voxel grids</li>\n  <li><strong>Conversion Tools</strong>: Typically includes preprocessing pipelines from ROS, Open3D, or custom CUDA kernels</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: Robotics, ADAS, and autonomous system engineers\n*   **Use Cases**: Real-time 3D object detection, SLAM (Simultaneous Localization and Mapping), point cloud segmentation, obstacle avoidance\n*   **Model Format**: Often custom or adapted from PyTorch/ONNX; serialized as tensors or voxel grids\n*   **Conversion Tools**: Typically includes preprocessing pipelines from ROS, Open3D, or custom CUDA kernels",
    "contentLength": 517,
    "wordCount": 55,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-33",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Architecture",
    "order": 33,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Sensor Input</strong>: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested</li>\n      <li><strong>Preprocessing</strong>: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)</li>\n      <li><strong>Inference</strong>: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)</li>\n      <li><strong>Postprocessing</strong>: Bounding boxes or semantic maps generated</li>\n      <li><strong>Fusion (Optional)</strong>: Sensor fusion with radar, camera, or odometry</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Spatial Encoder</strong>: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)</li>\n      <li><strong>Sparse CNNs or VoxelNet Layers</strong>: Specialized convolution ops for irregular input data</li>\n      <li><strong>Temporal Modules</strong>: Optional RNN, attention, or transformer blocks for sequential scans</li>\n      <li><strong>Hardware Abstraction</strong>: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)</li>\n    </ul>\n  </li>\n</ul>\n<p>LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Sensor Input</strong>: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested</li>\n      <li><strong>Preprocessing</strong>: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)</li>\n      <li><strong>Inference</strong>: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)</li>\n      <li><strong>Postprocessing</strong>: Bounding boxes or semantic maps generated</li>\n      <li><strong>Fusion (Optional)</strong>: Sensor fusion with radar, camera, or odometry</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Spatial Encoder</strong>: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)</li>\n      <li><strong>Sparse CNNs or VoxelNet Layers</strong>: Specialized convolution ops for irregular input data</li>\n      <li><strong>Temporal Modules</strong>: Optional RNN, attention, or transformer blocks for sequential scans</li>\n      <li><strong>Hardware Abstraction</strong>: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)</li>\n    </ul>",
    "contentMarkdown": "*   LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.\n    \n*   **Execution Flow**:\n    \n    1.  **Sensor Input**: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested\n    2.  **Preprocessing**: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)\n    3.  **Inference**: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)\n    4.  **Postprocessing**: Bounding boxes or semantic maps generated\n    5.  **Fusion (Optional)**: Sensor fusion with radar, camera, or odometry\n*   **Key Components**:\n    \n    *   **Spatial Encoder**: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)\n    *   **Sparse CNNs or VoxelNet Layers**: Specialized convolution ops for irregular input data\n    *   **Temporal Modules**: Optional RNN, attention, or transformer blocks for sequential scans\n    *   **Hardware Abstraction**: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)\n\nLidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.\n\n**Execution Flow**:\n\n1.  **Sensor Input**: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested\n2.  **Preprocessing**: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)\n3.  **Inference**: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)\n4.  **Postprocessing**: Bounding boxes or semantic maps generated\n5.  **Fusion (Optional)**: Sensor fusion with radar, camera, or odometry\n\n**Key Components**:\n\n*   **Spatial Encoder**: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)\n*   **Sparse CNNs or VoxelNet Layers**: Specialized convolution ops for irregular input data\n*   **Temporal Modules**: Optional RNN, attention, or transformer blocks for sequential scans\n*   **Hardware Abstraction**: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)",
    "contentLength": 2875,
    "wordCount": 289,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-34",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Implementation Details",
    "order": 34,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Tensor Representation</strong>:</p>\n\n    <ul>\n      <li>Often uses sparse tensors or hybrid dense-sparse structures</li>\n      <li>Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops</li>\n      <li>Quantization may be used to reduce memory footprint in embedded settings</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Techniques</strong>:</p>\n\n    <ul>\n      <li>Efficient neighbor search (KD-trees, octrees) for local feature aggregation</li>\n      <li>Temporal caching of features from prior scans</li>\n      <li>Batch fusion for multi-sensor inputs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment</strong>:</p>\n\n    <ul>\n      <li>Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers</li>\n      <li>Often integrated with ROS (Robot Operating System) for I/O and control flow</li>\n      <li>May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Tensor Representation</strong>:</p>\n<ul>\n      <li>Often uses sparse tensors or hybrid dense-sparse structures</li>\n      <li>Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops</li>\n      <li>Quantization may be used to reduce memory footprint in embedded settings</li>\n    </ul>\n<p><strong>Optimization Techniques</strong>:</p>\n<ul>\n      <li>Efficient neighbor search (KD-trees, octrees) for local feature aggregation</li>\n      <li>Temporal caching of features from prior scans</li>\n      <li>Batch fusion for multi-sensor inputs</li>\n    </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n      <li>Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers</li>\n      <li>Often integrated with ROS (Robot Operating System) for I/O and control flow</li>\n      <li>May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance</li>\n    </ul>",
    "contentMarkdown": "*   **Tensor Representation**:\n    \n    *   Often uses sparse tensors or hybrid dense-sparse structures\n    *   Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops\n    *   Quantization may be used to reduce memory footprint in embedded settings\n*   **Optimization Techniques**:\n    \n    *   Efficient neighbor search (KD-trees, octrees) for local feature aggregation\n    *   Temporal caching of features from prior scans\n    *   Batch fusion for multi-sensor inputs\n*   **Deployment**:\n    \n    *   Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers\n    *   Often integrated with ROS (Robot Operating System) for I/O and control flow\n    *   May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance\n\n**Tensor Representation**:\n\n*   Often uses sparse tensors or hybrid dense-sparse structures\n*   Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops\n*   Quantization may be used to reduce memory footprint in embedded settings\n\n**Optimization Techniques**:\n\n*   Efficient neighbor search (KD-trees, octrees) for local feature aggregation\n*   Temporal caching of features from prior scans\n*   Batch fusion for multi-sensor inputs\n\n**Deployment**:\n\n*   Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers\n*   Often integrated with ROS (Robot Operating System) for I/O and control flow\n*   May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance",
    "contentLength": 1920,
    "wordCount": 201,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-35",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Pros and Cons",
    "order": 35,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Designed for spatial and temporal data, not just 2D tensors</li>\n      <li>Optimized for sparse inputs and low-latency inference</li>\n      <li>Supports sensor fusion pipelines, enabling richer context</li>\n      <li>Can run on edge-grade GPUs or embedded NPUs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Fragmented tooling, often bespoke or tightly coupled to hardware</li>\n      <li>Lack of standardized runtime interface (unlike ONNX or TFLite)</li>\n      <li>Difficult to deploy across platforms without custom engineering</li>\n      <li>Sparse community and documentation; often buried in academic or industrial codebases</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Designed for spatial and temporal data, not just 2D tensors</li>\n      <li>Optimized for sparse inputs and low-latency inference</li>\n      <li>Supports sensor fusion pipelines, enabling richer context</li>\n      <li>Can run on edge-grade GPUs or embedded NPUs</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Fragmented tooling, often bespoke or tightly coupled to hardware</li>\n      <li>Lack of standardized runtime interface (unlike ONNX or TFLite)</li>\n      <li>Difficult to deploy across platforms without custom engineering</li>\n      <li>Sparse community and documentation; often buried in academic or industrial codebases</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Designed for spatial and temporal data, not just 2D tensors\n    *   Optimized for sparse inputs and low-latency inference\n    *   Supports sensor fusion pipelines, enabling richer context\n    *   Can run on edge-grade GPUs or embedded NPUs\n*   **Cons:**\n    \n    *   Fragmented tooling, often bespoke or tightly coupled to hardware\n    *   Lack of standardized runtime interface (unlike ONNX or TFLite)\n    *   Difficult to deploy across platforms without custom engineering\n    *   Sparse community and documentation; often buried in academic or industrial codebases\n\n**Pros:**\n\n*   Designed for spatial and temporal data, not just 2D tensors\n*   Optimized for sparse inputs and low-latency inference\n*   Supports sensor fusion pipelines, enabling richer context\n*   Can run on edge-grade GPUs or embedded NPUs\n\n**Cons:**\n\n*   Fragmented tooling, often bespoke or tightly coupled to hardware\n*   Lack of standardized runtime interface (unlike ONNX or TFLite)\n*   Difficult to deploy across platforms without custom engineering\n*   Sparse community and documentation; often buried in academic or industrial codebases",
    "contentLength": 1464,
    "wordCount": 160,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-pseudocode-flow-36",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Example Pseudocode Flow",
    "order": 36,
    "orderInChapter": 5,
    "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"c1\"># Step 1: Load point cloud\n</span><span class=\"n\">point_cloud</span> <span class=\"o\">=</span> <span class=\"n\">load_lidar_scan</span><span class=\"p\">(</span><span class=\"s\">\"/scans/frame_001.bin\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 2: Convert to voxel grid\n</span><span class=\"n\">voxel_grid</span> <span class=\"o\">=</span> <span class=\"n\">voxelize</span><span class=\"p\">(</span><span class=\"n\">point_cloud</span><span class=\"p\">,</span> <span class=\"n\">grid_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Step 3: Pass through 3D CNN\n</span><span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">sparse_conv_net</span><span class=\"p\">(</span><span class=\"n\">voxel_grid</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 4: Predict bounding boxes or labels\n</span><span class=\"n\">detections</span> <span class=\"o\">=</span> <span class=\"n\">decode_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Fuse with other sensors (optional)\n</span><span class=\"n\">fused_output</span> <span class=\"o\">=</span> <span class=\"n\">fuse_with_camera</span><span class=\"p\">(</span><span class=\"n\">detections</span><span class=\"p\">,</span> <span class=\"n\">rgb_frame</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"c1\"># Step 1: Load point cloud\n</span><span class=\"n\">point_cloud</span> <span class=\"o\">=</span> <span class=\"n\">load_lidar_scan</span><span class=\"p\">(</span><span class=\"s\">\"/scans/frame_001.bin\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 2: Convert to voxel grid\n</span><span class=\"n\">voxel_grid</span> <span class=\"o\">=</span> <span class=\"n\">voxelize</span><span class=\"p\">(</span><span class=\"n\">point_cloud</span><span class=\"p\">,</span> <span class=\"n\">grid_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Step 3: Pass through 3D CNN\n</span><span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">sparse_conv_net</span><span class=\"p\">(</span><span class=\"n\">voxel_grid</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 4: Predict bounding boxes or labels\n</span><span class=\"n\">detections</span> <span class=\"o\">=</span> <span class=\"n\">decode_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Fuse with other sensors (optional)\n</span><span class=\"n\">fused_output</span> <span class=\"o\">=</span> <span class=\"n\">fuse_with_camera</span><span class=\"p\">(</span><span class=\"n\">detections</span><span class=\"p\">,</span> <span class=\"n\">rgb_frame</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`# Step 1: Load point cloud point_cloud = load_lidar_scan(\"/scans/frame_001.bin\")  # Step 2: Convert to voxel grid voxel_grid = voxelize(point_cloud, grid_size=(0.1, 0.1, 0.1))  # Step 3: Pass through 3D CNN features = sparse_conv_net(voxel_grid)  # Step 4: Predict bounding boxes or labels detections = decode_bounding_boxes(features)  # Step 5: Fuse with other sensors (optional) fused_output = fuse_with_camera(detections, rgb_frame)`\n\n![](https://aman.ai/images/copy.png)\n\n`# Step 1: Load point cloud point_cloud = load_lidar_scan(\"/scans/frame_001.bin\")  # Step 2: Convert to voxel grid voxel_grid = voxelize(point_cloud, grid_size=(0.1, 0.1, 0.1))  # Step 3: Pass through 3D CNN features = sparse_conv_net(voxel_grid)  # Step 4: Predict bounding boxes or labels detections = decode_bounding_boxes(features)  # Step 5: Fuse with other sensors (optional) fused_output = fuse_with_camera(detections, rgb_frame)`",
    "contentLength": 3663,
    "wordCount": 112,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-pseudocode-flow"
  },
  {
    "id": "ai-ml-runtimes-suitable-applications-37",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Suitable Applications",
    "order": 37,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Autonomous vehicles (3D perception stacks)</li>\n  <li>Warehouse robots and drones</li>\n  <li>Industrial inspection systems</li>\n  <li>Advanced driver-assistance systems (ADAS)</li>\n  <li>\n    <p>SLAM systems for robotics</p>\n  </li>\n  <li>LidarTLM-like runtimes are not meant for general ML workloads but are highly optimized for 3D spatiotemporal inference, where conventional 2D model runtimes fall short. They tend to be integrated deep into hardware-specific SDKs or research frameworks.</li>\n</ul>\n<p>SLAM systems for robotics</p>",
    "contentMarkdown": "*   Autonomous vehicles (3D perception stacks)\n*   Warehouse robots and drones\n*   Industrial inspection systems\n*   Advanced driver-assistance systems (ADAS)\n*   SLAM systems for robotics\n    \n*   LidarTLM-like runtimes are not meant for general ML workloads but are highly optimized for 3D spatiotemporal inference, where conventional 2D model runtimes fall short. They tend to be integrated deep into hardware-specific SDKs or research frameworks.\n\nSLAM systems for robotics",
    "contentLength": 546,
    "wordCount": 66,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications"
  },
  {
    "id": "ai-ml-runtimes-llamacpp-deep-dive-38",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "llama.cpp Deep Dive",
    "order": 38,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is an open-source, C++-based implementation of inference for large language models (LLMs), originally inspired by Meta’s LLaMA family. It focuses on efficient CPU (and optionally GPU) inference for quantized transformer models. Unlike full ML runtimes, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is specialized, minimalist, and optimized for running LLMs—particularly on devices with constrained memory and compute budgets such as laptops, desktops, and even smartphones.</li>\n</ul>",
    "contentMarkdown": "*   `llama.cpp` is an open-source, C++-based implementation of inference for large language models (LLMs), originally inspired by Meta’s LLaMA family. It focuses on efficient CPU (and optionally GPU) inference for quantized transformer models. Unlike full ML runtimes, `llama.cpp` is specialized, minimalist, and optimized for running LLMs—particularly on devices with constrained memory and compute budgets such as laptops, desktops, and even smartphones.",
    "contentLength": 586,
    "wordCount": 61,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#llama.cpp-deep-dive"
  },
  {
    "id": "ai-ml-runtimes-overview-39",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Overview",
    "order": 39,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: LLM researchers, app developers, hobbyists</li>\n  <li><strong>Use Cases</strong>: Local chatbots, privacy-preserving LLM apps, embedded NLP on edge devices</li>\n  <li><strong>Model Format</strong>: Quantized GGUF (GPT-generated GGML Unified Format)</li>\n  <li><strong>Conversion Tools</strong>: Python conversion scripts from PyTorch checkpoints to GGUF</li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: LLM researchers, app developers, hobbyists\n*   **Use Cases**: Local chatbots, privacy-preserving LLM apps, embedded NLP on edge devices\n*   **Model Format**: Quantized GGUF (GPT-generated GGML Unified Format)\n*   **Conversion Tools**: Python conversion scripts from PyTorch checkpoints to GGUF",
    "contentLength": 410,
    "wordCount": 41,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-architecture-40",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Architecture",
    "order": 40,
    "orderInChapter": 9,
    "contentHtml": "<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: Quantized GGUF file loaded into memory</li>\n      <li><strong>KV Cache Allocation</strong>: Allocates buffers for key/value attention caching</li>\n      <li><strong>Token Embedding &amp; Input Prep</strong>: Maps token IDs to embeddings</li>\n      <li><strong>Layer Execution Loop</strong>: Runs transformer blocks sequentially</li>\n      <li><strong>Logits Output</strong>: Computes next-token logits, passed to sampler</li>\n      <li><strong>Sampling &amp; Token Generation</strong>: Greedy, top-k, nucleus, or temperature sampling</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>GGML Backend</strong>: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)</li>\n      <li><strong>Quantization Layers</strong>: 4-bit, 5-bit, and 8-bit quantized matmuls</li>\n      <li><strong>Inference Loop</strong>: Manually unrolled transformer stack—one layer at a time</li>\n      <li><strong>KV Cache Management</strong>: Token sequence history for autoregressive decoding</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optional GPU Support</strong>:</p>\n\n    <ul>\n      <li>Metal (macOS), OpenCL, CUDA support via modular backends</li>\n      <li>Offloading options: attention only, matmuls only, or full GPU</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: Quantized GGUF file loaded into memory</li>\n      <li><strong>KV Cache Allocation</strong>: Allocates buffers for key/value attention caching</li>\n      <li><strong>Token Embedding &amp; Input Prep</strong>: Maps token IDs to embeddings</li>\n      <li><strong>Layer Execution Loop</strong>: Runs transformer blocks sequentially</li>\n      <li><strong>Logits Output</strong>: Computes next-token logits, passed to sampler</li>\n      <li><strong>Sampling &amp; Token Generation</strong>: Greedy, top-k, nucleus, or temperature sampling</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>GGML Backend</strong>: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)</li>\n      <li><strong>Quantization Layers</strong>: 4-bit, 5-bit, and 8-bit quantized matmuls</li>\n      <li><strong>Inference Loop</strong>: Manually unrolled transformer stack—one layer at a time</li>\n      <li><strong>KV Cache Management</strong>: Token sequence history for autoregressive decoding</li>\n    </ul>\n<p><strong>Optional GPU Support</strong>:</p>\n<ul>\n      <li>Metal (macOS), OpenCL, CUDA support via modular backends</li>\n      <li>Offloading options: attention only, matmuls only, or full GPU</li>\n    </ul>",
    "contentMarkdown": "*   `llama.cpp` does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Load**: Quantized GGUF file loaded into memory\n    2.  **KV Cache Allocation**: Allocates buffers for key/value attention caching\n    3.  **Token Embedding & Input Prep**: Maps token IDs to embeddings\n    4.  **Layer Execution Loop**: Runs transformer blocks sequentially\n    5.  **Logits Output**: Computes next-token logits, passed to sampler\n    6.  **Sampling & Token Generation**: Greedy, top-k, nucleus, or temperature sampling\n*   **Key Components**:\n    \n    *   **GGML Backend**: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)\n    *   **Quantization Layers**: 4-bit, 5-bit, and 8-bit quantized matmuls\n    *   **Inference Loop**: Manually unrolled transformer stack—one layer at a time\n    *   **KV Cache Management**: Token sequence history for autoregressive decoding\n*   **Optional GPU Support**:\n    \n    *   Metal (macOS), OpenCL, CUDA support via modular backends\n    *   Offloading options: attention only, matmuls only, or full GPU\n\n`llama.cpp` does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.\n\n**Execution Flow**:\n\n1.  **Model Load**: Quantized GGUF file loaded into memory\n2.  **KV Cache Allocation**: Allocates buffers for key/value attention caching\n3.  **Token Embedding & Input Prep**: Maps token IDs to embeddings\n4.  **Layer Execution Loop**: Runs transformer blocks sequentially\n5.  **Logits Output**: Computes next-token logits, passed to sampler\n6.  **Sampling & Token Generation**: Greedy, top-k, nucleus, or temperature sampling\n\n**Key Components**:\n\n*   **GGML Backend**: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)\n*   **Quantization Layers**: 4-bit, 5-bit, and 8-bit quantized matmuls\n*   **Inference Loop**: Manually unrolled transformer stack—one layer at a time\n*   **KV Cache Management**: Token sequence history for autoregressive decoding\n\n**Optional GPU Support**:\n\n*   Metal (macOS), OpenCL, CUDA support via modular backends\n*   Offloading options: attention only, matmuls only, or full GPU",
    "contentLength": 3221,
    "wordCount": 320,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-41",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Implementation Details",
    "order": 41,
    "orderInChapter": 10,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Quantization</strong>:</p>\n\n    <ul>\n      <li>Tools like <code class=\"language-plaintext highlighter-rouge\">quantize.py</code> convert PyTorch models to GGUF format</li>\n      <li>Supports several quantization strategies (Q4_0, Q5_K, Q8_0, etc.)</li>\n      <li>Tradeoff between model size and accuracy</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Tensor Engine</strong>:</p>\n\n    <ul>\n      <li>No external libraries like BLAS, cuDNN, or MKL used by default</li>\n      <li>Uses hand-optimized C++ with platform-specific intrinsics</li>\n      <li>Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Optimization</strong>:</p>\n\n    <ul>\n      <li>Memory mapped file support (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Low memory mode: restricts KV cache or context length</li>\n      <li>Paging and streaming support for large contexts (e.g., <code class=\"language-plaintext highlighter-rouge\">llama.cpp + vLLM</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Integration</strong>:</p>\n\n    <ul>\n      <li>C API and Python bindings (<code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</li>\n      <li>Works with tools like LangChain, OpenRouter, and Ollama</li>\n      <li>Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Quantization</strong>:</p>\n<ul>\n      <li>Tools like <code class=\"language-plaintext highlighter-rouge\">quantize.py</code> convert PyTorch models to GGUF format</li>\n      <li>Supports several quantization strategies (Q4_0, Q5_K, Q8_0, etc.)</li>\n      <li>Tradeoff between model size and accuracy</li>\n    </ul>\n<p><strong>Tensor Engine</strong>:</p>\n<ul>\n      <li>No external libraries like BLAS, cuDNN, or MKL used by default</li>\n      <li>Uses hand-optimized C++ with platform-specific intrinsics</li>\n      <li>Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)</li>\n    </ul>\n<p><strong>Memory Optimization</strong>:</p>\n<ul>\n      <li>Memory mapped file support (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Low memory mode: restricts KV cache or context length</li>\n      <li>Paging and streaming support for large contexts (e.g., <code class=\"language-plaintext highlighter-rouge\">llama.cpp + vLLM</code>)</li>\n    </ul>\n<p><strong>Integration</strong>:</p>\n<ul>\n      <li>C API and Python bindings (<code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</li>\n      <li>Works with tools like LangChain, OpenRouter, and Ollama</li>\n      <li>Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.</li>\n    </ul>",
    "contentMarkdown": "*   **Model Quantization**:\n    \n    *   Tools like `quantize.py` convert PyTorch models to GGUF format\n    *   Supports several quantization strategies (Q4\\_0, Q5\\_K, Q8\\_0, etc.)\n    *   Tradeoff between model size and accuracy\n*   **Tensor Engine**:\n    \n    *   No external libraries like BLAS, cuDNN, or MKL used by default\n    *   Uses hand-optimized C++ with platform-specific intrinsics\n    *   Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)\n*   **Memory Optimization**:\n    \n    *   Memory mapped file support (`mmap`)\n    *   Low memory mode: restricts KV cache or context length\n    *   Paging and streaming support for large contexts (e.g., `llama.cpp + vLLM`)\n*   **Integration**:\n    \n    *   C API and Python bindings (`llama-cpp-python`)\n    *   Works with tools like LangChain, OpenRouter, and Ollama\n    *   Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.\n\n**Model Quantization**:\n\n*   Tools like `quantize.py` convert PyTorch models to GGUF format\n*   Supports several quantization strategies (Q4\\_0, Q5\\_K, Q8\\_0, etc.)\n*   Tradeoff between model size and accuracy\n\n**Tensor Engine**:\n\n*   No external libraries like BLAS, cuDNN, or MKL used by default\n*   Uses hand-optimized C++ with platform-specific intrinsics\n*   Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)\n\n**Memory Optimization**:\n\n*   Memory mapped file support (`mmap`)\n*   Low memory mode: restricts KV cache or context length\n*   Paging and streaming support for large contexts (e.g., `llama.cpp + vLLM`)\n\n**Integration**:\n\n*   C API and Python bindings (`llama-cpp-python`)\n*   Works with tools like LangChain, OpenRouter, and Ollama\n*   Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.",
    "contentLength": 2764,
    "wordCount": 234,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-42",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Pros and Cons",
    "order": 42,
    "orderInChapter": 11,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)</li>\n      <li>Portable and minimal dependencies</li>\n      <li>Quantization enables running models with &lt;4 GB RAM</li>\n      <li>Easily embedded into apps, games, and command-line tools</li>\n      <li>Active community and ecosystem (used in projects like Ollama and LM Studio)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Transformer-only; not a general ML runtime</li>\n      <li>No training support—strictly for inference</li>\n      <li>Manual conversion and tuning process required</li>\n      <li>Limited ops support; cannot easily add new ML layers</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)</li>\n      <li>Portable and minimal dependencies</li>\n      <li>Quantization enables running models with &lt;4 GB RAM</li>\n      <li>Easily embedded into apps, games, and command-line tools</li>\n      <li>Active community and ecosystem (used in projects like Ollama and LM Studio)</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Transformer-only; not a general ML runtime</li>\n      <li>No training support—strictly for inference</li>\n      <li>Manual conversion and tuning process required</li>\n      <li>Limited ops support; cannot easily add new ML layers</li>\n    </ul>",
    "contentMarkdown": "*   **Pros:**\n    \n    *   Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)\n    *   Portable and minimal dependencies\n    *   Quantization enables running models with <4 GB RAM\n    *   Easily embedded into apps, games, and command-line tools\n    *   Active community and ecosystem (used in projects like Ollama and LM Studio)\n*   **Cons:**\n    \n    *   Transformer-only; not a general ML runtime\n    *   No training support—strictly for inference\n    *   Manual conversion and tuning process required\n    *   Limited ops support; cannot easily add new ML layers\n\n**Pros:**\n\n*   Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)\n*   Portable and minimal dependencies\n*   Quantization enables running models with <4 GB RAM\n*   Easily embedded into apps, games, and command-line tools\n*   Active community and ecosystem (used in projects like Ollama and LM Studio)\n\n**Cons:**\n\n*   Transformer-only; not a general ML runtime\n*   No training support—strictly for inference\n*   Manual conversion and tuning process required\n*   Limited ops support; cannot easily add new ML layers",
    "contentLength": 1488,
    "wordCount": 166,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-cli-inference-43",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Example CLI Inference",
    "order": 43,
    "orderInChapter": 12,
    "contentHtml": "<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">./main <span class=\"nt\">-m</span> models/llama-7B.Q4_0.gguf <span class=\"nt\">-p</span> <span class=\"s2\">\"What is the capital of France?\"</span> <span class=\"nt\">-n</span> 64\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">./main <span class=\"nt\">-m</span> models/llama-7B.Q4_0.gguf <span class=\"nt\">-p</span> <span class=\"s2\">\"What is the capital of France?\"</span> <span class=\"nt\">-n</span> 64\n</code></pre>\n<ul>\n  <li><strong>Python Inference (via <code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</strong>:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">from</span> <span class=\"nn\">llama_cpp</span> <span class=\"kn\">import</span> <span class=\"n\">Llama</span>\n\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">Llama</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">(</span><span class=\"s\">\"Q: What is the capital of France?</span><span class=\"se\">\\n</span><span class=\"s\">A:\"</span><span class=\"p\">,</span> <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s\">\"choices\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">\"text\"</span><span class=\"p\">])</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">from</span> <span class=\"nn\">llama_cpp</span> <span class=\"kn\">import</span> <span class=\"n\">Llama</span>\n\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">Llama</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">(</span><span class=\"s\">\"Q: What is the capital of France?</span><span class=\"se\">\\n</span><span class=\"s\">A:\"</span><span class=\"p\">,</span> <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s\">\"choices\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">\"text\"</span><span class=\"p\">])</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>WebAssembly Example (Browser):</strong></p>\n\n    <ul>\n      <li>Precompiled WASM version can run LLMs client-side using WebGPU</li>\n      <li>Useful for private, offline AI assistants directly in browser</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>WebAssembly Example (Browser):</strong></p>\n<ul>\n      <li>Precompiled WASM version can run LLMs client-side using WebGPU</li>\n      <li>Useful for private, offline AI assistants directly in browser</li>\n    </ul>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`./main -m models/llama-7B.Q4_0.gguf -p \"What is the capital of France?\" -n 64`\n\n![](https://aman.ai/images/copy.png)\n\n`./main -m models/llama-7B.Q4_0.gguf -p \"What is the capital of France?\" -n 64`\n\n*   **Python Inference (via `llama-cpp-python`)**:\n\n![](https://aman.ai/images/copy.png)\n\n`from llama_cpp import Llama  llm = Llama(model_path=\"llama-7B.Q4_0.gguf\") output = llm(\"Q: What is the capital of France?\\nA:\", max_tokens=32) print(output[\"choices\"][0][\"text\"])`\n\n![](https://aman.ai/images/copy.png)\n\n`from llama_cpp import Llama  llm = Llama(model_path=\"llama-7B.Q4_0.gguf\") output = llm(\"Q: What is the capital of France?\\nA:\", max_tokens=32) print(output[\"choices\"][0][\"text\"])`\n\n*   **WebAssembly Example (Browser):**\n    \n    *   Precompiled WASM version can run LLMs client-side using WebGPU\n    *   Useful for private, offline AI assistants directly in browser\n\n**WebAssembly Example (Browser):**\n\n*   Precompiled WASM version can run LLMs client-side using WebGPU\n*   Useful for private, offline AI assistants directly in browser",
    "contentLength": 4222,
    "wordCount": 116,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-cli-inference"
  },
  {
    "id": "ai-ml-runtimes-suitable-applications-44",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "LidarTLM Deep Dive",
    "title": "Suitable Applications",
    "order": 44,
    "orderInChapter": 13,
    "contentHtml": "<ul>\n  <li>Private, offline chatbots</li>\n  <li>Voice assistants embedded in hardware</li>\n  <li>Context-aware agents in games or productivity apps</li>\n  <li>\n    <p>Developer tools with local NLP capabilities</p>\n  </li>\n  <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> showcases what is possible with small, optimized transformer runtimes and CPU-centric design. It’s not a general-purpose ML runtime but a powerful engine for language inference where privacy, portability, or internet-free operation is desired.</li>\n</ul>\n<p>Developer tools with local NLP capabilities</p>",
    "contentMarkdown": "*   Private, offline chatbots\n*   Voice assistants embedded in hardware\n*   Context-aware agents in games or productivity apps\n*   Developer tools with local NLP capabilities\n    \n*   `llama.cpp` showcases what is possible with small, optimized transformer runtimes and CPU-centric design. It’s not a general-purpose ML runtime but a powerful engine for language inference where privacy, portability, or internet-free operation is desired.\n\nDeveloper tools with local NLP capabilities",
    "contentLength": 602,
    "wordCount": 66,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications"
  },
  {
    "id": "ai-ml-runtimes-overview-45",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Overview",
    "order": 45,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Mobile developers, embedded engineers, production ML ops</li>\n  <li><strong>Use Cases</strong>: Real-time image classification, object detection, audio processing, NLP, edge analytics</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.tflite</code> (FlatBuffer format)</li>\n  <li><strong>Conversion Tools</strong>: TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">\\rightarrow</script> TFLite via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n</ul>",
    "contentMarkdown": "*   **Developer Target**: Mobile developers, embedded engineers, production ML ops\n*   **Use Cases**: Real-time image classification, object detection, audio processing, NLP, edge analytics\n*   **Model Format**: `.tflite` (FlatBuffer format)\n*   **Conversion Tools**: TensorFlow →→\\\\rightarrow TFLite via `TFLiteConverter`",
    "contentLength": 1750,
    "wordCount": 37,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-tensorflow-lite-architecture-46",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "TensorFlow Lite Architecture",
    "order": 46,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Model Conversion</strong>:</p>\n\n        <ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Model Load</strong>:</p>\n\n        <ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Tensor Allocation</strong>:</p>\n\n        <ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Postprocessing</strong>:</p>\n\n        <ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>FlatBuffer Model</strong>: Compact, zero-copy, serializable model format</li>\n      <li><strong>Interpreter</strong>: Core engine that evaluates the model graph</li>\n      <li><strong>Delegate Interface</strong>: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)</li>\n      <li><strong>Kernel Registry</strong>: Maps ops to optimized C++ implementations (or delegates)</li>\n    </ul>\n  </li>\n</ul>\n<p>TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Model Conversion</strong>:</p>\n\n        <ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Model Load</strong>:</p>\n\n        <ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Tensor Allocation</strong>:</p>\n\n        <ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Postprocessing</strong>:</p>\n\n        <ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Model Conversion</strong>:</p>\n<ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n<p><strong>Model Load</strong>:</p>\n<ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n<p><strong>Tensor Allocation</strong>:</p>\n<ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n<p><strong>Inference Execution</strong>:</p>\n<ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n<p><strong>Postprocessing</strong>:</p>\n<ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>FlatBuffer Model</strong>: Compact, zero-copy, serializable model format</li>\n      <li><strong>Interpreter</strong>: Core engine that evaluates the model graph</li>\n      <li><strong>Delegate Interface</strong>: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)</li>\n      <li><strong>Kernel Registry</strong>: Maps ops to optimized C++ implementations (or delegates)</li>\n    </ul>",
    "contentMarkdown": "*   TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Conversion**:\n        \n        *   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n    2.  **Model Load**:\n        \n        *   The model is loaded by the `Interpreter` class on the target device.\n    3.  **Tensor Allocation**:\n        \n        *   Memory buffers for input/output tensors are allocated.\n    4.  **Inference Execution**:\n        \n        *   The interpreter evaluates the computation graph, optionally using delegates.\n    5.  **Postprocessing**:\n        \n        *   Output tensors are read and interpreted by the application.\n*   **Key Components**:\n    \n    *   **FlatBuffer Model**: Compact, zero-copy, serializable model format\n    *   **Interpreter**: Core engine that evaluates the model graph\n    *   **Delegate Interface**: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)\n    *   **Kernel Registry**: Maps ops to optimized C++ implementations (or delegates)\n\nTFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.\n\n**Execution Flow**:\n\n1.  **Model Conversion**:\n    \n    *   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n2.  **Model Load**:\n    \n    *   The model is loaded by the `Interpreter` class on the target device.\n3.  **Tensor Allocation**:\n    \n    *   Memory buffers for input/output tensors are allocated.\n4.  **Inference Execution**:\n    \n    *   The interpreter evaluates the computation graph, optionally using delegates.\n5.  **Postprocessing**:\n    \n    *   Output tensors are read and interpreted by the application.\n\n**Model Conversion**:\n\n*   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n\n**Model Load**:\n\n*   The model is loaded by the `Interpreter` class on the target device.\n\n**Tensor Allocation**:\n\n*   Memory buffers for input/output tensors are allocated.\n\n**Inference Execution**:\n\n*   The interpreter evaluates the computation graph, optionally using delegates.\n\n**Postprocessing**:\n\n*   Output tensors are read and interpreted by the application.\n\n**Key Components**:\n\n*   **FlatBuffer Model**: Compact, zero-copy, serializable model format\n*   **Interpreter**: Core engine that evaluates the model graph\n*   **Delegate Interface**: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)\n*   **Kernel Registry**: Maps ops to optimized C++ implementations (or delegates)",
    "contentLength": 4736,
    "wordCount": 349,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#tensorflow-lite-architecture"
  },
  {
    "id": "ai-ml-runtimes-implementation-details-47",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Implementation Details",
    "order": 47,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Conversion</strong>:</p>\n\n    <ul>\n      <li>Converts SavedModels, Keras <code class=\"language-plaintext highlighter-rouge\">.h5</code>, or concrete functions to <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n      <li>Supports post-training quantization (dynamic, full integer, float16)</li>\n      <li>Model optimizations include constant folding, op fusion, and pruning</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Delegates</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Optional hardware acceleration backends:</p>\n\n        <ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n      </li>\n      <li>\n        <p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Threading and Performance</strong>:</p>\n\n    <ul>\n      <li>Supports multi-threaded inference</li>\n      <li>Interpreter can be run in C++, Java, Kotlin, Python, Swift</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Conversion</strong>:</p>\n<ul>\n      <li>Converts SavedModels, Keras <code class=\"language-plaintext highlighter-rouge\">.h5</code>, or concrete functions to <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n      <li>Supports post-training quantization (dynamic, full integer, float16)</li>\n      <li>Model optimizations include constant folding, op fusion, and pruning</li>\n    </ul>\n<p><strong>Delegates</strong>:</p>\n<ul>\n      <li>\n        <p>Optional hardware acceleration backends:</p>\n\n        <ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n      </li>\n      <li>\n        <p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n      </li>\n    </ul>\n<p>Optional hardware acceleration backends:</p>\n<ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n<p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n<p><strong>Threading and Performance</strong>:</p>\n<ul>\n      <li>Supports multi-threaded inference</li>\n      <li>Interpreter can be run in C++, Java, Kotlin, Python, Swift</li>\n    </ul>",
    "contentMarkdown": "*   **Model Conversion**:\n    \n    *   Converts SavedModels, Keras `.h5`, or concrete functions to `.tflite`\n    *   Supports post-training quantization (dynamic, full integer, float16)\n    *   Model optimizations include constant folding, op fusion, and pruning\n*   **Delegates**:\n    \n    *   Optional hardware acceleration backends:\n        \n        *   **NNAPI** (Android)\n        *   **GPU Delegate** (OpenCL, Metal)\n        *   **Hexagon Delegate** (Qualcomm DSP)\n        *   **Core ML Delegate** (iOS/macOS)\n        *   **EdgeTPU Delegate** (Coral devices)\n    *   Delegates work by “claiming” supported subgraphs during interpreter initialization\n        \n*   **Threading and Performance**:\n    \n    *   Supports multi-threaded inference\n    *   Interpreter can be run in C++, Java, Kotlin, Python, Swift\n\n**Model Conversion**:\n\n*   Converts SavedModels, Keras `.h5`, or concrete functions to `.tflite`\n*   Supports post-training quantization (dynamic, full integer, float16)\n*   Model optimizations include constant folding, op fusion, and pruning\n\n**Delegates**:\n\n*   Optional hardware acceleration backends:\n    \n    *   **NNAPI** (Android)\n    *   **GPU Delegate** (OpenCL, Metal)\n    *   **Hexagon Delegate** (Qualcomm DSP)\n    *   **Core ML Delegate** (iOS/macOS)\n    *   **EdgeTPU Delegate** (Coral devices)\n*   Delegates work by “claiming” supported subgraphs during interpreter initialization\n    \n\nOptional hardware acceleration backends:\n\n*   **NNAPI** (Android)\n*   **GPU Delegate** (OpenCL, Metal)\n*   **Hexagon Delegate** (Qualcomm DSP)\n*   **Core ML Delegate** (iOS/macOS)\n*   **EdgeTPU Delegate** (Coral devices)\n\nDelegates work by “claiming” supported subgraphs during interpreter initialization\n\n**Threading and Performance**:\n\n*   Supports multi-threaded inference\n*   Interpreter can be run in C++, Java, Kotlin, Python, Swift",
    "contentLength": 2974,
    "wordCount": 213,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details"
  },
  {
    "id": "ai-ml-runtimes-tensorflow-serving-short-overview-48",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "TensorFlow Serving (Short Overview)",
    "order": 48,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Designed for scalable deployment of TensorFlow models on servers</li>\n  <li>Models are exposed as REST/gRPC endpoints</li>\n  <li>Automatically loads, unloads, and versions models</li>\n  <li>Uses <code class=\"language-plaintext highlighter-rouge\">SavedModel</code> format, not <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n  <li>\n    <p>Not suitable for offline or embedded deployment</p>\n  </li>\n  <li><strong>Use Case Comparison</strong>:</li>\n</ul>\n<p>Not suitable for offline or embedded deployment</p>\n<p>Here is your formatted table following the provided style:</p>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Device</td>\n<td class=\"tg-tleft-valign-first\">Mobile/Edge</td>\n<td class=\"tg-tleft-valign-second\">Cloud/Server</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Format</td>\n<td class=\"tg-tleft-valign-first\"><code>.tflite</code></td>\n<td class=\"tg-tleft-valign-second\">SavedModel</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Communication</td>\n<td class=\"tg-tleft-valign-first\">In-process / Local</td>\n<td class=\"tg-tleft-valign-second\">gRPC / REST</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Milliseconds</td>\n<td class=\"tg-tleft-valign-second\">Sub-second to seconds</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Training Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No (inference only)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Size</td>\n<td class=\"tg-tleft-valign-first\">Small (~100s of KB)</td>\n<td class=\"tg-tleft-valign-second\">Large, server framework</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Device</td>\n<td class=\"tg-tleft-valign-first\">Mobile/Edge</td>\n<td class=\"tg-tleft-valign-second\">Cloud/Server</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Format</td>\n<td class=\"tg-tleft-valign-first\"><code>.tflite</code></td>\n<td class=\"tg-tleft-valign-second\">SavedModel</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Communication</td>\n<td class=\"tg-tleft-valign-first\">In-process / Local</td>\n<td class=\"tg-tleft-valign-second\">gRPC / REST</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Milliseconds</td>\n<td class=\"tg-tleft-valign-second\">Sub-second to seconds</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Training Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No (inference only)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Size</td>\n<td class=\"tg-tleft-valign-first\">Small (~100s of KB)</td>\n<td class=\"tg-tleft-valign-second\">Large, server framework</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "*   Designed for scalable deployment of TensorFlow models on servers\n*   Models are exposed as REST/gRPC endpoints\n*   Automatically loads, unloads, and versions models\n*   Uses `SavedModel` format, not `.tflite`\n*   Not suitable for offline or embedded deployment\n    \n*   **Use Case Comparison**:\n\nNot suitable for offline or embedded deployment\n\nHere is your formatted table following the provided style:\n\n**Feature**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Device\n\nMobile/Edge\n\nCloud/Server\n\nModel Format\n\n`.tflite`\n\nSavedModel\n\nCommunication\n\nIn-process / Local\n\ngRPC / REST\n\nLatency\n\nMilliseconds\n\nSub-second to seconds\n\nTraining Support\n\nNo\n\nNo (inference only)\n\nDeployment Size\n\nSmall (~100s of KB)\n\nLarge, server framework\n\n**Feature**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Device\n\nMobile/Edge\n\nCloud/Server\n\nModel Format\n\n`.tflite`\n\nSavedModel\n\nCommunication\n\nIn-process / Local\n\ngRPC / REST\n\nLatency\n\nMilliseconds\n\nSub-second to seconds\n\nTraining Support\n\nNo\n\nNo (inference only)\n\nDeployment Size\n\nSmall (~100s of KB)\n\nLarge, server framework",
    "contentLength": 3309,
    "wordCount": 138,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#tensorflow-serving-(short-overview)"
  },
  {
    "id": "ai-ml-runtimes-pros-and-cons-49",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Pros and Cons",
    "order": 49,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros (TensorFlow Lite):</strong></p>\n\n    <ul>\n      <li>Compact and efficient format (FlatBuffer)</li>\n      <li>Broad hardware delegate support</li>\n      <li>Quantization-aware and post-training optimizations</li>\n      <li>Cross-platform support (iOS, Android, Linux, microcontrollers)</li>\n      <li>Strong ecosystem and pre-trained model zoo (<code class=\"language-plaintext highlighter-rouge\">tflite-model-maker</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons (TensorFlow Lite):</strong></p>\n\n    <ul>\n      <li>Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)</li>\n      <li>Delegate behavior can be opaque and platform-dependent</li>\n      <li>Conversion can fail silently if unsupported ops are encountered</li>\n      <li>Debugging delegate fallbacks can be non-trivial</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros (TensorFlow Lite):</strong></p>\n<ul>\n      <li>Compact and efficient format (FlatBuffer)</li>\n      <li>Broad hardware delegate support</li>\n      <li>Quantization-aware and post-training optimizations</li>\n      <li>Cross-platform support (iOS, Android, Linux, microcontrollers)</li>\n      <li>Strong ecosystem and pre-trained model zoo (<code class=\"language-plaintext highlighter-rouge\">tflite-model-maker</code>)</li>\n    </ul>\n<p><strong>Cons (TensorFlow Lite):</strong></p>\n<ul>\n      <li>Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)</li>\n      <li>Delegate behavior can be opaque and platform-dependent</li>\n      <li>Conversion can fail silently if unsupported ops are encountered</li>\n      <li>Debugging delegate fallbacks can be non-trivial</li>\n    </ul>",
    "contentMarkdown": "*   **Pros (TensorFlow Lite):**\n    \n    *   Compact and efficient format (FlatBuffer)\n    *   Broad hardware delegate support\n    *   Quantization-aware and post-training optimizations\n    *   Cross-platform support (iOS, Android, Linux, microcontrollers)\n    *   Strong ecosystem and pre-trained model zoo (`tflite-model-maker`)\n*   **Cons (TensorFlow Lite):**\n    \n    *   Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)\n    *   Delegate behavior can be opaque and platform-dependent\n    *   Conversion can fail silently if unsupported ops are encountered\n    *   Debugging delegate fallbacks can be non-trivial\n\n**Pros (TensorFlow Lite):**\n\n*   Compact and efficient format (FlatBuffer)\n*   Broad hardware delegate support\n*   Quantization-aware and post-training optimizations\n*   Cross-platform support (iOS, Android, Linux, microcontrollers)\n*   Strong ecosystem and pre-trained model zoo (`tflite-model-maker`)\n\n**Cons (TensorFlow Lite):**\n\n*   Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)\n*   Delegate behavior can be opaque and platform-dependent\n*   Conversion can fail silently if unsupported ops are encountered\n*   Debugging delegate fallbacks can be non-trivial",
    "contentLength": 1688,
    "wordCount": 154,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons"
  },
  {
    "id": "ai-ml-runtimes-example-inference-python---tflite-50",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Example Inference (Python - TFLite)",
    "order": 50,
    "orderInChapter": 6,
    "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load model\n</span><span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">()</span>\n<span class=\"n\">output_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_output_details</span><span class=\"p\">()</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">set_tensor</span><span class=\"p\">(</span><span class=\"n\">input_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">],</span> <span class=\"n\">input_data</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">()</span>\n<span class=\"n\">output_data</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_tensor</span><span class=\"p\">(</span><span class=\"n\">output_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output_data</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load model\n</span><span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">()</span>\n<span class=\"n\">output_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_output_details</span><span class=\"p\">()</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">set_tensor</span><span class=\"p\">(</span><span class=\"n\">input_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">],</span> <span class=\"n\">input_data</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">()</span>\n<span class=\"n\">output_data</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_tensor</span><span class=\"p\">(</span><span class=\"n\">output_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output_data</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>Delegate usage (Android NNAPI, example via Java/Kotlin):</strong></li>\n</ul>\n<div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span><span class=\"o\">();</span>\n<span class=\"n\">options</span><span class=\"o\">.</span><span class=\"na\">addDelegate</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">NnApiDelegate</span><span class=\"o\">());</span>\n<span class=\"nc\">Interpreter</span> <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">(</span><span class=\"n\">tfliteModel</span><span class=\"o\">,</span> <span class=\"n\">options</span><span class=\"o\">);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span><span class=\"o\">();</span>\n<span class=\"n\">options</span><span class=\"o\">.</span><span class=\"na\">addDelegate</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">NnApiDelegate</span><span class=\"o\">());</span>\n<span class=\"nc\">Interpreter</span> <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">(</span><span class=\"n\">tfliteModel</span><span class=\"o\">,</span> <span class=\"n\">options</span><span class=\"o\">);</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import numpy as np  # Load model interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\") interpreter.allocate_tensors()  # Prepare input input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Run inference interpreter.set_tensor(input_details[0]['index'], input_data) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction:\", output_data)`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import numpy as np  # Load model interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\") interpreter.allocate_tensors()  # Prepare input input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Run inference interpreter.set_tensor(input_details[0]['index'], input_data) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction:\", output_data)`\n\n*   **Delegate usage (Android NNAPI, example via Java/Kotlin):**\n\n![](https://aman.ai/images/copy.png)\n\n`Interpreter.Options options = new Interpreter.Options(); options.addDelegate(new NnApiDelegate()); Interpreter interpreter = new Interpreter(tfliteModel, options);`\n\n![](https://aman.ai/images/copy.png)\n\n`Interpreter.Options options = new Interpreter.Options(); options.addDelegate(new NnApiDelegate()); Interpreter interpreter = new Interpreter(tfliteModel, options);`",
    "contentLength": 8461,
    "wordCount": 120,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#example-inference-(python---tflite)"
  },
  {
    "id": "ai-ml-runtimes-suitable-applications-51",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Suitable Applications",
    "order": 51,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>On-device health and fitness apps</li>\n  <li>Real-time object detection in AR</li>\n  <li>Offline voice recognition</li>\n  <li>Edge anomaly detection</li>\n  <li>\n    <p>TinyML deployments with <code class=\"language-plaintext highlighter-rouge\">TensorFlow Lite for Microcontrollers</code></p>\n  </li>\n  <li>TensorFlow Lite remains one of the most production-hardened and flexible runtimes for on-device ML, particularly in mobile and embedded contexts. Its support for multiple delegates and optimizations makes it a go-to choice for developers deploying models outside the cloud.</li>\n</ul>\n<p>TinyML deployments with <code class=\"language-plaintext highlighter-rouge\">TensorFlow Lite for Microcontrollers</code></p>",
    "contentMarkdown": "*   On-device health and fitness apps\n*   Real-time object detection in AR\n*   Offline voice recognition\n*   Edge anomaly detection\n*   TinyML deployments with `TensorFlow Lite for Microcontrollers`\n    \n*   TensorFlow Lite remains one of the most production-hardened and flexible runtimes for on-device ML, particularly in mobile and embedded contexts. Its support for multiple delegates and optimizations makes it a go-to choice for developers deploying models outside the cloud.\n\nTinyML deployments with `TensorFlow Lite for Microcontrollers`",
    "contentLength": 726,
    "wordCount": 75,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications"
  },
  {
    "id": "ai-ml-runtimes-comparative-analysis-52",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Comparative Analysis",
    "order": 52,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>Here are detailed tabular comparisons that encapsulates all key aspects across the different on-device ML runtimes discussed in the primer.</li>\n</ul>\n<h4 id=\"general-characteristics\">General Characteristics</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Platform(s)</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA Jetson, Desktop, Server</td>\n<td class=\"tg-tleft-valign-first\">Apple devices (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS only)</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform</td>\n<td class=\"tg-tleft-valign-first\">Embedded, mobile, MCU</td>\n<td class=\"tg-tleft-valign-first\">Robotics, automotive, ADAS</td>\n<td class=\"tg-tleft-valign-first\">Desktop, mobile, browser</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (mobile/edge)</td>\n<td class=\"tg-tleft-valign-second\">Cloud / server environments</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ML Task Focus</td>\n<td class=\"tg-tleft-valign-first\">Optimized inference</td>\n<td class=\"tg-tleft-valign-first\">General ML (vision, NLP)</td>\n<td class=\"tg-tleft-valign-first\">Research, transformer/NLP</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light inference</td>\n<td class=\"tg-tleft-valign-first\">3D spatial perception</td>\n<td class=\"tg-tleft-valign-first\">Large language model inference</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-second\">Scalable inference serving</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Only?</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">No (supports training)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Open Source?</td>\n<td class=\"tg-tleft-valign-first\">Partially (binaries open, tools closed)</td>\n<td class=\"tg-tleft-valign-first\">Partially (via tools)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Partially / variable</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Platform(s)</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA Jetson, Desktop, Server</td>\n<td class=\"tg-tleft-valign-first\">Apple devices (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS only)</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform</td>\n<td class=\"tg-tleft-valign-first\">Embedded, mobile, MCU</td>\n<td class=\"tg-tleft-valign-first\">Robotics, automotive, ADAS</td>\n<td class=\"tg-tleft-valign-first\">Desktop, mobile, browser</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (mobile/edge)</td>\n<td class=\"tg-tleft-valign-second\">Cloud / server environments</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ML Task Focus</td>\n<td class=\"tg-tleft-valign-first\">Optimized inference</td>\n<td class=\"tg-tleft-valign-first\">General ML (vision, NLP)</td>\n<td class=\"tg-tleft-valign-first\">Research, transformer/NLP</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light inference</td>\n<td class=\"tg-tleft-valign-first\">3D spatial perception</td>\n<td class=\"tg-tleft-valign-first\">Large language model inference</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-second\">Scalable inference serving</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Only?</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">No (supports training)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Open Source?</td>\n<td class=\"tg-tleft-valign-first\">Partially (binaries open, tools closed)</td>\n<td class=\"tg-tleft-valign-first\">Partially (via tools)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Partially / variable</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"model-formats-and-conversion\">Model Formats and Conversion</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Format</td>\n<td class=\"tg-tleft-valign-first\">.plan (TensorRT engine file)</td>\n<td class=\"tg-tleft-valign-first\">.mlmodelc</td>\n<td class=\"tg-tleft-valign-first\">Python-defined layers</td>\n<td class=\"tg-tleft-valign-first\">.onnx</td>\n<td class=\"tg-tleft-valign-first\">.ptc (compiled TorchScript)</td>\n<td class=\"tg-tleft-valign-first\">Custom / converted .onnx / raw tensors</td>\n<td class=\"tg-tleft-valign-first\">.gguf (quantized LLMs)</td>\n<td class=\"tg-tleft-valign-first\">.tflite (FlatBuffer)</td>\n<td class=\"tg-tleft-valign-second\">SavedModel (.pb, .pbtxt)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Supported Frameworks</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, ONNX</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TF (via converters)</td>\n<td class=\"tg-tleft-valign-first\">Native Python API</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow, others</td>\n<td class=\"tg-tleft-valign-first\">PyTorch (TorchScript subset)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow (via export)</td>\n<td class=\"tg-tleft-valign-first\">LLaMA-family only</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, Keras</td>\n<td class=\"tg-tleft-valign-second\">TensorFlow only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conversion Required?</td>\n<td class=\"tg-tleft-valign-first\">Yes (from ONNX or PyTorch export)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via coremltools)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (usually from PyTorch)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via AOT compiler)</td>\n<td class=\"tg-tleft-valign-first\">Yes, often includes preprocessing</td>\n<td class=\"tg-tleft-valign-first\">Yes (convert + quantize)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TFLiteConverter)</td>\n<td class=\"tg-tleft-valign-second\">No (already in target format)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Format</td>\n<td class=\"tg-tleft-valign-first\">.plan (TensorRT engine file)</td>\n<td class=\"tg-tleft-valign-first\">.mlmodelc</td>\n<td class=\"tg-tleft-valign-first\">Python-defined layers</td>\n<td class=\"tg-tleft-valign-first\">.onnx</td>\n<td class=\"tg-tleft-valign-first\">.ptc (compiled TorchScript)</td>\n<td class=\"tg-tleft-valign-first\">Custom / converted .onnx / raw tensors</td>\n<td class=\"tg-tleft-valign-first\">.gguf (quantized LLMs)</td>\n<td class=\"tg-tleft-valign-first\">.tflite (FlatBuffer)</td>\n<td class=\"tg-tleft-valign-second\">SavedModel (.pb, .pbtxt)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Supported Frameworks</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, ONNX</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TF (via converters)</td>\n<td class=\"tg-tleft-valign-first\">Native Python API</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow, others</td>\n<td class=\"tg-tleft-valign-first\">PyTorch (TorchScript subset)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow (via export)</td>\n<td class=\"tg-tleft-valign-first\">LLaMA-family only</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, Keras</td>\n<td class=\"tg-tleft-valign-second\">TensorFlow only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conversion Required?</td>\n<td class=\"tg-tleft-valign-first\">Yes (from ONNX or PyTorch export)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via coremltools)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (usually from PyTorch)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via AOT compiler)</td>\n<td class=\"tg-tleft-valign-first\">Yes, often includes preprocessing</td>\n<td class=\"tg-tleft-valign-first\">Yes (convert + quantize)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TFLiteConverter)</td>\n<td class=\"tg-tleft-valign-second\">No (already in target format)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"execution-model-and-hardware-support\">Execution Model and Hardware Support</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Type</td>\n<td class=\"tg-tleft-valign-first\">AOT compiled CUDA graph</td>\n<td class=\"tg-tleft-valign-first\">Eager, dynamic hardware assignment</td>\n<td class=\"tg-tleft-valign-first\">Eager + compiled graph</td>\n<td class=\"tg-tleft-valign-first\">Static graph with runtime optimizations</td>\n<td class=\"tg-tleft-valign-first\">Bytecode VM interpreter</td>\n<td class=\"tg-tleft-valign-first\">Sparse 3D graph + temporal flow</td>\n<td class=\"tg-tleft-valign-first\">Manual loop over transformer layers</td>\n<td class=\"tg-tleft-valign-first\">Static interpreter + delegates</td>\n<td class=\"tg-tleft-valign-second\">REST/gRPC inference pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU Support</td>\n<td class=\"tg-tleft-valign-first\">No (GPU only)</td>\n<td class=\"tg-tleft-valign-first\">Yes (fallback)</td>\n<td class=\"tg-tleft-valign-first\">Yes (M1/M2 optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes (default EP)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes (highly optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, Tensor Cores)</td>\n<td class=\"tg-tleft-valign-first\">Yes (Metal)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via MPS)</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, DirectML, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, embedded GPUs)</td>\n<td class=\"tg-tleft-valign-first\">Optional (Metal, CUDA, OpenCL)</td>\n<td class=\"tg-tleft-valign-first\">Yes (OpenCL, Metal)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU / DSP Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (Apple ANE)</td>\n<td class=\"tg-tleft-valign-first\">Emerging ANE support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via NNAPI, OpenVINO, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Potential via backend interface</td>\n<td class=\"tg-tleft-valign-first\">Yes (TI, Nvidia, ADAS accelerators)</td>\n<td class=\"tg-tleft-valign-first\">No (LLM-focused, CPU-oriented)</td>\n<td class=\"tg-tleft-valign-first\">Yes (NNAPI, EdgeTPU, Hexagon)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Abstraction</td>\n<td class=\"tg-tleft-valign-first\">Low-level plugin engine, manual tuning</td>\n<td class=\"tg-tleft-valign-first\">Automatic</td>\n<td class=\"tg-tleft-valign-first\">Manual tuning via MLX</td>\n<td class=\"tg-tleft-valign-first\">Modular Execution Providers (EPs)</td>\n<td class=\"tg-tleft-valign-first\">Compiled dispatcher with targets</td>\n<td class=\"tg-tleft-valign-first\">Device-specific optimization required</td>\n<td class=\"tg-tleft-valign-first\">Low-level SIMD/CUDA offload</td>\n<td class=\"tg-tleft-valign-first\">Delegate-based (pluggable)</td>\n<td class=\"tg-tleft-valign-second\">N/A</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Type</td>\n<td class=\"tg-tleft-valign-first\">AOT compiled CUDA graph</td>\n<td class=\"tg-tleft-valign-first\">Eager, dynamic hardware assignment</td>\n<td class=\"tg-tleft-valign-first\">Eager + compiled graph</td>\n<td class=\"tg-tleft-valign-first\">Static graph with runtime optimizations</td>\n<td class=\"tg-tleft-valign-first\">Bytecode VM interpreter</td>\n<td class=\"tg-tleft-valign-first\">Sparse 3D graph + temporal flow</td>\n<td class=\"tg-tleft-valign-first\">Manual loop over transformer layers</td>\n<td class=\"tg-tleft-valign-first\">Static interpreter + delegates</td>\n<td class=\"tg-tleft-valign-second\">REST/gRPC inference pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU Support</td>\n<td class=\"tg-tleft-valign-first\">No (GPU only)</td>\n<td class=\"tg-tleft-valign-first\">Yes (fallback)</td>\n<td class=\"tg-tleft-valign-first\">Yes (M1/M2 optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes (default EP)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes (highly optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, Tensor Cores)</td>\n<td class=\"tg-tleft-valign-first\">Yes (Metal)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via MPS)</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, DirectML, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, embedded GPUs)</td>\n<td class=\"tg-tleft-valign-first\">Optional (Metal, CUDA, OpenCL)</td>\n<td class=\"tg-tleft-valign-first\">Yes (OpenCL, Metal)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU / DSP Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (Apple ANE)</td>\n<td class=\"tg-tleft-valign-first\">Emerging ANE support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via NNAPI, OpenVINO, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Potential via backend interface</td>\n<td class=\"tg-tleft-valign-first\">Yes (TI, Nvidia, ADAS accelerators)</td>\n<td class=\"tg-tleft-valign-first\">No (LLM-focused, CPU-oriented)</td>\n<td class=\"tg-tleft-valign-first\">Yes (NNAPI, EdgeTPU, Hexagon)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Abstraction</td>\n<td class=\"tg-tleft-valign-first\">Low-level plugin engine, manual tuning</td>\n<td class=\"tg-tleft-valign-first\">Automatic</td>\n<td class=\"tg-tleft-valign-first\">Manual tuning via MLX</td>\n<td class=\"tg-tleft-valign-first\">Modular Execution Providers (EPs)</td>\n<td class=\"tg-tleft-valign-first\">Compiled dispatcher with targets</td>\n<td class=\"tg-tleft-valign-first\">Device-specific optimization required</td>\n<td class=\"tg-tleft-valign-first\">Low-level SIMD/CUDA offload</td>\n<td class=\"tg-tleft-valign-first\">Delegate-based (pluggable)</td>\n<td class=\"tg-tleft-valign-second\">N/A</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"optimization-size-and-constraints\">Optimization, Size, and Constraints</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Optimization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (kernel tuning, quantization, `float16`/`int8`)</td>\n<td class=\"tg-tleft-valign-first\">Yes (ANE targeting, quantization)</td>\n<td class=\"tg-tleft-valign-first\">No built-in, manual scripting</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, pruning, graph fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (operator pruning, bytecode fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (3D-aware compression and fusions)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantized GGUF)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, fusion)</td>\n<td class=\"tg-tleft-valign-second\">Yes (batching, threading)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Runtime Size</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–15 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–10 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Large (5–30 MB)</td>\n<td class=\"tg-tleft-valign-first\">Very small (&lt;1 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium–Large</td>\n<td class=\"tg-tleft-valign-first\">Small–Medium</td>\n<td class=\"tg-tleft-valign-first\">Small (~0.5–5 MB)</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint (Inference)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate (GPU memory bound)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate</td>\n<td class=\"tg-tleft-valign-first\">Moderate (GPU-heavy)</td>\n<td class=\"tg-tleft-valign-first\">Variable (depends on EPs)</td>\n<td class=\"tg-tleft-valign-first\">Ultra-low (sub-MB possible)</td>\n<td class=\"tg-tleft-valign-first\">High (large point cloud buffers)</td>\n<td class=\"tg-tleft-valign-first\">Low (~3–6 GB RAM for 7B models)</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Very low (sub-ms possible)</td>\n<td class=\"tg-tleft-valign-first\">Low (with ANE/GPU)</td>\n<td class=\"tg-tleft-valign-first\">Medium (eager mode)</td>\n<td class=\"tg-tleft-valign-first\">Variable (highly EP dependent)</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on density)</td>\n<td class=\"tg-tleft-valign-first\">Low (for small LLMs)</td>\n<td class=\"tg-tleft-valign-first\">Low (under 10ms typical)</td>\n<td class=\"tg-tleft-valign-second\">Moderate to high</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Optimization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (kernel tuning, quantization, `float16`/`int8`)</td>\n<td class=\"tg-tleft-valign-first\">Yes (ANE targeting, quantization)</td>\n<td class=\"tg-tleft-valign-first\">No built-in, manual scripting</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, pruning, graph fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (operator pruning, bytecode fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (3D-aware compression and fusions)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantized GGUF)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, fusion)</td>\n<td class=\"tg-tleft-valign-second\">Yes (batching, threading)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Runtime Size</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–15 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–10 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Large (5–30 MB)</td>\n<td class=\"tg-tleft-valign-first\">Very small (&lt;1 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium–Large</td>\n<td class=\"tg-tleft-valign-first\">Small–Medium</td>\n<td class=\"tg-tleft-valign-first\">Small (~0.5–5 MB)</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint (Inference)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate (GPU memory bound)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate</td>\n<td class=\"tg-tleft-valign-first\">Moderate (GPU-heavy)</td>\n<td class=\"tg-tleft-valign-first\">Variable (depends on EPs)</td>\n<td class=\"tg-tleft-valign-first\">Ultra-low (sub-MB possible)</td>\n<td class=\"tg-tleft-valign-first\">High (large point cloud buffers)</td>\n<td class=\"tg-tleft-valign-first\">Low (~3–6 GB RAM for 7B models)</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Very low (sub-ms possible)</td>\n<td class=\"tg-tleft-valign-first\">Low (with ANE/GPU)</td>\n<td class=\"tg-tleft-valign-first\">Medium (eager mode)</td>\n<td class=\"tg-tleft-valign-first\">Variable (highly EP dependent)</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on density)</td>\n<td class=\"tg-tleft-valign-first\">Low (for small LLMs)</td>\n<td class=\"tg-tleft-valign-first\">Low (under 10ms typical)</td>\n<td class=\"tg-tleft-valign-second\">Moderate to high</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"flexibility-debugging-and-ecosystem\">Flexibility, Debugging, and Ecosystem</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Custom Ops Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin library API)</td>\n<td class=\"tg-tleft-valign-first\">Limited (via <code>MLCustomModel</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full (via Python subclassing)</td>\n<td class=\"tg-tleft-valign-first\">Yes (custom EPs and ops)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++ op authoring)</td>\n<td class=\"tg-tleft-valign-first\">Yes (often required)</td>\n<td class=\"tg-tleft-valign-first\">No (fixed transformer kernel set)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++/C custom kernels)</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Community &amp; Documentation</td>\n<td class=\"tg-tleft-valign-first\">Strong NVIDIA developer support, active forums</td>\n<td class=\"tg-tleft-valign-first\">Strong, Apple developer-centric</td>\n<td class=\"tg-tleft-valign-first\">Niche, growing</td>\n<td class=\"tg-tleft-valign-first\">Very strong</td>\n<td class=\"tg-tleft-valign-first\">Growing (Meta-sponsored)</td>\n<td class=\"tg-tleft-valign-first\">Limited / hardware-vendor specific</td>\n<td class=\"tg-tleft-valign-first\">Active open-source base</td>\n<td class=\"tg-tleft-valign-first\">Mature, large community</td>\n<td class=\"tg-tleft-valign-second\">Very mature in production</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Debugger Support</td>\n<td class=\"tg-tleft-valign-first\">Nsight Systems, profiling tools, verbose logging</td>\n<td class=\"tg-tleft-valign-first\">Xcode tools</td>\n<td class=\"tg-tleft-valign-first\">Python debug console</td>\n<td class=\"tg-tleft-valign-first\">Moderate (model inspection tools)</td>\n<td class=\"tg-tleft-valign-first\">Minimal (CLI, log-based)</td>\n<td class=\"tg-tleft-valign-first\">Custom tooling per device</td>\n<td class=\"tg-tleft-valign-first\">Log-level output only</td>\n<td class=\"tg-tleft-valign-first\">TensorBoard-lite, CLI tools</td>\n<td class=\"tg-tleft-valign-second\">Monitoring via Prometheus, etc.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ease of Use</td>\n<td class=\"tg-tleft-valign-first\">Medium (manual optimization, engine building)</td>\n<td class=\"tg-tleft-valign-first\">High for Apple developers</td>\n<td class=\"tg-tleft-valign-first\">Medium (researchers, tinkerers)</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on EP)</td>\n<td class=\"tg-tleft-valign-first\">Medium (steep setup curve)</td>\n<td class=\"tg-tleft-valign-first\">Low (requires system integration)</td>\n<td class=\"tg-tleft-valign-first\">High (once model is quantized)</td>\n<td class=\"tg-tleft-valign-first\">High (especially with <code>model maker</code>)</td>\n<td class=\"tg-tleft-valign-second\">Medium to high (requires infra)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Custom Ops Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin library API)</td>\n<td class=\"tg-tleft-valign-first\">Limited (via <code>MLCustomModel</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full (via Python subclassing)</td>\n<td class=\"tg-tleft-valign-first\">Yes (custom EPs and ops)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++ op authoring)</td>\n<td class=\"tg-tleft-valign-first\">Yes (often required)</td>\n<td class=\"tg-tleft-valign-first\">No (fixed transformer kernel set)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++/C custom kernels)</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Community &amp; Documentation</td>\n<td class=\"tg-tleft-valign-first\">Strong NVIDIA developer support, active forums</td>\n<td class=\"tg-tleft-valign-first\">Strong, Apple developer-centric</td>\n<td class=\"tg-tleft-valign-first\">Niche, growing</td>\n<td class=\"tg-tleft-valign-first\">Very strong</td>\n<td class=\"tg-tleft-valign-first\">Growing (Meta-sponsored)</td>\n<td class=\"tg-tleft-valign-first\">Limited / hardware-vendor specific</td>\n<td class=\"tg-tleft-valign-first\">Active open-source base</td>\n<td class=\"tg-tleft-valign-first\">Mature, large community</td>\n<td class=\"tg-tleft-valign-second\">Very mature in production</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Debugger Support</td>\n<td class=\"tg-tleft-valign-first\">Nsight Systems, profiling tools, verbose logging</td>\n<td class=\"tg-tleft-valign-first\">Xcode tools</td>\n<td class=\"tg-tleft-valign-first\">Python debug console</td>\n<td class=\"tg-tleft-valign-first\">Moderate (model inspection tools)</td>\n<td class=\"tg-tleft-valign-first\">Minimal (CLI, log-based)</td>\n<td class=\"tg-tleft-valign-first\">Custom tooling per device</td>\n<td class=\"tg-tleft-valign-first\">Log-level output only</td>\n<td class=\"tg-tleft-valign-first\">TensorBoard-lite, CLI tools</td>\n<td class=\"tg-tleft-valign-second\">Monitoring via Prometheus, etc.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ease of Use</td>\n<td class=\"tg-tleft-valign-first\">Medium (manual optimization, engine building)</td>\n<td class=\"tg-tleft-valign-first\">High for Apple developers</td>\n<td class=\"tg-tleft-valign-first\">Medium (researchers, tinkerers)</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on EP)</td>\n<td class=\"tg-tleft-valign-first\">Medium (steep setup curve)</td>\n<td class=\"tg-tleft-valign-first\">Low (requires system integration)</td>\n<td class=\"tg-tleft-valign-first\">High (once model is quantized)</td>\n<td class=\"tg-tleft-valign-first\">High (especially with <code>model maker</code>)</td>\n<td class=\"tg-tleft-valign-second\">Medium to high (requires infra)</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "*   Here are detailed tabular comparisons that encapsulates all key aspects across the different on-device ML runtimes discussed in the primer.\n\n#### General Characteristics\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Platform(s)\n\nNVIDIA Jetson, Desktop, Server\n\nApple devices (iOS/macOS)\n\nApple Silicon (macOS only)\n\nCross-platform\n\nEmbedded, mobile, MCU\n\nRobotics, automotive, ADAS\n\nDesktop, mobile, browser\n\nCross-platform (mobile/edge)\n\nCloud / server environments\n\nML Task Focus\n\nOptimized inference\n\nGeneral ML (vision, NLP)\n\nResearch, transformer/NLP\n\nGeneral ML\n\nUltra-light inference\n\n3D spatial perception\n\nLarge language model inference\n\nGeneral ML\n\nScalable inference serving\n\nInference Only?\n\nYes\n\nYes\n\nNo (supports training)\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nOpen Source?\n\nPartially (binaries open, tools closed)\n\nPartially (via tools)\n\nYes\n\nYes\n\nYes\n\nPartially / variable\n\nYes\n\nYes\n\nYes\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Platform(s)\n\nNVIDIA Jetson, Desktop, Server\n\nApple devices (iOS/macOS)\n\nApple Silicon (macOS only)\n\nCross-platform\n\nEmbedded, mobile, MCU\n\nRobotics, automotive, ADAS\n\nDesktop, mobile, browser\n\nCross-platform (mobile/edge)\n\nCloud / server environments\n\nML Task Focus\n\nOptimized inference\n\nGeneral ML (vision, NLP)\n\nResearch, transformer/NLP\n\nGeneral ML\n\nUltra-light inference\n\n3D spatial perception\n\nLarge language model inference\n\nGeneral ML\n\nScalable inference serving\n\nInference Only?\n\nYes\n\nYes\n\nNo (supports training)\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nOpen Source?\n\nPartially (binaries open, tools closed)\n\nPartially (via tools)\n\nYes\n\nYes\n\nYes\n\nPartially / variable\n\nYes\n\nYes\n\nYes\n\n#### Model Formats and Conversion\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nPrimary Format\n\n.plan (TensorRT engine file)\n\n.mlmodelc\n\nPython-defined layers\n\n.onnx\n\n.ptc (compiled TorchScript)\n\nCustom / converted .onnx / raw tensors\n\n.gguf (quantized LLMs)\n\n.tflite (FlatBuffer)\n\nSavedModel (.pb, .pbtxt)\n\nSupported Frameworks\n\nPyTorch, ONNX\n\nPyTorch, TF (via converters)\n\nNative Python API\n\nPyTorch, TensorFlow, others\n\nPyTorch (TorchScript subset)\n\nPyTorch, TensorFlow (via export)\n\nLLaMA-family only\n\nTensorFlow, Keras\n\nTensorFlow only\n\nConversion Required?\n\nYes (from ONNX or PyTorch export)\n\nYes (via coremltools)\n\nNo\n\nYes (usually from PyTorch)\n\nYes (via AOT compiler)\n\nYes, often includes preprocessing\n\nYes (convert + quantize)\n\nYes (TFLiteConverter)\n\nNo (already in target format)\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nPrimary Format\n\n.plan (TensorRT engine file)\n\n.mlmodelc\n\nPython-defined layers\n\n.onnx\n\n.ptc (compiled TorchScript)\n\nCustom / converted .onnx / raw tensors\n\n.gguf (quantized LLMs)\n\n.tflite (FlatBuffer)\n\nSavedModel (.pb, .pbtxt)\n\nSupported Frameworks\n\nPyTorch, ONNX\n\nPyTorch, TF (via converters)\n\nNative Python API\n\nPyTorch, TensorFlow, others\n\nPyTorch (TorchScript subset)\n\nPyTorch, TensorFlow (via export)\n\nLLaMA-family only\n\nTensorFlow, Keras\n\nTensorFlow only\n\nConversion Required?\n\nYes (from ONNX or PyTorch export)\n\nYes (via coremltools)\n\nNo\n\nYes (usually from PyTorch)\n\nYes (via AOT compiler)\n\nYes, often includes preprocessing\n\nYes (convert + quantize)\n\nYes (TFLiteConverter)\n\nNo (already in target format)\n\n#### Execution Model and Hardware Support\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nExecution Type\n\nAOT compiled CUDA graph\n\nEager, dynamic hardware assignment\n\nEager + compiled graph\n\nStatic graph with runtime optimizations\n\nBytecode VM interpreter\n\nSparse 3D graph + temporal flow\n\nManual loop over transformer layers\n\nStatic interpreter + delegates\n\nREST/gRPC inference pipeline\n\nCPU Support\n\nNo (GPU only)\n\nYes (fallback)\n\nYes (M1/M2 optimized)\n\nYes (default EP)\n\nYes\n\nYes\n\nYes (highly optimized)\n\nYes\n\nYes\n\nGPU Support\n\nYes (CUDA, Tensor Cores)\n\nYes (Metal)\n\nYes (via MPS)\n\nYes (CUDA, DirectML, etc.)\n\nLimited\n\nYes (CUDA, embedded GPUs)\n\nOptional (Metal, CUDA, OpenCL)\n\nYes (OpenCL, Metal)\n\nNo\n\nNPU / DSP Support\n\nNo\n\nYes (Apple ANE)\n\nEmerging ANE support\n\nYes (via NNAPI, OpenVINO, etc.)\n\nPotential via backend interface\n\nYes (TI, Nvidia, ADAS accelerators)\n\nNo (LLM-focused, CPU-oriented)\n\nYes (NNAPI, EdgeTPU, Hexagon)\n\nNo\n\nHardware Abstraction\n\nLow-level plugin engine, manual tuning\n\nAutomatic\n\nManual tuning via MLX\n\nModular Execution Providers (EPs)\n\nCompiled dispatcher with targets\n\nDevice-specific optimization required\n\nLow-level SIMD/CUDA offload\n\nDelegate-based (pluggable)\n\nN/A\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nExecution Type\n\nAOT compiled CUDA graph\n\nEager, dynamic hardware assignment\n\nEager + compiled graph\n\nStatic graph with runtime optimizations\n\nBytecode VM interpreter\n\nSparse 3D graph + temporal flow\n\nManual loop over transformer layers\n\nStatic interpreter + delegates\n\nREST/gRPC inference pipeline\n\nCPU Support\n\nNo (GPU only)\n\nYes (fallback)\n\nYes (M1/M2 optimized)\n\nYes (default EP)\n\nYes\n\nYes\n\nYes (highly optimized)\n\nYes\n\nYes\n\nGPU Support\n\nYes (CUDA, Tensor Cores)\n\nYes (Metal)\n\nYes (via MPS)\n\nYes (CUDA, DirectML, etc.)\n\nLimited\n\nYes (CUDA, embedded GPUs)\n\nOptional (Metal, CUDA, OpenCL)\n\nYes (OpenCL, Metal)\n\nNo\n\nNPU / DSP Support\n\nNo\n\nYes (Apple ANE)\n\nEmerging ANE support\n\nYes (via NNAPI, OpenVINO, etc.)\n\nPotential via backend interface\n\nYes (TI, Nvidia, ADAS accelerators)\n\nNo (LLM-focused, CPU-oriented)\n\nYes (NNAPI, EdgeTPU, Hexagon)\n\nNo\n\nHardware Abstraction\n\nLow-level plugin engine, manual tuning\n\nAutomatic\n\nManual tuning via MLX\n\nModular Execution Providers (EPs)\n\nCompiled dispatcher with targets\n\nDevice-specific optimization required\n\nLow-level SIMD/CUDA offload\n\nDelegate-based (pluggable)\n\nN/A\n\n#### Optimization, Size, and Constraints\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nModel Optimization Support\n\nYes (kernel tuning, quantization, \\`float16\\`/\\`int8\\`)\n\nYes (ANE targeting, quantization)\n\nNo built-in, manual scripting\n\nYes (quantization, pruning, graph fusion)\n\nYes (operator pruning, bytecode fusion)\n\nYes (3D-aware compression and fusions)\n\nYes (quantized GGUF)\n\nYes (quantization, fusion)\n\nYes (batching, threading)\n\nRuntime Size\n\nMedium (~5–15 MB)\n\nMedium (~5–10 MB)\n\nMedium\n\nLarge (5–30 MB)\n\nVery small (<1 MB)\n\nMedium–Large\n\nSmall–Medium\n\nSmall (~0.5–5 MB)\n\nVery large (>100 MB)\n\nMemory Footprint (Inference)\n\nLow to moderate (GPU memory bound)\n\nLow to moderate\n\nModerate (GPU-heavy)\n\nVariable (depends on EPs)\n\nUltra-low (sub-MB possible)\n\nHigh (large point cloud buffers)\n\nLow (~3–6 GB RAM for 7B models)\n\nLow\n\nHigh\n\nLatency\n\nVery low (sub-ms possible)\n\nLow (with ANE/GPU)\n\nMedium (eager mode)\n\nVariable (highly EP dependent)\n\nVery low\n\nModerate to high (depends on density)\n\nLow (for small LLMs)\n\nLow (under 10ms typical)\n\nModerate to high\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nModel Optimization Support\n\nYes (kernel tuning, quantization, \\`float16\\`/\\`int8\\`)\n\nYes (ANE targeting, quantization)\n\nNo built-in, manual scripting\n\nYes (quantization, pruning, graph fusion)\n\nYes (operator pruning, bytecode fusion)\n\nYes (3D-aware compression and fusions)\n\nYes (quantized GGUF)\n\nYes (quantization, fusion)\n\nYes (batching, threading)\n\nRuntime Size\n\nMedium (~5–15 MB)\n\nMedium (~5–10 MB)\n\nMedium\n\nLarge (5–30 MB)\n\nVery small (<1 MB)\n\nMedium–Large\n\nSmall–Medium\n\nSmall (~0.5–5 MB)\n\nVery large (>100 MB)\n\nMemory Footprint (Inference)\n\nLow to moderate (GPU memory bound)\n\nLow to moderate\n\nModerate (GPU-heavy)\n\nVariable (depends on EPs)\n\nUltra-low (sub-MB possible)\n\nHigh (large point cloud buffers)\n\nLow (~3–6 GB RAM for 7B models)\n\nLow\n\nHigh\n\nLatency\n\nVery low (sub-ms possible)\n\nLow (with ANE/GPU)\n\nMedium (eager mode)\n\nVariable (highly EP dependent)\n\nVery low\n\nModerate to high (depends on density)\n\nLow (for small LLMs)\n\nLow (under 10ms typical)\n\nModerate to high\n\n#### Flexibility, Debugging, and Ecosystem\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nCustom Ops Support\n\nYes (via plugin library API)\n\nLimited (via `MLCustomModel`)\n\nFull (via Python subclassing)\n\nYes (custom EPs and ops)\n\nYes (C++ op authoring)\n\nYes (often required)\n\nNo (fixed transformer kernel set)\n\nYes (C++/C custom kernels)\n\nYes\n\nCommunity & Documentation\n\nStrong NVIDIA developer support, active forums\n\nStrong, Apple developer-centric\n\nNiche, growing\n\nVery strong\n\nGrowing (Meta-sponsored)\n\nLimited / hardware-vendor specific\n\nActive open-source base\n\nMature, large community\n\nVery mature in production\n\nDebugger Support\n\nNsight Systems, profiling tools, verbose logging\n\nXcode tools\n\nPython debug console\n\nModerate (model inspection tools)\n\nMinimal (CLI, log-based)\n\nCustom tooling per device\n\nLog-level output only\n\nTensorBoard-lite, CLI tools\n\nMonitoring via Prometheus, etc.\n\nEase of Use\n\nMedium (manual optimization, engine building)\n\nHigh for Apple developers\n\nMedium (researchers, tinkerers)\n\nModerate to high (depends on EP)\n\nMedium (steep setup curve)\n\nLow (requires system integration)\n\nHigh (once model is quantized)\n\nHigh (especially with `model maker`)\n\nMedium to high (requires infra)\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nCustom Ops Support\n\nYes (via plugin library API)\n\nLimited (via `MLCustomModel`)\n\nFull (via Python subclassing)\n\nYes (custom EPs and ops)\n\nYes (C++ op authoring)\n\nYes (often required)\n\nNo (fixed transformer kernel set)\n\nYes (C++/C custom kernels)\n\nYes\n\nCommunity & Documentation\n\nStrong NVIDIA developer support, active forums\n\nStrong, Apple developer-centric\n\nNiche, growing\n\nVery strong\n\nGrowing (Meta-sponsored)\n\nLimited / hardware-vendor specific\n\nActive open-source base\n\nMature, large community\n\nVery mature in production\n\nDebugger Support\n\nNsight Systems, profiling tools, verbose logging\n\nXcode tools\n\nPython debug console\n\nModerate (model inspection tools)\n\nMinimal (CLI, log-based)\n\nCustom tooling per device\n\nLog-level output only\n\nTensorBoard-lite, CLI tools\n\nMonitoring via Prometheus, etc.\n\nEase of Use\n\nMedium (manual optimization, engine building)\n\nHigh for Apple developers\n\nMedium (researchers, tinkerers)\n\nModerate to high (depends on EP)\n\nMedium (steep setup curve)\n\nLow (requires system integration)\n\nHigh (once model is quantized)\n\nHigh (especially with `model maker`)\n\nMedium to high (requires infra)",
    "contentLength": 33435,
    "wordCount": 1403,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis"
  },
  {
    "id": "ai-ml-runtimes-comparative-summary-and-guidance-53",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Comparative Summary and Guidance",
    "order": 53,
    "orderInChapter": 9,
    "contentHtml": "<h4 id=\"feature-comparison-table\">Feature Comparison Table</h4>\n<ul>\n  <li>This section provides a side-by-side comparison of the on-device ML runtimes discussed, highlighting their architectural differences, platform support, performance characteristics, and ideal use cases. This helps clarify which runtime best fits various project needs, from embedded development to mobile apps and language model inference.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform Support</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Model Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Hardware Acceleration</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Optimized For</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Custom Ops</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Size Footprint</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA GPUs (desktop, Jetson, server)</td>\n<td class=\"tg-tleft-valign-first\">ONNX, `.plan` (engine file)</td>\n<td class=\"tg-tleft-valign-first\">CUDA, Tensor Cores</td>\n<td class=\"tg-tleft-valign-first\">Low-latency GPU inference</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin system)</td>\n<td class=\"tg-tleft-valign-second\">Medium (~5–15 MB)</td>\n</tr>    \n<tr>\n<td class=\"tg-tleft-valign-first\">Core ML</td>\n<td class=\"tg-tleft-valign-first\">Apple only (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">`.mlmodelc`</td>\n<td class=\"tg-tleft-valign-first\">CPU, GPU, ANE</td>\n<td class=\"tg-tleft-valign-first\">App integration on Apple devices</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–10 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MLX</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS)</td>\n<td class=\"tg-tleft-valign-first\">Python code</td>\n<td class=\"tg-tleft-valign-first\">MPS, ANE (partial)</td>\n<td class=\"tg-tleft-valign-first\">Research &amp; experimentation</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ONNX Runtime</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (Mobile &amp; Desktop)</td>\n<td class=\"tg-tleft-valign-first\">`.onnx`</td>\n<td class=\"tg-tleft-valign-first\">CUDA, NNAPI, DirectML, etc.</td>\n<td class=\"tg-tleft-valign-first\">Cross-framework interoperability</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Large (~5–30 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Embedded, MCUs, Android</td>\n<td class=\"tg-tleft-valign-first\">Compiled TorchScript (`.ptc`)</td>\n<td class=\"tg-tleft-valign-first\">CPU, MCU, DSP</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light edge inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very small (&lt;1 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LidarTLM</td>\n<td class=\"tg-tleft-valign-first\">Embedded/Robotics</td>\n<td class=\"tg-tleft-valign-first\">Custom/ONNX</td>\n<td class=\"tg-tleft-valign-first\">CUDA, DSP, NPU</td>\n<td class=\"tg-tleft-valign-first\">Sparse point cloud inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium–Large</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Desktop, Mobile, WASM</td>\n<td class=\"tg-tleft-valign-first\">Quantized GGUF</td>\n<td class=\"tg-tleft-valign-first\">CPU, Optional GPU</td>\n<td class=\"tg-tleft-valign-first\">Efficient LLM inference</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Small–Medium (CPU)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TFLite</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (MCU to mobile)</td>\n<td class=\"tg-tleft-valign-first\">`.tflite`</td>\n<td class=\"tg-tleft-valign-first\">NNAPI, GPU, DSP, EdgeTPU</td>\n<td class=\"tg-tleft-valign-first\">Mobile and embedded AI</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Small (~500 KB–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TF Serving</td>\n<td class=\"tg-tleft-valign-first\">Cloud/Server</td>\n<td class=\"tg-tleft-valign-first\">SavedModel</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Scalable online inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform Support</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Model Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Hardware Acceleration</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Optimized For</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Custom Ops</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Size Footprint</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA GPUs (desktop, Jetson, server)</td>\n<td class=\"tg-tleft-valign-first\">ONNX, `.plan` (engine file)</td>\n<td class=\"tg-tleft-valign-first\">CUDA, Tensor Cores</td>\n<td class=\"tg-tleft-valign-first\">Low-latency GPU inference</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin system)</td>\n<td class=\"tg-tleft-valign-second\">Medium (~5–15 MB)</td>\n</tr>    \n<tr>\n<td class=\"tg-tleft-valign-first\">Core ML</td>\n<td class=\"tg-tleft-valign-first\">Apple only (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">`.mlmodelc`</td>\n<td class=\"tg-tleft-valign-first\">CPU, GPU, ANE</td>\n<td class=\"tg-tleft-valign-first\">App integration on Apple devices</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–10 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MLX</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS)</td>\n<td class=\"tg-tleft-valign-first\">Python code</td>\n<td class=\"tg-tleft-valign-first\">MPS, ANE (partial)</td>\n<td class=\"tg-tleft-valign-first\">Research &amp; experimentation</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ONNX Runtime</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (Mobile &amp; Desktop)</td>\n<td class=\"tg-tleft-valign-first\">`.onnx`</td>\n<td class=\"tg-tleft-valign-first\">CUDA, NNAPI, DirectML, etc.</td>\n<td class=\"tg-tleft-valign-first\">Cross-framework interoperability</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Large (~5–30 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Embedded, MCUs, Android</td>\n<td class=\"tg-tleft-valign-first\">Compiled TorchScript (`.ptc`)</td>\n<td class=\"tg-tleft-valign-first\">CPU, MCU, DSP</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light edge inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very small (&lt;1 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LidarTLM</td>\n<td class=\"tg-tleft-valign-first\">Embedded/Robotics</td>\n<td class=\"tg-tleft-valign-first\">Custom/ONNX</td>\n<td class=\"tg-tleft-valign-first\">CUDA, DSP, NPU</td>\n<td class=\"tg-tleft-valign-first\">Sparse point cloud inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium–Large</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Desktop, Mobile, WASM</td>\n<td class=\"tg-tleft-valign-first\">Quantized GGUF</td>\n<td class=\"tg-tleft-valign-first\">CPU, Optional GPU</td>\n<td class=\"tg-tleft-valign-first\">Efficient LLM inference</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Small–Medium (CPU)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TFLite</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (MCU to mobile)</td>\n<td class=\"tg-tleft-valign-first\">`.tflite`</td>\n<td class=\"tg-tleft-valign-first\">NNAPI, GPU, DSP, EdgeTPU</td>\n<td class=\"tg-tleft-valign-first\">Mobile and embedded AI</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Small (~500 KB–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TF Serving</td>\n<td class=\"tg-tleft-valign-first\">Cloud/Server</td>\n<td class=\"tg-tleft-valign-first\">SavedModel</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Scalable online inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"strengths-by-runtime\">Strengths by Runtime</h4>\n<ul>\n  <li>\n    <p><strong>Core ML</strong>: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.</p>\n  </li>\n  <li>\n    <p><strong>MLX</strong>: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.</p>\n  </li>\n  <li>\n    <p><strong>ONNX Runtime</strong>: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.</p>\n  </li>\n  <li>\n    <p><strong>ExecuTorch</strong>: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.</p>\n  </li>\n  <li>\n    <p><strong>LidarTLM</strong>: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.</p>\n  </li>\n  <li>\n    <p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.</p>\n  </li>\n  <li>\n    <p><strong>TFLite</strong>: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.</p>\n  </li>\n  <li>\n    <p><strong>TF Serving</strong>: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.</p>\n  </li>\n</ul>\n<p><strong>Core ML</strong>: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.</p>\n<p><strong>MLX</strong>: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.</p>\n<p><strong>ONNX Runtime</strong>: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.</p>\n<p><strong>ExecuTorch</strong>: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.</p>\n<p><strong>LidarTLM</strong>: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.</p>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.</p>\n<p><strong>TFLite</strong>: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.</p>\n<p><strong>TF Serving</strong>: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.</p>",
    "contentMarkdown": "#### Feature Comparison Table\n\n*   This section provides a side-by-side comparison of the on-device ML runtimes discussed, highlighting their architectural differences, platform support, performance characteristics, and ideal use cases. This helps clarify which runtime best fits various project needs, from embedded development to mobile apps and language model inference.\n\n**Runtime**\n\n**Platform Support**\n\n**Model Format**\n\n**Hardware Acceleration**\n\n**Optimized For**\n\n**Custom Ops**\n\n**Size Footprint**\n\nTensorRT\n\nNVIDIA GPUs (desktop, Jetson, server)\n\nONNX, \\`.plan\\` (engine file)\n\nCUDA, Tensor Cores\n\nLow-latency GPU inference\n\nYes (via plugin system)\n\nMedium (~5–15 MB)\n\nCore ML\n\nApple only (iOS/macOS)\n\n\\`.mlmodelc\\`\n\nCPU, GPU, ANE\n\nApp integration on Apple devices\n\nLimited\n\nMedium (~2–10 MB)\n\nMLX\n\nApple Silicon (macOS)\n\nPython code\n\nMPS, ANE (partial)\n\nResearch & experimentation\n\nYes\n\nMedium (~2–5 MB)\n\nONNX Runtime\n\nCross-platform (Mobile & Desktop)\n\n\\`.onnx\\`\n\nCUDA, NNAPI, DirectML, etc.\n\nCross-framework interoperability\n\nYes\n\nLarge (~5–30 MB)\n\nExecuTorch\n\nEmbedded, MCUs, Android\n\nCompiled TorchScript (\\`.ptc\\`)\n\nCPU, MCU, DSP\n\nUltra-light edge inference\n\nYes\n\nVery small (<1 MB)\n\nLidarTLM\n\nEmbedded/Robotics\n\nCustom/ONNX\n\nCUDA, DSP, NPU\n\nSparse point cloud inference\n\nYes\n\nMedium–Large\n\n`llama.cpp`\n\nDesktop, Mobile, WASM\n\nQuantized GGUF\n\nCPU, Optional GPU\n\nEfficient LLM inference\n\nLimited\n\nSmall–Medium (CPU)\n\nTFLite\n\nCross-platform (MCU to mobile)\n\n\\`.tflite\\`\n\nNNAPI, GPU, DSP, EdgeTPU\n\nMobile and embedded AI\n\nYes\n\nSmall (~500 KB–5 MB)\n\nTF Serving\n\nCloud/Server\n\nSavedModel\n\nN/A\n\nScalable online inference\n\nYes\n\nVery large (>100 MB)\n\n**Runtime**\n\n**Platform Support**\n\n**Model Format**\n\n**Hardware Acceleration**\n\n**Optimized For**\n\n**Custom Ops**\n\n**Size Footprint**\n\nTensorRT\n\nNVIDIA GPUs (desktop, Jetson, server)\n\nONNX, \\`.plan\\` (engine file)\n\nCUDA, Tensor Cores\n\nLow-latency GPU inference\n\nYes (via plugin system)\n\nMedium (~5–15 MB)\n\nCore ML\n\nApple only (iOS/macOS)\n\n\\`.mlmodelc\\`\n\nCPU, GPU, ANE\n\nApp integration on Apple devices\n\nLimited\n\nMedium (~2–10 MB)\n\nMLX\n\nApple Silicon (macOS)\n\nPython code\n\nMPS, ANE (partial)\n\nResearch & experimentation\n\nYes\n\nMedium (~2–5 MB)\n\nONNX Runtime\n\nCross-platform (Mobile & Desktop)\n\n\\`.onnx\\`\n\nCUDA, NNAPI, DirectML, etc.\n\nCross-framework interoperability\n\nYes\n\nLarge (~5–30 MB)\n\nExecuTorch\n\nEmbedded, MCUs, Android\n\nCompiled TorchScript (\\`.ptc\\`)\n\nCPU, MCU, DSP\n\nUltra-light edge inference\n\nYes\n\nVery small (<1 MB)\n\nLidarTLM\n\nEmbedded/Robotics\n\nCustom/ONNX\n\nCUDA, DSP, NPU\n\nSparse point cloud inference\n\nYes\n\nMedium–Large\n\n`llama.cpp`\n\nDesktop, Mobile, WASM\n\nQuantized GGUF\n\nCPU, Optional GPU\n\nEfficient LLM inference\n\nLimited\n\nSmall–Medium (CPU)\n\nTFLite\n\nCross-platform (MCU to mobile)\n\n\\`.tflite\\`\n\nNNAPI, GPU, DSP, EdgeTPU\n\nMobile and embedded AI\n\nYes\n\nSmall (~500 KB–5 MB)\n\nTF Serving\n\nCloud/Server\n\nSavedModel\n\nN/A\n\nScalable online inference\n\nYes\n\nVery large (>100 MB)\n\n#### Strengths by Runtime\n\n*   **Core ML**: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.\n    \n*   **MLX**: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.\n    \n*   **ONNX Runtime**: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.\n    \n*   **ExecuTorch**: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.\n    \n*   **LidarTLM**: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.\n    \n*   **`llama.cpp`**: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.\n    \n*   **TFLite**: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.\n    \n*   **TF Serving**: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.\n    \n\n**Core ML**: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.\n\n**MLX**: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.\n\n**ONNX Runtime**: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.\n\n**ExecuTorch**: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.\n\n**LidarTLM**: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.\n\n**`llama.cpp`**: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.\n\n**TFLite**: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.\n\n**TF Serving**: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.",
    "contentLength": 11989,
    "wordCount": 713,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#comparative-summary-and-guidance"
  },
  {
    "id": "ai-ml-runtimes-runtime-selection-guidance-54",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Runtime Selection Guidance",
    "order": 54,
    "orderInChapter": 10,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>If you’re deploying to iOS or macOS</strong>:</p>\n\n    <ul>\n      <li>Use <strong>Core ML</strong> for production apps.</li>\n      <li>Use <strong>MLX</strong> for research, local experimentation, or custom modeling.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re deploying to embedded edge devices</strong>:</p>\n\n    <ul>\n      <li>Use <strong>ExecuTorch</strong> for PyTorch-based workflows.</li>\n      <li>Use <strong>TensorFlow Lite for Microcontrollers</strong> for tight memory constraints.</li>\n      <li>Consider <strong>LidarTLM</strong>-style tools if dealing with 3D spatial data.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re targeting Android or need portability</strong>:</p>\n\n    <ul>\n      <li>Use <strong>TensorFlow Lite</strong> or <strong>ONNX Runtime</strong> with delegates like NNAPI or GPU.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re working with LLMs locally</strong>:</p>\n\n    <ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong> for best CPU-based inference and minimal setup.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you want cross-framework model portability</strong>:</p>\n\n    <ul>\n      <li>Use <strong>ONNX Runtime</strong> with models exported from PyTorch, TensorFlow, or others.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you require real-time, high-volume cloud inference</strong>:</p>\n\n    <ul>\n      <li>Use <strong>TensorFlow Serving</strong> or ONNX Runtime Server.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>If you’re deploying to iOS or macOS</strong>:</p>\n<ul>\n      <li>Use <strong>Core ML</strong> for production apps.</li>\n      <li>Use <strong>MLX</strong> for research, local experimentation, or custom modeling.</li>\n    </ul>\n<p><strong>If you’re deploying to embedded edge devices</strong>:</p>\n<ul>\n      <li>Use <strong>ExecuTorch</strong> for PyTorch-based workflows.</li>\n      <li>Use <strong>TensorFlow Lite for Microcontrollers</strong> for tight memory constraints.</li>\n      <li>Consider <strong>LidarTLM</strong>-style tools if dealing with 3D spatial data.</li>\n    </ul>\n<p><strong>If you’re targeting Android or need portability</strong>:</p>\n<ul>\n      <li>Use <strong>TensorFlow Lite</strong> or <strong>ONNX Runtime</strong> with delegates like NNAPI or GPU.</li>\n    </ul>\n<p><strong>If you’re working with LLMs locally</strong>:</p>\n<ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong> for best CPU-based inference and minimal setup.</li>\n    </ul>\n<p><strong>If you want cross-framework model portability</strong>:</p>\n<ul>\n      <li>Use <strong>ONNX Runtime</strong> with models exported from PyTorch, TensorFlow, or others.</li>\n    </ul>\n<p><strong>If you require real-time, high-volume cloud inference</strong>:</p>\n<ul>\n      <li>Use <strong>TensorFlow Serving</strong> or ONNX Runtime Server.</li>\n    </ul>",
    "contentMarkdown": "*   **If you’re deploying to iOS or macOS**:\n    \n    *   Use **Core ML** for production apps.\n    *   Use **MLX** for research, local experimentation, or custom modeling.\n*   **If you’re deploying to embedded edge devices**:\n    \n    *   Use **ExecuTorch** for PyTorch-based workflows.\n    *   Use **TensorFlow Lite for Microcontrollers** for tight memory constraints.\n    *   Consider **LidarTLM**\\-style tools if dealing with 3D spatial data.\n*   **If you’re targeting Android or need portability**:\n    \n    *   Use **TensorFlow Lite** or **ONNX Runtime** with delegates like NNAPI or GPU.\n*   **If you’re working with LLMs locally**:\n    \n    *   Use **`llama.cpp`** for best CPU-based inference and minimal setup.\n*   **If you want cross-framework model portability**:\n    \n    *   Use **ONNX Runtime** with models exported from PyTorch, TensorFlow, or others.\n*   **If you require real-time, high-volume cloud inference**:\n    \n    *   Use **TensorFlow Serving** or ONNX Runtime Server.\n\n**If you’re deploying to iOS or macOS**:\n\n*   Use **Core ML** for production apps.\n*   Use **MLX** for research, local experimentation, or custom modeling.\n\n**If you’re deploying to embedded edge devices**:\n\n*   Use **ExecuTorch** for PyTorch-based workflows.\n*   Use **TensorFlow Lite for Microcontrollers** for tight memory constraints.\n*   Consider **LidarTLM**\\-style tools if dealing with 3D spatial data.\n\n**If you’re targeting Android or need portability**:\n\n*   Use **TensorFlow Lite** or **ONNX Runtime** with delegates like NNAPI or GPU.\n\n**If you’re working with LLMs locally**:\n\n*   Use **`llama.cpp`** for best CPU-based inference and minimal setup.\n\n**If you want cross-framework model portability**:\n\n*   Use **ONNX Runtime** with models exported from PyTorch, TensorFlow, or others.\n\n**If you require real-time, high-volume cloud inference**:\n\n*   Use **TensorFlow Serving** or ONNX Runtime Server.",
    "contentLength": 2950,
    "wordCount": 258,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#runtime-selection-guidance"
  },
  {
    "id": "ai-ml-runtimes-final-thoughts-55",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
    "title": "Final Thoughts",
    "order": 55,
    "orderInChapter": 11,
    "contentHtml": "<ul>\n  <li>\n    <p>Choosing the right on-device ML runtime depends heavily on the following factors:</p>\n\n    <ul>\n      <li>Deployment environment (mobile, embedded, desktop, web, cloud)</li>\n      <li>Model architecture (CNN, RNN, transformer, etc.)</li>\n      <li>Performance requirements (latency, FPS, memory usage)</li>\n      <li>Development preferences (PyTorch, TensorFlow, raw C++, etc.)</li>\n      <li>Hardware capabilities (CPU, GPU, NPU, DSP, etc.)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:</p>\n\n    <ul>\n      <li><strong>Best for Apple-native app development</strong>: <em>Core ML</em></li>\n      <li><strong>Best for Apple-based model experimentation</strong>: <em>MLX</em></li>\n      <li><strong>Best for cross-platform portability and hardware access</strong>: <em>ONNX Runtime</em></li>\n      <li><strong>Best for minimal embedded inference</strong>: <em>ExecuTorch</em></li>\n      <li><strong>Best for 3D LiDAR/robotics</strong>: <em>LidarTLM-like stacks</em></li>\n      <li><strong>Best for on-device LLM inference</strong>: <em><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></em></li>\n      <li><strong>Best for mobile/embedded general ML</strong>: <em>TensorFlow Lite</em></li>\n      <li><strong>Best for scalable cloud inference</strong>: <em>TensorFlow Serving</em></li>\n    </ul>\n  </li>\n</ul>\n<p>Choosing the right on-device ML runtime depends heavily on the following factors:</p>\n<ul>\n      <li>Deployment environment (mobile, embedded, desktop, web, cloud)</li>\n      <li>Model architecture (CNN, RNN, transformer, etc.)</li>\n      <li>Performance requirements (latency, FPS, memory usage)</li>\n      <li>Development preferences (PyTorch, TensorFlow, raw C++, etc.)</li>\n      <li>Hardware capabilities (CPU, GPU, NPU, DSP, etc.)</li>\n    </ul>\n<p>Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:</p>\n<ul>\n      <li><strong>Best for Apple-native app development</strong>: <em>Core ML</em></li>\n      <li><strong>Best for Apple-based model experimentation</strong>: <em>MLX</em></li>\n      <li><strong>Best for cross-platform portability and hardware access</strong>: <em>ONNX Runtime</em></li>\n      <li><strong>Best for minimal embedded inference</strong>: <em>ExecuTorch</em></li>\n      <li><strong>Best for 3D LiDAR/robotics</strong>: <em>LidarTLM-like stacks</em></li>\n      <li><strong>Best for on-device LLM inference</strong>: <em><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></em></li>\n      <li><strong>Best for mobile/embedded general ML</strong>: <em>TensorFlow Lite</em></li>\n      <li><strong>Best for scalable cloud inference</strong>: <em>TensorFlow Serving</em></li>\n    </ul>",
    "contentMarkdown": "*   Choosing the right on-device ML runtime depends heavily on the following factors:\n    \n    *   Deployment environment (mobile, embedded, desktop, web, cloud)\n    *   Model architecture (CNN, RNN, transformer, etc.)\n    *   Performance requirements (latency, FPS, memory usage)\n    *   Development preferences (PyTorch, TensorFlow, raw C++, etc.)\n    *   Hardware capabilities (CPU, GPU, NPU, DSP, etc.)\n*   Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:\n    \n    *   **Best for Apple-native app development**: _Core ML_\n    *   **Best for Apple-based model experimentation**: _MLX_\n    *   **Best for cross-platform portability and hardware access**: _ONNX Runtime_\n    *   **Best for minimal embedded inference**: _ExecuTorch_\n    *   **Best for 3D LiDAR/robotics**: _LidarTLM-like stacks_\n    *   **Best for on-device LLM inference**: _`llama.cpp`_\n    *   **Best for mobile/embedded general ML**: _TensorFlow Lite_\n    *   **Best for scalable cloud inference**: _TensorFlow Serving_\n\nChoosing the right on-device ML runtime depends heavily on the following factors:\n\n*   Deployment environment (mobile, embedded, desktop, web, cloud)\n*   Model architecture (CNN, RNN, transformer, etc.)\n*   Performance requirements (latency, FPS, memory usage)\n*   Development preferences (PyTorch, TensorFlow, raw C++, etc.)\n*   Hardware capabilities (CPU, GPU, NPU, DSP, etc.)\n\nEach runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:\n\n*   **Best for Apple-native app development**: _Core ML_\n*   **Best for Apple-based model experimentation**: _MLX_\n*   **Best for cross-platform portability and hardware access**: _ONNX Runtime_\n*   **Best for minimal embedded inference**: _ExecuTorch_\n*   **Best for 3D LiDAR/robotics**: _LidarTLM-like stacks_\n*   **Best for on-device LLM inference**: _`llama.cpp`_\n*   **Best for mobile/embedded general ML**: _TensorFlow Lite_\n*   **Best for scalable cloud inference**: _TensorFlow Serving_",
    "contentLength": 3268,
    "wordCount": 326,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#final-thoughts"
  },
  {
    "id": "ai-ml-runtimes-protocol-buffers-protobuf-56",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: Serialization Formats Across Runtimes",
    "title": "Protocol Buffers (Protobuf)",
    "order": 56,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: TensorFlow (SavedModel, <code class=\"language-plaintext highlighter-rouge\">.pb</code>), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Google</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Binary serialization framework</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Encodes structured data using <code class=\"language-plaintext highlighter-rouge\">.proto</code> schemas</li>\n      <li>Supports code generation in multiple languages (Python, C++, Java, etc.)</li>\n      <li>Strict type definitions with schema versioning</li>\n      <li>Produces portable, efficient, extensible binary files</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Highly compact, faster than JSON/XML</li>\n      <li>Strong backward and forward compatibility through schema evolution</li>\n      <li>Ideal for representing complex hierarchical graphs (e.g., model computation trees)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li><strong>TensorFlow</strong>: Stores entire computation graph, tensor shapes, and metadata in <code class=\"language-plaintext highlighter-rouge\">.pb</code> (protobuf binary)</li>\n      <li><strong>ONNX</strong>: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Parsing requires full message decoding into memory</li>\n      <li>Less suited for minimal-footprint scenarios (e.g., MCUs)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n      </li>\n      <li>\n        <p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n      </li>\n      <li>\n        <p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n\n        <div class=\"language-protobuf highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Example Python Usage (ONNX-style):</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n\n        <ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: TensorFlow (SavedModel, <code class=\"language-plaintext highlighter-rouge\">.pb</code>), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</p>\n<p><strong>Developed by</strong>: Google</p>\n<p><strong>Type</strong>: Binary serialization framework</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Encodes structured data using <code class=\"language-plaintext highlighter-rouge\">.proto</code> schemas</li>\n      <li>Supports code generation in multiple languages (Python, C++, Java, etc.)</li>\n      <li>Strict type definitions with schema versioning</li>\n      <li>Produces portable, efficient, extensible binary files</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Highly compact, faster than JSON/XML</li>\n      <li>Strong backward and forward compatibility through schema evolution</li>\n      <li>Ideal for representing complex hierarchical graphs (e.g., model computation trees)</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li><strong>TensorFlow</strong>: Stores entire computation graph, tensor shapes, and metadata in <code class=\"language-plaintext highlighter-rouge\">.pb</code> (protobuf binary)</li>\n      <li><strong>ONNX</strong>: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Parsing requires full message decoding into memory</li>\n      <li>Less suited for minimal-footprint scenarios (e.g., MCUs)</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n      </li>\n      <li>\n        <p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n      </li>\n      <li>\n        <p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n\n        <div class=\"language-protobuf highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Example Python Usage (ONNX-style):</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n\n        <ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n<p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n<p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre>\n<p><strong>Example Python Usage (ONNX-style):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>",
    "contentMarkdown": "*   **Used by**: TensorFlow (SavedModel, `.pb`), ONNX (`.onnx`)\n    \n*   **Developed by**: Google\n    \n*   **Type**: Binary serialization framework\n    \n*   **Key Characteristics**:\n    \n    *   Encodes structured data using `.proto` schemas\n    *   Supports code generation in multiple languages (Python, C++, Java, etc.)\n    *   Strict type definitions with schema versioning\n    *   Produces portable, efficient, extensible binary files\n*   **Advantages**:\n    \n    *   Highly compact, faster than JSON/XML\n    *   Strong backward and forward compatibility through schema evolution\n    *   Ideal for representing complex hierarchical graphs (e.g., model computation trees)\n*   **In ML context**:\n    \n    *   **TensorFlow**: Stores entire computation graph, tensor shapes, and metadata in `.pb` (protobuf binary)\n    *   **ONNX**: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema\n*   **Limitations**:\n    \n    *   Parsing requires full message decoding into memory\n    *   Less suited for minimal-footprint scenarios (e.g., MCUs)\n*   **Example**:\n    \n    *   _Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n        \n    *   Protobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n        \n    *   **Schema Definition (`graph.proto`):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n        \n    *   **Example Python Usage (ONNX-style):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n        \n    *   **Serialized File:**\n        \n        *   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.\n\n**Used by**: TensorFlow (SavedModel, `.pb`), ONNX (`.onnx`)\n\n**Developed by**: Google\n\n**Type**: Binary serialization framework\n\n**Key Characteristics**:\n\n*   Encodes structured data using `.proto` schemas\n*   Supports code generation in multiple languages (Python, C++, Java, etc.)\n*   Strict type definitions with schema versioning\n*   Produces portable, efficient, extensible binary files\n\n**Advantages**:\n\n*   Highly compact, faster than JSON/XML\n*   Strong backward and forward compatibility through schema evolution\n*   Ideal for representing complex hierarchical graphs (e.g., model computation trees)\n\n**In ML context**:\n\n*   **TensorFlow**: Stores entire computation graph, tensor shapes, and metadata in `.pb` (protobuf binary)\n*   **ONNX**: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema\n\n**Limitations**:\n\n*   Parsing requires full message decoding into memory\n*   Less suited for minimal-footprint scenarios (e.g., MCUs)\n\n**Example**:\n\n*   _Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n    \n*   Protobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n    \n*   **Schema Definition (`graph.proto`):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n    \n*   **Example Python Usage (ONNX-style):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n    \n*   **Serialized File:**\n    \n    *   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.\n\n_Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n\nProtobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n\n**Schema Definition (`graph.proto`):**\n\n![](https://aman.ai/images/copy.png)\n\n  `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n\n**Example Python Usage (ONNX-style):**\n\n![](https://aman.ai/images/copy.png)\n\n  `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n\n**Serialized File:**\n\n*   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.",
    "contentLength": 15303,
    "wordCount": 624,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#protocol-buffers-(protobuf)"
  },
  {
    "id": "ai-ml-runtimes-flatbuffer-57",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: Serialization Formats Across Runtimes",
    "title": "FlatBuffer",
    "order": 57,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Google</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Binary serialization library with zero-copy design</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Allows direct access to data without unpacking (zero-copy reads)</li>\n      <li>Compact binary representation optimized for low-latency parsing</li>\n      <li>Built-in schema evolution support</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Near-instantaneous loading—no deserialization overhead</li>\n      <li>Perfect for mobile/embedded devices with tight latency or startup constraints</li>\n      <li>Schema-aware tooling for validation</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">.tflite</code> files store computation graphs, tensors, and metadata using FlatBuffer encoding</li>\n      <li>Facilitates runtime interpretation without converting the graph into a different memory format</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Harder to inspect/debug than JSON or Protobuf</li>\n      <li>Limited dynamic structure capabilities compared to Protobuf</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n      </li>\n      <li>\n        <p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n      </li>\n      <li>\n        <p><strong>FlatBuffer Schema (simplified):</strong></p>\n\n        <pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n      </li>\n      <li>\n        <p><strong>Example Python Usage:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</p>\n<p><strong>Developed by</strong>: Google</p>\n<p><strong>Type</strong>: Binary serialization library with zero-copy design</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Allows direct access to data without unpacking (zero-copy reads)</li>\n      <li>Compact binary representation optimized for low-latency parsing</li>\n      <li>Built-in schema evolution support</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Near-instantaneous loading—no deserialization overhead</li>\n      <li>Perfect for mobile/embedded devices with tight latency or startup constraints</li>\n      <li>Schema-aware tooling for validation</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">.tflite</code> files store computation graphs, tensors, and metadata using FlatBuffer encoding</li>\n      <li>Facilitates runtime interpretation without converting the graph into a different memory format</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Harder to inspect/debug than JSON or Protobuf</li>\n      <li>Limited dynamic structure capabilities compared to Protobuf</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n      </li>\n      <li>\n        <p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n      </li>\n      <li>\n        <p><strong>FlatBuffer Schema (simplified):</strong></p>\n\n        <pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n      </li>\n      <li>\n        <p><strong>Example Python Usage:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n<p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n<p><strong>FlatBuffer Schema (simplified):</strong></p>\n<pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n<p><strong>Example Python Usage:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>",
    "contentMarkdown": "*   **Used by**: TensorFlow Lite (`.tflite`)\n    \n*   **Developed by**: Google\n    \n*   **Type**: Binary serialization library with zero-copy design\n    \n*   **Key Characteristics**:\n    \n    *   Allows direct access to data without unpacking (zero-copy reads)\n    *   Compact binary representation optimized for low-latency parsing\n    *   Built-in schema evolution support\n*   **Advantages**:\n    \n    *   Near-instantaneous loading—no deserialization overhead\n    *   Perfect for mobile/embedded devices with tight latency or startup constraints\n    *   Schema-aware tooling for validation\n*   **In ML context**:\n    \n    *   `.tflite` files store computation graphs, tensors, and metadata using FlatBuffer encoding\n    *   Facilitates runtime interpretation without converting the graph into a different memory format\n*   **Limitations**:\n    \n    *   Harder to inspect/debug than JSON or Protobuf\n    *   Limited dynamic structure capabilities compared to Protobuf\n*   **Example**:\n    \n    *   _Used in: TensorFlow Lite (`.tflite`)_\n        \n    *   FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n        \n    *   **FlatBuffer Schema (simplified):**\n        \n        ```idl\n          table Tensor {\n            shape: [int];\n            type: int;\n            buffer: int;\n          }\n        \n          table Operator {\n            opcode_index: int;\n            inputs: [int];\n            outputs: [int];\n          }\n        \n          table Model {\n            tensors: [Tensor];\n            operators: [Operator];\n          }\n        ```\n        \n    *   **Example Python Usage:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n        \n    *   **Serialized File:**\n        \n        *   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.\n\n**Used by**: TensorFlow Lite (`.tflite`)\n\n**Developed by**: Google\n\n**Type**: Binary serialization library with zero-copy design\n\n**Key Characteristics**:\n\n*   Allows direct access to data without unpacking (zero-copy reads)\n*   Compact binary representation optimized for low-latency parsing\n*   Built-in schema evolution support\n\n**Advantages**:\n\n*   Near-instantaneous loading—no deserialization overhead\n*   Perfect for mobile/embedded devices with tight latency or startup constraints\n*   Schema-aware tooling for validation\n\n**In ML context**:\n\n*   `.tflite` files store computation graphs, tensors, and metadata using FlatBuffer encoding\n*   Facilitates runtime interpretation without converting the graph into a different memory format\n\n**Limitations**:\n\n*   Harder to inspect/debug than JSON or Protobuf\n*   Limited dynamic structure capabilities compared to Protobuf\n\n**Example**:\n\n*   _Used in: TensorFlow Lite (`.tflite`)_\n    \n*   FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n    \n*   **FlatBuffer Schema (simplified):**\n    \n    ```idl\n      table Tensor {\n        shape: [int];\n        type: int;\n        buffer: int;\n      }\n    \n      table Operator {\n        opcode_index: int;\n        inputs: [int];\n        outputs: [int];\n      }\n    \n      table Model {\n        tensors: [Tensor];\n        operators: [Operator];\n      }\n    ```\n    \n*   **Example Python Usage:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n    \n*   **Serialized File:**\n    \n    *   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.\n\n_Used in: TensorFlow Lite (`.tflite`)_\n\nFlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n\n**FlatBuffer Schema (simplified):**\n\n```idl\n  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n```\n\n**Example Python Usage:**\n\n![](https://aman.ai/images/copy.png)\n\n  `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n\n**Serialized File:**\n\n*   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.",
    "contentLength": 9244,
    "wordCount": 510,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#flatbuffer"
  },
  {
    "id": "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: Serialization Formats Across Runtimes",
    "title": "GGUF (GPT-generated GGML Unified Format)",
    "order": 58,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and its LLM-compatible ecosystem</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Community (successor to GGML model format)</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Lightweight binary tensor format for large language models</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Encodes quantized transformer weights and architecture metadata</li>\n      <li>Designed for efficient memory mapping and low-RAM usage</li>\n      <li>Built for CPU-first inference (with optional GPU support)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Extremely compact, especially with quantization (4–8 bit)</li>\n      <li>Simple, fast memory-mapped loading (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Compatible with CPU-based inference engines (no dependencies)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li>Stores models like LLaMA, Mistral, Alpaca after quantization</li>\n      <li>Used by <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">llm.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">text-generation-webui</code>, and other local LLM tools</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Not general-purpose—only suitable for transformer LLMs</li>\n      <li>Lacks complex graph control (branching, dynamic ops)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n      </li>\n      <li>\n        <p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n      </li>\n      <li>\n        <p><strong>Header Block (example layout in binary format):</strong></p>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Python conversion (from PyTorch):</strong></p>\n\n        <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and its LLM-compatible ecosystem</p>\n<p><strong>Developed by</strong>: Community (successor to GGML model format)</p>\n<p><strong>Type</strong>: Lightweight binary tensor format for large language models</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Encodes quantized transformer weights and architecture metadata</li>\n      <li>Designed for efficient memory mapping and low-RAM usage</li>\n      <li>Built for CPU-first inference (with optional GPU support)</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Extremely compact, especially with quantization (4–8 bit)</li>\n      <li>Simple, fast memory-mapped loading (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Compatible with CPU-based inference engines (no dependencies)</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li>Stores models like LLaMA, Mistral, Alpaca after quantization</li>\n      <li>Used by <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">llm.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">text-generation-webui</code>, and other local LLM tools</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Not general-purpose—only suitable for transformer LLMs</li>\n      <li>Lacks complex graph control (branching, dynamic ops)</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n      </li>\n      <li>\n        <p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n      </li>\n      <li>\n        <p><strong>Header Block (example layout in binary format):</strong></p>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Python conversion (from PyTorch):</strong></p>\n\n        <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>\n      </li>\n    </ul>\n<p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n<p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n<p><strong>Header Block (example layout in binary format):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre>\n<p><strong>Python conversion (from PyTorch):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre>\n<p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>",
    "contentMarkdown": "*   **Used by**: `llama.cpp` and its LLM-compatible ecosystem\n    \n*   **Developed by**: Community (successor to GGML model format)\n    \n*   **Type**: Lightweight binary tensor format for large language models\n    \n*   **Key Characteristics**:\n    \n    *   Encodes quantized transformer weights and architecture metadata\n    *   Designed for efficient memory mapping and low-RAM usage\n    *   Built for CPU-first inference (with optional GPU support)\n*   **Advantages**:\n    \n    *   Extremely compact, especially with quantization (4–8 bit)\n    *   Simple, fast memory-mapped loading (`mmap`)\n    *   Compatible with CPU-based inference engines (no dependencies)\n*   **In ML context**:\n    \n    *   Stores models like LLaMA, Mistral, Alpaca after quantization\n    *   Used by `llama.cpp`, `llm.cpp`, `text-generation-webui`, and other local LLM tools\n*   **Limitations**:\n    \n    *   Not general-purpose—only suitable for transformer LLMs\n    *   Lacks complex graph control (branching, dynamic ops)\n*   **Example**:\n    \n    *   Used in: `llama.cpp`, quantized LLMs\\*\n        \n    *   GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n        \n    *   **Header Block (example layout in binary format):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n        \n    *   **Python conversion (from PyTorch):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `python convert.py --input model.bin --output model.gguf --format Q4_0`\n        \n    *   **Reading from `llama.cpp`:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n        \n    *   **Serialized File:**\n        \n        *   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.\n\n**Used by**: `llama.cpp` and its LLM-compatible ecosystem\n\n**Developed by**: Community (successor to GGML model format)\n\n**Type**: Lightweight binary tensor format for large language models\n\n**Key Characteristics**:\n\n*   Encodes quantized transformer weights and architecture metadata\n*   Designed for efficient memory mapping and low-RAM usage\n*   Built for CPU-first inference (with optional GPU support)\n\n**Advantages**:\n\n*   Extremely compact, especially with quantization (4–8 bit)\n*   Simple, fast memory-mapped loading (`mmap`)\n*   Compatible with CPU-based inference engines (no dependencies)\n\n**In ML context**:\n\n*   Stores models like LLaMA, Mistral, Alpaca after quantization\n*   Used by `llama.cpp`, `llm.cpp`, `text-generation-webui`, and other local LLM tools\n\n**Limitations**:\n\n*   Not general-purpose—only suitable for transformer LLMs\n*   Lacks complex graph control (branching, dynamic ops)\n\n**Example**:\n\n*   Used in: `llama.cpp`, quantized LLMs\\*\n    \n*   GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n    \n*   **Header Block (example layout in binary format):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n    \n*   **Python conversion (from PyTorch):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `python convert.py --input model.bin --output model.gguf --format Q4_0`\n    \n*   **Reading from `llama.cpp`:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n    \n*   **Serialized File:**\n    \n    *   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.\n\nUsed in: `llama.cpp`, quantized LLMs\\*\n\nGGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n\n**Header Block (example layout in binary format):**\n\n![](https://aman.ai/images/copy.png)\n\n  `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n\n**Python conversion (from PyTorch):**\n\n![](https://aman.ai/images/copy.png)\n\n  `python convert.py --input model.bin --output model.gguf --format Q4_0`\n\n**Reading from `llama.cpp`:**\n\n![](https://aman.ai/images/copy.png)\n\n  `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n\n**Serialized File:**\n\n*   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.",
    "contentLength": 11325,
    "wordCount": 493,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#gguf-(gpt-generated-ggml-unified-format)"
  },
  {
    "id": "ai-ml-runtimes-bytecode-format-executorch-59",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: Serialization Formats Across Runtimes",
    "title": "Bytecode Format (ExecuTorch)",
    "order": 59,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: ExecuTorch</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Meta</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Custom AOT-compiled bytecode</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Outputs compact bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>) from PyTorch models via TorchScript tracing</li>\n      <li>Prunes unused operators to reduce binary size</li>\n      <li>Embeds minimal op metadata needed for runtime VM</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Highly portable and minimal—can run on MCUs and RTOS platforms</li>\n      <li>Deterministic memory usage and low overhead</li>\n      <li>Enables static linking of models and kernels for bare-metal systems</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li>Targets constrained devices (sub-MB RAM)</li>\n      <li>Supports fixed operator sets with predictable memory and runtime behavior</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Rigid format—not well suited for dynamic models or rich graph structures</li>\n      <li>Tied closely to PyTorch tracing and compilation pipeline.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n      </li>\n      <li>\n        <p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n      </li>\n      <li>\n        <p><strong>Model Compilation:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Runtime Use in C++:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: ExecuTorch</p>\n<p><strong>Developed by</strong>: Meta</p>\n<p><strong>Type</strong>: Custom AOT-compiled bytecode</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Outputs compact bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>) from PyTorch models via TorchScript tracing</li>\n      <li>Prunes unused operators to reduce binary size</li>\n      <li>Embeds minimal op metadata needed for runtime VM</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Highly portable and minimal—can run on MCUs and RTOS platforms</li>\n      <li>Deterministic memory usage and low overhead</li>\n      <li>Enables static linking of models and kernels for bare-metal systems</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li>Targets constrained devices (sub-MB RAM)</li>\n      <li>Supports fixed operator sets with predictable memory and runtime behavior</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Rigid format—not well suited for dynamic models or rich graph structures</li>\n      <li>Tied closely to PyTorch tracing and compilation pipeline.</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n      </li>\n      <li>\n        <p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n      </li>\n      <li>\n        <p><strong>Model Compilation:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Runtime Use in C++:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n<p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n<p><strong>Model Compilation:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre>\n<p><strong>Runtime Use in C++:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>",
    "contentMarkdown": "*   **Used by**: ExecuTorch\n    \n*   **Developed by**: Meta\n    \n*   **Type**: Custom AOT-compiled bytecode\n    \n*   **Key Characteristics**:\n    \n    *   Outputs compact bytecode (`.ptc`) from PyTorch models via TorchScript tracing\n    *   Prunes unused operators to reduce binary size\n    *   Embeds minimal op metadata needed for runtime VM\n*   **Advantages**:\n    \n    *   Highly portable and minimal—can run on MCUs and RTOS platforms\n    *   Deterministic memory usage and low overhead\n    *   Enables static linking of models and kernels for bare-metal systems\n*   **In ML context**:\n    \n    *   Targets constrained devices (sub-MB RAM)\n    *   Supports fixed operator sets with predictable memory and runtime behavior\n*   **Limitations**:\n    \n    *   Rigid format—not well suited for dynamic models or rich graph structures\n    *   Tied closely to PyTorch tracing and compilation pipeline.\n*   **Example**:\n    \n    *   _Used in: ExecuTorch (`.ptc` format)_\n        \n    *   ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n        \n    *   **Model Compilation:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n        \n    *   **Runtime Use in C++:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n        \n    *   **Serialized File:**\n        \n        *   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.\n\n**Used by**: ExecuTorch\n\n**Developed by**: Meta\n\n**Type**: Custom AOT-compiled bytecode\n\n**Key Characteristics**:\n\n*   Outputs compact bytecode (`.ptc`) from PyTorch models via TorchScript tracing\n*   Prunes unused operators to reduce binary size\n*   Embeds minimal op metadata needed for runtime VM\n\n**Advantages**:\n\n*   Highly portable and minimal—can run on MCUs and RTOS platforms\n*   Deterministic memory usage and low overhead\n*   Enables static linking of models and kernels for bare-metal systems\n\n**In ML context**:\n\n*   Targets constrained devices (sub-MB RAM)\n*   Supports fixed operator sets with predictable memory and runtime behavior\n\n**Limitations**:\n\n*   Rigid format—not well suited for dynamic models or rich graph structures\n*   Tied closely to PyTorch tracing and compilation pipeline.\n\n**Example**:\n\n*   _Used in: ExecuTorch (`.ptc` format)_\n    \n*   ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n    \n*   **Model Compilation:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n    \n*   **Runtime Use in C++:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n    \n*   **Serialized File:**\n    \n    *   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.\n\n_Used in: ExecuTorch (`.ptc` format)_\n\nExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n\n**Model Compilation:**\n\n![](https://aman.ai/images/copy.png)\n\n  `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n\n**Runtime Use in C++:**\n\n![](https://aman.ai/images/copy.png)\n\n  `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n\n**Serialized File:**\n\n*   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.",
    "contentLength": 13069,
    "wordCount": 475,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#bytecode-format-(executorch)"
  },
  {
    "id": "ai-ml-runtimes-comparative-analysis-60",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: Serialization Formats Across Runtimes",
    "title": "Comparative Analysis",
    "order": 60,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>Understanding the serialization format is crucial when choosing a runtime—especially for performance, portability, and debugging. Developers targeting mobile and embedded environments often prefer FlatBuffer or bytecode for efficiency, while cloud/server or cross-platform projects benefit from Protobuf’s rich graph encoding.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Used By</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Format Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Example File</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Viewability</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Tool to Inspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Protobuf</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, ONNX</td>\n<td class=\"tg-tleft-valign-first\">Binary (schema-driven)</td>\n<td class=\"tg-tleft-valign-first\"><code>model.onnx</code>, <code>model.pb</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>onnx</code>, <code>tf.saved_model_cli</code></td>\n<td class=\"tg-tleft-valign-first\">Cross-platform, schema evolution, rich structure</td>\n<td class=\"tg-tleft-valign-second\">Larger footprint, full deserialization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlatBuffer</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow Lite</td>\n<td class=\"tg-tleft-valign-first\">Zero-copy binary</td>\n<td class=\"tg-tleft-valign-first\"><code>model.tflite</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>flatc</code>, <code>tflite</code> API</td>\n<td class=\"tg-tleft-valign-first\">Instant loading, ideal for embedded use</td>\n<td class=\"tg-tleft-valign-second\">Harder to inspect/debug</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GGUF</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Binary tensor map</td>\n<td class=\"tg-tleft-valign-first\"><code>llama-7B.Q4_0.gguf</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code>, <code>gguf_dump.py</code></td>\n<td class=\"tg-tleft-valign-first\">Ultra-compact, mmap-friendly, quantized</td>\n<td class=\"tg-tleft-valign-second\">LLM-specific only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Bytecode</td>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Compiled AOT VM</td>\n<td class=\"tg-tleft-valign-first\"><code>model.ptc</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>executorchc</code>, ExecuTorch API</td>\n<td class=\"tg-tleft-valign-first\">Tiny runtime, embedded-friendly</td>\n<td class=\"tg-tleft-valign-second\">Limited flexibility, PyTorch-only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT Engine</td>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">Binary CUDA engine</td>\n<td class=\"tg-tleft-valign-first\"><code>model.plan</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\">TensorRT API (<code>trtexec</code>)</td>\n<td class=\"tg-tleft-valign-first\">Hardware-optimized, precompiled inference</td>\n<td class=\"tg-tleft-valign-second\">NVIDIA-only, not portable</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Used By</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Format Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Example File</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Viewability</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Tool to Inspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Protobuf</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, ONNX</td>\n<td class=\"tg-tleft-valign-first\">Binary (schema-driven)</td>\n<td class=\"tg-tleft-valign-first\"><code>model.onnx</code>, <code>model.pb</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>onnx</code>, <code>tf.saved_model_cli</code></td>\n<td class=\"tg-tleft-valign-first\">Cross-platform, schema evolution, rich structure</td>\n<td class=\"tg-tleft-valign-second\">Larger footprint, full deserialization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlatBuffer</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow Lite</td>\n<td class=\"tg-tleft-valign-first\">Zero-copy binary</td>\n<td class=\"tg-tleft-valign-first\"><code>model.tflite</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>flatc</code>, <code>tflite</code> API</td>\n<td class=\"tg-tleft-valign-first\">Instant loading, ideal for embedded use</td>\n<td class=\"tg-tleft-valign-second\">Harder to inspect/debug</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GGUF</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Binary tensor map</td>\n<td class=\"tg-tleft-valign-first\"><code>llama-7B.Q4_0.gguf</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code>, <code>gguf_dump.py</code></td>\n<td class=\"tg-tleft-valign-first\">Ultra-compact, mmap-friendly, quantized</td>\n<td class=\"tg-tleft-valign-second\">LLM-specific only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Bytecode</td>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Compiled AOT VM</td>\n<td class=\"tg-tleft-valign-first\"><code>model.ptc</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>executorchc</code>, ExecuTorch API</td>\n<td class=\"tg-tleft-valign-first\">Tiny runtime, embedded-friendly</td>\n<td class=\"tg-tleft-valign-second\">Limited flexibility, PyTorch-only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT Engine</td>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">Binary CUDA engine</td>\n<td class=\"tg-tleft-valign-first\"><code>model.plan</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\">TensorRT API (<code>trtexec</code>)</td>\n<td class=\"tg-tleft-valign-first\">Hardware-optimized, precompiled inference</td>\n<td class=\"tg-tleft-valign-second\">NVIDIA-only, not portable</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "*   Understanding the serialization format is crucial when choosing a runtime—especially for performance, portability, and debugging. Developers targeting mobile and embedded environments often prefer FlatBuffer or bytecode for efficiency, while cloud/server or cross-platform projects benefit from Protobuf’s rich graph encoding.\n\n**Format**\n\n**Used By**\n\n**Format Type**\n\n**Example File**\n\n**Viewability**\n\n**Tool to Inspect**\n\n**Strengths**\n\n**Limitations**\n\nProtobuf\n\nTensorFlow, ONNX\n\nBinary (schema-driven)\n\n`model.onnx`, `model.pb`\n\nBinary\n\n`onnx`, `tf.saved_model_cli`\n\nCross-platform, schema evolution, rich structure\n\nLarger footprint, full deserialization\n\nFlatBuffer\n\nTensorFlow Lite\n\nZero-copy binary\n\n`model.tflite`\n\nBinary\n\n`flatc`, `tflite` API\n\nInstant loading, ideal for embedded use\n\nHarder to inspect/debug\n\nGGUF\n\n`llama.cpp`\n\nBinary tensor map\n\n`llama-7B.Q4_0.gguf`\n\nBinary\n\n`llama.cpp`, `gguf_dump.py`\n\nUltra-compact, mmap-friendly, quantized\n\nLLM-specific only\n\nBytecode\n\nExecuTorch\n\nCompiled AOT VM\n\n`model.ptc`\n\nBinary\n\n`executorchc`, ExecuTorch API\n\nTiny runtime, embedded-friendly\n\nLimited flexibility, PyTorch-only\n\nTensorRT Engine\n\nTensorRT\n\nBinary CUDA engine\n\n`model.plan`\n\nBinary\n\nTensorRT API (`trtexec`)\n\nHardware-optimized, precompiled inference\n\nNVIDIA-only, not portable\n\n**Format**\n\n**Used By**\n\n**Format Type**\n\n**Example File**\n\n**Viewability**\n\n**Tool to Inspect**\n\n**Strengths**\n\n**Limitations**\n\nProtobuf\n\nTensorFlow, ONNX\n\nBinary (schema-driven)\n\n`model.onnx`, `model.pb`\n\nBinary\n\n`onnx`, `tf.saved_model_cli`\n\nCross-platform, schema evolution, rich structure\n\nLarger footprint, full deserialization\n\nFlatBuffer\n\nTensorFlow Lite\n\nZero-copy binary\n\n`model.tflite`\n\nBinary\n\n`flatc`, `tflite` API\n\nInstant loading, ideal for embedded use\n\nHarder to inspect/debug\n\nGGUF\n\n`llama.cpp`\n\nBinary tensor map\n\n`llama-7B.Q4_0.gguf`\n\nBinary\n\n`llama.cpp`, `gguf_dump.py`\n\nUltra-compact, mmap-friendly, quantized\n\nLLM-specific only\n\nBytecode\n\nExecuTorch\n\nCompiled AOT VM\n\n`model.ptc`\n\nBinary\n\n`executorchc`, ExecuTorch API\n\nTiny runtime, embedded-friendly\n\nLimited flexibility, PyTorch-only\n\nTensorRT Engine\n\nTensorRT\n\nBinary CUDA engine\n\n`model.plan`\n\nBinary\n\nTensorRT API (`trtexec`)\n\nHardware-optimized, precompiled inference\n\nNVIDIA-only, not portable",
    "contentLength": 6816,
    "wordCount": 236,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis"
  },
  {
    "id": "ai-ml-runtimes-general-workflow-from-model-to-inference-61",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Model Execution Lifecycle Across ML Runtimes",
    "title": "General Workflow: from Model to Inference",
    "order": 61,
    "orderInChapter": 1,
    "contentHtml": "<h4 id=\"model-training\">Model Training</h4>\n<ul>\n  <li>\n    <p>Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be <strong>exported or converted</strong> into a format compatible with the intended runtime.</p>\n  </li>\n  <li>\n    <p>This stage outputs:</p>\n\n    <ul>\n      <li>A trained model file (e.g., <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, etc.)</li>\n      <li>Associated metadata (input/output shapes, quantization info, etc.)</li>\n    </ul>\n  </li>\n</ul>\n<p>Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be <strong>exported or converted</strong> into a format compatible with the intended runtime.</p>\n<p>This stage outputs:</p>\n<ul>\n      <li>A trained model file (e.g., <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, etc.)</li>\n      <li>Associated metadata (input/output shapes, quantization info, etc.)</li>\n    </ul>\n<h4 id=\"model-conversion\">Model Conversion</h4>\n<ul>\n  <li>\n    <p>This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply <strong>graph simplification</strong>, <strong>quantization</strong>, or <strong>operator fusion</strong>.</p>\n  </li>\n  <li>\n    <p>Typical tools used:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">torch.onnx.export()</code> (PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\rightarrow</script> ONNX)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\rightarrow</script> Core ML)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> (TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">executorchc</code> (TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-22\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\rightarrow</script> ExecuTorch bytecode)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">quantize.py</code> (for GGUF / <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p>This phase outputs:</p>\n\n    <ul>\n      <li>Serialized model file tailored for the target runtime</li>\n      <li>Optional quantized or optimized variant</li>\n    </ul>\n  </li>\n</ul>\n<p>This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply <strong>graph simplification</strong>, <strong>quantization</strong>, or <strong>operator fusion</strong>.</p>\n<p>Typical tools used:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">torch.onnx.export()</code> (PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\rightarrow</script> ONNX)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\rightarrow</script> Core ML)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> (TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">executorchc</code> (TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-22\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\rightarrow</script> ExecuTorch bytecode)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">quantize.py</code> (for GGUF / <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n<p>This phase outputs:</p>\n<ul>\n      <li>Serialized model file tailored for the target runtime</li>\n      <li>Optional quantized or optimized variant</li>\n    </ul>\n<h4 id=\"model-loading\">Model Loading</h4>\n<ul>\n  <li>\n    <p>At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:</p>\n\n    <ul>\n      <li>Internal intermediate representation (IR)</li>\n      <li>Execution graph</li>\n      <li>Bytecode or linear transformer stack (as in <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Some runtimes use <strong>zero-copy formats</strong> (e.g., FlatBuffer in TFLite) to avoid overhead.</p>\n  </li>\n</ul>\n<p>At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:</p>\n<ul>\n      <li>Internal intermediate representation (IR)</li>\n      <li>Execution graph</li>\n      <li>Bytecode or linear transformer stack (as in <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n<p>Some runtimes use <strong>zero-copy formats</strong> (e.g., FlatBuffer in TFLite) to avoid overhead.</p>\n<h4 id=\"memory-allocation\">Memory Allocation</h4>\n<ul>\n  <li>\n    <p>Before inference can occur, the runtime must allocate:</p>\n\n    <ul>\n      <li>Input and output tensor buffers</li>\n      <li>Working memory for intermediate computations</li>\n      <li>(If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers</li>\n    </ul>\n  </li>\n  <li>\n    <p>Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>).</p>\n  </li>\n</ul>\n<p>Before inference can occur, the runtime must allocate:</p>\n<ul>\n      <li>Input and output tensor buffers</li>\n      <li>Working memory for intermediate computations</li>\n      <li>(If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers</li>\n    </ul>\n<p>Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>).</p>\n<h4 id=\"inference-execution\">Inference Execution</h4>\n<ul>\n  <li>\n    <p>The core execution stage involves:</p>\n\n    <ul>\n      <li>Running the model graph or stack</li>\n      <li>Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)</li>\n      <li>Managing control flow, caching, and batching</li>\n    </ul>\n  </li>\n  <li>\n    <p>Different runtimes handle scheduling and dispatch differently:</p>\n\n    <ul>\n      <li>TensorRT: CUDA engine with explicit graph scheduling</li>\n      <li>TFLite: Static interpreter with delegate hand-off</li>\n      <li>ONNX Runtime: Execution Providers (EPs)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>: Single-threaded or parallel transformer loop</li>\n    </ul>\n  </li>\n</ul>\n<p>The core execution stage involves:</p>\n<ul>\n      <li>Running the model graph or stack</li>\n      <li>Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)</li>\n      <li>Managing control flow, caching, and batching</li>\n    </ul>\n<p>Different runtimes handle scheduling and dispatch differently:</p>\n<ul>\n      <li>TensorRT: CUDA engine with explicit graph scheduling</li>\n      <li>TFLite: Static interpreter with delegate hand-off</li>\n      <li>ONNX Runtime: Execution Providers (EPs)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>: Single-threaded or parallel transformer loop</li>\n    </ul>\n<h4 id=\"postprocessing--output\">Postprocessing &amp; Output</h4>\n<ul>\n  <li>\n    <p>The final outputs are:</p>\n\n    <ul>\n      <li>Raw logits, class probabilities, bounding boxes, text, etc.</li>\n      <li>Returned via API calls (C++, Python, Swift, etc.)</li>\n    </ul>\n  </li>\n  <li>\n    <p>This stage may also include:</p>\n\n    <ul>\n      <li>Dequantization</li>\n      <li>Formatting into app-native types (e.g., Swift structs in Core ML)</li>\n      <li>Logging and telemetry</li>\n    </ul>\n  </li>\n</ul>\n<p>The final outputs are:</p>\n<ul>\n      <li>Raw logits, class probabilities, bounding boxes, text, etc.</li>\n      <li>Returned via API calls (C++, Python, Swift, etc.)</li>\n    </ul>\n<p>This stage may also include:</p>\n<ul>\n      <li>Dequantization</li>\n      <li>Formatting into app-native types (e.g., Swift structs in Core ML)</li>\n      <li>Logging and telemetry</li>\n    </ul>\n<h4 id=\"lifecycle-optimization-optional-but-critical\">Lifecycle Optimization (Optional but Critical)</h4>\n<ul>\n  <li>\n    <p>For deployment, optimization techniques may be inserted at multiple points:</p>\n\n    <ul>\n      <li>Quantization (during conversion)</li>\n      <li>Delegate configuration (runtime initialization)</li>\n      <li>Memory pruning and op fusion (during compile/AOT phase)</li>\n      <li>Execution profiling and tuning</li>\n    </ul>\n  </li>\n</ul>\n<p>For deployment, optimization techniques may be inserted at multiple points:</p>\n<ul>\n      <li>Quantization (during conversion)</li>\n      <li>Delegate configuration (runtime initialization)</li>\n      <li>Memory pruning and op fusion (during compile/AOT phase)</li>\n      <li>Execution profiling and tuning</li>\n    </ul>",
    "contentMarkdown": "#### Model Training\n\n*   Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be **exported or converted** into a format compatible with the intended runtime.\n    \n*   This stage outputs:\n    \n    *   A trained model file (e.g., `.onnx`, `.mlmodel`, `.pt`, `.tflite`, etc.)\n    *   Associated metadata (input/output shapes, quantization info, etc.)\n\nAlthough training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be **exported or converted** into a format compatible with the intended runtime.\n\nThis stage outputs:\n\n*   A trained model file (e.g., `.onnx`, `.mlmodel`, `.pt`, `.tflite`, etc.)\n*   Associated metadata (input/output shapes, quantization info, etc.)\n\n#### Model Conversion\n\n*   This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply **graph simplification**, **quantization**, or **operator fusion**.\n    \n*   Typical tools used:\n    \n    *   `torch.onnx.export()` (PyTorch →→\\\\rightarrow ONNX)\n    *   `coremltools.convert()` (→→\\\\rightarrow Core ML)\n    *   `TFLiteConverter` (TensorFlow →→\\\\rightarrow `.tflite`)\n    *   `executorchc` (TorchScript →→\\\\rightarrow ExecuTorch bytecode)\n    *   `quantize.py` (for GGUF / `llama.cpp`)\n*   This phase outputs:\n    \n    *   Serialized model file tailored for the target runtime\n    *   Optional quantized or optimized variant\n\nThis phase adapts the trained model into a runtime-specific format. Conversion tools may also apply **graph simplification**, **quantization**, or **operator fusion**.\n\nTypical tools used:\n\n*   `torch.onnx.export()` (PyTorch →→\\\\rightarrow ONNX)\n*   `coremltools.convert()` (→→\\\\rightarrow Core ML)\n*   `TFLiteConverter` (TensorFlow →→\\\\rightarrow `.tflite`)\n*   `executorchc` (TorchScript →→\\\\rightarrow ExecuTorch bytecode)\n*   `quantize.py` (for GGUF / `llama.cpp`)\n\nThis phase outputs:\n\n*   Serialized model file tailored for the target runtime\n*   Optional quantized or optimized variant\n\n#### Model Loading\n\n*   At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:\n    \n    *   Internal intermediate representation (IR)\n    *   Execution graph\n    *   Bytecode or linear transformer stack (as in `llama.cpp`)\n*   Some runtimes use **zero-copy formats** (e.g., FlatBuffer in TFLite) to avoid overhead.\n    \n\nAt runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:\n\n*   Internal intermediate representation (IR)\n*   Execution graph\n*   Bytecode or linear transformer stack (as in `llama.cpp`)\n\nSome runtimes use **zero-copy formats** (e.g., FlatBuffer in TFLite) to avoid overhead.\n\n#### Memory Allocation\n\n*   Before inference can occur, the runtime must allocate:\n    \n    *   Input and output tensor buffers\n    *   Working memory for intermediate computations\n    *   (If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers\n*   Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, `llama.cpp`).\n    \n\nBefore inference can occur, the runtime must allocate:\n\n*   Input and output tensor buffers\n*   Working memory for intermediate computations\n*   (If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers\n\nAdvanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, `llama.cpp`).\n\n#### Inference Execution\n\n*   The core execution stage involves:\n    \n    *   Running the model graph or stack\n    *   Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)\n    *   Managing control flow, caching, and batching\n*   Different runtimes handle scheduling and dispatch differently:\n    \n    *   TensorRT: CUDA engine with explicit graph scheduling\n    *   TFLite: Static interpreter with delegate hand-off\n    *   ONNX Runtime: Execution Providers (EPs)\n    *   `llama.cpp`: Single-threaded or parallel transformer loop\n\nThe core execution stage involves:\n\n*   Running the model graph or stack\n*   Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)\n*   Managing control flow, caching, and batching\n\nDifferent runtimes handle scheduling and dispatch differently:\n\n*   TensorRT: CUDA engine with explicit graph scheduling\n*   TFLite: Static interpreter with delegate hand-off\n*   ONNX Runtime: Execution Providers (EPs)\n*   `llama.cpp`: Single-threaded or parallel transformer loop\n\n#### Postprocessing & Output\n\n*   The final outputs are:\n    \n    *   Raw logits, class probabilities, bounding boxes, text, etc.\n    *   Returned via API calls (C++, Python, Swift, etc.)\n*   This stage may also include:\n    \n    *   Dequantization\n    *   Formatting into app-native types (e.g., Swift structs in Core ML)\n    *   Logging and telemetry\n\nThe final outputs are:\n\n*   Raw logits, class probabilities, bounding boxes, text, etc.\n*   Returned via API calls (C++, Python, Swift, etc.)\n\nThis stage may also include:\n\n*   Dequantization\n*   Formatting into app-native types (e.g., Swift structs in Core ML)\n*   Logging and telemetry\n\n#### Lifecycle Optimization (Optional but Critical)\n\n*   For deployment, optimization techniques may be inserted at multiple points:\n    \n    *   Quantization (during conversion)\n    *   Delegate configuration (runtime initialization)\n    *   Memory pruning and op fusion (during compile/AOT phase)\n    *   Execution profiling and tuning\n\nFor deployment, optimization techniques may be inserted at multiple points:\n\n*   Quantization (during conversion)\n*   Delegate configuration (runtime initialization)\n*   Memory pruning and op fusion (during compile/AOT phase)\n*   Execution profiling and tuning",
    "contentLength": 18791,
    "wordCount": 759,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#general-workflow:-from-model-to-inference"
  },
  {
    "id": "ai-ml-runtimes-runtime-specific-execution-lifecycles-62",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Model Execution Lifecycle Across ML Runtimes",
    "title": "Runtime-Specific Execution Lifecycles",
    "order": 62,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>While the general lifecycle described earlier applies to all runtimes, each ML runtime adapts or specializes this flow to match its architectural goals and supported hardware.</li>\n  <li>This section provides an <strong>execution lifecycle breakdown</strong> for each runtime discussed in the original primer, with particular focus on runtime-specific logic during model loading, graph execution, memory management, and hardware dispatch.</li>\n</ul>\n<h4 id=\"tensorrt-execution-lifecycle-nvidia-gpus\">TensorRT Execution Lifecycle (NVIDIA GPUs)</h4>\n<ul>\n  <li>\n    <p>TensorRT uses an <strong>Ahead-of-Time (AOT) engine-building</strong> process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the <code class=\"language-plaintext highlighter-rouge\">.plan</code> file encapsulates a pre-fused, quantized, and hardware-specific execution graph.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Import &amp; Parsing:</strong> Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.</li>\n      <li><strong>Builder Optimization:</strong> Applies kernel selection, op fusion, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, and layer scheduling.</li>\n      <li><strong>Engine Generation:</strong> Outputs a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file containing the serialized CUDA engine.</li>\n      <li><strong>Runtime Load:</strong> Loads the plan into memory via <code class=\"language-plaintext highlighter-rouge\">IRuntime</code>, allocates CUDA buffers.</li>\n      <li><strong>Execution Context:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">ExecutionContext</code> with shape bindings, input/output memory views.</li>\n      <li><strong>Inference Loop:</strong> Launches CUDA kernels via streams with async execution.</li>\n      <li><strong>Output Retrieval:</strong> Copies GPU output buffers back to host (if needed).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Extremely low latency, precompiled execution</li>\n      <li>Requires regeneration if model shape changes</li>\n      <li>All ops dispatched on GPU only</li>\n    </ul>\n  </li>\n</ul>\n<p>TensorRT uses an <strong>Ahead-of-Time (AOT) engine-building</strong> process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the <code class=\"language-plaintext highlighter-rouge\">.plan</code> file encapsulates a pre-fused, quantized, and hardware-specific execution graph.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Import &amp; Parsing:</strong> Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.</li>\n      <li><strong>Builder Optimization:</strong> Applies kernel selection, op fusion, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, and layer scheduling.</li>\n      <li><strong>Engine Generation:</strong> Outputs a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file containing the serialized CUDA engine.</li>\n      <li><strong>Runtime Load:</strong> Loads the plan into memory via <code class=\"language-plaintext highlighter-rouge\">IRuntime</code>, allocates CUDA buffers.</li>\n      <li><strong>Execution Context:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">ExecutionContext</code> with shape bindings, input/output memory views.</li>\n      <li><strong>Inference Loop:</strong> Launches CUDA kernels via streams with async execution.</li>\n      <li><strong>Output Retrieval:</strong> Copies GPU output buffers back to host (if needed).</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Extremely low latency, precompiled execution</li>\n      <li>Requires regeneration if model shape changes</li>\n      <li>All ops dispatched on GPU only</li>\n    </ul>\n<h4 id=\"core-ml-execution-lifecycle-apple-platforms\">Core ML Execution Lifecycle (Apple Platforms)</h4>\n<ul>\n  <li>\n    <p>Core ML performs inference via <strong>runtime graph execution</strong> of a compiled <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> package. It abstracts backend selection and heavily integrates with Apple’s APIs.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Compilation:</strong> <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-25\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> via Xcode or <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n      <li><strong>App Initialization:</strong> Loads model via <code class=\"language-plaintext highlighter-rouge\">MLModel(configuration:)</code></li>\n      <li><strong>Backend Dispatch:</strong> Chooses CPU, GPU, or ANE depending on hardware availability and op support.</li>\n      <li><strong>Inference Call:</strong> <code class=\"language-plaintext highlighter-rouge\">model.prediction(input:)</code> executes internal graph</li>\n      <li><strong>Result Handling:</strong> Outputs are returned as native Swift types (e.g., strings, arrays, dicts)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Dynamic backend selection with op-level granularity</li>\n      <li>Opaque execution graph, no public access to IR</li>\n      <li>Secure, sandboxed memory isolation for inference</li>\n    </ul>\n  </li>\n</ul>\n<p>Core ML performs inference via <strong>runtime graph execution</strong> of a compiled <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> package. It abstracts backend selection and heavily integrates with Apple’s APIs.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Compilation:</strong> <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-25\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> via Xcode or <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n      <li><strong>App Initialization:</strong> Loads model via <code class=\"language-plaintext highlighter-rouge\">MLModel(configuration:)</code></li>\n      <li><strong>Backend Dispatch:</strong> Chooses CPU, GPU, or ANE depending on hardware availability and op support.</li>\n      <li><strong>Inference Call:</strong> <code class=\"language-plaintext highlighter-rouge\">model.prediction(input:)</code> executes internal graph</li>\n      <li><strong>Result Handling:</strong> Outputs are returned as native Swift types (e.g., strings, arrays, dicts)</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Dynamic backend selection with op-level granularity</li>\n      <li>Opaque execution graph, no public access to IR</li>\n      <li>Secure, sandboxed memory isolation for inference</li>\n    </ul>\n<h4 id=\"mlx-execution-lifecycle-apple-silicon\">MLX Execution Lifecycle (Apple Silicon)</h4>\n<ul>\n  <li>\n    <p>MLX uses a <strong>Python-based tensor programming model</strong> and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Definition:</strong> Model is defined in Python using <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li><strong>Eager Execution (default):</strong> Runs ops immediately using Metal Performance Shaders (MPS)</li>\n      <li><strong>Compiled Graph (optional):</strong> <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> transforms a function into a static kernel sequence</li>\n      <li><strong>Tensor Handling:</strong> All tensors are immutable; memory reuse is managed by the MLX runtime</li>\n      <li><strong>Execution:</strong> Kernel invocations are dispatched via Metal; ANE support is under development</li>\n      <li><strong>Output:</strong> Results returned as MLX tensors, convertible to NumPy or PyTorch</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Developer-centric and Pythonic</li>\n      <li>Targets M1/M2 GPU via Metal</li>\n      <li>No external model serialization—code <em>is</em> the model</li>\n    </ul>\n  </li>\n</ul>\n<p>MLX uses a <strong>Python-based tensor programming model</strong> and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Definition:</strong> Model is defined in Python using <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li><strong>Eager Execution (default):</strong> Runs ops immediately using Metal Performance Shaders (MPS)</li>\n      <li><strong>Compiled Graph (optional):</strong> <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> transforms a function into a static kernel sequence</li>\n      <li><strong>Tensor Handling:</strong> All tensors are immutable; memory reuse is managed by the MLX runtime</li>\n      <li><strong>Execution:</strong> Kernel invocations are dispatched via Metal; ANE support is under development</li>\n      <li><strong>Output:</strong> Results returned as MLX tensors, convertible to NumPy or PyTorch</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Developer-centric and Pythonic</li>\n      <li>Targets M1/M2 GPU via Metal</li>\n      <li>No external model serialization—code <em>is</em> the model</li>\n    </ul>\n<h4 id=\"onnx-runtime-execution-lifecycle\">ONNX Runtime Execution Lifecycle</h4>\n<ul>\n  <li>\n    <p>ONNX Runtime is built around an <strong>intermediate computation graph</strong>, modular kernel registry, and <strong>Execution Providers (EPs)</strong> that delegate ops to appropriate hardware.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Load:</strong> Parses <code class=\"language-plaintext highlighter-rouge\">.onnx</code> file (protobuf format) into IR</li>\n      <li><strong>Graph Optimization:</strong> Applies passes (e.g., constant folding, op fusion, node elimination)</li>\n      <li><strong>EP Assignment:</strong> Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)</li>\n      <li><strong>Session Initialization:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> with input/output bindings</li>\n      <li><strong>Execution:</strong> Each partition of the graph is dispatched to its EP</li>\n      <li><strong>Result Aggregation:</strong> Output tensors are collected and returned in native types</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Pluggable backend system for flexible hardware support</li>\n      <li>Static graph, dynamic shape support with constraints</li>\n      <li>Strong cross-platform model portability</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime is built around an <strong>intermediate computation graph</strong>, modular kernel registry, and <strong>Execution Providers (EPs)</strong> that delegate ops to appropriate hardware.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Load:</strong> Parses <code class=\"language-plaintext highlighter-rouge\">.onnx</code> file (protobuf format) into IR</li>\n      <li><strong>Graph Optimization:</strong> Applies passes (e.g., constant folding, op fusion, node elimination)</li>\n      <li><strong>EP Assignment:</strong> Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)</li>\n      <li><strong>Session Initialization:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> with input/output bindings</li>\n      <li><strong>Execution:</strong> Each partition of the graph is dispatched to its EP</li>\n      <li><strong>Result Aggregation:</strong> Output tensors are collected and returned in native types</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Pluggable backend system for flexible hardware support</li>\n      <li>Static graph, dynamic shape support with constraints</li>\n      <li>Strong cross-platform model portability</li>\n    </ul>\n<h4 id=\"executorch-execution-lifecycle-mcuembedded-focus\">ExecuTorch Execution Lifecycle (MCU/Embedded Focus)</h4>\n<ul>\n  <li>\n    <p>ExecuTorch employs a <strong>bytecode VM</strong> model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>TorchScript Compilation:</strong> PyTorch model scripted and converted into <code class=\"language-plaintext highlighter-rouge\">.pt</code> (TorchScript)</li>\n      <li><strong>AOT Bytecode Generation:</strong> <code class=\"language-plaintext highlighter-rouge\">executorchc</code> compiles model to <code class=\"language-plaintext highlighter-rouge\">.ptc</code> (bytecode)</li>\n      <li><strong>Runtime Embedding:</strong> Bytecode and interpreter embedded into firmware or C++ app</li>\n      <li><strong>Interpreter Loop:</strong> Model execution performed by a tiny VM that reads bytecode</li>\n      <li><strong>Op Dispatch:</strong> Ops are routed to statically compiled function pointers</li>\n      <li><strong>Output Return:</strong> Inference results written to statically allocated output buffer</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Deterministic memory, static allocation only</li>\n      <li>Supports sub-MB runtime environments</li>\n      <li>Highly tunable; model format ≠ PyTorch IR</li>\n    </ul>\n  </li>\n</ul>\n<p>ExecuTorch employs a <strong>bytecode VM</strong> model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>TorchScript Compilation:</strong> PyTorch model scripted and converted into <code class=\"language-plaintext highlighter-rouge\">.pt</code> (TorchScript)</li>\n      <li><strong>AOT Bytecode Generation:</strong> <code class=\"language-plaintext highlighter-rouge\">executorchc</code> compiles model to <code class=\"language-plaintext highlighter-rouge\">.ptc</code> (bytecode)</li>\n      <li><strong>Runtime Embedding:</strong> Bytecode and interpreter embedded into firmware or C++ app</li>\n      <li><strong>Interpreter Loop:</strong> Model execution performed by a tiny VM that reads bytecode</li>\n      <li><strong>Op Dispatch:</strong> Ops are routed to statically compiled function pointers</li>\n      <li><strong>Output Return:</strong> Inference results written to statically allocated output buffer</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Deterministic memory, static allocation only</li>\n      <li>Supports sub-MB runtime environments</li>\n      <li>Highly tunable; model format ≠ PyTorch IR</li>\n    </ul>\n<h4 id=\"lidartlm-execution-lifecycle-lidar-focused-embedded-stacks\">LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)</h4>\n<ul>\n  <li>\n    <p>LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Sensor Input:</strong> LiDAR frames streamed in real-time</li>\n      <li><strong>Preprocessing:</strong> Voxelization or range transformation into tensor-friendly formats</li>\n      <li><strong>Tensor Pipeline:</strong> Sparse CNNs, 3D convolutions, and attention modules process data</li>\n      <li><strong>Temporal Fusion:</strong> RNN or transformer-based modules optionally applied across frames</li>\n      <li><strong>Postprocessing:</strong> Generates semantic maps or bounding boxes</li>\n      <li><strong>Sensor Fusion:</strong> Optionally integrates radar or camera data for final outputs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Sparse tensors and voxel grids dominate memory model</li>\n      <li>CUDA, Open3D, or MinkowskiEngine often used</li>\n      <li>Hard real-time constraints for robotics/ADAS</li>\n    </ul>\n  </li>\n</ul>\n<p>LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Sensor Input:</strong> LiDAR frames streamed in real-time</li>\n      <li><strong>Preprocessing:</strong> Voxelization or range transformation into tensor-friendly formats</li>\n      <li><strong>Tensor Pipeline:</strong> Sparse CNNs, 3D convolutions, and attention modules process data</li>\n      <li><strong>Temporal Fusion:</strong> RNN or transformer-based modules optionally applied across frames</li>\n      <li><strong>Postprocessing:</strong> Generates semantic maps or bounding boxes</li>\n      <li><strong>Sensor Fusion:</strong> Optionally integrates radar or camera data for final outputs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Sparse tensors and voxel grids dominate memory model</li>\n      <li>CUDA, Open3D, or MinkowskiEngine often used</li>\n      <li>Hard real-time constraints for robotics/ADAS</li>\n    </ul>\n<h4 id=\"llamacpp-execution-lifecycle-quantized-llms\"><code class=\"language-plaintext Highlighter-rouge\">llama.cpp</code> Execution Lifecycle (Quantized LLMs)</h4>\n<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Load:</strong> GGUF model memory-mapped into RAM</li>\n      <li><strong>KV Cache Setup:</strong> Pre-allocates attention buffers</li>\n      <li><strong>Embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-28\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\rightarrow</script> Transformer Loop:</strong> Sequentially executes transformer layers</li>\n      <li><strong>Sampling:</strong> Next token is selected via greedy/top-k/top-p logic</li>\n      <li><strong>Tokenization:</strong> Output string is constructed from sampled token IDs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Highly portable, CPU-optimized, extremely low memory usage</li>\n      <li>No dynamic graph, no scheduler, no intermediate representation</li>\n      <li>Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Load:</strong> GGUF model memory-mapped into RAM</li>\n      <li><strong>KV Cache Setup:</strong> Pre-allocates attention buffers</li>\n      <li><strong>Embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-28\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\rightarrow</script> Transformer Loop:</strong> Sequentially executes transformer layers</li>\n      <li><strong>Sampling:</strong> Next token is selected via greedy/top-k/top-p logic</li>\n      <li><strong>Tokenization:</strong> Output string is constructed from sampled token IDs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Highly portable, CPU-optimized, extremely low memory usage</li>\n      <li>No dynamic graph, no scheduler, no intermediate representation</li>\n      <li>Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle</li>\n    </ul>\n<h4 id=\"tensorflow-lite-execution-lifecycle\">TensorFlow Lite Execution Lifecycle</h4>\n<ul>\n  <li>\n    <p>TFLite uses a <strong>FlatBuffer interpreter</strong> architecture with optional delegates for acceleration.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Conversion:</strong> TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code> via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n      <li><strong>FlatBuffer Load:</strong> Model loaded with <code class=\"language-plaintext highlighter-rouge\">Interpreter(model_path=...)</code></li>\n      <li><strong>Tensor Allocation:</strong> Input/output buffers allocated via <code class=\"language-plaintext highlighter-rouge\">allocate_tensors()</code></li>\n      <li><strong>Delegate Attachment (optional):</strong> NNAPI, GPU, Hexagon delegate claims subgraphs</li>\n      <li><strong>Inference:</strong> Static interpreter walks the computation graph</li>\n      <li><strong>Output Access:</strong> Results extracted via <code class=\"language-plaintext highlighter-rouge\">get_tensor()</code> APIs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Very compact format with zero-copy access</li>\n      <li>Delegate design separates concerns for CPU vs. accelerators</li>\n      <li>Strong ecosystem with tooling (e.g., Model Maker, Visualizer)</li>\n    </ul>\n  </li>\n</ul>\n<p>TFLite uses a <strong>FlatBuffer interpreter</strong> architecture with optional delegates for acceleration.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Conversion:</strong> TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code> via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n      <li><strong>FlatBuffer Load:</strong> Model loaded with <code class=\"language-plaintext highlighter-rouge\">Interpreter(model_path=...)</code></li>\n      <li><strong>Tensor Allocation:</strong> Input/output buffers allocated via <code class=\"language-plaintext highlighter-rouge\">allocate_tensors()</code></li>\n      <li><strong>Delegate Attachment (optional):</strong> NNAPI, GPU, Hexagon delegate claims subgraphs</li>\n      <li><strong>Inference:</strong> Static interpreter walks the computation graph</li>\n      <li><strong>Output Access:</strong> Results extracted via <code class=\"language-plaintext highlighter-rouge\">get_tensor()</code> APIs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Very compact format with zero-copy access</li>\n      <li>Delegate design separates concerns for CPU vs. accelerators</li>\n      <li>Strong ecosystem with tooling (e.g., Model Maker, Visualizer)</li>\n    </ul>",
    "contentMarkdown": "*   While the general lifecycle described earlier applies to all runtimes, each ML runtime adapts or specializes this flow to match its architectural goals and supported hardware.\n*   This section provides an **execution lifecycle breakdown** for each runtime discussed in the original primer, with particular focus on runtime-specific logic during model loading, graph execution, memory management, and hardware dispatch.\n\n#### TensorRT Execution Lifecycle (NVIDIA GPUs)\n\n*   TensorRT uses an **Ahead-of-Time (AOT) engine-building** process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the `.plan` file encapsulates a pre-fused, quantized, and hardware-specific execution graph.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Import & Parsing:** Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.\n    *   **Builder Optimization:** Applies kernel selection, op fusion, `int8`/`float16` quantization, and layer scheduling.\n    *   **Engine Generation:** Outputs a `.plan` file containing the serialized CUDA engine.\n    *   **Runtime Load:** Loads the plan into memory via `IRuntime`, allocates CUDA buffers.\n    *   **Execution Context:** Prepares `ExecutionContext` with shape bindings, input/output memory views.\n    *   **Inference Loop:** Launches CUDA kernels via streams with async execution.\n    *   **Output Retrieval:** Copies GPU output buffers back to host (if needed).\n*   **Unique Characteristics:**\n    \n    *   Extremely low latency, precompiled execution\n    *   Requires regeneration if model shape changes\n    *   All ops dispatched on GPU only\n\nTensorRT uses an **Ahead-of-Time (AOT) engine-building** process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the `.plan` file encapsulates a pre-fused, quantized, and hardware-specific execution graph.\n\n**Lifecycle Stages:**\n\n*   **Model Import & Parsing:** Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.\n*   **Builder Optimization:** Applies kernel selection, op fusion, `int8`/`float16` quantization, and layer scheduling.\n*   **Engine Generation:** Outputs a `.plan` file containing the serialized CUDA engine.\n*   **Runtime Load:** Loads the plan into memory via `IRuntime`, allocates CUDA buffers.\n*   **Execution Context:** Prepares `ExecutionContext` with shape bindings, input/output memory views.\n*   **Inference Loop:** Launches CUDA kernels via streams with async execution.\n*   **Output Retrieval:** Copies GPU output buffers back to host (if needed).\n\n**Unique Characteristics:**\n\n*   Extremely low latency, precompiled execution\n*   Requires regeneration if model shape changes\n*   All ops dispatched on GPU only\n\n#### Core ML Execution Lifecycle (Apple Platforms)\n\n*   Core ML performs inference via **runtime graph execution** of a compiled `.mlmodelc` package. It abstracts backend selection and heavily integrates with Apple’s APIs.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Compilation:** `.mlmodel` →→\\\\rightarrow `.mlmodelc` via Xcode or `coremltools`\n    *   **App Initialization:** Loads model via `MLModel(configuration:)`\n    *   **Backend Dispatch:** Chooses CPU, GPU, or ANE depending on hardware availability and op support.\n    *   **Inference Call:** `model.prediction(input:)` executes internal graph\n    *   **Result Handling:** Outputs are returned as native Swift types (e.g., strings, arrays, dicts)\n*   **Unique Characteristics:**\n    \n    *   Dynamic backend selection with op-level granularity\n    *   Opaque execution graph, no public access to IR\n    *   Secure, sandboxed memory isolation for inference\n\nCore ML performs inference via **runtime graph execution** of a compiled `.mlmodelc` package. It abstracts backend selection and heavily integrates with Apple’s APIs.\n\n**Lifecycle Stages:**\n\n*   **Model Compilation:** `.mlmodel` →→\\\\rightarrow `.mlmodelc` via Xcode or `coremltools`\n*   **App Initialization:** Loads model via `MLModel(configuration:)`\n*   **Backend Dispatch:** Chooses CPU, GPU, or ANE depending on hardware availability and op support.\n*   **Inference Call:** `model.prediction(input:)` executes internal graph\n*   **Result Handling:** Outputs are returned as native Swift types (e.g., strings, arrays, dicts)\n\n**Unique Characteristics:**\n\n*   Dynamic backend selection with op-level granularity\n*   Opaque execution graph, no public access to IR\n*   Secure, sandboxed memory isolation for inference\n\n#### MLX Execution Lifecycle (Apple Silicon)\n\n*   MLX uses a **Python-based tensor programming model** and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Definition:** Model is defined in Python using `mlx.nn.Module`\n    *   **Eager Execution (default):** Runs ops immediately using Metal Performance Shaders (MPS)\n    *   **Compiled Graph (optional):** `@mlx.compile` transforms a function into a static kernel sequence\n    *   **Tensor Handling:** All tensors are immutable; memory reuse is managed by the MLX runtime\n    *   **Execution:** Kernel invocations are dispatched via Metal; ANE support is under development\n    *   **Output:** Results returned as MLX tensors, convertible to NumPy or PyTorch\n*   **Unique Characteristics:**\n    \n    *   Developer-centric and Pythonic\n    *   Targets M1/M2 GPU via Metal\n    *   No external model serialization—code _is_ the model\n\nMLX uses a **Python-based tensor programming model** and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.\n\n**Lifecycle Stages:**\n\n*   **Model Definition:** Model is defined in Python using `mlx.nn.Module`\n*   **Eager Execution (default):** Runs ops immediately using Metal Performance Shaders (MPS)\n*   **Compiled Graph (optional):** `@mlx.compile` transforms a function into a static kernel sequence\n*   **Tensor Handling:** All tensors are immutable; memory reuse is managed by the MLX runtime\n*   **Execution:** Kernel invocations are dispatched via Metal; ANE support is under development\n*   **Output:** Results returned as MLX tensors, convertible to NumPy or PyTorch\n\n**Unique Characteristics:**\n\n*   Developer-centric and Pythonic\n*   Targets M1/M2 GPU via Metal\n*   No external model serialization—code _is_ the model\n\n#### ONNX Runtime Execution Lifecycle\n\n*   ONNX Runtime is built around an **intermediate computation graph**, modular kernel registry, and **Execution Providers (EPs)** that delegate ops to appropriate hardware.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Load:** Parses `.onnx` file (protobuf format) into IR\n    *   **Graph Optimization:** Applies passes (e.g., constant folding, op fusion, node elimination)\n    *   **EP Assignment:** Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)\n    *   **Session Initialization:** Prepares `InferenceSession` with input/output bindings\n    *   **Execution:** Each partition of the graph is dispatched to its EP\n    *   **Result Aggregation:** Output tensors are collected and returned in native types\n*   **Unique Characteristics:**\n    \n    *   Pluggable backend system for flexible hardware support\n    *   Static graph, dynamic shape support with constraints\n    *   Strong cross-platform model portability\n\nONNX Runtime is built around an **intermediate computation graph**, modular kernel registry, and **Execution Providers (EPs)** that delegate ops to appropriate hardware.\n\n**Lifecycle Stages:**\n\n*   **Model Load:** Parses `.onnx` file (protobuf format) into IR\n*   **Graph Optimization:** Applies passes (e.g., constant folding, op fusion, node elimination)\n*   **EP Assignment:** Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)\n*   **Session Initialization:** Prepares `InferenceSession` with input/output bindings\n*   **Execution:** Each partition of the graph is dispatched to its EP\n*   **Result Aggregation:** Output tensors are collected and returned in native types\n\n**Unique Characteristics:**\n\n*   Pluggable backend system for flexible hardware support\n*   Static graph, dynamic shape support with constraints\n*   Strong cross-platform model portability\n\n#### ExecuTorch Execution Lifecycle (MCU/Embedded Focus)\n\n*   ExecuTorch employs a **bytecode VM** model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.\n    \n*   **Lifecycle Stages:**\n    \n    *   **TorchScript Compilation:** PyTorch model scripted and converted into `.pt` (TorchScript)\n    *   **AOT Bytecode Generation:** `executorchc` compiles model to `.ptc` (bytecode)\n    *   **Runtime Embedding:** Bytecode and interpreter embedded into firmware or C++ app\n    *   **Interpreter Loop:** Model execution performed by a tiny VM that reads bytecode\n    *   **Op Dispatch:** Ops are routed to statically compiled function pointers\n    *   **Output Return:** Inference results written to statically allocated output buffer\n*   **Unique Characteristics:**\n    \n    *   Deterministic memory, static allocation only\n    *   Supports sub-MB runtime environments\n    *   Highly tunable; model format ≠ PyTorch IR\n\nExecuTorch employs a **bytecode VM** model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.\n\n**Lifecycle Stages:**\n\n*   **TorchScript Compilation:** PyTorch model scripted and converted into `.pt` (TorchScript)\n*   **AOT Bytecode Generation:** `executorchc` compiles model to `.ptc` (bytecode)\n*   **Runtime Embedding:** Bytecode and interpreter embedded into firmware or C++ app\n*   **Interpreter Loop:** Model execution performed by a tiny VM that reads bytecode\n*   **Op Dispatch:** Ops are routed to statically compiled function pointers\n*   **Output Return:** Inference results written to statically allocated output buffer\n\n**Unique Characteristics:**\n\n*   Deterministic memory, static allocation only\n*   Supports sub-MB runtime environments\n*   Highly tunable; model format ≠ PyTorch IR\n\n#### LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)\n\n*   LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Sensor Input:** LiDAR frames streamed in real-time\n    *   **Preprocessing:** Voxelization or range transformation into tensor-friendly formats\n    *   **Tensor Pipeline:** Sparse CNNs, 3D convolutions, and attention modules process data\n    *   **Temporal Fusion:** RNN or transformer-based modules optionally applied across frames\n    *   **Postprocessing:** Generates semantic maps or bounding boxes\n    *   **Sensor Fusion:** Optionally integrates radar or camera data for final outputs\n*   **Unique Characteristics:**\n    \n    *   Sparse tensors and voxel grids dominate memory model\n    *   CUDA, Open3D, or MinkowskiEngine often used\n    *   Hard real-time constraints for robotics/ADAS\n\nLidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.\n\n**Lifecycle Stages:**\n\n*   **Sensor Input:** LiDAR frames streamed in real-time\n*   **Preprocessing:** Voxelization or range transformation into tensor-friendly formats\n*   **Tensor Pipeline:** Sparse CNNs, 3D convolutions, and attention modules process data\n*   **Temporal Fusion:** RNN or transformer-based modules optionally applied across frames\n*   **Postprocessing:** Generates semantic maps or bounding boxes\n*   **Sensor Fusion:** Optionally integrates radar or camera data for final outputs\n\n**Unique Characteristics:**\n\n*   Sparse tensors and voxel grids dominate memory model\n*   CUDA, Open3D, or MinkowskiEngine often used\n*   Hard real-time constraints for robotics/ADAS\n\n#### `llama.cpp` Execution Lifecycle (Quantized LLMs)\n\n*   `llama.cpp` is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Load:** GGUF model memory-mapped into RAM\n    *   **KV Cache Setup:** Pre-allocates attention buffers\n    *   **Embedding →→\\\\rightarrow Transformer Loop:** Sequentially executes transformer layers\n    *   **Sampling:** Next token is selected via greedy/top-k/top-p logic\n    *   **Tokenization:** Output string is constructed from sampled token IDs\n*   **Unique Characteristics:**\n    \n    *   Highly portable, CPU-optimized, extremely low memory usage\n    *   No dynamic graph, no scheduler, no intermediate representation\n    *   Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle\n\n`llama.cpp` is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.\n\n**Lifecycle Stages:**\n\n*   **Model Load:** GGUF model memory-mapped into RAM\n*   **KV Cache Setup:** Pre-allocates attention buffers\n*   **Embedding →→\\\\rightarrow Transformer Loop:** Sequentially executes transformer layers\n*   **Sampling:** Next token is selected via greedy/top-k/top-p logic\n*   **Tokenization:** Output string is constructed from sampled token IDs\n\n**Unique Characteristics:**\n\n*   Highly portable, CPU-optimized, extremely low memory usage\n*   No dynamic graph, no scheduler, no intermediate representation\n*   Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle\n\n#### TensorFlow Lite Execution Lifecycle\n\n*   TFLite uses a **FlatBuffer interpreter** architecture with optional delegates for acceleration.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Conversion:** TensorFlow →→\\\\rightarrow `.tflite` via `TFLiteConverter`\n    *   **FlatBuffer Load:** Model loaded with `Interpreter(model_path=...)`\n    *   **Tensor Allocation:** Input/output buffers allocated via `allocate_tensors()`\n    *   **Delegate Attachment (optional):** NNAPI, GPU, Hexagon delegate claims subgraphs\n    *   **Inference:** Static interpreter walks the computation graph\n    *   **Output Access:** Results extracted via `get_tensor()` APIs\n*   **Unique Characteristics:**\n    \n    *   Very compact format with zero-copy access\n    *   Delegate design separates concerns for CPU vs. accelerators\n    *   Strong ecosystem with tooling (e.g., Model Maker, Visualizer)\n\nTFLite uses a **FlatBuffer interpreter** architecture with optional delegates for acceleration.\n\n**Lifecycle Stages:**\n\n*   **Model Conversion:** TensorFlow →→\\\\rightarrow `.tflite` via `TFLiteConverter`\n*   **FlatBuffer Load:** Model loaded with `Interpreter(model_path=...)`\n*   **Tensor Allocation:** Input/output buffers allocated via `allocate_tensors()`\n*   **Delegate Attachment (optional):** NNAPI, GPU, Hexagon delegate claims subgraphs\n*   **Inference:** Static interpreter walks the computation graph\n*   **Output Access:** Results extracted via `get_tensor()` APIs\n\n**Unique Characteristics:**\n\n*   Very compact format with zero-copy access\n*   Delegate design separates concerns for CPU vs. accelerators\n*   Strong ecosystem with tooling (e.g., Model Maker, Visualizer)",
    "contentLength": 30010,
    "wordCount": 1895,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#runtime-specific-execution-lifecycles"
  },
  {
    "id": "ai-ml-runtimes-overview-63",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "Overview",
    "order": 63,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>This quick overview shows how <a href=\"https://github.com/pytorch/FBGEMM\"><code class=\"language-plaintext highlighter-rouge\">FBGEMM</code></a>, <a href=\"https://github.com/pytorch/QNNPACK\"><code class=\"language-plaintext highlighter-rouge\">QNNPACK</code></a>, and <a href=\"https://github.com/google/XNNPACK\"><code class=\"language-plaintext highlighter-rouge\">XNNPACK</code></a> fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:</p>\n\n    <ul>\n      <li><strong>Primary purpose</strong>: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.</li>\n      <li><strong>They do not</strong>: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.</li>\n      <li><strong>Where they fit</strong>: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.</li>\n    </ul>\n  </li>\n</ul>\n<p>This quick overview shows how <a href=\"https://github.com/pytorch/FBGEMM\"><code class=\"language-plaintext highlighter-rouge\">FBGEMM</code></a>, <a href=\"https://github.com/pytorch/QNNPACK\"><code class=\"language-plaintext highlighter-rouge\">QNNPACK</code></a>, and <a href=\"https://github.com/google/XNNPACK\"><code class=\"language-plaintext highlighter-rouge\">XNNPACK</code></a> fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:</p>\n<ul>\n      <li><strong>Primary purpose</strong>: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.</li>\n      <li><strong>They do not</strong>: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.</li>\n      <li><strong>Where they fit</strong>: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.</li>\n    </ul>",
    "contentMarkdown": "*   This quick overview shows how [`FBGEMM`](https://github.com/pytorch/FBGEMM), [`QNNPACK`](https://github.com/pytorch/QNNPACK), and [`XNNPACK`](https://github.com/google/XNNPACK) fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:\n    \n    *   **Primary purpose**: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.\n    *   **They do not**: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.\n    *   **Where they fit**: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.\n\nThis quick overview shows how [`FBGEMM`](https://github.com/pytorch/FBGEMM), [`QNNPACK`](https://github.com/pytorch/QNNPACK), and [`XNNPACK`](https://github.com/google/XNNPACK) fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:\n\n*   **Primary purpose**: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.\n*   **They do not**: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.\n*   **Where they fit**: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.",
    "contentLength": 2034,
    "wordCount": 171,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#overview"
  },
  {
    "id": "ai-ml-runtimes-fbgemm-by-meta-server-cpus-64",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "FBGEMM (by Meta,; Server CPUs)",
    "order": 64,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><strong>Target platforms</strong>: x86-64 server/desktop CPUs with SIMD (AVX2, AVX-512; newer stacks can leverage AMX on recent Intel parts via higher-level integrations).</li>\n  <li><strong>Data types and quant schemes</strong>: <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">uint8</code> activations (affine per-tensor), <code class=\"language-plaintext highlighter-rouge\">int8</code> weights (often symmetric per-channel). 32-bit accumulation with requantization to <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">uint8</code> or dequantization to float. Also provides row-wise and 4-bit embedding quantization utilities for recommendation models.</li>\n  <li><strong>Operator coverage</strong>: Linear/GEMM and convolution (including groupwise), prepacked weight paths; optimized im2col/IGEMM; embedding bag and sparse length ops for recsys.</li>\n  <li><strong>Optimizations</strong>: Weight pre-packing into cache-friendly blocked layouts; vectorized micro-kernels; cache- and register-blocking; fused bias+activation+requant paths; threadpool parallelism.</li>\n  <li>\n    <p><strong>Typical use</strong>: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static <code class=\"language-plaintext highlighter-rouge\">int8</code> conv/linear). Best when you need maximum x86 performance for <code class=\"language-plaintext highlighter-rouge\">int8</code> inference.\n; Mobile CPUs)</p>\n  </li>\n  <li><strong>Target platforms</strong>: ARM/ARM64 mobile CPUs with NEON (Android/iOS); designed for low-power cores.</li>\n  <li><strong>Data types and quant schemes</strong>: <code class=\"language-plaintext highlighter-rouge\">uint8</code>/<code class=\"language-plaintext highlighter-rouge\">int8</code> activations (affine per-tensor), <code class=\"language-plaintext highlighter-rouge\">int8</code> per-channel weights; 32-bit accumulation with efficient requantization.</li>\n  <li><strong>Operator coverage</strong>: Quantized convolution/IGEMM, depthwise conv, deconvolution, fully connected (GEMM), pooling, various activation/elementwise ops.</li>\n  <li><strong>Optimizations</strong>: NHWC-friendly kernels; careful cache use for small batch/small filters; per-thread micro-kernels; fused post-ops to reduce memory traffic.</li>\n  <li><strong>Typical use</strong>: PyTorch Mobile’s quantized back end on ARM; good default for mobile <code class=\"language-plaintext highlighter-rouge\">int8</code> CNNs and fully connected layers where you need predictable latency on phones.</li>\n</ul>\n<p><strong>Typical use</strong>: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static <code class=\"language-plaintext highlighter-rouge\">int8</code> conv/linear). Best when you need maximum x86 performance for <code class=\"language-plaintext highlighter-rouge\">int8</code> inference.\n; Mobile CPUs)</p>",
    "contentMarkdown": "*   **Target platforms**: x86-64 server/desktop CPUs with SIMD (AVX2, AVX-512; newer stacks can leverage AMX on recent Intel parts via higher-level integrations).\n*   **Data types and quant schemes**: `int8`/`uint8` activations (affine per-tensor), `int8` weights (often symmetric per-channel). 32-bit accumulation with requantization to `int8`/`uint8` or dequantization to float. Also provides row-wise and 4-bit embedding quantization utilities for recommendation models.\n*   **Operator coverage**: Linear/GEMM and convolution (including groupwise), prepacked weight paths; optimized im2col/IGEMM; embedding bag and sparse length ops for recsys.\n*   **Optimizations**: Weight pre-packing into cache-friendly blocked layouts; vectorized micro-kernels; cache- and register-blocking; fused bias+activation+requant paths; threadpool parallelism.\n*   **Typical use**: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static `int8` conv/linear). Best when you need maximum x86 performance for `int8` inference. ; Mobile CPUs)\n    \n*   **Target platforms**: ARM/ARM64 mobile CPUs with NEON (Android/iOS); designed for low-power cores.\n*   **Data types and quant schemes**: `uint8`/`int8` activations (affine per-tensor), `int8` per-channel weights; 32-bit accumulation with efficient requantization.\n*   **Operator coverage**: Quantized convolution/IGEMM, depthwise conv, deconvolution, fully connected (GEMM), pooling, various activation/elementwise ops.\n*   **Optimizations**: NHWC-friendly kernels; careful cache use for small batch/small filters; per-thread micro-kernels; fused post-ops to reduce memory traffic.\n*   **Typical use**: PyTorch Mobile’s quantized back end on ARM; good default for mobile `int8` CNNs and fully connected layers where you need predictable latency on phones.\n\n**Typical use**: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static `int8` conv/linear). Best when you need maximum x86 performance for `int8` inference. ; Mobile CPUs)",
    "contentLength": 2997,
    "wordCount": 246,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#fbgemm-(by-meta,;-server-cpus)"
  },
  {
    "id": "ai-ml-runtimes-xnnpack-by-google-both-server-and-mobile-cpus-65",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "XNNPACK (by Google,; Both Server and Mobile CPUs)",
    "order": 65,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><strong>Target platforms</strong>: ARM/ARM64, x86-64, and WebAssembly (WASM); broadly portable and actively maintained.</li>\n  <li><strong>Data types and quant schemes</strong>: Strong <code class=\"language-plaintext highlighter-rouge\">float32</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code> coverage; mature QS8/QU8 (signed/unsigned 8-bit) inference for conv/GEMM/elementwise with per-channel weight scales. 32-bit accumulation and precise requantization.</li>\n  <li><strong>Operator coverage</strong>: Convolution (standard and depthwise), fully connected, pooling, deconvolution, elementwise math, softmax, activation functions, resize, etc.</li>\n  <li><strong>Optimizations</strong>: Handwritten micro-kernels per ISA (NEON/AVX/AVX512), NHWC dataflow, weight prepacking, GEMM/IGEMM families with cache-aware blocking, parallel work-stealing.</li>\n  <li><strong>Typical use</strong>: TensorFlow Lite’s XNNPACK delegate on CPU (float and <code class=\"language-plaintext highlighter-rouge\">int8</code>), and increasingly as a CPU backend in other frameworks for both float and quantized inference.</li>\n</ul>",
    "contentMarkdown": "*   **Target platforms**: ARM/ARM64, x86-64, and WebAssembly (WASM); broadly portable and actively maintained.\n*   **Data types and quant schemes**: Strong `float32`/`float16`/`bfloat16` coverage; mature QS8/QU8 (signed/unsigned 8-bit) inference for conv/GEMM/elementwise with per-channel weight scales. 32-bit accumulation and precise requantization.\n*   **Operator coverage**: Convolution (standard and depthwise), fully connected, pooling, deconvolution, elementwise math, softmax, activation functions, resize, etc.\n*   **Optimizations**: Handwritten micro-kernels per ISA (NEON/AVX/AVX512), NHWC dataflow, weight prepacking, GEMM/IGEMM families with cache-aware blocking, parallel work-stealing.\n*   **Typical use**: TensorFlow Lite’s XNNPACK delegate on CPU (float and `int8`), and increasingly as a CPU backend in other frameworks for both float and quantized inference.",
    "contentLength": 1212,
    "wordCount": 101,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#xnnpack-(by-google,;-both-server-and-mobile-cpus)"
  },
  {
    "id": "ai-ml-runtimes-what-to-choose-when-66",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "What to Choose When?",
    "order": 66,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li><strong>PyTorch (desktop/server CPU)</strong>: <code class=\"language-plaintext highlighter-rouge\">FBGEMM</code> is the usual backend for quantized ops; dynamic quantized Linear/LSTM also route here.</li>\n  <li><strong>PyTorch Mobile (ARM)</strong>: <code class=\"language-plaintext highlighter-rouge\">QNNPACK</code> is the historical default for quantized ops; some float operators can use XNNPACK.</li>\n  <li><strong>TensorFlow Lite (CPU)</strong>: <code class=\"language-plaintext highlighter-rouge\">XNNPACK</code> delegate accelerates many <code class=\"language-plaintext highlighter-rouge\">float32</code> and <code class=\"language-plaintext highlighter-rouge\">int8</code> ops; the interpreter falls back to reference kernels when needed.</li>\n  <li><strong>ONNX Runtime (CPU)</strong>: Uses its own CPU kernels by default, but can be built/integrated with these libraries in certain configurations; on mobile, builds commonly leverage <code class=\"language-plaintext highlighter-rouge\">XNNPACK</code>.</li>\n</ul>",
    "contentMarkdown": "*   **PyTorch (desktop/server CPU)**: `FBGEMM` is the usual backend for quantized ops; dynamic quantized Linear/LSTM also route here.\n*   **PyTorch Mobile (ARM)**: `QNNPACK` is the historical default for quantized ops; some float operators can use XNNPACK.\n*   **TensorFlow Lite (CPU)**: `XNNPACK` delegate accelerates many `float32` and `int8` ops; the interpreter falls back to reference kernels when needed.\n*   **ONNX Runtime (CPU)**: Uses its own CPU kernels by default, but can be built/integrated with these libraries in certain configurations; on mobile, builds commonly leverage `XNNPACK`.",
    "contentLength": 1025,
    "wordCount": 84,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#what-to-choose-when?"
  },
  {
    "id": "ai-ml-runtimes-design-notes-67",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "Design Notes",
    "order": 67,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li><strong>Quant params are part of tensors</strong>: Kernels need correct scales/zero-points. For per-channel weights, pass channel-wise scales; activations are usually per-tensor.</li>\n  <li><strong>Accumulation width</strong>: 8-bit multiply-accumulates are summed into 32-bit accumulators to avoid overflow, then requantized. Watch for saturation when chaining ops.</li>\n  <li><strong>Prepack once</strong>: Pre-pack and reuse weights to avoid paying packing costs per inference. Many APIs expose prepacked weight objects.</li>\n  <li><strong>Layout matters</strong>: These libraries typically prefer NHWC for conv on mobile; mismatched layouts cause costly transposes.</li>\n  <li><strong>Dynamic vs static quant</strong>: Dynamic quantizes activations on-the-fly (common for Linear/LSTM), static uses calibration ranges. FBGEMM has strong dynamic Linear/LSTM paths.</li>\n  <li><strong>Activation ranges</strong>: Calibrate representative inputs to pick good scales and avoid clamp-heavy requantization.</li>\n</ul>",
    "contentMarkdown": "*   **Quant params are part of tensors**: Kernels need correct scales/zero-points. For per-channel weights, pass channel-wise scales; activations are usually per-tensor.\n*   **Accumulation width**: 8-bit multiply-accumulates are summed into 32-bit accumulators to avoid overflow, then requantized. Watch for saturation when chaining ops.\n*   **Prepack once**: Pre-pack and reuse weights to avoid paying packing costs per inference. Many APIs expose prepacked weight objects.\n*   **Layout matters**: These libraries typically prefer NHWC for conv on mobile; mismatched layouts cause costly transposes.\n*   **Dynamic vs static quant**: Dynamic quantizes activations on-the-fly (common for Linear/LSTM), static uses calibration ranges. FBGEMM has strong dynamic Linear/LSTM paths.\n*   **Activation ranges**: Calibrate representative inputs to pick good scales and avoid clamp-heavy requantization.",
    "contentLength": 1025,
    "wordCount": 115,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#design-notes"
  },
  {
    "id": "ai-ml-runtimes-comparative-analysis-68",
    "articleSlug": "ml-runtimes",
    "articleTitle": "ML Runtimes",
    "category": "Miscellaneous",
    "chapter": "Related: CPU Operator Libraries/Backends",
    "title": "Comparative Analysis",
    "order": 68,
    "orderInChapter": 6,
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FBGEMM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QNNPACK</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>XNNPACK</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary target</td>\n<td class=\"tg-tleft-valign-first\">x86-64 servers/desktops</td>\n<td class=\"tg-tleft-valign-first\">ARM/ARM64 mobile</td>\n<td class=\"tg-tleft-valign-second\">ARM/ARM64, x86-64, WASM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best precision</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (server)</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (mobile)</td>\n<td class=\"tg-tleft-valign-second\">`float32`/`float16` plus `int8`</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical consumers</td>\n<td class=\"tg-tleft-valign-first\">PyTorch quant (server)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch Mobile quant</td>\n<td class=\"tg-tleft-valign-second\">TFLite delegate; some PyTorch CPU paths</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conv layout</td>\n<td class=\"tg-tleft-valign-first\">NCHW/NHWC with prepack</td>\n<td class=\"tg-tleft-valign-first\">NHWC</td>\n<td class=\"tg-tleft-valign-second\">NHWC</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Weight handling</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-second\">Prepacked per-channel `int8`</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FBGEMM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QNNPACK</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>XNNPACK</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary target</td>\n<td class=\"tg-tleft-valign-first\">x86-64 servers/desktops</td>\n<td class=\"tg-tleft-valign-first\">ARM/ARM64 mobile</td>\n<td class=\"tg-tleft-valign-second\">ARM/ARM64, x86-64, WASM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best precision</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (server)</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (mobile)</td>\n<td class=\"tg-tleft-valign-second\">`float32`/`float16` plus `int8`</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical consumers</td>\n<td class=\"tg-tleft-valign-first\">PyTorch quant (server)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch Mobile quant</td>\n<td class=\"tg-tleft-valign-second\">TFLite delegate; some PyTorch CPU paths</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conv layout</td>\n<td class=\"tg-tleft-valign-first\">NCHW/NHWC with prepack</td>\n<td class=\"tg-tleft-valign-first\">NHWC</td>\n<td class=\"tg-tleft-valign-second\">NHWC</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Weight handling</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-second\">Prepacked per-channel `int8`</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Attribute**\n\n**FBGEMM**\n\n**QNNPACK**\n\n**XNNPACK**\n\nPrimary target\n\nx86-64 servers/desktops\n\nARM/ARM64 mobile\n\nARM/ARM64, x86-64, WASM\n\nBest precision\n\n\\`int8\\` quant (server)\n\n\\`int8\\` quant (mobile)\n\n\\`float32\\`/\\`float16\\` plus \\`int8\\`\n\nTypical consumers\n\nPyTorch quant (server)\n\nPyTorch Mobile quant\n\nTFLite delegate; some PyTorch CPU paths\n\nConv layout\n\nNCHW/NHWC with prepack\n\nNHWC\n\nNHWC\n\nWeight handling\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\n**Attribute**\n\n**FBGEMM**\n\n**QNNPACK**\n\n**XNNPACK**\n\nPrimary target\n\nx86-64 servers/desktops\n\nARM/ARM64 mobile\n\nARM/ARM64, x86-64, WASM\n\nBest precision\n\n\\`int8\\` quant (server)\n\n\\`int8\\` quant (mobile)\n\n\\`float32\\`/\\`float16\\` plus \\`int8\\`\n\nTypical consumers\n\nPyTorch quant (server)\n\nPyTorch Mobile quant\n\nTFLite delegate; some PyTorch CPU paths\n\nConv layout\n\nNCHW/NHWC with prepack\n\nNHWC\n\nNHWC\n\nWeight handling\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`",
    "contentLength": 3227,
    "wordCount": 112,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis"
  }
]