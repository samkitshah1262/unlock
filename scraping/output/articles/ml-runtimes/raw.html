<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • ML Runtimes</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/ml-runtimes/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fml-runtimes&amp;ers=2"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWmCJFzv5FKZRwms_6R5vJ2EQiMRwhekqiEcQh776cmZ7j4yMQsmSEkUuxasoHun746zo6mpW3H3Sz7rxe2lCVvRpJ8VwGygLcLPDxzW4DCmxIjVDxc2MIo8AEYxKphsAAKNVyl0w==?fccs=W1siQUtzUm9sOGhZZG1jd21WTWN6TUh4d0RITXVVYWFIOWZuNUJJRXlCTUVqLU1WdlAzV2FfRkpMWVRBcjNfcU9IdTNIS2t0T3VlaTU4RDJqbEo3RklCak1Odlk5ZHlONTdzUjB1bE1vcHZpX3FsdDJUQWV6NWNqNEhkNkt5OFowNGtzSW9KZU9BQmhXdFBkc0NvUHgxX3NRSFFESkIxaXdibGJ3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwMDksNDIxMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9tbC1ydW50aW1lcy8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxUa6ghoD97IDxCtz4yG_MHUyz8-4uae-dVBg6m_DcEdFedWI3xJXi6rwYUbd2yMMJGnlLKTcBCmU7kUB4-ifWfVHbQksC1S02CQ7NeyJAfCh65R8rFprwBsjRcdOgWUf-rv4bjanw==?fccs=W1siQUtzUm9sOGhZZG1jd21WTWN6TUh4d0RITXVVYWFIOWZuNUJJRXlCTUVqLU1WdlAzV2FfRkpMWVRBcjNfcU9IdTNIS2t0T3VlaTU4RDJqbEo3RklCak1Odlk5ZHlONTdzUjB1bE1vcHZpX3FsdDJUQWV6NWNqNEhkNkt5OFowNGtzSW9KZU9BQmhXdFBkc0NvUHgxX3NRSFFESkIxaXdibGJ3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwMDksNjEwMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5XSxudWxsLDIsbnVsbCwiZW4iXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvbWwtcnVudGltZXMvIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI5LCJmYWxzZSJdXV0"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxVMrzocExCeEyr0ld-9HoSqob829nayhibIjASUSlvZ7gaTVCFGiDG7nGaHcSWdZddKuAO8FBbB6iud4RBaNkCrpssj6xRzno3_tflEwlc9ycR-wiCHFNLMcHd9driAggSy7Osikw==?fccs=W1siQUtzUm9sOGhZZG1jd21WTWN6TUh4d0RITXVVYWFIOWZuNUJJRXlCTUVqLU1WdlAzV2FfRkpMWVRBcjNfcU9IdTNIS2t0T3VlaTU4RDJqbEo3RklCak1Odlk5ZHlONTdzUjB1bE1vcHZpX3FsdDJUQWV6NWNqNEhkNkt5OFowNGtzSW9KZU9BQmhXdFBkc0NvUHgxX3NRSFFESkIxaXdibGJ3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjMwMTAsNDgwMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5LDZdLG51bGwsMixudWxsLCJlbiIsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLDFdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9tbC1ydW50aW1lcy8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjksImZhbHNlIl1dXQ"></script></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • ML Runtimes</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#architecture-overview-of-on-device-ml-runtimes" id="markdown-toc-architecture-overview-of-on-device-ml-runtimes">Architecture Overview of On-Device ML Runtimes</a>    <ul>
      <li><a href="#common-architectural-layers" id="markdown-toc-common-architectural-layers">Common Architectural Layers</a></li>
      <li><a href="#architecture-by-runtime" id="markdown-toc-architecture-by-runtime">Architecture by Runtime</a>        <ul>
          <li><a href="#tensorrt" id="markdown-toc-tensorrt">TensorRT</a></li>
          <li><a href="#core-ml" id="markdown-toc-core-ml">Core ML</a></li>
          <li><a href="#mlx-apple-mlx" id="markdown-toc-mlx-apple-mlx">MLX (Apple MLX)</a></li>
          <li><a href="#onnx-runtime" id="markdown-toc-onnx-runtime">ONNX Runtime</a></li>
          <li><a href="#executorch" id="markdown-toc-executorch">ExecuTorch</a></li>
          <li><a href="#lidartlm" id="markdown-toc-lidartlm">LidarTLM</a></li>
          <li><a href="#llamacpp" id="markdown-toc-llamacpp"><code class="language-plaintext Highlighter-rouge">llama.cpp</code></a></li>
          <li><a href="#tensorflow-lite--serving" id="markdown-toc-tensorflow-lite--serving">TensorFlow Lite / Serving</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#tensorrt-deep-dive" id="markdown-toc-tensorrt-deep-dive">TensorRT Deep Dive</a>    <ul>
      <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
      <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
      <li><a href="#implementation-details" id="markdown-toc-implementation-details">Implementation Details</a></li>
      <li><a href="#pros-and-cons" id="markdown-toc-pros-and-cons">Pros and Cons</a></li>
      <li><a href="#example-workflow" id="markdown-toc-example-workflow">Example Workflow</a></li>
      <li><a href="#suitable-applications" id="markdown-toc-suitable-applications">Suitable Applications</a></li>
    </ul>
  </li>
  <li><a href="#core-ml-deep-dive" id="markdown-toc-core-ml-deep-dive">Core ML Deep Dive</a>    <ul>
      <li><a href="#overview-1" id="markdown-toc-overview-1">Overview</a></li>
      <li><a href="#architecture-1" id="markdown-toc-architecture-1">Architecture</a></li>
      <li><a href="#supported-model-types" id="markdown-toc-supported-model-types">Supported Model Types</a></li>
      <li><a href="#implementation-details-1" id="markdown-toc-implementation-details-1">Implementation Details</a></li>
      <li><a href="#pros-and-cons-1" id="markdown-toc-pros-and-cons-1">Pros and Cons</a></li>
      <li><a href="#example-code-snippet" id="markdown-toc-example-code-snippet">Example Code Snippet</a></li>
    </ul>
  </li>
  <li><a href="#mlx-deep-dive" id="markdown-toc-mlx-deep-dive">MLX Deep Dive</a>    <ul>
      <li><a href="#overview-2" id="markdown-toc-overview-2">Overview</a></li>
      <li><a href="#architecture-2" id="markdown-toc-architecture-2">Architecture</a></li>
      <li><a href="#implementation-details-2" id="markdown-toc-implementation-details-2">Implementation Details</a></li>
      <li><a href="#pros-and-cons-2" id="markdown-toc-pros-and-cons-2">Pros and Cons</a></li>
      <li><a href="#example-code-snippet-1" id="markdown-toc-example-code-snippet-1">Example Code Snippet</a></li>
    </ul>
  </li>
  <li><a href="#onnx-runtime-deep-dive" id="markdown-toc-onnx-runtime-deep-dive">ONNX Runtime Deep Dive</a>    <ul>
      <li><a href="#overview-3" id="markdown-toc-overview-3">Overview</a></li>
      <li><a href="#architecture-3" id="markdown-toc-architecture-3">Architecture</a></li>
      <li><a href="#implementation-details-3" id="markdown-toc-implementation-details-3">Implementation Details</a></li>
      <li><a href="#pros-and-cons-3" id="markdown-toc-pros-and-cons-3">Pros and Cons</a></li>
      <li><a href="#example-code-snippet-python" id="markdown-toc-example-code-snippet-python">Example Code Snippet (Python)</a></li>
      <li><a href="#use-in-edge--on-device-scenarios" id="markdown-toc-use-in-edge--on-device-scenarios">Use in Edge / On-Device Scenarios</a></li>
    </ul>
  </li>
  <li><a href="#executorch-deep-dive" id="markdown-toc-executorch-deep-dive">ExecuTorch Deep Dive</a>    <ul>
      <li><a href="#overview-4" id="markdown-toc-overview-4">Overview</a></li>
      <li><a href="#architecture-4" id="markdown-toc-architecture-4">Architecture</a></li>
      <li><a href="#implementation-details-4" id="markdown-toc-implementation-details-4">Implementation Details</a></li>
      <li><a href="#pros-and-cons-4" id="markdown-toc-pros-and-cons-4">Pros and Cons</a></li>
      <li><a href="#example-workflow-1" id="markdown-toc-example-workflow-1">Example Workflow</a></li>
      <li><a href="#suitable-applications-1" id="markdown-toc-suitable-applications-1">Suitable Applications</a></li>
    </ul>
  </li>
  <li><a href="#lidartlm-deep-dive" id="markdown-toc-lidartlm-deep-dive">LidarTLM Deep Dive</a>    <ul>
      <li><a href="#overview-5" id="markdown-toc-overview-5">Overview</a></li>
      <li><a href="#architecture-5" id="markdown-toc-architecture-5">Architecture</a></li>
      <li><a href="#implementation-details-5" id="markdown-toc-implementation-details-5">Implementation Details</a></li>
      <li><a href="#pros-and-cons-5" id="markdown-toc-pros-and-cons-5">Pros and Cons</a></li>
      <li><a href="#example-pseudocode-flow" id="markdown-toc-example-pseudocode-flow">Example Pseudocode Flow</a></li>
      <li><a href="#suitable-applications-2" id="markdown-toc-suitable-applications-2">Suitable Applications</a></li>
      <li><a href="#llamacpp-deep-dive" id="markdown-toc-llamacpp-deep-dive"><code class="language-plaintext Highlighter-rouge">llama.cpp</code> Deep Dive</a></li>
      <li><a href="#overview-6" id="markdown-toc-overview-6">Overview</a></li>
      <li><a href="#architecture-6" id="markdown-toc-architecture-6">Architecture</a></li>
      <li><a href="#implementation-details-6" id="markdown-toc-implementation-details-6">Implementation Details</a></li>
      <li><a href="#pros-and-cons-6" id="markdown-toc-pros-and-cons-6">Pros and Cons</a></li>
      <li><a href="#example-cli-inference" id="markdown-toc-example-cli-inference">Example CLI Inference</a></li>
      <li><a href="#suitable-applications-3" id="markdown-toc-suitable-applications-3">Suitable Applications</a></li>
    </ul>
  </li>
  <li><a href="#tensorflow-lite--tensorflow-serving-deep-dive" id="markdown-toc-tensorflow-lite--tensorflow-serving-deep-dive">TensorFlow Lite / TensorFlow Serving Deep Dive</a>    <ul>
      <li><a href="#overview-7" id="markdown-toc-overview-7">Overview</a></li>
      <li><a href="#tensorflow-lite-architecture" id="markdown-toc-tensorflow-lite-architecture">TensorFlow Lite Architecture</a></li>
      <li><a href="#implementation-details-7" id="markdown-toc-implementation-details-7">Implementation Details</a></li>
      <li><a href="#tensorflow-serving-short-overview" id="markdown-toc-tensorflow-serving-short-overview">TensorFlow Serving (Short Overview)</a></li>
      <li><a href="#pros-and-cons-7" id="markdown-toc-pros-and-cons-7">Pros and Cons</a></li>
      <li><a href="#example-inference-python---tflite" id="markdown-toc-example-inference-python---tflite">Example Inference (Python - TFLite)</a></li>
      <li><a href="#suitable-applications-4" id="markdown-toc-suitable-applications-4">Suitable Applications</a></li>
      <li><a href="#comparative-analysis" id="markdown-toc-comparative-analysis">Comparative Analysis</a>        <ul>
          <li><a href="#general-characteristics" id="markdown-toc-general-characteristics">General Characteristics</a></li>
          <li><a href="#model-formats-and-conversion" id="markdown-toc-model-formats-and-conversion">Model Formats and Conversion</a></li>
          <li><a href="#execution-model-and-hardware-support" id="markdown-toc-execution-model-and-hardware-support">Execution Model and Hardware Support</a></li>
          <li><a href="#optimization-size-and-constraints" id="markdown-toc-optimization-size-and-constraints">Optimization, Size, and Constraints</a></li>
          <li><a href="#flexibility-debugging-and-ecosystem" id="markdown-toc-flexibility-debugging-and-ecosystem">Flexibility, Debugging, and Ecosystem</a></li>
        </ul>
      </li>
      <li><a href="#comparative-summary-and-guidance" id="markdown-toc-comparative-summary-and-guidance">Comparative Summary and Guidance</a>        <ul>
          <li><a href="#feature-comparison-table" id="markdown-toc-feature-comparison-table">Feature Comparison Table</a></li>
          <li><a href="#strengths-by-runtime" id="markdown-toc-strengths-by-runtime">Strengths by Runtime</a></li>
        </ul>
      </li>
      <li><a href="#runtime-selection-guidance" id="markdown-toc-runtime-selection-guidance">Runtime Selection Guidance</a></li>
      <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
    </ul>
  </li>
  <li><a href="#related-serialization-formats-across-runtimes" id="markdown-toc-related-serialization-formats-across-runtimes">Related: Serialization Formats Across Runtimes</a>    <ul>
      <li><a href="#protocol-buffers-protobuf" id="markdown-toc-protocol-buffers-protobuf">Protocol Buffers (Protobuf)</a></li>
      <li><a href="#flatbuffer" id="markdown-toc-flatbuffer">FlatBuffer</a></li>
      <li><a href="#gguf-gpt-generated-ggml-unified-format" id="markdown-toc-gguf-gpt-generated-ggml-unified-format">GGUF (GPT-generated GGML Unified Format)</a></li>
      <li><a href="#bytecode-format-executorch" id="markdown-toc-bytecode-format-executorch">Bytecode Format (ExecuTorch)</a></li>
      <li><a href="#comparative-analysis-1" id="markdown-toc-comparative-analysis-1">Comparative Analysis</a></li>
    </ul>
  </li>
  <li><a href="#model-execution-lifecycle-across-ml-runtimes" id="markdown-toc-model-execution-lifecycle-across-ml-runtimes">Model Execution Lifecycle Across ML Runtimes</a>    <ul>
      <li><a href="#general-workflow-from-model-to-inference" id="markdown-toc-general-workflow-from-model-to-inference">General Workflow: from Model to Inference</a>        <ul>
          <li><a href="#model-training" id="markdown-toc-model-training">Model Training</a></li>
          <li><a href="#model-conversion" id="markdown-toc-model-conversion">Model Conversion</a></li>
          <li><a href="#model-loading" id="markdown-toc-model-loading">Model Loading</a></li>
          <li><a href="#memory-allocation" id="markdown-toc-memory-allocation">Memory Allocation</a></li>
          <li><a href="#inference-execution" id="markdown-toc-inference-execution">Inference Execution</a></li>
          <li><a href="#postprocessing--output" id="markdown-toc-postprocessing--output">Postprocessing &amp; Output</a></li>
          <li><a href="#lifecycle-optimization-optional-but-critical" id="markdown-toc-lifecycle-optimization-optional-but-critical">Lifecycle Optimization (Optional but Critical)</a></li>
        </ul>
      </li>
      <li><a href="#runtime-specific-execution-lifecycles" id="markdown-toc-runtime-specific-execution-lifecycles">Runtime-Specific Execution Lifecycles</a>        <ul>
          <li><a href="#tensorrt-execution-lifecycle-nvidia-gpus" id="markdown-toc-tensorrt-execution-lifecycle-nvidia-gpus">TensorRT Execution Lifecycle (NVIDIA GPUs)</a></li>
          <li><a href="#core-ml-execution-lifecycle-apple-platforms" id="markdown-toc-core-ml-execution-lifecycle-apple-platforms">Core ML Execution Lifecycle (Apple Platforms)</a></li>
          <li><a href="#mlx-execution-lifecycle-apple-silicon" id="markdown-toc-mlx-execution-lifecycle-apple-silicon">MLX Execution Lifecycle (Apple Silicon)</a></li>
          <li><a href="#onnx-runtime-execution-lifecycle" id="markdown-toc-onnx-runtime-execution-lifecycle">ONNX Runtime Execution Lifecycle</a></li>
          <li><a href="#executorch-execution-lifecycle-mcuembedded-focus" id="markdown-toc-executorch-execution-lifecycle-mcuembedded-focus">ExecuTorch Execution Lifecycle (MCU/Embedded Focus)</a></li>
          <li><a href="#lidartlm-execution-lifecycle-lidar-focused-embedded-stacks" id="markdown-toc-lidartlm-execution-lifecycle-lidar-focused-embedded-stacks">LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)</a></li>
          <li><a href="#llamacpp-execution-lifecycle-quantized-llms" id="markdown-toc-llamacpp-execution-lifecycle-quantized-llms"><code class="language-plaintext Highlighter-rouge">llama.cpp</code> Execution Lifecycle (Quantized LLMs)</a></li>
          <li><a href="#tensorflow-lite-execution-lifecycle" id="markdown-toc-tensorflow-lite-execution-lifecycle">TensorFlow Lite Execution Lifecycle</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#related-cpu-operator-librariesbackends" id="markdown-toc-related-cpu-operator-librariesbackends">Related: CPU Operator Libraries/Backends</a>    <ul>
      <li><a href="#overview-8" id="markdown-toc-overview-8">Overview</a></li>
      <li><a href="#fbgemm-by-meta-server-cpus" id="markdown-toc-fbgemm-by-meta-server-cpus"><code class="language-plaintext Highlighter-rouge">FBGEMM</code> (by Meta,; Server CPUs)</a></li>
      <li><a href="#xnnpack-by-google-both-server-and-mobile-cpus" id="markdown-toc-xnnpack-by-google-both-server-and-mobile-cpus"><code class="language-plaintext Highlighter-rouge">XNNPACK</code> (by Google,; Both Server and Mobile CPUs)</a></li>
      <li><a href="#what-to-choose-when" id="markdown-toc-what-to-choose-when">What to Choose When?</a></li>
      <li><a href="#design-notes" id="markdown-toc-design-notes">Design Notes</a></li>
      <li><a href="#comparative-analysis-2" id="markdown-toc-comparative-analysis-2">Comparative Analysis</a></li>
    </ul>
  </li>
  <li><a href="#further-reading" id="markdown-toc-further-reading">Further Reading</a></li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>As AI becomes increasingly integral to modern software applications, deploying models directly on devices—such as smartphones, embedded systems, wearables, and edge computing nodes—has gained prominence. This approach, known as <strong>on-device machine learning</strong>, enables faster inference, improved privacy, offline capabilities, and lower latency compared to cloud-based alternatives.</p>
  </li>
  <li>
    <p>Several runtimes/inference engines have been developed to facilitate the efficient execution of ML models on diverse hardware architectures. These runtimes vary significantly in terms of platform compatibility, supported model formats, execution optimizations, and hardware acceleration. This primer covers a detailed comparison of key ML runtimes that support on-device inference:</p>

    <ul>
      <li>TensorRT</li>
      <li>Core ML</li>
      <li>MLX (Apple MLX)</li>
      <li>ONNX Runtime</li>
      <li>ExecuTorch</li>
      <li>LidarTLM</li>
      <li><code class="language-plaintext highlighter-rouge">llama.cpp</code></li>
      <li>TensorFlow Lite / TensorFlow Serving</li>
    </ul>
  </li>
  <li>
    <p>This primer includes both general-purpose and specialized runtimes, ranging from Core ML and TensorFlow Lite to transformer-specific tools like <code class="language-plaintext highlighter-rouge">llama.cpp</code> and GPU-optimized engines such as TensorRT.</p>
  </li>
</ul>

<h2 id="architecture-overview-of-on-device-ml-runtimes">Architecture Overview of On-Device ML Runtimes</h2>

<ul>
  <li>On-device machine learning runtimes are engineered to execute pre-trained models efficiently within the constraints of mobile devices, embedded platforms, and personal computers. Despite the diversity of runtimes, they typically share core architectural components that manage model parsing, hardware abstraction, and execution flow.</li>
  <li>This section outlines common architectural patterns and then provides architecture summaries for each runtime discussed in this primer.</li>
</ul>

<h3 id="common-architectural-layers">Common Architectural Layers</h3>

<ul>
  <li>
    <p>Most on-device ML runtimes follow a layered architecture consisting of the following components:</p>

    <ul>
      <li>
        <p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class="language-plaintext highlighter-rouge">.mlmodel</code>, <code class="language-plaintext highlighter-rouge">.tflite</code>, <code class="language-plaintext highlighter-rouge">.onnx</code>, <code class="language-plaintext highlighter-rouge">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>
      </li>
      <li>
        <p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class="language-plaintext highlighter-rouge">.onnx</code>) and TensorFlow (<code class="language-plaintext highlighter-rouge">.pb</code>) models.</p>
      </li>
      <li>
        <p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>
      </li>
      <li>
        <p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>
      </li>
      <li>
        <p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>
      </li>
      <li>
        <p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="architecture-by-runtime">Architecture by Runtime</h3>

<h4 id="tensorrt">TensorRT</h4>

<ul>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.plan</code> (TensorRT Engine)</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Accepts models in ONNX, TensorFlow, or Caffe formats</li>
      <li>Optimizes and compiles model into a serialized CUDA engine (<code class="language-plaintext highlighter-rouge">.plan</code>)</li>
      <li>Engine executes directly via CUDA on supported NVIDIA GPUs</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: NVIDIA GPUs (desktop, embedded, server)</li>
  <li><strong>Backend Design</strong>: Layer fusion, kernel autotuning, <code class="language-plaintext highlighter-rouge">int8</code>/<code class="language-plaintext highlighter-rouge">float16</code> quantization, Tensor Cores</li>
  <li><strong>Strengths</strong>: Extreme inference speed on NVIDIA hardware, minimal latency, quantization support</li>
  <li><strong>Weaknesses</strong>: GPU-only, requires CUDA, less flexible for model updates at runtime</li>
</ul>

<h4 id="core-ml">Core ML</h4>

<ul>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.mlmodel</code>, optionally converted from other formats using <code class="language-plaintext highlighter-rouge">coremltools</code></li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Model is compiled into a Core ML model package (<code class="language-plaintext highlighter-rouge">.mlmodelc</code>)</li>
      <li>Uses internal execution graph</li>
      <li>Runtime determines target hardware (CPU, GPU, or ANE) dynamically</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: CPU, GPU, Apple Neural Engine (ANE)</li>
  <li><strong>Backend Design</strong>: Proprietary graph engine, no direct user-accessible IR</li>
  <li><strong>Strengths</strong>: Seamless Apple integration, high-level API, automatic hardware optimization</li>
  <li><strong>Weaknesses</strong>: Apple-platform only, opaque architecture, limited transparency for debugging</li>
</ul>

<h4 id="mlx-apple-mlx">MLX (Apple MLX)</h4>

<ul>
  <li><strong>Model Format</strong>: Python-based tensor operations with PyTorch-like syntax</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Eager mode and graph execution both supported</li>
      <li>Uses Metal Performance Shaders and ANE backend where possible</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: Primarily Apple Silicon (M-series CPU, GPU, ANE)</li>
  <li><strong>Backend Design</strong>: Dynamic execution engine; uses MLX backend API</li>
  <li><strong>Strengths</strong>: Developer flexibility, research-oriented, direct tensor ops</li>
  <li><strong>Weaknesses</strong>: Early-stage, Apple-only, smaller community, fewer pre-built models</li>
</ul>

<h4 id="onnx-runtime">ONNX Runtime</h4>

<ul>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.onnx</code></li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Loads ONNX graph and converts to optimized IR</li>
      <li>Graph optimization passes applied (e.g., constant folding, fusion)</li>
      <li>Execution providers (EPs) handle hardware-specific execution</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: CPU, GPU (CUDA, ROCm), NNAPI, DirectML, ARM, OpenVINO</li>
  <li><strong>Backend Design</strong>: Pluggable EP system, modular kernel dispatch</li>
  <li><strong>Strengths</strong>: Cross-platform, flexible, highly optimized</li>
  <li><strong>Weaknesses</strong>: Model conversion may be lossy or complex, mobile-specific tuning needed</li>
</ul>

<h4 id="executorch">ExecuTorch</h4>

<ul>
  <li><strong>Model Format</strong>: PyTorch Lite models, <code class="language-plaintext highlighter-rouge">ptc</code> compiled bytecode</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>TorchScript traced models compiled using Ahead-of-Time (AOT) compiler</li>
      <li>Produces a minimal runtime with only needed ops</li>
      <li>Bytecode is executed on microcontroller or mobile device</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: CPU, MCU, potentially DSP/NPU</li>
  <li><strong>Backend Design</strong>: AOT compiler, custom micro runtime, graph executor</li>
  <li><strong>Strengths</strong>: Lightweight, optimized for resource-constrained environments</li>
  <li><strong>Weaknesses</strong>: Limited model format support, newer toolchain</li>
</ul>

<h4 id="lidartlm">LidarTLM</h4>

<ul>
  <li><strong>Model Format</strong>: Custom or converted models for lidar data processing</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Ingests sparse point cloud or voxel data</li>
      <li>Uses spatial and temporal inference pipelines</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: ARM CPUs, embedded GPU, or AI co-processors</li>
  <li><strong>Backend Design</strong>: Spatially-aware computation graph; sensor-fusion modules</li>
  <li><strong>Strengths</strong>: Specialized for lidar, supports sensor fusion</li>
  <li><strong>Weaknesses</strong>: Niche use case, limited community and documentation</li>
</ul>

<h4 id="llamacpp"><code class="language-plaintext Highlighter-rouge">llama.cpp</code></h4>

<ul>
  <li><strong>Model Format</strong>: Quantized LLM formats (GGUF, etc.)</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>Loads quantized model into memory</li>
      <li>Performs batched matmul-based transformer inference</li>
      <li>Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)</li>
    </ul>
  </li>
  <li><strong>Hardware Support</strong>: CPU, optionally GPU</li>
  <li><strong>Backend Design</strong>: Minimalist tensor framework, custom linear algebra, no IR</li>
  <li><strong>Strengths</strong>: Extremely portable, optimized for low-RAM devices, self-contained</li>
  <li><strong>Weaknesses</strong>: Focused only on LLMs, lower-level interface</li>
</ul>

<h4 id="tensorflow-lite--serving">TensorFlow Lite / Serving</h4>

<ul>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.tflite</code> (Lite), <code class="language-plaintext highlighter-rouge">.pb</code> or SavedModel (Serving)</li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ul>
      <li>TFLite: uses FlatBuffer model, loads and interprets ops</li>
      <li>Serving: REST/gRPC server for remote model inference</li>
    </ul>
  </li>
  <li>
    <p><strong>Hardware Support</strong>:</p>

    <ul>
      <li>TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP</li>
      <li>Serving: Primarily server-side; not for on-device use</li>
    </ul>
  </li>
  <li>
    <p><strong>Backend Design</strong>:</p>

    <ul>
      <li>TFLite: statically compiled interpreters with kernel registry</li>
      <li>TFLite delegates for hardware acceleration</li>
    </ul>
  </li>
  <li><strong>Strengths</strong>: Broad compatibility, active ecosystem, stable</li>
  <li><strong>Weaknesses</strong>: Delegate configuration can be tricky, Serving not suitable for offline use</li>
</ul>

<h2 id="tensorrt-deep-dive">TensorRT Deep Dive</h2>

<ul>
  <li>TensorRT is NVIDIA’s high-performance, low-latency inference runtime for deep learning models. It is purpose-built for GPU-accelerated inference and heavily optimized for NVIDIA’s hardware, including desktop GPUs, Jetson embedded boards, and datacenter GPUs with Tensor Cores.</li>
</ul>

<h3 id="overview">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: Engineers deploying deep learning models on NVIDIA hardware</li>
  <li><strong>Use Cases</strong>: Vision inference, robotics, autonomous vehicles, embedded AI with Jetson, high-throughput servers</li>
  <li><strong>Model Format</strong>: ONNX, Caffe, TensorFlow (converted to <code class="language-plaintext highlighter-rouge">.plan</code> engine)</li>
  <li><strong>Conversion Tools</strong>: <code class="language-plaintext highlighter-rouge">trtexec</code>, TensorRT Python/C++ APIs</li>
</ul>

<h3 id="architecture">Architecture</h3>

<ul>
  <li>
    <p>TensorRT transforms trained models into an optimized engine using multiple optimization passes:</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li><strong>Model Import</strong>: Loads model (typically ONNX) using TensorRT parser</li>
      <li>
        <p><strong>Optimization</strong>:</p>

        <ul>
          <li>Layer fusion</li>
          <li>Precision calibration (<code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">int8</code>)</li>
          <li>Kernel selection and scheduling</li>
        </ul>
      </li>
      <li>
        <p><strong>Engine Building</strong>:</p>

        <ul>
          <li>Generates a <code class="language-plaintext highlighter-rouge">.plan</code> file (serialized CUDA engine)</li>
          <li>This engine can be reused for fast deployment</li>
        </ul>
      </li>
      <li>
        <p><strong>Inference Execution</strong>:</p>

        <ul>
          <li>Input data fed through pre-allocated CUDA buffers</li>
          <li>Execution is entirely GPU-bound using CUDA streams</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>Builder</strong>: Optimizes and generates runtime engine</li>
      <li><strong>Runtime</strong>: Loads and executes serialized engine</li>
      <li><strong>Execution Context</strong>: Holds all buffers and workspace</li>
      <li><strong>Calibrator</strong>: Generates <code class="language-plaintext highlighter-rouge">int8</code> quantization scale factors using sample data</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Quantization Support</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">float32</code>, <code class="language-plaintext highlighter-rouge">float16</code>, and <code class="language-plaintext highlighter-rouge">int8</code> precision modes</li>
      <li><code class="language-plaintext highlighter-rouge">int8</code> requires calibration dataset (representative samples)</li>
    </ul>
  </li>
  <li>
    <p><strong>Layer Fusion</strong>:</p>

    <ul>
      <li>Combines ops like conv + bias + activation into a single kernel</li>
      <li>Reduces memory overhead and execution latency</li>
    </ul>
  </li>
  <li>
    <p><strong>Dynamic Shapes</strong>:</p>

    <ul>
      <li>Supports engines that accept varying input sizes with shape profiles</li>
    </ul>
  </li>
  <li>
    <p><strong>Deployment</strong>:</p>

    <ul>
      <li>Supports inference from Python or C++</li>
      <li>Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros</strong>:</p>

    <ul>
      <li>Best-in-class GPU inference performance</li>
      <li>Optimized for Tensor Cores (Ampere, Hopper, etc.)</li>
      <li>Rich tooling (e.g., <code class="language-plaintext highlighter-rouge">trtexec</code>, calibration tools)</li>
      <li>Integration with Jetson for embedded AI</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons</strong>:</p>

    <ul>
      <li>Requires NVIDIA GPU and CUDA runtime</li>
      <li>Not suitable for CPU or cross-platform apps</li>
      <li>Build/optimization pipeline adds complexity</li>
      <li>Engine regeneration needed if input shape or model changes significantly</li>
    </ul>
  </li>
</ul>

<h3 id="example-workflow">Example Workflow</h3>

<ul>
  <li><strong>Model Conversion (ONNX <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 1.191em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">\rightarrow</script> Engine):</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code0"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code0">trtexec <span class="nt">--onnx</span><span class="o">=</span>model.onnx <span class="nt">--saveEngine</span><span class="o">=</span>model.plan <span class="nt">--</span><span class="sb">`</span>float16<span class="sb">`</span>
</code></pre></div></div>

<ul>
  <li><strong>C++ Inference:</strong></li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code1"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code1"><span class="n">nvinfer1</span><span class="o">::</span><span class="n">IRuntime</span><span class="o">*</span> <span class="n">runtime</span> <span class="o">=</span> <span class="n">nvinfer1</span><span class="o">::</span><span class="n">createInferRuntime</span><span class="p">(</span><span class="n">logger</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">ifstream</span> <span class="nf">engineFile</span><span class="p">(</span><span class="s">"model.plan"</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="n">nvinfer1</span><span class="o">::</span><span class="n">ICudaEngine</span><span class="o">*</span> <span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">-&gt;</span><span class="n">deserializeCudaEngine</span><span class="p">(...);</span>
</code></pre></div></div>

<ul>
  <li><strong>Python Inference:</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code2"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code2"><span class="kn">import</span> <span class="nn">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>
<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"model.plan"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">).</span><span class="n">deserialize_cuda_engine</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
</code></pre></div></div>

<h3 id="suitable-applications">Suitable Applications</h3>

<ul>
  <li>Real-time object detection on Jetson Nano/Xavier</li>
  <li>Batch inference in ML inference servers</li>
  <li><code class="language-plaintext highlighter-rouge">int8</code>-quantized NLP models for chatbots</li>
  <li>
    <p>High-throughput video analytics (via DeepStream)</p>
  </li>
  <li>TensorRT excels in performance-critical scenarios where latency, batch throughput, or GPU utilization is a bottleneck. It’s a specialized, production-grade runtime for teams fully committed to NVIDIA’s platform.</li>
</ul>

<h2 id="core-ml-deep-dive">Core ML Deep Dive</h2>

<ul>
  <li>Core ML is Apple’s on-device machine learning framework, designed to provide seamless model deployment and execution across the Apple ecosystem. It’s tailored for iOS, macOS, watchOS, and tvOS, offering tight integration with system-level APIs and hardware acceleration units like the Apple Neural Engine (ANE).</li>
</ul>

<h3 id="overview-1">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: iOS/macOS developers</li>
  <li><strong>Use Cases</strong>: Image recognition, natural language processing, AR/VR, real-time gesture and object detection</li>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.mlmodel</code> (converted to <code class="language-plaintext highlighter-rouge">.mlmodelc</code> at compile time)</li>
  <li><strong>Conversion Tools</strong>: <code class="language-plaintext highlighter-rouge">coremltools</code>, Apple Create ML, ONNX to Core ML converters</li>
</ul>

<h3 id="architecture-1">Architecture</h3>

<ul>
  <li>
    <p><strong>Model Compiler</strong>: Converts <code class="language-plaintext highlighter-rouge">.mlmodel</code> to <code class="language-plaintext highlighter-rouge">.mlmodelc</code>, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.</p>
  </li>
  <li>
    <p><strong>Execution Pipeline</strong>:</p>

    <ol>
      <li><strong>Model Load</strong>: App loads the <code class="language-plaintext highlighter-rouge">.mlmodelc</code> file at runtime using the <code class="language-plaintext highlighter-rouge">MLModel</code> API.</li>
      <li><strong>Prediction API</strong>: Developer calls <code class="language-plaintext highlighter-rouge">prediction(input:)</code>, which triggers the internal compute graph.</li>
      <li><strong>Backend Selection</strong>: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.</li>
      <li><strong>Execution Engine</strong>: Executes the optimized graph using Apple’s proprietary kernel implementations.</li>
      <li><strong>Output</strong>: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.</li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><em>MLModel Interface</em>: Main interaction point for inference</li>
      <li><em>MLMultiArray</em>: N-dimensional tensor abstraction</li>
      <li><em>MLFeatureValue / MLFeatureProvider</em>: Input-output containers</li>
      <li><em>NeuralNetwork.proto</em>: Defines underlying graph schema for neural network layers</li>
    </ul>
  </li>
</ul>

<h3 id="supported-model-types">Supported Model Types</h3>

<ul>
  <li>Neural Networks (CNNs, RNNs, Transformers)</li>
  <li>Decision Trees and Ensembles (from XGBoost, scikit-learn)</li>
  <li>Natural Language models (tokenizers, embeddings)</li>
  <li>Audio signal processing</li>
  <li>Custom models using Core ML’s custom layers</li>
</ul>

<h3 id="implementation-details-1">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Conversion Process</strong>:</p>

    <ul>
      <li>Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format</li>
      <li><code class="language-plaintext highlighter-rouge">coremltools.convert()</code> maps ops to Core ML equivalents and produces <code class="language-plaintext highlighter-rouge">.mlmodel</code></li>
      <li>Optional model quantization (e.g., 16-bit float) can be applied to reduce size</li>
    </ul>
  </li>
  <li>
    <p><strong>Hardware Utilization</strong>:</p>

    <ul>
      <li>Automatically uses ANE if available (iPhone 8 and later)</li>
      <li>Fallback to Metal GPU or CPU if ANE doesn’t support all ops</li>
      <li>Internal heuristics determine fallback patterns and op partitioning</li>
    </ul>
  </li>
  <li>
    <p><strong>Custom Layers</strong>:</p>

    <ul>
      <li>Developers can define <code class="language-plaintext highlighter-rouge">MLCustomModel</code> classes</li>
      <li>Useful when Core ML lacks certain ops</li>
      <li>Requires manual tensor handling and native Swift/Obj-C implementation</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-1">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Deep Apple integration (Vision, AVFoundation, ARKit, etc.)</li>
      <li>Seamless use of hardware accelerators</li>
      <li>High-level Swift API for rapid development</li>
      <li>Secure and privacy-focused (no data leaves device)</li>
      <li>Optimized runtime with minimal latency</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Apple-only ecosystem</li>
      <li>Conversion limitations (unsupported ops in some models)</li>
      <li>Limited visibility into runtime internals</li>
      <li>Custom layer interface can be verbose and inflexible</li>
    </ul>
  </li>
</ul>

<h3 id="example-code-snippet">Example Code Snippet</h3>

<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code3"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code3"><span class="k">guard</span> <span class="k">let</span> <span class="nv">model</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="kt">MyImageClassifier</span><span class="p">(</span><span class="nv">configuration</span><span class="p">:</span> <span class="kt">MLModelConfiguration</span><span class="p">())</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fatalError</span><span class="p">(</span><span class="s">"Model failed to load"</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">let</span> <span class="nv">input</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="kt">MLMultiArray</span><span class="p">(</span><span class="nv">shape</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span> <span class="nv">dataType</span><span class="p">:</span> <span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1">// Fill input array with pixel data</span>

<span class="k">let</span> <span class="nv">output</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="n">model</span><span class="o">.</span><span class="nf">prediction</span><span class="p">(</span><span class="nv">input</span><span class="p">:</span> <span class="n">input</span><span class="o">!</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">?</span><span class="o">.</span><span class="n">classLabel</span> <span class="p">??</span> <span class="s">"Prediction failed"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="mlx-deep-dive">MLX Deep Dive</h2>

<ul>
  <li>MLX (Machine Learning eXperimentation) is a relatively new Apple-developed machine learning framework built specifically for Apple Silicon. It is designed for flexibility, research, and experimentation, offering a PyTorch-like Python API with eager and compiled execution. Unlike Core ML, which targets app integration and production deployment, MLX is meant for model development, prototyping, and edge inference—while taking full advantage of Apple hardware like the M-series chips.</li>
  <li>Put simply, MLX is particularly well-suited for developers focused on rapid iteration and fine-tuning of models on Apple devices. It’s promising for LLMs and vision transformers on MacBooks and other Apple Silicon-powered hardware.</li>
</ul>

<h3 id="overview-2">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: ML researchers and developers using Apple Silicon</li>
  <li><strong>Use Cases</strong>: Research, fine-tuning models on-device, LLM inference, Apple-optimized ML pipelines</li>
  <li><strong>Model Format</strong>: No proprietary serialized model format; models are expressed in Python source code using <code class="language-plaintext highlighter-rouge">mlx.nn</code> layers</li>
  <li><strong>Conversion Tools</strong>: Emerging support for PyTorch model import via <code class="language-plaintext highlighter-rouge">mlx-trace</code> and ONNX conversion</li>
</ul>

<h3 id="architecture-2">Architecture</h3>

<ul>
  <li>
    <p>MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.</p>
  </li>
  <li>
    <p><strong>Execution Modes</strong>:</p>

    <ul>
      <li><strong>Eager Execution</strong>: Immediate computation for prototyping/debugging</li>
      <li><strong>Compiled Graph</strong>: Via <code class="language-plaintext highlighter-rouge">mlx.compile()</code> for performance-critical inference</li>
    </ul>
  </li>
  <li>
    <p><strong>Core Components</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">mlx.core</code>: Tensor definitions and low-level math operations</li>
      <li><code class="language-plaintext highlighter-rouge">mlx.nn</code>: High-level neural network module abstraction (analogous to PyTorch’s <code class="language-plaintext highlighter-rouge">nn.Module</code>)</li>
      <li><code class="language-plaintext highlighter-rouge">mlx.optimizers</code>: Gradient-based optimizers for training</li>
      <li><code class="language-plaintext highlighter-rouge">mlx.transforms</code>: Preprocessing utilities (e.g., normalization, resizing)</li>
    </ul>
  </li>
  <li>
    <p><strong>Hardware Abstraction</strong>:</p>

    <ul>
      <li>Primarily targets the GPU via MPS</li>
      <li>MLX compiler performs static analysis to optimize kernel dispatch and memory usage</li>
      <li>ANE support is still evolving and model-dependent</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-2">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Tensor Memory Model</strong>:</p>

    <ul>
      <li>MLX tensors are immutable</li>
      <li>Operations generate new tensors rather than mutating in-place</li>
      <li>Enables functional purity and easier graph compilation</li>
    </ul>
  </li>
  <li>
    <p><strong>JIT Compilation</strong>:</p>

    <ul>
      <li>While code is typically run in Python, MLX allows functions to be decorated with <code class="language-plaintext highlighter-rouge">@mlx.compile</code> to trace and compile computation graphs</li>
      <li>Reduces memory allocations and kernel overhead</li>
    </ul>
  </li>
  <li>
    <p><strong>Custom Modules</strong>:</p>

    <ul>
      <li>Developers can create custom layers by subclassing <code class="language-plaintext highlighter-rouge">mlx.nn.Module</code></li>
      <li>Supports standard layers like <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Conv2d</code>, <code class="language-plaintext highlighter-rouge">LayerNorm</code>, etc.</li>
    </ul>
  </li>
  <li>
    <p><strong>Interoperability</strong>:</p>

    <ul>
      <li>MLX includes tools to convert PyTorch models using tracing (WIP)</li>
      <li>No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-2">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Highly optimized for Apple Silicon (especially M1/M2)</li>
      <li>Lightweight and minimalist API with functional programming style</li>
      <li>Supports training and inference on-device</li>
      <li>Fast experimentation with eager mode and compilation toggle</li>
      <li>Tensor API is intuitive for PyTorch users</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)</li>
      <li>Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)</li>
      <li>No official deployment format—source code is the model</li>
      <li>Interop with other frameworks is under active development but not production-ready</li>
    </ul>
  </li>
</ul>

<h3 id="example-code-snippet-1">Example Code Snippet</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code4"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code4"><span class="kn">import</span> <span class="nn">mlx.core</span> <span class="k">as</span> <span class="n">mx</span>
<span class="kn">import</span> <span class="nn">mlx.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">SimpleMLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleMLP</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">mx</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Prediction:"</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>For accelerated inference:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code5"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code5"><span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">mx</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="onnx-runtime-deep-dive">ONNX Runtime Deep Dive</h2>

<ul>
  <li>ONNX Runtime (ORT) is a cross-platform, high-performance inference engine for deploying models in the Open Neural Network Exchange (ONNX) format. Maintained by Microsoft, it is widely adopted due to its flexibility, extensibility, and support for numerous hardware backends. ONNX itself is an open standard that enables interoperability between ML frameworks like PyTorch, TensorFlow, and scikit-learn.</li>
</ul>

<h3 id="overview-3">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: Application developers, MLOps teams, platform architects</li>
  <li><strong>Use Cases</strong>: Cross-framework inference, model portability, production deployments (cloud + edge), hardware acceleration</li>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.onnx</code> (Open Neural Network Exchange format)</li>
  <li><strong>Conversion Tools</strong>: <code class="language-plaintext highlighter-rouge">torch.onnx.export</code>, <code class="language-plaintext highlighter-rouge">tf2onnx</code>, <code class="language-plaintext highlighter-rouge">skl2onnx</code>, and many others</li>
</ul>

<h3 id="architecture-3">Architecture</h3>

<ul>
  <li>
    <p>ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li><strong>Model Load</strong>: Parses the <code class="language-plaintext highlighter-rouge">.onnx</code> model file into an internal graph representation.</li>
      <li><strong>Graph Optimization</strong>: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.</li>
      <li><strong>Execution Provider Selection</strong>: Based on available hardware and EP priorities, operators are assigned to execution backends.</li>
      <li><strong>Execution</strong>: ORT schedules and dispatches kernel calls for each partition of the graph.</li>
      <li><strong>Output Handling</strong>: Results are returned in native types or via C/C++/Python APIs.</li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>Session</strong>: <code class="language-plaintext highlighter-rouge">InferenceSession</code> is the main object for loading and running models.</li>
      <li>
        <p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>

        <ul>
          <li>CPU (default)</li>
          <li>CUDA (NVIDIA GPUs)</li>
          <li>DirectML (Windows GPU)</li>
          <li>OpenVINO (Intel accelerators)</li>
          <li>NNAPI (Android)</li>
          <li>CoreML (iOS/macOS)</li>
          <li>TensorRT</li>
          <li>QNN (Qualcomm AI Engine)</li>
        </ul>
      </li>
      <li><strong>Graph Transformer</strong>: Rewrites and optimizes the computation graph</li>
      <li><strong>Kernel Registry</strong>: Maps ONNX ops to optimized implementations</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-3">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Model Format</strong>:</p>

    <ul>
      <li>ONNX models are stored in protobuf format</li>
      <li>Static computation graph with explicit type and shape information</li>
      <li>Supports operator versioning to ensure backward compatibility</li>
    </ul>
  </li>
  <li>
    <p><strong>Customization</strong>:</p>

    <ul>
      <li>Developers can register custom ops and execution providers</li>
      <li>Optional use of external initializers and custom inference contexts</li>
    </ul>
  </li>
  <li>
    <p><strong>Execution Optimization</strong>:</p>

    <ul>
      <li>Graph transformation level can be controlled (basic, extended, all)</li>
      <li>EPs can share execution (e.g., some layers on CPU, others on GPU)</li>
      <li>Quantization and sparsity-aware execution supported via tools like <code class="language-plaintext highlighter-rouge">onnxruntime-tools</code></li>
    </ul>
  </li>
  <li>
    <p><strong>Mobile Support</strong>:</p>

    <ul>
      <li>ONNX Runtime Mobile: A statically linked, size-reduced runtime</li>
      <li>Works with Android and iOS, using NNAPI, Core ML, or CPU fallback</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-3">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Framework agnostic and highly interoperable</li>
      <li>Broad hardware support via modular execution providers</li>
      <li>Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)</li>
      <li>Mobile support with optimized builds and quantized execution</li>
      <li>Extensive language bindings (Python, C++, C#, Java)</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Debugging can be complex across EPs</li>
      <li>Conversion process from other frameworks may require custom scripts</li>
      <li>ONNX opset compatibility issues can arise across versions</li>
      <li>Mobile optimization (size, latency) requires manual tuning</li>
    </ul>
  </li>
</ul>

<h3 id="example-code-snippet-python">Example Code Snippet (Python)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code6"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code6"><span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="n">ort</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Load ONNX model
</span><span class="n">session</span> <span class="o">=</span> <span class="n">ort</span><span class="p">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s">"resnet50.onnx"</span><span class="p">)</span>

<span class="c1"># Prepare input
</span><span class="n">input_name</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Run inference
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Prediction shape:"</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Using CUDA Execution Provider</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code7"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code7"><span class="n">session</span> <span class="o">=</span> <span class="n">ort</span><span class="p">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s">"resnet50.onnx"</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s">'CUDAExecutionProvider'</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="use-in-edge--on-device-scenarios">Use in Edge / On-Device Scenarios</h3>

<ul>
  <li>
    <p>ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:</p>

    <ul>
      <li>Stripped-down build (~1–2 MB)</li>
      <li>FlatBuffer format support in preview</li>
      <li>Android NNAPI and iOS Core ML integration</li>
      <li>Prebuilt minimal runtime packages for specific models</li>
    </ul>
  </li>
  <li>
    <p>ONNX Runtime is best suited for applications where:</p>

    <ul>
      <li>Portability across hardware is essential</li>
      <li>Mixed execution (CPU + accelerator) is beneficial</li>
      <li>The model pipeline involves multiple frameworks</li>
    </ul>
  </li>
</ul>

<h2 id="executorch-deep-dive">ExecuTorch Deep Dive</h2>

<ul>
  <li>ExecuTorch is a lightweight runtime and deployment framework built by Meta (Facebook) to run PyTorch models on constrained edge devices, including microcontrollers (MCUs), embedded systems, and mobile hardware. It is designed with the principles of minimalism, portability, and execution efficiency. Unlike full PyTorch runtimes, ExecuTorch leverages Ahead-of-Time (AOT) compilation and produces compact bytecode representations of models.</li>
</ul>

<h3 id="overview-4">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: Embedded ML engineers, mobile and edge system developers</li>
  <li><strong>Use Cases</strong>: Sensor fusion, vision at the edge, voice command detection, ultra-low-power AI applications</li>
  <li><strong>Model Format</strong>: Compiled TorchScript bytecode (<code class="language-plaintext highlighter-rouge">.ptc</code>)</li>
  <li><strong>Conversion Tools</strong>: PyTorch <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">\rightarrow</script> TorchScript <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mo" id="MathJax-Span-9" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-3">\rightarrow</script> ExecuTorch via AOT pipeline</li>
</ul>

<h3 id="architecture-4">Architecture</h3>

<ul>
  <li>
    <p>ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li>
        <p><strong>Model Export</strong>:</p>

        <ul>
          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>
          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>
        </ul>
      </li>
      <li>
        <p><strong>Runtime Embedding</strong>:</p>

        <ul>
          <li>The bytecode and necessary ops are compiled with the target runtime.</li>
          <li>Optional op pruning removes unused operations.</li>
        </ul>
      </li>
      <li>
        <p><strong>Deployment</strong>:</p>

        <ul>
          <li>Model and runtime are flashed onto the device.</li>
          <li>Inference is run via a lightweight VM interpreter.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>Bytecode Format</strong>: <code class="language-plaintext highlighter-rouge">.ptc</code> files contain compiled operators and control flow</li>
      <li><strong>VM Runtime</strong>: A minimal interpreter that reads and executes bytecode</li>
      <li><strong>Dispatcher</strong>: Routes ops to backend implementations</li>
      <li><strong>Memory Arena</strong>: Static memory model, optionally no dynamic allocation</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-4">Implementation Details</h3>

<ul>
  <li>
    <p><strong>AOT Compiler</strong>:</p>

    <ul>
      <li>Converts scripted TorchScript models into bytecode and op kernels</li>
      <li>Includes a model linker that statically binds required ops</li>
      <li>Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)</li>
    </ul>
  </li>
  <li>
    <p><strong>Operator Handling</strong>:</p>

    <ul>
      <li>Customizable op kernels allow device-specific optimization</li>
      <li>Optional kernel fusion via compiler passes for performance</li>
    </ul>
  </li>
  <li>
    <p><strong>Runtime Constraints</strong>:</p>

    <ul>
      <li>Code size: Can be &lt;500 KB with aggressive pruning</li>
      <li>No reliance on dynamic memory allocation (static buffer planning)</li>
      <li>Designed for devices with as little as 256 KB RAM</li>
    </ul>
  </li>
  <li>
    <p><strong>Integration</strong>:</p>

    <ul>
      <li>Written in C++</li>
      <li>Can integrate with sensor pipelines, real-time OS, or MCU firmware</li>
      <li>Open-sourced with tooling for building and flashing models to hardware</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-4">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Extremely lightweight, MCU-ready</li>
      <li>AOT compilation reduces runtime overhead</li>
      <li>Deterministic memory usage (good for real-time applications)</li>
      <li>Modular and open-source with low-level control</li>
      <li>PyTorch-compatible workflow for training and export</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Requires model to be written in a static subset of PyTorch</li>
      <li>Limited dynamic control flow (must be scriptable)</li>
      <li>Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite</li>
      <li>Focused on inference only; no training support on-device</li>
    </ul>
  </li>
</ul>

<h3 id="example-workflow-1">Example Workflow</h3>

<ul>
  <li><strong>Model Export (Python):</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code8"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code8"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">TinyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TinyModel</span><span class="p">()</span>
<span class="n">scripted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">scripted</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"model.pt"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>ExecuTorch AOT Compilation (CLI or CMake):</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code9"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code9">executorchc compile <span class="nt">--model</span> model.pt <span class="nt">--output</span> model.ptc <span class="nt">--target</span> cortex-m
</code></pre></div></div>

<ul>
  <li><strong>Embedded Runtime Integration (C++):</strong></li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code10"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code10"><span class="cp">#include</span> <span class="cpf">"executorch/runtime/runtime.h"</span><span class="cp">
</span>
<span class="n">executorch</span><span class="o">::</span><span class="n">load_model</span><span class="p">(</span><span class="s">"model.ptc"</span><span class="p">);</span>
<span class="n">executorch</span><span class="o">::</span><span class="n">run_model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">);</span>
</code></pre></div></div>

<h3 id="suitable-applications-1">Suitable Applications</h3>

<ul>
  <li>Wake-word detection on MCUs</li>
  <li>Gesture recognition using MEMS sensors</li>
  <li>Smart agriculture (tiny vision models)</li>
  <li>
    <p>Battery-powered health monitoring devices</p>
  </li>
  <li>ExecuTorch fills a critical niche for deploying PyTorch-trained models on hardware where traditional runtimes like TensorFlow Lite or ONNX Runtime are too heavy.</li>
</ul>

<h2 id="lidartlm-deep-dive">LidarTLM Deep Dive</h2>

<ul>
  <li>
    <p>LidarTLM (LiDAR Tensor Layer Module) is a specialized, lower-profile runtime or processing pipeline designed for inference on LiDAR data using neural networks. It is not a mainstream or widely standardized runtime like TensorFlow Lite or ONNX Runtime, but rather refers to a class of embedded software tools tailored for 3D point cloud inference and fusion with temporal data—typically in autonomous systems, robotics, or advanced driver-assistance systems (ADAS).</p>
  </li>
  <li>
    <p>Because LidarTLM is less commonly documented and may refer to proprietary or research-centric toolkits, this section will focus on generalized design principles, use cases, and what distinguishes LiDAR-focused runtimes from general-purpose ML engines.</p>
  </li>
</ul>

<h3 id="overview-5">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: Robotics, ADAS, and autonomous system engineers</li>
  <li><strong>Use Cases</strong>: Real-time 3D object detection, SLAM (Simultaneous Localization and Mapping), point cloud segmentation, obstacle avoidance</li>
  <li><strong>Model Format</strong>: Often custom or adapted from PyTorch/ONNX; serialized as tensors or voxel grids</li>
  <li><strong>Conversion Tools</strong>: Typically includes preprocessing pipelines from ROS, Open3D, or custom CUDA kernels</li>
</ul>

<h3 id="architecture-5">Architecture</h3>

<ul>
  <li>
    <p>LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li><strong>Sensor Input</strong>: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested</li>
      <li><strong>Preprocessing</strong>: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)</li>
      <li><strong>Inference</strong>: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)</li>
      <li><strong>Postprocessing</strong>: Bounding boxes or semantic maps generated</li>
      <li><strong>Fusion (Optional)</strong>: Sensor fusion with radar, camera, or odometry</li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>Spatial Encoder</strong>: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)</li>
      <li><strong>Sparse CNNs or VoxelNet Layers</strong>: Specialized convolution ops for irregular input data</li>
      <li><strong>Temporal Modules</strong>: Optional RNN, attention, or transformer blocks for sequential scans</li>
      <li><strong>Hardware Abstraction</strong>: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-5">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Tensor Representation</strong>:</p>

    <ul>
      <li>Often uses sparse tensors or hybrid dense-sparse structures</li>
      <li>Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops</li>
      <li>Quantization may be used to reduce memory footprint in embedded settings</li>
    </ul>
  </li>
  <li>
    <p><strong>Optimization Techniques</strong>:</p>

    <ul>
      <li>Efficient neighbor search (KD-trees, octrees) for local feature aggregation</li>
      <li>Temporal caching of features from prior scans</li>
      <li>Batch fusion for multi-sensor inputs</li>
    </ul>
  </li>
  <li>
    <p><strong>Deployment</strong>:</p>

    <ul>
      <li>Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers</li>
      <li>Often integrated with ROS (Robot Operating System) for I/O and control flow</li>
      <li>May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-5">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Designed for spatial and temporal data, not just 2D tensors</li>
      <li>Optimized for sparse inputs and low-latency inference</li>
      <li>Supports sensor fusion pipelines, enabling richer context</li>
      <li>Can run on edge-grade GPUs or embedded NPUs</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Fragmented tooling, often bespoke or tightly coupled to hardware</li>
      <li>Lack of standardized runtime interface (unlike ONNX or TFLite)</li>
      <li>Difficult to deploy across platforms without custom engineering</li>
      <li>Sparse community and documentation; often buried in academic or industrial codebases</li>
    </ul>
  </li>
</ul>

<h3 id="example-pseudocode-flow">Example Pseudocode Flow</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code11"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code11"><span class="c1"># Step 1: Load point cloud
</span><span class="n">point_cloud</span> <span class="o">=</span> <span class="n">load_lidar_scan</span><span class="p">(</span><span class="s">"/scans/frame_001.bin"</span><span class="p">)</span>

<span class="c1"># Step 2: Convert to voxel grid
</span><span class="n">voxel_grid</span> <span class="o">=</span> <span class="n">voxelize</span><span class="p">(</span><span class="n">point_cloud</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>

<span class="c1"># Step 3: Pass through 3D CNN
</span><span class="n">features</span> <span class="o">=</span> <span class="n">sparse_conv_net</span><span class="p">(</span><span class="n">voxel_grid</span><span class="p">)</span>

<span class="c1"># Step 4: Predict bounding boxes or labels
</span><span class="n">detections</span> <span class="o">=</span> <span class="n">decode_bounding_boxes</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Step 5: Fuse with other sensors (optional)
</span><span class="n">fused_output</span> <span class="o">=</span> <span class="n">fuse_with_camera</span><span class="p">(</span><span class="n">detections</span><span class="p">,</span> <span class="n">rgb_frame</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="suitable-applications-2">Suitable Applications</h3>

<ul>
  <li>Autonomous vehicles (3D perception stacks)</li>
  <li>Warehouse robots and drones</li>
  <li>Industrial inspection systems</li>
  <li>Advanced driver-assistance systems (ADAS)</li>
  <li>
    <p>SLAM systems for robotics</p>
  </li>
  <li>LidarTLM-like runtimes are not meant for general ML workloads but are highly optimized for 3D spatiotemporal inference, where conventional 2D model runtimes fall short. They tend to be integrated deep into hardware-specific SDKs or research frameworks.</li>
</ul>

<h3 id="llamacpp-deep-dive"><code class="language-plaintext Highlighter-rouge">llama.cpp</code> Deep Dive</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">llama.cpp</code> is an open-source, C++-based implementation of inference for large language models (LLMs), originally inspired by Meta’s LLaMA family. It focuses on efficient CPU (and optionally GPU) inference for quantized transformer models. Unlike full ML runtimes, <code class="language-plaintext highlighter-rouge">llama.cpp</code> is specialized, minimalist, and optimized for running LLMs—particularly on devices with constrained memory and compute budgets such as laptops, desktops, and even smartphones.</li>
</ul>

<h3 id="overview-6">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: LLM researchers, app developers, hobbyists</li>
  <li><strong>Use Cases</strong>: Local chatbots, privacy-preserving LLM apps, embedded NLP on edge devices</li>
  <li><strong>Model Format</strong>: Quantized GGUF (GPT-generated GGML Unified Format)</li>
  <li><strong>Conversion Tools</strong>: Python conversion scripts from PyTorch checkpoints to GGUF</li>
</ul>

<h3 id="architecture-6">Architecture</h3>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">llama.cpp</code> does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li><strong>Model Load</strong>: Quantized GGUF file loaded into memory</li>
      <li><strong>KV Cache Allocation</strong>: Allocates buffers for key/value attention caching</li>
      <li><strong>Token Embedding &amp; Input Prep</strong>: Maps token IDs to embeddings</li>
      <li><strong>Layer Execution Loop</strong>: Runs transformer blocks sequentially</li>
      <li><strong>Logits Output</strong>: Computes next-token logits, passed to sampler</li>
      <li><strong>Sampling &amp; Token Generation</strong>: Greedy, top-k, nucleus, or temperature sampling</li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>GGML Backend</strong>: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)</li>
      <li><strong>Quantization Layers</strong>: 4-bit, 5-bit, and 8-bit quantized matmuls</li>
      <li><strong>Inference Loop</strong>: Manually unrolled transformer stack—one layer at a time</li>
      <li><strong>KV Cache Management</strong>: Token sequence history for autoregressive decoding</li>
    </ul>
  </li>
  <li>
    <p><strong>Optional GPU Support</strong>:</p>

    <ul>
      <li>Metal (macOS), OpenCL, CUDA support via modular backends</li>
      <li>Offloading options: attention only, matmuls only, or full GPU</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-6">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Model Quantization</strong>:</p>

    <ul>
      <li>Tools like <code class="language-plaintext highlighter-rouge">quantize.py</code> convert PyTorch models to GGUF format</li>
      <li>Supports several quantization strategies (Q4_0, Q5_K, Q8_0, etc.)</li>
      <li>Tradeoff between model size and accuracy</li>
    </ul>
  </li>
  <li>
    <p><strong>Tensor Engine</strong>:</p>

    <ul>
      <li>No external libraries like BLAS, cuDNN, or MKL used by default</li>
      <li>Uses hand-optimized C++ with platform-specific intrinsics</li>
      <li>Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)</li>
    </ul>
  </li>
  <li>
    <p><strong>Memory Optimization</strong>:</p>

    <ul>
      <li>Memory mapped file support (<code class="language-plaintext highlighter-rouge">mmap</code>)</li>
      <li>Low memory mode: restricts KV cache or context length</li>
      <li>Paging and streaming support for large contexts (e.g., <code class="language-plaintext highlighter-rouge">llama.cpp + vLLM</code>)</li>
    </ul>
  </li>
  <li>
    <p><strong>Integration</strong>:</p>

    <ul>
      <li>C API and Python bindings (<code class="language-plaintext highlighter-rouge">llama-cpp-python</code>)</li>
      <li>Works with tools like LangChain, OpenRouter, and Ollama</li>
      <li>Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.</li>
    </ul>
  </li>
</ul>

<h3 id="pros-and-cons-6">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros:</strong></p>

    <ul>
      <li>Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)</li>
      <li>Portable and minimal dependencies</li>
      <li>Quantization enables running models with &lt;4 GB RAM</li>
      <li>Easily embedded into apps, games, and command-line tools</li>
      <li>Active community and ecosystem (used in projects like Ollama and LM Studio)</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons:</strong></p>

    <ul>
      <li>Transformer-only; not a general ML runtime</li>
      <li>No training support—strictly for inference</li>
      <li>Manual conversion and tuning process required</li>
      <li>Limited ops support; cannot easily add new ML layers</li>
    </ul>
  </li>
</ul>

<h3 id="example-cli-inference">Example CLI Inference</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code12"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code12">./main <span class="nt">-m</span> models/llama-7B.Q4_0.gguf <span class="nt">-p</span> <span class="s2">"What is the capital of France?"</span> <span class="nt">-n</span> 64
</code></pre></div></div>

<ul>
  <li><strong>Python Inference (via <code class="language-plaintext highlighter-rouge">llama-cpp-python</code>)</strong>:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code13"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code13"><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"llama-7B.Q4_0.gguf"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="s">"Q: What is the capital of France?</span><span class="se">\n</span><span class="s">A:"</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s">"choices"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"text"</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>
    <p><strong>WebAssembly Example (Browser):</strong></p>

    <ul>
      <li>Precompiled WASM version can run LLMs client-side using WebGPU</li>
      <li>Useful for private, offline AI assistants directly in browser</li>
    </ul>
  </li>
</ul>

<h3 id="suitable-applications-3">Suitable Applications</h3>

<ul>
  <li>Private, offline chatbots</li>
  <li>Voice assistants embedded in hardware</li>
  <li>Context-aware agents in games or productivity apps</li>
  <li>
    <p>Developer tools with local NLP capabilities</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">llama.cpp</code> showcases what is possible with small, optimized transformer runtimes and CPU-centric design. It’s not a general-purpose ML runtime but a powerful engine for language inference where privacy, portability, or internet-free operation is desired.</li>
</ul>

<h2 id="tensorflow-lite--tensorflow-serving-deep-dive">TensorFlow Lite / TensorFlow Serving Deep Dive</h2>

<ul>
  <li>
    <p>TensorFlow Lite (TFLite) and TensorFlow Serving are two distinct components from the TensorFlow ecosystem optimized for inference, but they serve different purposes and deployment environments.</p>
  </li>
  <li><strong>TensorFlow Lite</strong> is designed for <strong>on-device inference</strong>, particularly for mobile, embedded, and IoT platforms.</li>
  <li>
    <p><strong>TensorFlow Serving</strong> is designed for <strong>cloud and server-side model deployment</strong>, providing high-throughput, low-latency model serving over gRPC or HTTP.</p>
  </li>
  <li>This section focuses primarily on <strong>TensorFlow Lite</strong> due to its relevance to on-device ML runtimes, with a comparative note on Serving at the end.</li>
</ul>

<h3 id="overview-7">Overview</h3>

<ul>
  <li><strong>Developer Target</strong>: Mobile developers, embedded engineers, production ML ops</li>
  <li><strong>Use Cases</strong>: Real-time image classification, object detection, audio processing, NLP, edge analytics</li>
  <li><strong>Model Format</strong>: <code class="language-plaintext highlighter-rouge">.tflite</code> (FlatBuffer format)</li>
  <li><strong>Conversion Tools</strong>: TensorFlow <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mo" id="MathJax-Span-12" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-4">\rightarrow</script> TFLite via <code class="language-plaintext highlighter-rouge">TFLiteConverter</code></li>
</ul>

<h3 id="tensorflow-lite-architecture">TensorFlow Lite Architecture</h3>

<ul>
  <li>
    <p>TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.</p>
  </li>
  <li>
    <p><strong>Execution Flow</strong>:</p>

    <ol>
      <li>
        <p><strong>Model Conversion</strong>:</p>

        <ul>
          <li>Uses <code class="language-plaintext highlighter-rouge">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class="language-plaintext highlighter-rouge">.tflite</code> model.</li>
        </ul>
      </li>
      <li>
        <p><strong>Model Load</strong>:</p>

        <ul>
          <li>The model is loaded by the <code class="language-plaintext highlighter-rouge">Interpreter</code> class on the target device.</li>
        </ul>
      </li>
      <li>
        <p><strong>Tensor Allocation</strong>:</p>

        <ul>
          <li>Memory buffers for input/output tensors are allocated.</li>
        </ul>
      </li>
      <li>
        <p><strong>Inference Execution</strong>:</p>

        <ul>
          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>
        </ul>
      </li>
      <li>
        <p><strong>Postprocessing</strong>:</p>

        <ul>
          <li>Output tensors are read and interpreted by the application.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Key Components</strong>:</p>

    <ul>
      <li><strong>FlatBuffer Model</strong>: Compact, zero-copy, serializable model format</li>
      <li><strong>Interpreter</strong>: Core engine that evaluates the model graph</li>
      <li><strong>Delegate Interface</strong>: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)</li>
      <li><strong>Kernel Registry</strong>: Maps ops to optimized C++ implementations (or delegates)</li>
    </ul>
  </li>
</ul>

<h3 id="implementation-details-7">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Model Conversion</strong>:</p>

    <ul>
      <li>Converts SavedModels, Keras <code class="language-plaintext highlighter-rouge">.h5</code>, or concrete functions to <code class="language-plaintext highlighter-rouge">.tflite</code></li>
      <li>Supports post-training quantization (dynamic, full integer, float16)</li>
      <li>Model optimizations include constant folding, op fusion, and pruning</li>
    </ul>
  </li>
  <li>
    <p><strong>Delegates</strong>:</p>

    <ul>
      <li>
        <p>Optional hardware acceleration backends:</p>

        <ul>
          <li><strong>NNAPI</strong> (Android)</li>
          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>
          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>
          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>
          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>
        </ul>
      </li>
      <li>
        <p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Threading and Performance</strong>:</p>

    <ul>
      <li>Supports multi-threaded inference</li>
      <li>Interpreter can be run in C++, Java, Kotlin, Python, Swift</li>
    </ul>
  </li>
</ul>

<h3 id="tensorflow-serving-short-overview">TensorFlow Serving (Short Overview)</h3>

<ul>
  <li>Designed for scalable deployment of TensorFlow models on servers</li>
  <li>Models are exposed as REST/gRPC endpoints</li>
  <li>Automatically loads, unloads, and versions models</li>
  <li>Uses <code class="language-plaintext highlighter-rouge">SavedModel</code> format, not <code class="language-plaintext highlighter-rouge">.tflite</code></li>
  <li>
    <p>Not suitable for offline or embedded deployment</p>
  </li>
  <li><strong>Use Case Comparison</strong>:</li>
</ul>

<p>Here is your formatted table following the provided style:</p>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Feature</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Target Device</td>
<td class="tg-tleft-valign-first">Mobile/Edge</td>
<td class="tg-tleft-valign-second">Cloud/Server</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Model Format</td>
<td class="tg-tleft-valign-first"><code>.tflite</code></td>
<td class="tg-tleft-valign-second">SavedModel</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Communication</td>
<td class="tg-tleft-valign-first">In-process / Local</td>
<td class="tg-tleft-valign-second">gRPC / REST</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Latency</td>
<td class="tg-tleft-valign-first">Milliseconds</td>
<td class="tg-tleft-valign-second">Sub-second to seconds</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Training Support</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-second">No (inference only)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Deployment Size</td>
<td class="tg-tleft-valign-first">Small (~100s of KB)</td>
<td class="tg-tleft-valign-second">Large, server framework</td>
</tr>
</tbody>
</table>
</div>

<h3 id="pros-and-cons-7">Pros and Cons</h3>

<ul>
  <li>
    <p><strong>Pros (TensorFlow Lite):</strong></p>

    <ul>
      <li>Compact and efficient format (FlatBuffer)</li>
      <li>Broad hardware delegate support</li>
      <li>Quantization-aware and post-training optimizations</li>
      <li>Cross-platform support (iOS, Android, Linux, microcontrollers)</li>
      <li>Strong ecosystem and pre-trained model zoo (<code class="language-plaintext highlighter-rouge">tflite-model-maker</code>)</li>
    </ul>
  </li>
  <li>
    <p><strong>Cons (TensorFlow Lite):</strong></p>

    <ul>
      <li>Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)</li>
      <li>Delegate behavior can be opaque and platform-dependent</li>
      <li>Conversion can fail silently if unsupported ops are encountered</li>
      <li>Debugging delegate fallbacks can be non-trivial</li>
    </ul>
  </li>
</ul>

<h3 id="example-inference-python---tflite">Example Inference (Python - TFLite)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code14"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code14"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Load model
</span><span class="n">interpreter</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">Interpreter</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"mobilenet_v2.tflite"</span><span class="p">)</span>
<span class="n">interpreter</span><span class="p">.</span><span class="n">allocate_tensors</span><span class="p">()</span>

<span class="c1"># Prepare input
</span><span class="n">input_details</span> <span class="o">=</span> <span class="n">interpreter</span><span class="p">.</span><span class="n">get_input_details</span><span class="p">()</span>
<span class="n">output_details</span> <span class="o">=</span> <span class="n">interpreter</span><span class="p">.</span><span class="n">get_output_details</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Run inference
</span><span class="n">interpreter</span><span class="p">.</span><span class="n">set_tensor</span><span class="p">(</span><span class="n">input_details</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'index'</span><span class="p">],</span> <span class="n">input_data</span><span class="p">)</span>
<span class="n">interpreter</span><span class="p">.</span><span class="n">invoke</span><span class="p">()</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">interpreter</span><span class="p">.</span><span class="n">get_tensor</span><span class="p">(</span><span class="n">output_details</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'index'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Prediction:"</span><span class="p">,</span> <span class="n">output_data</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Delegate usage (Android NNAPI, example via Java/Kotlin):</strong></li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code15"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code15"><span class="nc">Interpreter</span><span class="o">.</span><span class="na">Options</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Interpreter</span><span class="o">.</span><span class="na">Options</span><span class="o">();</span>
<span class="n">options</span><span class="o">.</span><span class="na">addDelegate</span><span class="o">(</span><span class="k">new</span> <span class="nc">NnApiDelegate</span><span class="o">());</span>
<span class="nc">Interpreter</span> <span class="n">interpreter</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Interpreter</span><span class="o">(</span><span class="n">tfliteModel</span><span class="o">,</span> <span class="n">options</span><span class="o">);</span>
</code></pre></div></div>

<h3 id="suitable-applications-4">Suitable Applications</h3>

<ul>
  <li>On-device health and fitness apps</li>
  <li>Real-time object detection in AR</li>
  <li>Offline voice recognition</li>
  <li>Edge anomaly detection</li>
  <li>
    <p>TinyML deployments with <code class="language-plaintext highlighter-rouge">TensorFlow Lite for Microcontrollers</code></p>
  </li>
  <li>TensorFlow Lite remains one of the most production-hardened and flexible runtimes for on-device ML, particularly in mobile and embedded contexts. Its support for multiple delegates and optimizations makes it a go-to choice for developers deploying models outside the cloud.</li>
</ul>

<h3 id="comparative-analysis">Comparative Analysis</h3>

<ul>
  <li>Here are detailed tabular comparisons that encapsulates all key aspects across the different on-device ML runtimes discussed in the primer.</li>
</ul>

<h4 id="general-characteristics">General Characteristics</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorRT</strong></th>
<th class="tg-hcenter-valign-first"><strong>Core ML</strong></th>
<th class="tg-hcenter-valign-first"><strong>MLX</strong></th>
<th class="tg-hcenter-valign-first"><strong>ONNX Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>ExecuTorch</strong></th>
<th class="tg-hcenter-valign-first"><strong>LidarTLM</strong></th>
<th class="tg-hcenter-valign-first"><strong><code>llama.cpp</code></strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Target Platform(s)</td>
<td class="tg-tleft-valign-first">NVIDIA Jetson, Desktop, Server</td>
<td class="tg-tleft-valign-first">Apple devices (iOS/macOS)</td>
<td class="tg-tleft-valign-first">Apple Silicon (macOS only)</td>
<td class="tg-tleft-valign-first">Cross-platform</td>
<td class="tg-tleft-valign-first">Embedded, mobile, MCU</td>
<td class="tg-tleft-valign-first">Robotics, automotive, ADAS</td>
<td class="tg-tleft-valign-first">Desktop, mobile, browser</td>
<td class="tg-tleft-valign-first">Cross-platform (mobile/edge)</td>
<td class="tg-tleft-valign-second">Cloud / server environments</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">ML Task Focus</td>
<td class="tg-tleft-valign-first">Optimized inference</td>
<td class="tg-tleft-valign-first">General ML (vision, NLP)</td>
<td class="tg-tleft-valign-first">Research, transformer/NLP</td>
<td class="tg-tleft-valign-first">General ML</td>
<td class="tg-tleft-valign-first">Ultra-light inference</td>
<td class="tg-tleft-valign-first">3D spatial perception</td>
<td class="tg-tleft-valign-first">Large language model inference</td>
<td class="tg-tleft-valign-first">General ML</td>
<td class="tg-tleft-valign-second">Scalable inference serving</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Inference Only?</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">No (supports training)</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Yes</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Open Source?</td>
<td class="tg-tleft-valign-first">Partially (binaries open, tools closed)</td>
<td class="tg-tleft-valign-first">Partially (via tools)</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Partially / variable</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Yes</td>
</tr>
</tbody>
</table>
</div>

<h4 id="model-formats-and-conversion">Model Formats and Conversion</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorRT</strong></th>
<th class="tg-hcenter-valign-first"><strong>Core ML</strong></th>
<th class="tg-hcenter-valign-first"><strong>MLX</strong></th>
<th class="tg-hcenter-valign-first"><strong>ONNX Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>ExecuTorch</strong></th>
<th class="tg-hcenter-valign-first"><strong>LidarTLM</strong></th>
<th class="tg-hcenter-valign-first"><strong><code>llama.cpp</code></strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Primary Format</td>
<td class="tg-tleft-valign-first">.plan (TensorRT engine file)</td>
<td class="tg-tleft-valign-first">.mlmodelc</td>
<td class="tg-tleft-valign-first">Python-defined layers</td>
<td class="tg-tleft-valign-first">.onnx</td>
<td class="tg-tleft-valign-first">.ptc (compiled TorchScript)</td>
<td class="tg-tleft-valign-first">Custom / converted .onnx / raw tensors</td>
<td class="tg-tleft-valign-first">.gguf (quantized LLMs)</td>
<td class="tg-tleft-valign-first">.tflite (FlatBuffer)</td>
<td class="tg-tleft-valign-second">SavedModel (.pb, .pbtxt)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Supported Frameworks</td>
<td class="tg-tleft-valign-first">PyTorch, ONNX</td>
<td class="tg-tleft-valign-first">PyTorch, TF (via converters)</td>
<td class="tg-tleft-valign-first">Native Python API</td>
<td class="tg-tleft-valign-first">PyTorch, TensorFlow, others</td>
<td class="tg-tleft-valign-first">PyTorch (TorchScript subset)</td>
<td class="tg-tleft-valign-first">PyTorch, TensorFlow (via export)</td>
<td class="tg-tleft-valign-first">LLaMA-family only</td>
<td class="tg-tleft-valign-first">TensorFlow, Keras</td>
<td class="tg-tleft-valign-second">TensorFlow only</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Conversion Required?</td>
<td class="tg-tleft-valign-first">Yes (from ONNX or PyTorch export)</td>
<td class="tg-tleft-valign-first">Yes (via coremltools)</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-first">Yes (usually from PyTorch)</td>
<td class="tg-tleft-valign-first">Yes (via AOT compiler)</td>
<td class="tg-tleft-valign-first">Yes, often includes preprocessing</td>
<td class="tg-tleft-valign-first">Yes (convert + quantize)</td>
<td class="tg-tleft-valign-first">Yes (TFLiteConverter)</td>
<td class="tg-tleft-valign-second">No (already in target format)</td>
</tr>
</tbody>
</table>
</div>

<h4 id="execution-model-and-hardware-support">Execution Model and Hardware Support</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorRT</strong></th>
<th class="tg-hcenter-valign-first"><strong>Core ML</strong></th>
<th class="tg-hcenter-valign-first"><strong>MLX</strong></th>
<th class="tg-hcenter-valign-first"><strong>ONNX Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>ExecuTorch</strong></th>
<th class="tg-hcenter-valign-first"><strong>LidarTLM</strong></th>
<th class="tg-hcenter-valign-first"><strong><code>llama.cpp</code></strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Execution Type</td>
<td class="tg-tleft-valign-first">AOT compiled CUDA graph</td>
<td class="tg-tleft-valign-first">Eager, dynamic hardware assignment</td>
<td class="tg-tleft-valign-first">Eager + compiled graph</td>
<td class="tg-tleft-valign-first">Static graph with runtime optimizations</td>
<td class="tg-tleft-valign-first">Bytecode VM interpreter</td>
<td class="tg-tleft-valign-first">Sparse 3D graph + temporal flow</td>
<td class="tg-tleft-valign-first">Manual loop over transformer layers</td>
<td class="tg-tleft-valign-first">Static interpreter + delegates</td>
<td class="tg-tleft-valign-second">REST/gRPC inference pipeline</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">CPU Support</td>
<td class="tg-tleft-valign-first">No (GPU only)</td>
<td class="tg-tleft-valign-first">Yes (fallback)</td>
<td class="tg-tleft-valign-first">Yes (M1/M2 optimized)</td>
<td class="tg-tleft-valign-first">Yes (default EP)</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-first">Yes (highly optimized)</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Yes</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">GPU Support</td>
<td class="tg-tleft-valign-first">Yes (CUDA, Tensor Cores)</td>
<td class="tg-tleft-valign-first">Yes (Metal)</td>
<td class="tg-tleft-valign-first">Yes (via MPS)</td>
<td class="tg-tleft-valign-first">Yes (CUDA, DirectML, etc.)</td>
<td class="tg-tleft-valign-first">Limited</td>
<td class="tg-tleft-valign-first">Yes (CUDA, embedded GPUs)</td>
<td class="tg-tleft-valign-first">Optional (Metal, CUDA, OpenCL)</td>
<td class="tg-tleft-valign-first">Yes (OpenCL, Metal)</td>
<td class="tg-tleft-valign-second">No</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">NPU / DSP Support</td>
<td class="tg-tleft-valign-first">No</td>
<td class="tg-tleft-valign-first">Yes (Apple ANE)</td>
<td class="tg-tleft-valign-first">Emerging ANE support</td>
<td class="tg-tleft-valign-first">Yes (via NNAPI, OpenVINO, etc.)</td>
<td class="tg-tleft-valign-first">Potential via backend interface</td>
<td class="tg-tleft-valign-first">Yes (TI, Nvidia, ADAS accelerators)</td>
<td class="tg-tleft-valign-first">No (LLM-focused, CPU-oriented)</td>
<td class="tg-tleft-valign-first">Yes (NNAPI, EdgeTPU, Hexagon)</td>
<td class="tg-tleft-valign-second">No</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Hardware Abstraction</td>
<td class="tg-tleft-valign-first">Low-level plugin engine, manual tuning</td>
<td class="tg-tleft-valign-first">Automatic</td>
<td class="tg-tleft-valign-first">Manual tuning via MLX</td>
<td class="tg-tleft-valign-first">Modular Execution Providers (EPs)</td>
<td class="tg-tleft-valign-first">Compiled dispatcher with targets</td>
<td class="tg-tleft-valign-first">Device-specific optimization required</td>
<td class="tg-tleft-valign-first">Low-level SIMD/CUDA offload</td>
<td class="tg-tleft-valign-first">Delegate-based (pluggable)</td>
<td class="tg-tleft-valign-second">N/A</td>
</tr>
</tbody>
</table>
</div>

<h4 id="optimization-size-and-constraints">Optimization, Size, and Constraints</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorRT</strong></th>
<th class="tg-hcenter-valign-first"><strong>Core ML</strong></th>
<th class="tg-hcenter-valign-first"><strong>MLX</strong></th>
<th class="tg-hcenter-valign-first"><strong>ONNX Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>ExecuTorch</strong></th>
<th class="tg-hcenter-valign-first"><strong>LidarTLM</strong></th>
<th class="tg-hcenter-valign-first"><strong><code>llama.cpp</code></strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Model Optimization Support</td>
<td class="tg-tleft-valign-first">Yes (kernel tuning, quantization, `float16`/`int8`)</td>
<td class="tg-tleft-valign-first">Yes (ANE targeting, quantization)</td>
<td class="tg-tleft-valign-first">No built-in, manual scripting</td>
<td class="tg-tleft-valign-first">Yes (quantization, pruning, graph fusion)</td>
<td class="tg-tleft-valign-first">Yes (operator pruning, bytecode fusion)</td>
<td class="tg-tleft-valign-first">Yes (3D-aware compression and fusions)</td>
<td class="tg-tleft-valign-first">Yes (quantized GGUF)</td>
<td class="tg-tleft-valign-first">Yes (quantization, fusion)</td>
<td class="tg-tleft-valign-second">Yes (batching, threading)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Runtime Size</td>
<td class="tg-tleft-valign-first">Medium (~5–15 MB)</td>
<td class="tg-tleft-valign-first">Medium (~5–10 MB)</td>
<td class="tg-tleft-valign-first">Medium</td>
<td class="tg-tleft-valign-first">Large (5–30 MB)</td>
<td class="tg-tleft-valign-first">Very small (&lt;1 MB)</td>
<td class="tg-tleft-valign-first">Medium–Large</td>
<td class="tg-tleft-valign-first">Small–Medium</td>
<td class="tg-tleft-valign-first">Small (~0.5–5 MB)</td>
<td class="tg-tleft-valign-second">Very large (&gt;100 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Memory Footprint (Inference)</td>
<td class="tg-tleft-valign-first">Low to moderate (GPU memory bound)</td>
<td class="tg-tleft-valign-first">Low to moderate</td>
<td class="tg-tleft-valign-first">Moderate (GPU-heavy)</td>
<td class="tg-tleft-valign-first">Variable (depends on EPs)</td>
<td class="tg-tleft-valign-first">Ultra-low (sub-MB possible)</td>
<td class="tg-tleft-valign-first">High (large point cloud buffers)</td>
<td class="tg-tleft-valign-first">Low (~3–6 GB RAM for 7B models)</td>
<td class="tg-tleft-valign-first">Low</td>
<td class="tg-tleft-valign-second">High</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Latency</td>
<td class="tg-tleft-valign-first">Very low (sub-ms possible)</td>
<td class="tg-tleft-valign-first">Low (with ANE/GPU)</td>
<td class="tg-tleft-valign-first">Medium (eager mode)</td>
<td class="tg-tleft-valign-first">Variable (highly EP dependent)</td>
<td class="tg-tleft-valign-first">Very low</td>
<td class="tg-tleft-valign-first">Moderate to high (depends on density)</td>
<td class="tg-tleft-valign-first">Low (for small LLMs)</td>
<td class="tg-tleft-valign-first">Low (under 10ms typical)</td>
<td class="tg-tleft-valign-second">Moderate to high</td>
</tr>
</tbody>
</table>
</div>

<h4 id="flexibility-debugging-and-ecosystem">Flexibility, Debugging, and Ecosystem</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorRT</strong></th>
<th class="tg-hcenter-valign-first"><strong>Core ML</strong></th>
<th class="tg-hcenter-valign-first"><strong>MLX</strong></th>
<th class="tg-hcenter-valign-first"><strong>ONNX Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>ExecuTorch</strong></th>
<th class="tg-hcenter-valign-first"><strong>LidarTLM</strong></th>
<th class="tg-hcenter-valign-first"><strong><code>llama.cpp</code></strong></th>
<th class="tg-hcenter-valign-first"><strong>TensorFlow Lite</strong></th>
<th class="tg-hcenter-valign-second"><strong>TensorFlow Serving</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Custom Ops Support</td>
<td class="tg-tleft-valign-first">Yes (via plugin library API)</td>
<td class="tg-tleft-valign-first">Limited (via <code>MLCustomModel</code>)</td>
<td class="tg-tleft-valign-first">Full (via Python subclassing)</td>
<td class="tg-tleft-valign-first">Yes (custom EPs and ops)</td>
<td class="tg-tleft-valign-first">Yes (C++ op authoring)</td>
<td class="tg-tleft-valign-first">Yes (often required)</td>
<td class="tg-tleft-valign-first">No (fixed transformer kernel set)</td>
<td class="tg-tleft-valign-first">Yes (C++/C custom kernels)</td>
<td class="tg-tleft-valign-second">Yes</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Community &amp; Documentation</td>
<td class="tg-tleft-valign-first">Strong NVIDIA developer support, active forums</td>
<td class="tg-tleft-valign-first">Strong, Apple developer-centric</td>
<td class="tg-tleft-valign-first">Niche, growing</td>
<td class="tg-tleft-valign-first">Very strong</td>
<td class="tg-tleft-valign-first">Growing (Meta-sponsored)</td>
<td class="tg-tleft-valign-first">Limited / hardware-vendor specific</td>
<td class="tg-tleft-valign-first">Active open-source base</td>
<td class="tg-tleft-valign-first">Mature, large community</td>
<td class="tg-tleft-valign-second">Very mature in production</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Debugger Support</td>
<td class="tg-tleft-valign-first">Nsight Systems, profiling tools, verbose logging</td>
<td class="tg-tleft-valign-first">Xcode tools</td>
<td class="tg-tleft-valign-first">Python debug console</td>
<td class="tg-tleft-valign-first">Moderate (model inspection tools)</td>
<td class="tg-tleft-valign-first">Minimal (CLI, log-based)</td>
<td class="tg-tleft-valign-first">Custom tooling per device</td>
<td class="tg-tleft-valign-first">Log-level output only</td>
<td class="tg-tleft-valign-first">TensorBoard-lite, CLI tools</td>
<td class="tg-tleft-valign-second">Monitoring via Prometheus, etc.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Ease of Use</td>
<td class="tg-tleft-valign-first">Medium (manual optimization, engine building)</td>
<td class="tg-tleft-valign-first">High for Apple developers</td>
<td class="tg-tleft-valign-first">Medium (researchers, tinkerers)</td>
<td class="tg-tleft-valign-first">Moderate to high (depends on EP)</td>
<td class="tg-tleft-valign-first">Medium (steep setup curve)</td>
<td class="tg-tleft-valign-first">Low (requires system integration)</td>
<td class="tg-tleft-valign-first">High (once model is quantized)</td>
<td class="tg-tleft-valign-first">High (especially with <code>model maker</code>)</td>
<td class="tg-tleft-valign-second">Medium to high (requires infra)</td>
</tr>
</tbody>
</table>
</div>

<h3 id="comparative-summary-and-guidance">Comparative Summary and Guidance</h3>

<h4 id="feature-comparison-table">Feature Comparison Table</h4>

<ul>
  <li>This section provides a side-by-side comparison of the on-device ML runtimes discussed, highlighting their architectural differences, platform support, performance characteristics, and ideal use cases. This helps clarify which runtime best fits various project needs, from embedded development to mobile apps and language model inference.</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Runtime</strong></th>
<th class="tg-hcenter-valign-first"><strong>Platform Support</strong></th>
<th class="tg-hcenter-valign-first"><strong>Model Format</strong></th>
<th class="tg-hcenter-valign-first"><strong>Hardware Acceleration</strong></th>
<th class="tg-hcenter-valign-first"><strong>Optimized For</strong></th>
<th class="tg-hcenter-valign-first"><strong>Custom Ops</strong></th>
<th class="tg-hcenter-valign-second"><strong>Size Footprint</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">TensorRT</td>
<td class="tg-tleft-valign-first">NVIDIA GPUs (desktop, Jetson, server)</td>
<td class="tg-tleft-valign-first">ONNX, `.plan` (engine file)</td>
<td class="tg-tleft-valign-first">CUDA, Tensor Cores</td>
<td class="tg-tleft-valign-first">Low-latency GPU inference</td>
<td class="tg-tleft-valign-first">Yes (via plugin system)</td>
<td class="tg-tleft-valign-second">Medium (~5–15 MB)</td>
</tr>    
<tr>
<td class="tg-tleft-valign-first">Core ML</td>
<td class="tg-tleft-valign-first">Apple only (iOS/macOS)</td>
<td class="tg-tleft-valign-first">`.mlmodelc`</td>
<td class="tg-tleft-valign-first">CPU, GPU, ANE</td>
<td class="tg-tleft-valign-first">App integration on Apple devices</td>
<td class="tg-tleft-valign-first">Limited</td>
<td class="tg-tleft-valign-second">Medium (~2–10 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">MLX</td>
<td class="tg-tleft-valign-first">Apple Silicon (macOS)</td>
<td class="tg-tleft-valign-first">Python code</td>
<td class="tg-tleft-valign-first">MPS, ANE (partial)</td>
<td class="tg-tleft-valign-first">Research &amp; experimentation</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Medium (~2–5 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">ONNX Runtime</td>
<td class="tg-tleft-valign-first">Cross-platform (Mobile &amp; Desktop)</td>
<td class="tg-tleft-valign-first">`.onnx`</td>
<td class="tg-tleft-valign-first">CUDA, NNAPI, DirectML, etc.</td>
<td class="tg-tleft-valign-first">Cross-framework interoperability</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Large (~5–30 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">ExecuTorch</td>
<td class="tg-tleft-valign-first">Embedded, MCUs, Android</td>
<td class="tg-tleft-valign-first">Compiled TorchScript (`.ptc`)</td>
<td class="tg-tleft-valign-first">CPU, MCU, DSP</td>
<td class="tg-tleft-valign-first">Ultra-light edge inference</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Very small (&lt;1 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">LidarTLM</td>
<td class="tg-tleft-valign-first">Embedded/Robotics</td>
<td class="tg-tleft-valign-first">Custom/ONNX</td>
<td class="tg-tleft-valign-first">CUDA, DSP, NPU</td>
<td class="tg-tleft-valign-first">Sparse point cloud inference</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Medium–Large</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><code>llama.cpp</code></td>
<td class="tg-tleft-valign-first">Desktop, Mobile, WASM</td>
<td class="tg-tleft-valign-first">Quantized GGUF</td>
<td class="tg-tleft-valign-first">CPU, Optional GPU</td>
<td class="tg-tleft-valign-first">Efficient LLM inference</td>
<td class="tg-tleft-valign-first">Limited</td>
<td class="tg-tleft-valign-second">Small–Medium (CPU)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">TFLite</td>
<td class="tg-tleft-valign-first">Cross-platform (MCU to mobile)</td>
<td class="tg-tleft-valign-first">`.tflite`</td>
<td class="tg-tleft-valign-first">NNAPI, GPU, DSP, EdgeTPU</td>
<td class="tg-tleft-valign-first">Mobile and embedded AI</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Small (~500 KB–5 MB)</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">TF Serving</td>
<td class="tg-tleft-valign-first">Cloud/Server</td>
<td class="tg-tleft-valign-first">SavedModel</td>
<td class="tg-tleft-valign-first">N/A</td>
<td class="tg-tleft-valign-first">Scalable online inference</td>
<td class="tg-tleft-valign-first">Yes</td>
<td class="tg-tleft-valign-second">Very large (&gt;100 MB)</td>
</tr>
</tbody>
</table>
</div>

<h4 id="strengths-by-runtime">Strengths by Runtime</h4>

<ul>
  <li>
    <p><strong>Core ML</strong>: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.</p>
  </li>
  <li>
    <p><strong>MLX</strong>: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.</p>
  </li>
  <li>
    <p><strong>ONNX Runtime</strong>: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.</p>
  </li>
  <li>
    <p><strong>ExecuTorch</strong>: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.</p>
  </li>
  <li>
    <p><strong>LidarTLM</strong>: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">llama.cpp</code></strong>: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.</p>
  </li>
  <li>
    <p><strong>TFLite</strong>: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.</p>
  </li>
  <li>
    <p><strong>TF Serving</strong>: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.</p>
  </li>
</ul>

<h3 id="runtime-selection-guidance">Runtime Selection Guidance</h3>

<ul>
  <li>
    <p><strong>If you’re deploying to iOS or macOS</strong>:</p>

    <ul>
      <li>Use <strong>Core ML</strong> for production apps.</li>
      <li>Use <strong>MLX</strong> for research, local experimentation, or custom modeling.</li>
    </ul>
  </li>
  <li>
    <p><strong>If you’re deploying to embedded edge devices</strong>:</p>

    <ul>
      <li>Use <strong>ExecuTorch</strong> for PyTorch-based workflows.</li>
      <li>Use <strong>TensorFlow Lite for Microcontrollers</strong> for tight memory constraints.</li>
      <li>Consider <strong>LidarTLM</strong>-style tools if dealing with 3D spatial data.</li>
    </ul>
  </li>
  <li>
    <p><strong>If you’re targeting Android or need portability</strong>:</p>

    <ul>
      <li>Use <strong>TensorFlow Lite</strong> or <strong>ONNX Runtime</strong> with delegates like NNAPI or GPU.</li>
    </ul>
  </li>
  <li>
    <p><strong>If you’re working with LLMs locally</strong>:</p>

    <ul>
      <li>Use <strong><code class="language-plaintext highlighter-rouge">llama.cpp</code></strong> for best CPU-based inference and minimal setup.</li>
    </ul>
  </li>
  <li>
    <p><strong>If you want cross-framework model portability</strong>:</p>

    <ul>
      <li>Use <strong>ONNX Runtime</strong> with models exported from PyTorch, TensorFlow, or others.</li>
    </ul>
  </li>
  <li>
    <p><strong>If you require real-time, high-volume cloud inference</strong>:</p>

    <ul>
      <li>Use <strong>TensorFlow Serving</strong> or ONNX Runtime Server.</li>
    </ul>
  </li>
</ul>

<h3 id="final-thoughts">Final Thoughts</h3>

<ul>
  <li>
    <p>Choosing the right on-device ML runtime depends heavily on the following factors:</p>

    <ul>
      <li>Deployment environment (mobile, embedded, desktop, web, cloud)</li>
      <li>Model architecture (CNN, RNN, transformer, etc.)</li>
      <li>Performance requirements (latency, FPS, memory usage)</li>
      <li>Development preferences (PyTorch, TensorFlow, raw C++, etc.)</li>
      <li>Hardware capabilities (CPU, GPU, NPU, DSP, etc.)</li>
    </ul>
  </li>
  <li>
    <p>Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:</p>

    <ul>
      <li><strong>Best for Apple-native app development</strong>: <em>Core ML</em></li>
      <li><strong>Best for Apple-based model experimentation</strong>: <em>MLX</em></li>
      <li><strong>Best for cross-platform portability and hardware access</strong>: <em>ONNX Runtime</em></li>
      <li><strong>Best for minimal embedded inference</strong>: <em>ExecuTorch</em></li>
      <li><strong>Best for 3D LiDAR/robotics</strong>: <em>LidarTLM-like stacks</em></li>
      <li><strong>Best for on-device LLM inference</strong>: <em><code class="language-plaintext highlighter-rouge">llama.cpp</code></em></li>
      <li><strong>Best for mobile/embedded general ML</strong>: <em>TensorFlow Lite</em></li>
      <li><strong>Best for scalable cloud inference</strong>: <em>TensorFlow Serving</em></li>
    </ul>
  </li>
</ul>

<h2 id="related-serialization-formats-across-runtimes">Related: Serialization Formats Across Runtimes</h2>

<ul>
  <li>In machine learning runtimes, how a model is <strong>serialized</strong>—i.e., stored and structured on disk—is critical for performance, compatibility, and portability. Serialization formats determine how the computation graph, parameters, metadata, and sometimes even execution plans are encoded and interpreted by the runtime. Each runtime typically adopts a format aligned with its optimization goals: whether that’s minimal size, fast loading, platform neutrality, or human-readability for debugging.</li>
  <li>Here we briefly compare four major serialization formats used across popular on-device ML runtimes: <strong>Protocol Buffers (Protobuf)</strong>, <strong>FlatBuffer</strong>, <strong>GGUF</strong>, and <strong>Bytecode formats</strong>, reinforcing how data structures are stored, loaded, and interpreted at runtime.</li>
</ul>

<h3 id="protocol-buffers-protobuf">Protocol Buffers (Protobuf)</h3>

<ul>
  <li>
    <p><strong>Used by</strong>: TensorFlow (SavedModel, <code class="language-plaintext highlighter-rouge">.pb</code>), ONNX (<code class="language-plaintext highlighter-rouge">.onnx</code>)</p>
  </li>
  <li>
    <p><strong>Developed by</strong>: Google</p>
  </li>
  <li>
    <p><strong>Type</strong>: Binary serialization framework</p>
  </li>
  <li>
    <p><strong>Key Characteristics</strong>:</p>

    <ul>
      <li>Encodes structured data using <code class="language-plaintext highlighter-rouge">.proto</code> schemas</li>
      <li>Supports code generation in multiple languages (Python, C++, Java, etc.)</li>
      <li>Strict type definitions with schema versioning</li>
      <li>Produces portable, efficient, extensible binary files</li>
    </ul>
  </li>
  <li>
    <p><strong>Advantages</strong>:</p>

    <ul>
      <li>Highly compact, faster than JSON/XML</li>
      <li>Strong backward and forward compatibility through schema evolution</li>
      <li>Ideal for representing complex hierarchical graphs (e.g., model computation trees)</li>
    </ul>
  </li>
  <li>
    <p><strong>In ML context</strong>:</p>

    <ul>
      <li><strong>TensorFlow</strong>: Stores entire computation graph, tensor shapes, and metadata in <code class="language-plaintext highlighter-rouge">.pb</code> (protobuf binary)</li>
      <li><strong>ONNX</strong>: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema</li>
    </ul>
  </li>
  <li>
    <p><strong>Limitations</strong>:</p>

    <ul>
      <li>Parsing requires full message decoding into memory</li>
      <li>Less suited for minimal-footprint scenarios (e.g., MCUs)</li>
    </ul>
  </li>
  <li>
    <p><strong>Example</strong>:</p>

    <ul>
      <li>
        <p><em>Used in: TensorFlow (<code class="language-plaintext highlighter-rouge">.pb</code>, SavedModel), ONNX (<code class="language-plaintext highlighter-rouge">.onnx</code>)</em></p>
      </li>
      <li>
        <p>Protobuf defines a schema in <code class="language-plaintext highlighter-rouge">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>
      </li>
      <li>
        <p><strong>Schema Definition (<code class="language-plaintext highlighter-rouge">graph.proto</code>):</strong></p>

        <div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code16"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code16">  <span class="kd">message</span> <span class="nc">TensorShape</span> <span class="p">{</span>
    <span class="k">repeated</span> <span class="kt">int64</span> <span class="na">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kd">message</span> <span class="nc">Node</span> <span class="p">{</span>
    <span class="kt">string</span> <span class="na">op_type</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="k">repeated</span> <span class="kt">string</span> <span class="na">input</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
    <span class="k">repeated</span> <span class="kt">string</span> <span class="na">output</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kd">message</span> <span class="nc">Graph</span> <span class="p">{</span>
    <span class="k">repeated</span> <span class="n">Node</span> <span class="na">node</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">repeated</span> <span class="n">TensorShape</span> <span class="na">input_shape</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="k">repeated</span> <span class="n">TensorShape</span> <span class="na">output_shape</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Example Python Usage (ONNX-style):</strong></p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code17"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code17">  <span class="kn">import</span> <span class="nn">onnx</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"resnet50.onnx"</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">node</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Shows first operation (e.g., Conv)
</span></code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Serialized File:</strong></p>

        <ul>
          <li>A binary <code class="language-plaintext highlighter-rouge">.onnx</code> or <code class="language-plaintext highlighter-rouge">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="flatbuffer">FlatBuffer</h3>

<ul>
  <li>
    <p><strong>Used by</strong>: TensorFlow Lite (<code class="language-plaintext highlighter-rouge">.tflite</code>)</p>
  </li>
  <li>
    <p><strong>Developed by</strong>: Google</p>
  </li>
  <li>
    <p><strong>Type</strong>: Binary serialization library with zero-copy design</p>
  </li>
  <li>
    <p><strong>Key Characteristics</strong>:</p>

    <ul>
      <li>Allows direct access to data without unpacking (zero-copy reads)</li>
      <li>Compact binary representation optimized for low-latency parsing</li>
      <li>Built-in schema evolution support</li>
    </ul>
  </li>
  <li>
    <p><strong>Advantages</strong>:</p>

    <ul>
      <li>Near-instantaneous loading—no deserialization overhead</li>
      <li>Perfect for mobile/embedded devices with tight latency or startup constraints</li>
      <li>Schema-aware tooling for validation</li>
    </ul>
  </li>
  <li>
    <p><strong>In ML context</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">.tflite</code> files store computation graphs, tensors, and metadata using FlatBuffer encoding</li>
      <li>Facilitates runtime interpretation without converting the graph into a different memory format</li>
    </ul>
  </li>
  <li>
    <p><strong>Limitations</strong>:</p>

    <ul>
      <li>Harder to inspect/debug than JSON or Protobuf</li>
      <li>Limited dynamic structure capabilities compared to Protobuf</li>
    </ul>
  </li>
  <li>
    <p><strong>Example</strong>:</p>

    <ul>
      <li>
        <p><em>Used in: TensorFlow Lite (<code class="language-plaintext highlighter-rouge">.tflite</code>)</em></p>
      </li>
      <li>
        <p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>
      </li>
      <li>
        <p><strong>FlatBuffer Schema (simplified):</strong></p>

        <pre><code class="language-idl">  table Tensor {
    shape: [int];
    type: int;
    buffer: int;
  }

  table Operator {
    opcode_index: int;
    inputs: [int];
    outputs: [int];
  }

  table Model {
    tensors: [Tensor];
    operators: [Operator];
  }
</code></pre>
      </li>
      <li>
        <p><strong>Example Python Usage:</strong></p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code18"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code18">  <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

  <span class="n">interpreter</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">Interpreter</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"mobilenet_v2.tflite"</span><span class="p">)</span>
  <span class="n">interpreter</span><span class="p">.</span><span class="n">allocate_tensors</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="n">interpreter</span><span class="p">.</span><span class="n">get_input_details</span><span class="p">())</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Serialized File:</strong></p>
        <ul>
          <li>A <code class="language-plaintext highlighter-rouge">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="gguf-gpt-generated-ggml-unified-format">GGUF (GPT-generated GGML Unified Format)</h3>

<ul>
  <li>
    <p><strong>Used by</strong>: <code class="language-plaintext highlighter-rouge">llama.cpp</code> and its LLM-compatible ecosystem</p>
  </li>
  <li>
    <p><strong>Developed by</strong>: Community (successor to GGML model format)</p>
  </li>
  <li>
    <p><strong>Type</strong>: Lightweight binary tensor format for large language models</p>
  </li>
  <li>
    <p><strong>Key Characteristics</strong>:</p>

    <ul>
      <li>Encodes quantized transformer weights and architecture metadata</li>
      <li>Designed for efficient memory mapping and low-RAM usage</li>
      <li>Built for CPU-first inference (with optional GPU support)</li>
    </ul>
  </li>
  <li>
    <p><strong>Advantages</strong>:</p>

    <ul>
      <li>Extremely compact, especially with quantization (4–8 bit)</li>
      <li>Simple, fast memory-mapped loading (<code class="language-plaintext highlighter-rouge">mmap</code>)</li>
      <li>Compatible with CPU-based inference engines (no dependencies)</li>
    </ul>
  </li>
  <li>
    <p><strong>In ML context</strong>:</p>

    <ul>
      <li>Stores models like LLaMA, Mistral, Alpaca after quantization</li>
      <li>Used by <code class="language-plaintext highlighter-rouge">llama.cpp</code>, <code class="language-plaintext highlighter-rouge">llm.cpp</code>, <code class="language-plaintext highlighter-rouge">text-generation-webui</code>, and other local LLM tools</li>
    </ul>
  </li>
  <li>
    <p><strong>Limitations</strong>:</p>

    <ul>
      <li>Not general-purpose—only suitable for transformer LLMs</li>
      <li>Lacks complex graph control (branching, dynamic ops)</li>
    </ul>
  </li>
  <li>
    <p><strong>Example</strong>:</p>

    <ul>
      <li>
        <p>Used in: <code class="language-plaintext highlighter-rouge">llama.cpp</code>, quantized LLMs*</p>
      </li>
      <li>
        <p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>
      </li>
      <li>
        <p><strong>Header Block (example layout in binary format):</strong></p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code19"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code19">  GGUF
  version: 3
  tensor_count: 397
  metadata:
    model_type: llama
    vocab_size: 32000
    quantization: Q4_0
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Python conversion (from PyTorch):</strong></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code20"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code20">  python convert.py <span class="nt">--input</span> model.bin <span class="nt">--output</span> model.gguf <span class="nt">--format</span> Q4_0
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Reading from <code class="language-plaintext highlighter-rouge">llama.cpp</code>:</strong></p>

        <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code21"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code21">  <span class="n">gguf_context</span> <span class="o">*</span><span class="n">ctx</span> <span class="o">=</span> <span class="n">gguf_init_from_file</span><span class="p">(</span><span class="s">"llama-7B.Q4_0.gguf"</span><span class="p">);</span>
  <span class="n">ggml_tensor</span> <span class="o">*</span><span class="n">wq</span> <span class="o">=</span> <span class="n">gguf_get_tensor_by_name</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="s">"layers.0.attn.wq"</span><span class="p">);</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Serialized File:</strong></p>
        <ul>
          <li>A <code class="language-plaintext highlighter-rouge">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="bytecode-format-executorch">Bytecode Format (ExecuTorch)</h3>

<ul>
  <li>
    <p><strong>Used by</strong>: ExecuTorch</p>
  </li>
  <li>
    <p><strong>Developed by</strong>: Meta</p>
  </li>
  <li>
    <p><strong>Type</strong>: Custom AOT-compiled bytecode</p>
  </li>
  <li>
    <p><strong>Key Characteristics</strong>:</p>

    <ul>
      <li>Outputs compact bytecode (<code class="language-plaintext highlighter-rouge">.ptc</code>) from PyTorch models via TorchScript tracing</li>
      <li>Prunes unused operators to reduce binary size</li>
      <li>Embeds minimal op metadata needed for runtime VM</li>
    </ul>
  </li>
  <li>
    <p><strong>Advantages</strong>:</p>

    <ul>
      <li>Highly portable and minimal—can run on MCUs and RTOS platforms</li>
      <li>Deterministic memory usage and low overhead</li>
      <li>Enables static linking of models and kernels for bare-metal systems</li>
    </ul>
  </li>
  <li>
    <p><strong>In ML context</strong>:</p>

    <ul>
      <li>Targets constrained devices (sub-MB RAM)</li>
      <li>Supports fixed operator sets with predictable memory and runtime behavior</li>
    </ul>
  </li>
  <li>
    <p><strong>Limitations</strong>:</p>

    <ul>
      <li>Rigid format—not well suited for dynamic models or rich graph structures</li>
      <li>Tied closely to PyTorch tracing and compilation pipeline.</li>
    </ul>
  </li>
  <li>
    <p><strong>Example</strong>:</p>

    <ul>
      <li>
        <p><em>Used in: ExecuTorch (<code class="language-plaintext highlighter-rouge">.ptc</code> format)</em></p>
      </li>
      <li>
        <p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>
      </li>
      <li>
        <p><strong>Model Compilation:</strong></p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code22"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code22">  <span class="kn">import</span> <span class="nn">torch</span>

  <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">scripted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span><span class="p">(</span><span class="n">Net</span><span class="p">())</span>
  <span class="n">scripted</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"net.pt"</span><span class="p">)</span>  <span class="c1"># TorchScript
</span>
  <span class="c1"># Compile to ExecuTorch format
</span>  <span class="err">!</span><span class="n">executorchc</span> <span class="nb">compile</span> <span class="o">--</span><span class="n">model</span> <span class="n">net</span><span class="p">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">output</span> <span class="n">net</span><span class="p">.</span><span class="n">ptc</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Runtime Use in C++:</strong></p>

        <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code23"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code23">  <span class="n">executorch</span><span class="o">::</span><span class="n">Runtime</span> <span class="n">runtime</span><span class="p">;</span>
  <span class="n">runtime</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">"net.ptc"</span><span class="p">);</span>
  <span class="n">runtime</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">);</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Serialized File:</strong></p>
        <ul>
          <li>A <code class="language-plaintext highlighter-rouge">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="comparative-analysis-1">Comparative Analysis</h3>

<ul>
  <li>Understanding the serialization format is crucial when choosing a runtime—especially for performance, portability, and debugging. Developers targeting mobile and embedded environments often prefer FlatBuffer or bytecode for efficiency, while cloud/server or cross-platform projects benefit from Protobuf’s rich graph encoding.</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Format</strong></th>
<th class="tg-hcenter-valign-first"><strong>Used By</strong></th>
<th class="tg-hcenter-valign-first"><strong>Format Type</strong></th>
<th class="tg-hcenter-valign-first"><strong>Example File</strong></th>
<th class="tg-hcenter-valign-first"><strong>Viewability</strong></th>
<th class="tg-hcenter-valign-first"><strong>Tool to Inspect</strong></th>
<th class="tg-hcenter-valign-first"><strong>Strengths</strong></th>
<th class="tg-hcenter-valign-second"><strong>Limitations</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Protobuf</td>
<td class="tg-tleft-valign-first">TensorFlow, ONNX</td>
<td class="tg-tleft-valign-first">Binary (schema-driven)</td>
<td class="tg-tleft-valign-first"><code>model.onnx</code>, <code>model.pb</code></td>
<td class="tg-tleft-valign-first">Binary</td>
<td class="tg-tleft-valign-first"><code>onnx</code>, <code>tf.saved_model_cli</code></td>
<td class="tg-tleft-valign-first">Cross-platform, schema evolution, rich structure</td>
<td class="tg-tleft-valign-second">Larger footprint, full deserialization</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">FlatBuffer</td>
<td class="tg-tleft-valign-first">TensorFlow Lite</td>
<td class="tg-tleft-valign-first">Zero-copy binary</td>
<td class="tg-tleft-valign-first"><code>model.tflite</code></td>
<td class="tg-tleft-valign-first">Binary</td>
<td class="tg-tleft-valign-first"><code>flatc</code>, <code>tflite</code> API</td>
<td class="tg-tleft-valign-first">Instant loading, ideal for embedded use</td>
<td class="tg-tleft-valign-second">Harder to inspect/debug</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">GGUF</td>
<td class="tg-tleft-valign-first"><code>llama.cpp</code></td>
<td class="tg-tleft-valign-first">Binary tensor map</td>
<td class="tg-tleft-valign-first"><code>llama-7B.Q4_0.gguf</code></td>
<td class="tg-tleft-valign-first">Binary</td>
<td class="tg-tleft-valign-first"><code>llama.cpp</code>, <code>gguf_dump.py</code></td>
<td class="tg-tleft-valign-first">Ultra-compact, mmap-friendly, quantized</td>
<td class="tg-tleft-valign-second">LLM-specific only</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Bytecode</td>
<td class="tg-tleft-valign-first">ExecuTorch</td>
<td class="tg-tleft-valign-first">Compiled AOT VM</td>
<td class="tg-tleft-valign-first"><code>model.ptc</code></td>
<td class="tg-tleft-valign-first">Binary</td>
<td class="tg-tleft-valign-first"><code>executorchc</code>, ExecuTorch API</td>
<td class="tg-tleft-valign-first">Tiny runtime, embedded-friendly</td>
<td class="tg-tleft-valign-second">Limited flexibility, PyTorch-only</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">TensorRT Engine</td>
<td class="tg-tleft-valign-first">TensorRT</td>
<td class="tg-tleft-valign-first">Binary CUDA engine</td>
<td class="tg-tleft-valign-first"><code>model.plan</code></td>
<td class="tg-tleft-valign-first">Binary</td>
<td class="tg-tleft-valign-first">TensorRT API (<code>trtexec</code>)</td>
<td class="tg-tleft-valign-first">Hardware-optimized, precompiled inference</td>
<td class="tg-tleft-valign-second">NVIDIA-only, not portable</td>
</tr>
</tbody>
</table>
</div>

<h2 id="model-execution-lifecycle-across-ml-runtimes">Model Execution Lifecycle Across ML Runtimes</h2>

<ul>
  <li>On-device and edge-focused ML runtimes vary widely in design, hardware support, and internal implementation. However, the overall lifecycle of executing a machine learning model—across any runtime—can be broken down into a common series of stages.</li>
  <li>This section provides a deep technical walkthrough of each stage in the lifecycle and sets the foundation for understanding how the specific runtimes (TensorRT, Core ML, MLX, ONNX Runtime, ExecuTorch, LidarTLM, llama.cpp, and TensorFlow Lite/Serving) customize or optimize these stages.</li>
  <li>Across all runtimes, model execution follows a common pipeline: convert the trained model into a runtime-compatible format, load and allocate memory, dispatch operations to hardware accelerators (CPU/GPU/NPU), and return structured outputs. Each runtime adapts this flow to its architecture—ranging from compiled CUDA engines (TensorRT) to VM-interpreted bytecode (ExecuTorch) to quantized transformer loops (llama.cpp)—to meet performance, portability, or resource constraints.</li>
</ul>

<h3 id="general-workflow-from-model-to-inference">General Workflow: from Model to Inference</h3>

<h4 id="model-training">Model Training</h4>

<ul>
  <li>
    <p>Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be <strong>exported or converted</strong> into a format compatible with the intended runtime.</p>
  </li>
  <li>
    <p>This stage outputs:</p>

    <ul>
      <li>A trained model file (e.g., <code class="language-plaintext highlighter-rouge">.onnx</code>, <code class="language-plaintext highlighter-rouge">.mlmodel</code>, <code class="language-plaintext highlighter-rouge">.pt</code>, <code class="language-plaintext highlighter-rouge">.tflite</code>, etc.)</li>
      <li>Associated metadata (input/output shapes, quantization info, etc.)</li>
    </ul>
  </li>
</ul>

<h4 id="model-conversion">Model Conversion</h4>

<ul>
  <li>
    <p>This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply <strong>graph simplification</strong>, <strong>quantization</strong>, or <strong>operator fusion</strong>.</p>
  </li>
  <li>
    <p>Typical tools used:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">torch.onnx.export()</code> (PyTorch <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mo" id="MathJax-Span-15" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-5">\rightarrow</script> ONNX)</li>
      <li><code class="language-plaintext highlighter-rouge">coremltools.convert()</code> (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mo" id="MathJax-Span-18" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">\rightarrow</script> Core ML)</li>
      <li><code class="language-plaintext highlighter-rouge">TFLiteConverter</code> (TensorFlow <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mo" id="MathJax-Span-21" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-7">\rightarrow</script> <code class="language-plaintext highlighter-rouge">.tflite</code>)</li>
      <li><code class="language-plaintext highlighter-rouge">executorchc</code> (TorchScript <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="mo" id="MathJax-Span-24" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-8">\rightarrow</script> ExecuTorch bytecode)</li>
      <li><code class="language-plaintext highlighter-rouge">quantize.py</code> (for GGUF / <code class="language-plaintext highlighter-rouge">llama.cpp</code>)</li>
    </ul>
  </li>
  <li>
    <p>This phase outputs:</p>

    <ul>
      <li>Serialized model file tailored for the target runtime</li>
      <li>Optional quantized or optimized variant</li>
    </ul>
  </li>
</ul>

<h4 id="model-loading">Model Loading</h4>

<ul>
  <li>
    <p>At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:</p>

    <ul>
      <li>Internal intermediate representation (IR)</li>
      <li>Execution graph</li>
      <li>Bytecode or linear transformer stack (as in <code class="language-plaintext highlighter-rouge">llama.cpp</code>)</li>
    </ul>
  </li>
  <li>
    <p>Some runtimes use <strong>zero-copy formats</strong> (e.g., FlatBuffer in TFLite) to avoid overhead.</p>
  </li>
</ul>

<h4 id="memory-allocation">Memory Allocation</h4>

<ul>
  <li>
    <p>Before inference can occur, the runtime must allocate:</p>

    <ul>
      <li>Input and output tensor buffers</li>
      <li>Working memory for intermediate computations</li>
      <li>(If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers</li>
    </ul>
  </li>
  <li>
    <p>Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, <code class="language-plaintext highlighter-rouge">llama.cpp</code>).</p>
  </li>
</ul>

<h4 id="inference-execution">Inference Execution</h4>

<ul>
  <li>
    <p>The core execution stage involves:</p>

    <ul>
      <li>Running the model graph or stack</li>
      <li>Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)</li>
      <li>Managing control flow, caching, and batching</li>
    </ul>
  </li>
  <li>
    <p>Different runtimes handle scheduling and dispatch differently:</p>

    <ul>
      <li>TensorRT: CUDA engine with explicit graph scheduling</li>
      <li>TFLite: Static interpreter with delegate hand-off</li>
      <li>ONNX Runtime: Execution Providers (EPs)</li>
      <li><code class="language-plaintext highlighter-rouge">llama.cpp</code>: Single-threaded or parallel transformer loop</li>
    </ul>
  </li>
</ul>

<h4 id="postprocessing--output">Postprocessing &amp; Output</h4>

<ul>
  <li>
    <p>The final outputs are:</p>

    <ul>
      <li>Raw logits, class probabilities, bounding boxes, text, etc.</li>
      <li>Returned via API calls (C++, Python, Swift, etc.)</li>
    </ul>
  </li>
  <li>
    <p>This stage may also include:</p>

    <ul>
      <li>Dequantization</li>
      <li>Formatting into app-native types (e.g., Swift structs in Core ML)</li>
      <li>Logging and telemetry</li>
    </ul>
  </li>
</ul>

<h4 id="lifecycle-optimization-optional-but-critical">Lifecycle Optimization (Optional but Critical)</h4>

<ul>
  <li>
    <p>For deployment, optimization techniques may be inserted at multiple points:</p>

    <ul>
      <li>Quantization (during conversion)</li>
      <li>Delegate configuration (runtime initialization)</li>
      <li>Memory pruning and op fusion (during compile/AOT phase)</li>
      <li>Execution profiling and tuning</li>
    </ul>
  </li>
</ul>

<h3 id="runtime-specific-execution-lifecycles">Runtime-Specific Execution Lifecycles</h3>

<ul>
  <li>While the general lifecycle described earlier applies to all runtimes, each ML runtime adapts or specializes this flow to match its architectural goals and supported hardware.</li>
  <li>This section provides an <strong>execution lifecycle breakdown</strong> for each runtime discussed in the original primer, with particular focus on runtime-specific logic during model loading, graph execution, memory management, and hardware dispatch.</li>
</ul>

<h4 id="tensorrt-execution-lifecycle-nvidia-gpus">TensorRT Execution Lifecycle (NVIDIA GPUs)</h4>

<ul>
  <li>
    <p>TensorRT uses an <strong>Ahead-of-Time (AOT) engine-building</strong> process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the <code class="language-plaintext highlighter-rouge">.plan</code> file encapsulates a pre-fused, quantized, and hardware-specific execution graph.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Import &amp; Parsing:</strong> Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.</li>
      <li><strong>Builder Optimization:</strong> Applies kernel selection, op fusion, <code class="language-plaintext highlighter-rouge">int8</code>/<code class="language-plaintext highlighter-rouge">float16</code> quantization, and layer scheduling.</li>
      <li><strong>Engine Generation:</strong> Outputs a <code class="language-plaintext highlighter-rouge">.plan</code> file containing the serialized CUDA engine.</li>
      <li><strong>Runtime Load:</strong> Loads the plan into memory via <code class="language-plaintext highlighter-rouge">IRuntime</code>, allocates CUDA buffers.</li>
      <li><strong>Execution Context:</strong> Prepares <code class="language-plaintext highlighter-rouge">ExecutionContext</code> with shape bindings, input/output memory views.</li>
      <li><strong>Inference Loop:</strong> Launches CUDA kernels via streams with async execution.</li>
      <li><strong>Output Retrieval:</strong> Copies GPU output buffers back to host (if needed).</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Extremely low latency, precompiled execution</li>
      <li>Requires regeneration if model shape changes</li>
      <li>All ops dispatched on GPU only</li>
    </ul>
  </li>
</ul>

<h4 id="core-ml-execution-lifecycle-apple-platforms">Core ML Execution Lifecycle (Apple Platforms)</h4>

<ul>
  <li>
    <p>Core ML performs inference via <strong>runtime graph execution</strong> of a compiled <code class="language-plaintext highlighter-rouge">.mlmodelc</code> package. It abstracts backend selection and heavily integrates with Apple’s APIs.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Compilation:</strong> <code class="language-plaintext highlighter-rouge">.mlmodel</code> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-25" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-26"><span class="mo" id="MathJax-Span-27" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-9">\rightarrow</script> <code class="language-plaintext highlighter-rouge">.mlmodelc</code> via Xcode or <code class="language-plaintext highlighter-rouge">coremltools</code></li>
      <li><strong>App Initialization:</strong> Loads model via <code class="language-plaintext highlighter-rouge">MLModel(configuration:)</code></li>
      <li><strong>Backend Dispatch:</strong> Chooses CPU, GPU, or ANE depending on hardware availability and op support.</li>
      <li><strong>Inference Call:</strong> <code class="language-plaintext highlighter-rouge">model.prediction(input:)</code> executes internal graph</li>
      <li><strong>Result Handling:</strong> Outputs are returned as native Swift types (e.g., strings, arrays, dicts)</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Dynamic backend selection with op-level granularity</li>
      <li>Opaque execution graph, no public access to IR</li>
      <li>Secure, sandboxed memory isolation for inference</li>
    </ul>
  </li>
</ul>

<h4 id="mlx-execution-lifecycle-apple-silicon">MLX Execution Lifecycle (Apple Silicon)</h4>

<ul>
  <li>
    <p>MLX uses a <strong>Python-based tensor programming model</strong> and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Definition:</strong> Model is defined in Python using <code class="language-plaintext highlighter-rouge">mlx.nn.Module</code></li>
      <li><strong>Eager Execution (default):</strong> Runs ops immediately using Metal Performance Shaders (MPS)</li>
      <li><strong>Compiled Graph (optional):</strong> <code class="language-plaintext highlighter-rouge">@mlx.compile</code> transforms a function into a static kernel sequence</li>
      <li><strong>Tensor Handling:</strong> All tensors are immutable; memory reuse is managed by the MLX runtime</li>
      <li><strong>Execution:</strong> Kernel invocations are dispatched via Metal; ANE support is under development</li>
      <li><strong>Output:</strong> Results returned as MLX tensors, convertible to NumPy or PyTorch</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Developer-centric and Pythonic</li>
      <li>Targets M1/M2 GPU via Metal</li>
      <li>No external model serialization—code <em>is</em> the model</li>
    </ul>
  </li>
</ul>

<h4 id="onnx-runtime-execution-lifecycle">ONNX Runtime Execution Lifecycle</h4>

<ul>
  <li>
    <p>ONNX Runtime is built around an <strong>intermediate computation graph</strong>, modular kernel registry, and <strong>Execution Providers (EPs)</strong> that delegate ops to appropriate hardware.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Load:</strong> Parses <code class="language-plaintext highlighter-rouge">.onnx</code> file (protobuf format) into IR</li>
      <li><strong>Graph Optimization:</strong> Applies passes (e.g., constant folding, op fusion, node elimination)</li>
      <li><strong>EP Assignment:</strong> Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)</li>
      <li><strong>Session Initialization:</strong> Prepares <code class="language-plaintext highlighter-rouge">InferenceSession</code> with input/output bindings</li>
      <li><strong>Execution:</strong> Each partition of the graph is dispatched to its EP</li>
      <li><strong>Result Aggregation:</strong> Output tensors are collected and returned in native types</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Pluggable backend system for flexible hardware support</li>
      <li>Static graph, dynamic shape support with constraints</li>
      <li>Strong cross-platform model portability</li>
    </ul>
  </li>
</ul>

<h4 id="executorch-execution-lifecycle-mcuembedded-focus">ExecuTorch Execution Lifecycle (MCU/Embedded Focus)</h4>

<ul>
  <li>
    <p>ExecuTorch employs a <strong>bytecode VM</strong> model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>TorchScript Compilation:</strong> PyTorch model scripted and converted into <code class="language-plaintext highlighter-rouge">.pt</code> (TorchScript)</li>
      <li><strong>AOT Bytecode Generation:</strong> <code class="language-plaintext highlighter-rouge">executorchc</code> compiles model to <code class="language-plaintext highlighter-rouge">.ptc</code> (bytecode)</li>
      <li><strong>Runtime Embedding:</strong> Bytecode and interpreter embedded into firmware or C++ app</li>
      <li><strong>Interpreter Loop:</strong> Model execution performed by a tiny VM that reads bytecode</li>
      <li><strong>Op Dispatch:</strong> Ops are routed to statically compiled function pointers</li>
      <li><strong>Output Return:</strong> Inference results written to statically allocated output buffer</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Deterministic memory, static allocation only</li>
      <li>Supports sub-MB runtime environments</li>
      <li>Highly tunable; model format ≠ PyTorch IR</li>
    </ul>
  </li>
</ul>

<h4 id="lidartlm-execution-lifecycle-lidar-focused-embedded-stacks">LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)</h4>

<ul>
  <li>
    <p>LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Sensor Input:</strong> LiDAR frames streamed in real-time</li>
      <li><strong>Preprocessing:</strong> Voxelization or range transformation into tensor-friendly formats</li>
      <li><strong>Tensor Pipeline:</strong> Sparse CNNs, 3D convolutions, and attention modules process data</li>
      <li><strong>Temporal Fusion:</strong> RNN or transformer-based modules optionally applied across frames</li>
      <li><strong>Postprocessing:</strong> Generates semantic maps or bounding boxes</li>
      <li><strong>Sensor Fusion:</strong> Optionally integrates radar or camera data for final outputs</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Sparse tensors and voxel grids dominate memory model</li>
      <li>CUDA, Open3D, or MinkowskiEngine often used</li>
      <li>Hard real-time constraints for robotics/ADAS</li>
    </ul>
  </li>
</ul>

<h4 id="llamacpp-execution-lifecycle-quantized-llms"><code class="language-plaintext Highlighter-rouge">llama.cpp</code> Execution Lifecycle (Quantized LLMs)</h4>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">llama.cpp</code> is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Load:</strong> GGUF model memory-mapped into RAM</li>
      <li><strong>KV Cache Setup:</strong> Pre-allocates attention buffers</li>
      <li><strong>Embedding <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-28" style="width: 1.191em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;"><span class="mrow" id="MathJax-Span-29"><span class="mo" id="MathJax-Span-30" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-10">\rightarrow</script> Transformer Loop:</strong> Sequentially executes transformer layers</li>
      <li><strong>Sampling:</strong> Next token is selected via greedy/top-k/top-p logic</li>
      <li><strong>Tokenization:</strong> Output string is constructed from sampled token IDs</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Highly portable, CPU-optimized, extremely low memory usage</li>
      <li>No dynamic graph, no scheduler, no intermediate representation</li>
      <li>Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle</li>
    </ul>
  </li>
</ul>

<h4 id="tensorflow-lite-execution-lifecycle">TensorFlow Lite Execution Lifecycle</h4>

<ul>
  <li>
    <p>TFLite uses a <strong>FlatBuffer interpreter</strong> architecture with optional delegates for acceleration.</p>
  </li>
  <li>
    <p><strong>Lifecycle Stages:</strong></p>

    <ul>
      <li><strong>Model Conversion:</strong> TensorFlow <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-31" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-32"><span class="mo" id="MathJax-Span-33" style="font-family: STIXGeneral-Regular;">→</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-11">\rightarrow</script> <code class="language-plaintext highlighter-rouge">.tflite</code> via <code class="language-plaintext highlighter-rouge">TFLiteConverter</code></li>
      <li><strong>FlatBuffer Load:</strong> Model loaded with <code class="language-plaintext highlighter-rouge">Interpreter(model_path=...)</code></li>
      <li><strong>Tensor Allocation:</strong> Input/output buffers allocated via <code class="language-plaintext highlighter-rouge">allocate_tensors()</code></li>
      <li><strong>Delegate Attachment (optional):</strong> NNAPI, GPU, Hexagon delegate claims subgraphs</li>
      <li><strong>Inference:</strong> Static interpreter walks the computation graph</li>
      <li><strong>Output Access:</strong> Results extracted via <code class="language-plaintext highlighter-rouge">get_tensor()</code> APIs</li>
    </ul>
  </li>
  <li>
    <p><strong>Unique Characteristics:</strong></p>

    <ul>
      <li>Very compact format with zero-copy access</li>
      <li>Delegate design separates concerns for CPU vs. accelerators</li>
      <li>Strong ecosystem with tooling (e.g., Model Maker, Visualizer)</li>
    </ul>
  </li>
</ul>

<h2 id="related-cpu-operator-librariesbackends">Related: CPU Operator Libraries/Backends</h2>

<ul>
  <li><a href="https://github.com/pytorch/FBGEMM"><code class="language-plaintext highlighter-rouge">FBGEMM</code></a>, <a href="https://github.com/pytorch/QNNPACK"><code class="language-plaintext highlighter-rouge">QNNPACK</code></a>, and <a href="https://github.com/google/XNNPACK"><code class="language-plaintext highlighter-rouge">XNNPACK</code></a> are high-performance CPU operator libraries used by runtimes (and frameworks embedding a runtime) to execute model operators efficiently.</li>
  <li>Note that quantization is performed by the framework either during inference (dynamic), training-time (QAT), or post-training (PTQ), producing per-tensor or per-channel scales/zero-points <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-34" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.93em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-35"><span class="mo" id="MathJax-Span-36" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-37" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-38" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-39" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">z</span><span class="mo" id="MathJax-Span-40" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-12">(s, z)</script> or <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-41" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.66em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-42"><span class="mo" id="MathJax-Span-43" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-44"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-45" style="font-family: STIXGeneral-Italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-46" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-47" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-48" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-49" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="mi" id="MathJax-Span-50" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-51" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>s</mi><mi>c</mi></msub><mo>,</mo><msub><mi>z</mi><mi>c</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-13">(s_c, z_c)</script> via the affine map <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;c&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;u&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-52" style="width: 15.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.284em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1013.23em, 2.763em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-53"><span class="msubsup" id="MathJax-Span-54"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-55" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-56" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-57" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-58" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 4.846em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.46em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="texatom" id="MathJax-Span-59"><span class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61" style="font-family: STIXGeneral-Regular;">c</span><span class="mi" id="MathJax-Span-62" style="font-family: STIXGeneral-Regular;">l</span><span class="mi" id="MathJax-Span-63" style="font-family: STIXGeneral-Regular;">i</span><span class="mi" id="MathJax-Span-64" style="font-family: STIXGeneral-Regular;">p</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.747em; left: 1.513em;"><span class="texatom" id="MathJax-Span-65"><span class="mrow" id="MathJax-Span-66"><span class="mo" id="MathJax-Span-67" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">[</span><span class="msubsup" id="MathJax-Span-68"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-69" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="texatom" id="MathJax-Span-70"><span class="mrow" id="MathJax-Span-71"><span class="mo" id="MathJax-Span-72" style="font-size: 50%; font-family: STIXGeneral-Regular;">min</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-73" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-74" style="height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-75"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-76" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.367em;"><span class="texatom" id="MathJax-Span-77"><span class="mrow" id="MathJax-Span-78"><span class="mo" id="MathJax-Span-79" style="font-size: 50%; font-family: STIXGeneral-Regular;">max</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-80" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">]</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-81" style="font-family: STIXGeneral-Regular;">(</span><span class="texatom" id="MathJax-Span-82"><span class="mrow" id="MathJax-Span-83"><span class="mi" id="MathJax-Span-84" style="font-family: STIXGeneral-Regular;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-85" style="font-family: STIXGeneral-Regular;">o</span><span class="mi" id="MathJax-Span-86" style="font-family: STIXGeneral-Regular;">u</span><span class="mi" id="MathJax-Span-87" style="font-family: STIXGeneral-Regular;">n</span><span class="mi" id="MathJax-Span-88" style="font-family: STIXGeneral-Regular;">d</span></span></span><span class="mo" id="MathJax-Span-89" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-90" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-91"><span class="mrow" id="MathJax-Span-92"><span class="mo" id="MathJax-Span-93" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mi" id="MathJax-Span-94" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-95" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-96" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">+</span><span class="mi" id="MathJax-Span-97" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">z</span><span class="mo" id="MathJax-Span-98" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>q</mi></msub><mo>=</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">p</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">[</mo><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mo movablelimits="true" form="prefix">min</mo></mrow></msub><mo>,</mo><mspace width="thinmathspace"></mspace><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mo movablelimits="true" form="prefix">max</mo></mrow></msub><mo stretchy="false">]</mo></mrow></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi></mrow><mo stretchy="false">(</mo><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-14">x_q=\mathrm{clip}_{[q_{\min},\,q_{\max}]}(\mathrm{round}(x/s)+z)</script> with dequantization <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-99" style="width: 6.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-100"><span class="mi" id="MathJax-Span-101" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-102" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mi" id="MathJax-Span-103" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">s</span><span class="mspace" id="MathJax-Span-104" style="height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-105" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-106"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-107" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-108" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-109" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">−</span><span class="mi" id="MathJax-Span-110" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">z</span><span class="mo" id="MathJax-Span-111" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>≈</mo><mi>s</mi><mspace width="thinmathspace"></mspace><mo stretchy="false">(</mo><msub><mi>x</mi><mi>q</mi></msub><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-15">x\approx s\,(x_q-z)</script>. These backends (<code class="language-plaintext highlighter-rouge">FBGEMM</code>, <code class="language-plaintext highlighter-rouge">QNNPACK</code>, and <code class="language-plaintext highlighter-rouge">XNNPACK</code>) do not perform quantization but rather consume these parameters and the resulting low-precision tensors (or unquantized floating-point tensors for XNNPACK paths) to run inference kernels efficiently.</li>
  <li><a href="https://github.com/pytorch/FBGEMM"><code class="language-plaintext highlighter-rouge">FBGEMM</code></a> targets x86/servers with fast <code class="language-plaintext highlighter-rouge">int8</code> GEMM/conv; <a href="https://github.com/pytorch/QNNPACK"><code class="language-plaintext highlighter-rouge">QNNPACK</code></a> targets ARM/mobile CPUs with optimized <code class="language-plaintext highlighter-rouge">int8</code> conv/GEMM/activations; <a href="https://github.com/google/XNNPACK"><code class="language-plaintext highlighter-rouge">XNNPACK</code></a> focuses on fast <code class="language-plaintext highlighter-rouge">float32</code> kernels with some <code class="language-plaintext highlighter-rouge">int8</code> paths and commonly handles float ops.</li>
</ul>

<h3 id="overview-8">Overview</h3>

<ul>
  <li>
    <p>This quick overview shows how <a href="https://github.com/pytorch/FBGEMM"><code class="language-plaintext highlighter-rouge">FBGEMM</code></a>, <a href="https://github.com/pytorch/QNNPACK"><code class="language-plaintext highlighter-rouge">QNNPACK</code></a>, and <a href="https://github.com/google/XNNPACK"><code class="language-plaintext highlighter-rouge">XNNPACK</code></a> fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:</p>

    <ul>
      <li><strong>Primary purpose</strong>: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.</li>
      <li><strong>They do not</strong>: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.</li>
      <li><strong>Where they fit</strong>: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.</li>
    </ul>
  </li>
</ul>

<h3 id="fbgemm-by-meta-server-cpus"><a href="https://github.com/pytorch/FBGEMM"><code class="language-plaintext Highlighter-rouge">FBGEMM</code></a> (by Meta,; Server CPUs)</h3>

<ul>
  <li><strong>Target platforms</strong>: x86-64 server/desktop CPUs with SIMD (AVX2, AVX-512; newer stacks can leverage AMX on recent Intel parts via higher-level integrations).</li>
  <li><strong>Data types and quant schemes</strong>: <code class="language-plaintext highlighter-rouge">int8</code>/<code class="language-plaintext highlighter-rouge">uint8</code> activations (affine per-tensor), <code class="language-plaintext highlighter-rouge">int8</code> weights (often symmetric per-channel). 32-bit accumulation with requantization to <code class="language-plaintext highlighter-rouge">int8</code>/<code class="language-plaintext highlighter-rouge">uint8</code> or dequantization to float. Also provides row-wise and 4-bit embedding quantization utilities for recommendation models.</li>
  <li><strong>Operator coverage</strong>: Linear/GEMM and convolution (including groupwise), prepacked weight paths; optimized im2col/IGEMM; embedding bag and sparse length ops for recsys.</li>
  <li><strong>Optimizations</strong>: Weight pre-packing into cache-friendly blocked layouts; vectorized micro-kernels; cache- and register-blocking; fused bias+activation+requant paths; threadpool parallelism.</li>
  <li>
    <p><strong>Typical use</strong>: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static <code class="language-plaintext highlighter-rouge">int8</code> conv/linear). Best when you need maximum x86 performance for <code class="language-plaintext highlighter-rouge">int8</code> inference.
; Mobile CPUs)</p>
  </li>
  <li><strong>Target platforms</strong>: ARM/ARM64 mobile CPUs with NEON (Android/iOS); designed for low-power cores.</li>
  <li><strong>Data types and quant schemes</strong>: <code class="language-plaintext highlighter-rouge">uint8</code>/<code class="language-plaintext highlighter-rouge">int8</code> activations (affine per-tensor), <code class="language-plaintext highlighter-rouge">int8</code> per-channel weights; 32-bit accumulation with efficient requantization.</li>
  <li><strong>Operator coverage</strong>: Quantized convolution/IGEMM, depthwise conv, deconvolution, fully connected (GEMM), pooling, various activation/elementwise ops.</li>
  <li><strong>Optimizations</strong>: NHWC-friendly kernels; careful cache use for small batch/small filters; per-thread micro-kernels; fused post-ops to reduce memory traffic.</li>
  <li><strong>Typical use</strong>: PyTorch Mobile’s quantized back end on ARM; good default for mobile <code class="language-plaintext highlighter-rouge">int8</code> CNNs and fully connected layers where you need predictable latency on phones.</li>
</ul>

<h3 id="xnnpack-by-google-both-server-and-mobile-cpus"><a href="https://github.com/google/XNNPACK"><code class="language-plaintext Highlighter-rouge">XNNPACK</code></a> (by Google,; Both Server and Mobile CPUs)</h3>

<ul>
  <li><strong>Target platforms</strong>: ARM/ARM64, x86-64, and WebAssembly (WASM); broadly portable and actively maintained.</li>
  <li><strong>Data types and quant schemes</strong>: Strong <code class="language-plaintext highlighter-rouge">float32</code>/<code class="language-plaintext highlighter-rouge">float16</code>/<code class="language-plaintext highlighter-rouge">bfloat16</code> coverage; mature QS8/QU8 (signed/unsigned 8-bit) inference for conv/GEMM/elementwise with per-channel weight scales. 32-bit accumulation and precise requantization.</li>
  <li><strong>Operator coverage</strong>: Convolution (standard and depthwise), fully connected, pooling, deconvolution, elementwise math, softmax, activation functions, resize, etc.</li>
  <li><strong>Optimizations</strong>: Handwritten micro-kernels per ISA (NEON/AVX/AVX512), NHWC dataflow, weight prepacking, GEMM/IGEMM families with cache-aware blocking, parallel work-stealing.</li>
  <li><strong>Typical use</strong>: TensorFlow Lite’s XNNPACK delegate on CPU (float and <code class="language-plaintext highlighter-rouge">int8</code>), and increasingly as a CPU backend in other frameworks for both float and quantized inference.</li>
</ul>

<h3 id="what-to-choose-when">What to Choose When?</h3>

<ul>
  <li><strong>PyTorch (desktop/server CPU)</strong>: <code class="language-plaintext highlighter-rouge">FBGEMM</code> is the usual backend for quantized ops; dynamic quantized Linear/LSTM also route here.</li>
  <li><strong>PyTorch Mobile (ARM)</strong>: <code class="language-plaintext highlighter-rouge">QNNPACK</code> is the historical default for quantized ops; some float operators can use XNNPACK.</li>
  <li><strong>TensorFlow Lite (CPU)</strong>: <code class="language-plaintext highlighter-rouge">XNNPACK</code> delegate accelerates many <code class="language-plaintext highlighter-rouge">float32</code> and <code class="language-plaintext highlighter-rouge">int8</code> ops; the interpreter falls back to reference kernels when needed.</li>
  <li><strong>ONNX Runtime (CPU)</strong>: Uses its own CPU kernels by default, but can be built/integrated with these libraries in certain configurations; on mobile, builds commonly leverage <code class="language-plaintext highlighter-rouge">XNNPACK</code>.</li>
</ul>

<h3 id="design-notes">Design Notes</h3>

<ul>
  <li><strong>Quant params are part of tensors</strong>: Kernels need correct scales/zero-points. For per-channel weights, pass channel-wise scales; activations are usually per-tensor.</li>
  <li><strong>Accumulation width</strong>: 8-bit multiply-accumulates are summed into 32-bit accumulators to avoid overflow, then requantized. Watch for saturation when chaining ops.</li>
  <li><strong>Prepack once</strong>: Pre-pack and reuse weights to avoid paying packing costs per inference. Many APIs expose prepacked weight objects.</li>
  <li><strong>Layout matters</strong>: These libraries typically prefer NHWC for conv on mobile; mismatched layouts cause costly transposes.</li>
  <li><strong>Dynamic vs static quant</strong>: Dynamic quantizes activations on-the-fly (common for Linear/LSTM), static uses calibration ranges. FBGEMM has strong dynamic Linear/LSTM paths.</li>
  <li><strong>Activation ranges</strong>: Calibrate representative inputs to pick good scales and avoid clamp-heavy requantization.</li>
</ul>

<h3 id="comparative-analysis-2">Comparative Analysis</h3>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Attribute</strong></th>
<th class="tg-hcenter-valign-first"><strong>FBGEMM</strong></th>
<th class="tg-hcenter-valign-first"><strong>QNNPACK</strong></th>
<th class="tg-hcenter-valign-second"><strong>XNNPACK</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Primary target</td>
<td class="tg-tleft-valign-first">x86-64 servers/desktops</td>
<td class="tg-tleft-valign-first">ARM/ARM64 mobile</td>
<td class="tg-tleft-valign-second">ARM/ARM64, x86-64, WASM</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Best precision</td>
<td class="tg-tleft-valign-first">`int8` quant (server)</td>
<td class="tg-tleft-valign-first">`int8` quant (mobile)</td>
<td class="tg-tleft-valign-second">`float32`/`float16` plus `int8`</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Typical consumers</td>
<td class="tg-tleft-valign-first">PyTorch quant (server)</td>
<td class="tg-tleft-valign-first">PyTorch Mobile quant</td>
<td class="tg-tleft-valign-second">TFLite delegate; some PyTorch CPU paths</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Conv layout</td>
<td class="tg-tleft-valign-first">NCHW/NHWC with prepack</td>
<td class="tg-tleft-valign-first">NHWC</td>
<td class="tg-tleft-valign-second">NHWC</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Weight handling</td>
<td class="tg-tleft-valign-first">Prepacked per-channel `int8`</td>
<td class="tg-tleft-valign-first">Prepacked per-channel `int8`</td>
<td class="tg-tleft-valign-second">Prepacked per-channel `int8`</td>
</tr>
</tbody>
</table>
</div>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2206.01859">Efficient Inference with Transformer Models on CPUs</a></li>
  <li><a href="https://arxiv.org/abs/2211.17192">Speculative Decoding for Accelerated Transformer Inference</a></li>
  <li><a href="https://arxiv.org/abs/2104.10807">Fast Transformers with Memory-Efficient Attention via KV Cache Optimization</a></li>
  <li><a href="https://arxiv.org/abs/2211.10438">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li>
  <li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-extension-for-pytorch.html">Intel Extension for PyTorch: Boosting Transformer Inference on CPUs</a></li>
  <li><a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer GitHub Repository (NVIDIA)</a></li>
  <li><a href="https://github.com/vllm-project/vllm">vLLM: Easy and Fast LLM Serving with State-of-the-Art Throughput</a></li>
  <li><a href="https://developer.nvidia.com/blog/deploying-transformer-models-on-the-edge-with-nvidia-tensorrt/">Deploying Transformer Models on Edge Devices with TensorRT</a></li>
  <li><a href="https://pytorch.org/docs/stable/quantization.html">Quantization Aware Training in PyTorch</a></li>
  <li><a href="https://onnxruntime.ai/docs/performance/transformers.html">ONNX Runtime: Accelerating Transformer Inference</a></li>
  <li><a href="https://medium.com/@pengzhang.dev/speculative-decoding-in-vllm-2ac5b5a8b1b1">Speculative Decoding in vLLM (Medium article)</a></li>
  <li><a href="https://medium.com/@eric.zelikman/distilling-and-quantizing-gpt-2-for-mobile-5dc68d4fbc8e">Running LLMs on Mobile: Lessons from Distilling and Quantizing GPT-2</a></li>
  <li><a href="https://developer.nvidia.com/blog/optimizing-llm-serving-on-nvidia-gpus-with-tensorrt-llm/">Optimizing LLM Serving on NVIDIA GPUs with TensorRT-LLM</a></li>
  <li><a href="https://onnxruntime.ai/blog/2024/llm-int4-inference/">LLM INT4 Inference with ONNX Runtime</a></li>
  <li><a href="https://coral.ai/docs/edgetpu/models-intro/#transformers">Efficient Transformer Inference on Edge with EdgeTPU</a></li>
  <li><a href="https://github.com/iree-org/iree">IREE: Intermediate Representation Execution Environment</a></li>
  <li><a href="https://github.com/openxla/xla">XLA</a></li>
  <li><a href="https://github.com/openxla/stablehlo">StableHLO</a></li>
</ul>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code24"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code24">@article{Chadha2020DistilledMLRuntimes,
  title   = {ML Runtimes},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895470&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fml-runtimes%2F&amp;pra=5&amp;wgl=1&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766923008864&amp;bpp=1&amp;bdt=64&amp;idt=12&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=5974023997247&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31096041%2C95376241%2C95376583%2C95378749%2C95379213%2C95379902&amp;oid=2&amp;pvsid=1428316129419823&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=33792&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=15" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe src="https://www.google.com/recaptcha/api2/aframe" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>