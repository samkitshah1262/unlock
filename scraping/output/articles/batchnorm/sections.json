[
  {
    "id": "ai-batchnorm-batchnorm-during-training-1",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Summary: BatchNorm During Training vs. Testing/Inference",
    "title": "BatchNorm During Training:",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Compute Mean and Variance</strong>: For each feature in the mini-batch, compute the mean and variance.\n    <ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 8.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1006.88em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"msubsup\" id=\"MathJax-Span-15\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-17\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"mi\" id=\"MathJax-Span-19\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-25\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-28\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-29\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-35\"><span class=\"mrow\" id=\"MathJax-Span-36\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><msub><mi>x</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">\\mu_{batch} = \\frac{1}{m} \\sum_{i=1}^{m} x_i</script>\n      </li>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-41\" style=\"width: 13.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1011.04em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"msubsup\" id=\"MathJax-Span-43\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-46\"><span class=\"mrow\" id=\"MathJax-Span-47\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-54\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-57\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-59\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-64\"><span class=\"mrow\" id=\"MathJax-Span-65\"><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-68\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-83\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-6\">\\sigma_{batch}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{batch})^2</script>\n      </li>\n    </ul>\n\n    <p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-84\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-85\"><span class=\"msubsup\" id=\"MathJax-Span-86\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">x_i</script> is the feature vector of a single data point, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-89\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-90\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">m</script> is the number of data points in the mini-batch.</p>\n  </li>\n  <li><strong>Normalize</strong>: Normalize the activations using the computed mean and variance:\n    <ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub></mrow><msqrt><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>&amp;#x03F5;</mi></msqrt></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.836em, 1007.14em, 4.169em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"munderover\" id=\"MathJax-Span-97\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-102\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 5.003em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1004.12em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -2.029em;\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"msubsup\" id=\"MathJax-Span-104\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-108\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-110\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.659em, 1004.9em, 4.846em, -999.997em); top: -2.862em; left: 50%; margin-left: -2.445em;\"><span class=\"msqrt\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1003.8em, 4.482em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"msubsup\" id=\"MathJax-Span-119\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-122\"><span class=\"mrow\" id=\"MathJax-Span-123\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1003.86em, 3.388em, -999.997em); top: -4.372em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 3.336em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.836em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.253em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.669em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.086em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.503em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.919em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.643em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.003em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.247em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub></mrow><msqrt><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></msqrt></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-9\">\\hat{x}_i = \\frac{x_i - \\mu_{batch}}{\\sqrt{\\sigma_{batch}^2 + \\epsilon}}</script>\n      </li>\n    </ul>\n\n    <p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.42em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ϵ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\epsilon</script> is a small constant added for numerical stability.</p>\n  </li>\n  <li><strong>Scale and Shift</strong>: This is a crucial step which allows the model to learn the optimal scale and mean for each feature. Two learnable parameters, gamma (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-134\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\gamma</script>) and beta (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">\\beta</script>), are introduced.\n    <ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>&amp;#x03B3;</mi><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-147\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-148\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"munderover\" id=\"MathJax-Span-150\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>γ</mi><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-13\">y_i = \\gamma \\hat{x}_i + \\beta</script>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Update Running Statistics</strong>: To use during inference, maintain a running mean and variance (usually via an exponential moving average) of the features during training. These statistics are updated every time a batch is processed.</li>\n</ul>\n<ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 8.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1006.88em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"msubsup\" id=\"MathJax-Span-15\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-17\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"mi\" id=\"MathJax-Span-19\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-25\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-28\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-29\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-35\"><span class=\"mrow\" id=\"MathJax-Span-36\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><msub><mi>x</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">\\mu_{batch} = \\frac{1}{m} \\sum_{i=1}^{m} x_i</script>\n      </li>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-41\" style=\"width: 13.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1011.04em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"msubsup\" id=\"MathJax-Span-43\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-46\"><span class=\"mrow\" id=\"MathJax-Span-47\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-54\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.84em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.836em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-57\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-59\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-64\"><span class=\"mrow\" id=\"MathJax-Span-65\"><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-68\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-83\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-6\">\\sigma_{batch}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{batch})^2</script>\n      </li>\n    </ul>\n<p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-84\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-85\"><span class=\"msubsup\" id=\"MathJax-Span-86\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">x_i</script> is the feature vector of a single data point, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-89\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-90\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">m</script> is the number of data points in the mini-batch.</p>\n<ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub></mrow><msqrt><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>&amp;#x03F5;</mi></msqrt></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.836em, 1007.14em, 4.169em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"munderover\" id=\"MathJax-Span-97\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-102\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 5.003em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1004.12em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -2.029em;\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"msubsup\" id=\"MathJax-Span-104\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-108\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-110\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.659em, 1004.9em, 4.846em, -999.997em); top: -2.862em; left: 50%; margin-left: -2.445em;\"><span class=\"msqrt\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1003.8em, 4.482em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"msubsup\" id=\"MathJax-Span-119\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-122\"><span class=\"mrow\" id=\"MathJax-Span-123\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1003.86em, 3.388em, -999.997em); top: -4.372em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 3.336em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.836em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.253em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.669em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.086em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.503em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.919em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.643em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.003em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.247em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub></mrow><msqrt><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></msqrt></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-9\">\\hat{x}_i = \\frac{x_i - \\mu_{batch}}{\\sqrt{\\sigma_{batch}^2 + \\epsilon}}</script>\n      </li>\n    </ul>\n<p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.42em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ϵ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\epsilon</script> is a small constant added for numerical stability.</p>\n<ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>&amp;#x03B3;</mi><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-147\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-148\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"munderover\" id=\"MathJax-Span-150\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>γ</mi><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-13\">y_i = \\gamma \\hat{x}_i + \\beta</script>\n      </li>\n    </ul>",
    "contentMarkdown": "*   **Compute Mean and Variance**: For each feature in the mini-batch, compute the mean and variance.\n    \n    *   μbatch\\=1m∑i\\=1mxiμbatch\\=1m∑i\\=1mxi\n        \n        \\\\mu\\_{batch} = \\\\frac{1}{m} \\\\sum\\_{i=1}^{m} x\\_i\n    *   σ2batch\\=1m∑i\\=1m(xi−μbatch)2σbatch2\\=1m∑i\\=1m(xi−μbatch)2\n        \n        \\\\sigma\\_{batch}^2 = \\\\frac{1}{m} \\\\sum\\_{i=1}^{m} (x\\_i - \\\\mu\\_{batch})^2\n    \n    where xixix\\_i is the feature vector of a single data point, and mmm is the number of data points in the mini-batch.\n    \n*   **Normalize**: Normalize the activations using the computed mean and variance:\n    \n    *   x̂ i\\=xi−μbatchσ2batch+ϵ‾‾‾‾‾‾‾‾‾√x^i\\=xi−μbatchσbatch2+ϵ\n        \n        \\\\hat{x}\\_i = \\\\frac{x\\_i - \\\\mu\\_{batch}}{\\\\sqrt{\\\\sigma\\_{batch}^2 + \\\\epsilon}}\n    \n    where ϵϵ\\\\epsilon is a small constant added for numerical stability.\n    \n*   **Scale and Shift**: This is a crucial step which allows the model to learn the optimal scale and mean for each feature. Two learnable parameters, gamma (γγ\\\\gamma) and beta (ββ\\\\beta), are introduced.\n    *   yi\\=γx̂ i+βyi\\=γx^i+β\n        \n        y\\_i = \\\\gamma \\\\hat{x}\\_i + \\\\beta\n*   **Update Running Statistics**: To use during inference, maintain a running mean and variance (usually via an exponential moving average) of the features during training. These statistics are updated every time a batch is processed.\n\n*   μbatch\\=1m∑i\\=1mxiμbatch\\=1m∑i\\=1mxi\n    \n    \\\\mu\\_{batch} = \\\\frac{1}{m} \\\\sum\\_{i=1}^{m} x\\_i\n*   σ2batch\\=1m∑i\\=1m(xi−μbatch)2σbatch2\\=1m∑i\\=1m(xi−μbatch)2\n    \n    \\\\sigma\\_{batch}^2 = \\\\frac{1}{m} \\\\sum\\_{i=1}^{m} (x\\_i - \\\\mu\\_{batch})^2\n\nwhere xixix\\_i is the feature vector of a single data point, and mmm is the number of data points in the mini-batch.\n\n*   x̂ i\\=xi−μbatchσ2batch+ϵ‾‾‾‾‾‾‾‾‾√x^i\\=xi−μbatchσbatch2+ϵ\n    \n    \\\\hat{x}\\_i = \\\\frac{x\\_i - \\\\mu\\_{batch}}{\\\\sqrt{\\\\sigma\\_{batch}^2 + \\\\epsilon}}\n\nwhere ϵϵ\\\\epsilon is a small constant added for numerical stability.\n\n*   yi\\=γx̂ i+βyi\\=γx^i+β\n    \n    y\\_i = \\\\gamma \\\\hat{x}\\_i + \\\\beta",
    "contentLength": 79102,
    "wordCount": 229,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#batchnorm-during-training:"
  },
  {
    "id": "ai-batchnorm-batchnorm-during-testing-inference-2",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Summary: BatchNorm During Training vs. Testing/Inference",
    "title": "BatchNorm During Testing (Inference):",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Use Running Statistics</strong>: Instead of computing the mean and variance for the current batch of data (which might not make sense, especially if you’re processing one example at a time), use the running mean and variance statistics computed during training.</p>\n  </li>\n  <li><strong>Normalize</strong>: Normalize the activations using the running mean and variance:\n    <ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub></mrow><msqrt><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>&amp;#x03F5;</mi></msqrt></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-156\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1007.82em, 4.169em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"msubsup\" id=\"MathJax-Span-158\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-159\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"munderover\" id=\"MathJax-Span-161\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-166\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 5.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1004.79em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -2.393em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"msubsup\" id=\"MathJax-Span-168\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-172\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-174\"><span class=\"mrow\" id=\"MathJax-Span-175\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">g</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.711em, 1005.58em, 4.898em, -999.997em); top: -2.914em; left: 50%; margin-left: -2.758em;\"><span class=\"msqrt\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 5.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1004.48em, 4.638em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"msubsup\" id=\"MathJax-Span-185\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.326em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-193\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-196\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">g</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1004.53em, 3.388em, -999.997em); top: -4.268em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 4.013em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.888em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.305em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.773em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.242em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.659em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.128em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.544em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.591em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.247em; border-left: 0px solid; width: 0px; height: 3.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub></mrow><msqrt><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></msqrt></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-14\">\\hat{x}_i = \\frac{x_i - \\mu_{running}}{\\sqrt{\\sigma_{running}^2 + \\epsilon}}</script>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Scale and Shift</strong>: Use the learned gamma (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-199\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">\\gamma</script>) and beta (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-202\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">\\beta</script>) values from training to scale and shift the normalized activations:\n    <ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>&amp;#x03B3;</mi><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-205\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"msubsup\" id=\"MathJax-Span-207\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-213\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"munderover\" id=\"MathJax-Span-215\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>γ</mi><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-17\">y_i = \\gamma \\hat{x}_i + \\beta</script>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Use Running Statistics</strong>: Instead of computing the mean and variance for the current batch of data (which might not make sense, especially if you’re processing one example at a time), use the running mean and variance statistics computed during training.</p>\n<ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub></mrow><msqrt><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>&amp;#x03F5;</mi></msqrt></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-156\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1007.82em, 4.169em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"msubsup\" id=\"MathJax-Span-158\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-159\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"munderover\" id=\"MathJax-Span-161\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-166\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 5.68em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1004.79em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -2.393em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"msubsup\" id=\"MathJax-Span-168\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-172\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-174\"><span class=\"mrow\" id=\"MathJax-Span-175\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">g</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.711em, 1005.58em, 4.898em, -999.997em); top: -2.914em; left: 50%; margin-left: -2.758em;\"><span class=\"msqrt\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 5.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.023em, 1004.48em, 4.638em, -999.997em); top: -4.008em; left: 1.044em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"msubsup\" id=\"MathJax-Span-185\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.326em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-193\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-196\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">g</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1004.53em, 3.388em, -999.997em); top: -4.268em; left: 1.044em;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 4.013em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.888em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.305em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.773em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.242em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.659em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.128em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.544em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1001.1em, 4.482em, -999.997em); top: -3.591em; left: 0em;\"><span style=\"font-family: STIXSizeOneSym;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1005.68em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.68em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.247em; border-left: 0px solid; width: 0px; height: 3.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub></mrow><msqrt><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>u</mi><mi>n</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></msqrt></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-14\">\\hat{x}_i = \\frac{x_i - \\mu_{running}}{\\sqrt{\\sigma_{running}^2 + \\epsilon}}</script>\n      </li>\n    </ul>\n<ul>\n      <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>&amp;#x03B3;</mi><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-205\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1004.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"msubsup\" id=\"MathJax-Span-207\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-213\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"munderover\" id=\"MathJax-Span-215\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>γ</mi><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-17\">y_i = \\gamma \\hat{x}_i + \\beta</script>\n      </li>\n    </ul>",
    "contentMarkdown": "*   **Use Running Statistics**: Instead of computing the mean and variance for the current batch of data (which might not make sense, especially if you’re processing one example at a time), use the running mean and variance statistics computed during training.\n    \n*   **Normalize**: Normalize the activations using the running mean and variance:\n    *   x̂ i\\=xi−μrunningσ2running+ϵ‾‾‾‾‾‾‾‾‾‾√x^i\\=xi−μrunningσrunning2+ϵ\n        \n        \\\\hat{x}\\_i = \\\\frac{x\\_i - \\\\mu\\_{running}}{\\\\sqrt{\\\\sigma\\_{running}^2 + \\\\epsilon}}\n*   **Scale and Shift**: Use the learned gamma (γγ\\\\gamma) and beta (ββ\\\\beta) values from training to scale and shift the normalized activations:\n    *   yi\\=γx̂ i+βyi\\=γx^i+β\n        \n        y\\_i = \\\\gamma \\\\hat{x}\\_i + \\\\beta\n\n**Use Running Statistics**: Instead of computing the mean and variance for the current batch of data (which might not make sense, especially if you’re processing one example at a time), use the running mean and variance statistics computed during training.\n\n*   x̂ i\\=xi−μrunningσ2running+ϵ‾‾‾‾‾‾‾‾‾‾√x^i\\=xi−μrunningσrunning2+ϵ\n    \n    \\\\hat{x}\\_i = \\\\frac{x\\_i - \\\\mu\\_{running}}{\\\\sqrt{\\\\sigma\\_{running}^2 + \\\\epsilon}}\n\n*   yi\\=γx̂ i+βyi\\=γx^i+β\n    \n    y\\_i = \\\\gamma \\\\hat{x}\\_i + \\\\beta",
    "contentLength": 38110,
    "wordCount": 152,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#batchnorm-during-testing-(inference):"
  },
  {
    "id": "ai-batchnorm-why-the-distinction-3",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Summary: BatchNorm During Training vs. Testing/Inference",
    "title": "Why the Distinction?",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>BatchNorm’s behavior difference between training and testing ensures a few things:</p>\n\n    <ul>\n      <li>\n        <p><strong>Stability</strong>: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.</p>\n      </li>\n      <li>\n        <p><strong>Scalability</strong>: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>In frameworks like TensorFlow and PyTorch, the distinction between training and inference for BatchNorm is handled automatically as long as you appropriately set the model’s mode (<code class=\"language-plaintext highlighter-rouge\">model.train()</code> vs. <code class=\"language-plaintext highlighter-rouge\">model.eval()</code> in PyTorch, for instance).</p>\n  </li>\n</ul>\n<p>BatchNorm’s behavior difference between training and testing ensures a few things:</p>\n<ul>\n      <li>\n        <p><strong>Stability</strong>: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.</p>\n      </li>\n      <li>\n        <p><strong>Scalability</strong>: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.</p>\n      </li>\n    </ul>\n<p><strong>Stability</strong>: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.</p>\n<p><strong>Scalability</strong>: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.</p>\n<p>In frameworks like TensorFlow and PyTorch, the distinction between training and inference for BatchNorm is handled automatically as long as you appropriately set the model’s mode (<code class=\"language-plaintext highlighter-rouge\">model.train()</code> vs. <code class=\"language-plaintext highlighter-rouge\">model.eval()</code> in PyTorch, for instance).</p>",
    "contentMarkdown": "*   BatchNorm’s behavior difference between training and testing ensures a few things:\n    \n    *   **Stability**: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.\n        \n    *   **Scalability**: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.\n        \n*   In frameworks like TensorFlow and PyTorch, the distinction between training and inference for BatchNorm is handled automatically as long as you appropriately set the model’s mode (`model.train()` vs. `model.eval()` in PyTorch, for instance).\n    \n\nBatchNorm’s behavior difference between training and testing ensures a few things:\n\n*   **Stability**: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.\n    \n*   **Scalability**: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.\n    \n\n**Stability**: Using running statistics during inference ensures that the network behaves more predictably. If we were to normalize using batch statistics during inference, the network’s output could vary significantly based on the composition and size of the input batch.\n\n**Scalability**: During inference, you might not always have a “batch” of data. Sometimes, you might want to make predictions for a single data point. Using running statistics from training allows you to do this without any issues.\n\nIn frameworks like TensorFlow and PyTorch, the distinction between training and inference for BatchNorm is handled automatically as long as you appropriately set the model’s mode (`model.train()` vs. `model.eval()` in PyTorch, for instance).",
    "contentLength": 2744,
    "wordCount": 322,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#why-the-distinction?"
  },
  {
    "id": "ai-batchnorm-use-with-different-network-types-4",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Use with Different Network Types",
    "order": 4,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Batch normalization is a general technique that can be used to normalize the inputs to a layer.</p>\n  </li>\n  <li>\n    <p>It can be used with most network types, such as Multilayer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks.</p>\n  </li>\n</ul>\n<p>Batch normalization is a general technique that can be used to normalize the inputs to a layer.</p>\n<p>It can be used with most network types, such as Multilayer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks.</p>",
    "contentMarkdown": "*   Batch normalization is a general technique that can be used to normalize the inputs to a layer.\n    \n*   It can be used with most network types, such as Multilayer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks.\n    \n\nBatch normalization is a general technique that can be used to normalize the inputs to a layer.\n\nIt can be used with most network types, such as Multilayer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks.",
    "contentLength": 534,
    "wordCount": 74,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#use-with-different-network-types"
  },
  {
    "id": "ai-batchnorm-probably-use-before-the-activation-5",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Probably Use Before the Activation",
    "order": 5,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer.</p>\n  </li>\n  <li>\n    <p>It may be more appropriate <strong>after</strong> the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.</p>\n  </li>\n  <li>\n    <p>It may be appropriate <strong>before</strong> the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function (ReLU), the modern default for most network types.</p>\n  </li>\n</ul>\n<p>Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer.</p>\n<p>It may be more appropriate <strong>after</strong> the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.</p>\n<p>It may be appropriate <strong>before</strong> the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function (ReLU), the modern default for most network types.</p>\n<blockquote>\n  <p>The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. - <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n</blockquote>\n<p>The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. - <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n<ul>\n  <li>Perhaps test both approaches with your network.</li>\n</ul>",
    "contentMarkdown": "*   Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer.\n    \n*   It may be more appropriate **after** the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.\n    \n*   It may be appropriate **before** the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function (ReLU), the modern default for most network types.\n    \n\nBatch normalization may be used on the inputs to the layer before or after the activation function in the previous layer.\n\nIt may be more appropriate **after** the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.\n\nIt may be appropriate **before** the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function (ReLU), the modern default for most network types.\n\n> The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\nThe goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\n*   Perhaps test both approaches with your network.",
    "contentLength": 2080,
    "wordCount": 270,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#probably-use-before-the-activation"
  },
  {
    "id": "ai-batchnorm-use-large-learning-rates-6",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Use Large Learning Rates",
    "order": 6,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Using batch normalization makes the network more stable during training.</p>\n  </li>\n  <li>\n    <p>This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.</p>\n  </li>\n</ul>\n<p>Using batch normalization makes the network more stable during training.</p>\n<p>This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.</p>\n<blockquote>\n  <p>In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects — <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n</blockquote>\n<p>In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects — <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n<ul>\n  <li>The faster training also means that the decay rate used for the learning rate may be increased.</li>\n</ul>",
    "contentMarkdown": "*   Using batch normalization makes the network more stable during training.\n    \n*   This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.\n    \n\nUsing batch normalization makes the network more stable during training.\n\nThis may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.\n\n> In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects — [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\nIn a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects — [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\n*   The faster training also means that the decay rate used for the learning rate may be increased.",
    "contentLength": 1189,
    "wordCount": 155,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#use-large-learning-rates"
  },
  {
    "id": "ai-batchnorm-less-sensitive-to-weight-initialization-7",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Less Sensitive to Weight Initialization",
    "order": 7,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>Deep neural networks can be quite sensitive to the technique used to initialize the weights prior to training.</p>\n  </li>\n  <li>\n    <p>The stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method.</p>\n  </li>\n</ul>\n<p>Deep neural networks can be quite sensitive to the technique used to initialize the weights prior to training.</p>\n<p>The stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method.</p>",
    "contentMarkdown": "*   Deep neural networks can be quite sensitive to the technique used to initialize the weights prior to training.\n    \n*   The stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method.\n    \n\nDeep neural networks can be quite sensitive to the technique used to initialize the weights prior to training.\n\nThe stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method.",
    "contentLength": 600,
    "wordCount": 82,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#less-sensitive-to-weight-initialization"
  },
  {
    "id": "ai-batchnorm-alternate-to-data-preparation-8",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Alternate to Data Preparation",
    "order": 8,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Batch normalization could be used to standardize raw input variables that have differing scales.</p>\n  </li>\n  <li>\n    <p>If the mean and standard deviations calculated for each input feature are calculated over the mini-batch instead of over the entire training dataset, then the batch size must be sufficiently representative of the range of each variable.</p>\n  </li>\n  <li>\n    <p>It may not be appropriate for variables that have a data distribution that is highly non-Gaussian, in which case it might be better to perform data scaling as a pre-processing step.</p>\n  </li>\n</ul>\n<p>Batch normalization could be used to standardize raw input variables that have differing scales.</p>\n<p>If the mean and standard deviations calculated for each input feature are calculated over the mini-batch instead of over the entire training dataset, then the batch size must be sufficiently representative of the range of each variable.</p>\n<p>It may not be appropriate for variables that have a data distribution that is highly non-Gaussian, in which case it might be better to perform data scaling as a pre-processing step.</p>",
    "contentMarkdown": "*   Batch normalization could be used to standardize raw input variables that have differing scales.\n    \n*   If the mean and standard deviations calculated for each input feature are calculated over the mini-batch instead of over the entire training dataset, then the batch size must be sufficiently representative of the range of each variable.\n    \n*   It may not be appropriate for variables that have a data distribution that is highly non-Gaussian, in which case it might be better to perform data scaling as a pre-processing step.\n    \n\nBatch normalization could be used to standardize raw input variables that have differing scales.\n\nIf the mean and standard deviations calculated for each input feature are calculated over the mini-batch instead of over the entire training dataset, then the batch size must be sufficiently representative of the range of each variable.\n\nIt may not be appropriate for variables that have a data distribution that is highly non-Gaussian, in which case it might be better to perform data scaling as a pre-processing step.",
    "contentLength": 1141,
    "wordCount": 167,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#alternate-to-data-preparation"
  },
  {
    "id": "ai-batchnorm-dont-use-with-dropout-9",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Tips for Using Batch Normalization",
    "title": "Don’t Use with Dropout",
    "order": 9,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.</li>\n</ul>\n<blockquote>\n  <p>Removing Dropout from Modified BN-Inception speeds up training, without increasing overfitting. — <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n</blockquote>\n<p>Removing Dropout from Modified BN-Inception speeds up training, without increasing overfitting. — <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015.</p>\n<ul>\n  <li>\n    <p>Further, it may not be a good idea to use batch normalization and dropout in the same network.</p>\n  </li>\n  <li>\n    <p>The reason is that the statistics used to normalize the activations of the prior layer may become noisy given the random dropping out of nodes during the dropout procedure.</p>\n  </li>\n</ul>\n<p>Further, it may not be a good idea to use batch normalization and dropout in the same network.</p>\n<p>The reason is that the statistics used to normalize the activations of the prior layer may become noisy given the random dropping out of nodes during the dropout procedure.</p>\n<blockquote>\n  <p>Batch normalization also sometimes reduces generalization error and allows dropout to be omitted, due to the noise in the estimate of the statistics used to normalize each variable. — Page 425, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016.</p>\n</blockquote>\n<p>Batch normalization also sometimes reduces generalization error and allows dropout to be omitted, due to the noise in the estimate of the statistics used to normalize each variable. — Page 425, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016.</p>",
    "contentMarkdown": "*   Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n\n> Removing Dropout from Modified BN-Inception speeds up training, without increasing overfitting. — [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\nRemoving Dropout from Modified BN-Inception speeds up training, without increasing overfitting. — [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015.\n\n*   Further, it may not be a good idea to use batch normalization and dropout in the same network.\n    \n*   The reason is that the statistics used to normalize the activations of the prior layer may become noisy given the random dropping out of nodes during the dropout procedure.\n    \n\nFurther, it may not be a good idea to use batch normalization and dropout in the same network.\n\nThe reason is that the statistics used to normalize the activations of the prior layer may become noisy given the random dropping out of nodes during the dropout procedure.\n\n> Batch normalization also sometimes reduces generalization error and allows dropout to be omitted, due to the noise in the estimate of the statistics used to normalize each variable. — Page 425, [Deep Learning](https://amzn.to/2NJW3gE), 2016.\n\nBatch normalization also sometimes reduces generalization error and allows dropout to be omitted, due to the noise in the estimate of the statistics used to normalize each variable. — Page 425, [Deep Learning](https://amzn.to/2NJW3gE), 2016.",
    "contentLength": 1869,
    "wordCount": 234,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#don’t-use-with-dropout"
  },
  {
    "id": "ai-batchnorm-does-batchnorm-lead-to-a-standard-normal-distribut-10",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "FAQs",
    "title": "Does BatchNorm Lead to a Standard Normal Distribution Among Layer Outputs?",
    "order": 10,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Batch Normalization (BatchNorm) in deep learning does not directly lead to a standard normal distribution among layer outputs, but it does move the outputs closer to a normal distribution. The primary purpose of BatchNorm is to normalize the inputs of each layer, i.e., to shift and scale the inputs so that they have a mean of zero and a standard deviation of one. This is somewhat similar to a standard normal distribution.</li>\n  <li>Here’s a breakdown of how BatchNorm works and its effects:\n    <ol>\n      <li>\n        <p><strong>Normalizing Layer Inputs:</strong> BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.</p>\n      </li>\n      <li>\n        <p><strong>Learnable Parameters:</strong> After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\gamma</script>) and a shift factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\beta</script>). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.</p>\n      </li>\n      <li>\n        <p><strong>Improving Training Stability:</strong> By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.</p>\n      </li>\n      <li>\n        <p><strong>Effect on Distribution:</strong> While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.</p>\n      </li>\n      <li>\n        <p><strong>Impact on Training and Activation Functions:</strong> BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.</p>\n      </li>\n    </ol>\n  </li>\n  <li>In summary, BatchNorm helps in normalizing the inputs to each layer, making them have properties similar to a standard normal distribution, but it does not enforce a strict standard normal distribution. The learnable parameters in BatchNorm give the network flexibility to learn the most effective distribution of inputs for each layer.</li>\n</ul>\n<ol>\n      <li>\n        <p><strong>Normalizing Layer Inputs:</strong> BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.</p>\n      </li>\n      <li>\n        <p><strong>Learnable Parameters:</strong> After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\gamma</script>) and a shift factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\beta</script>). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.</p>\n      </li>\n      <li>\n        <p><strong>Improving Training Stability:</strong> By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.</p>\n      </li>\n      <li>\n        <p><strong>Effect on Distribution:</strong> While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.</p>\n      </li>\n      <li>\n        <p><strong>Impact on Training and Activation Functions:</strong> BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.</p>\n      </li>\n    </ol>\n<p><strong>Normalizing Layer Inputs:</strong> BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.</p>\n<p><strong>Learnable Parameters:</strong> After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">γ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>γ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\gamma</script>) and a shift factor (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B2;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"mi\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>β</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\beta</script>). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.</p>\n<p><strong>Improving Training Stability:</strong> By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.</p>\n<p><strong>Effect on Distribution:</strong> While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.</p>\n<p><strong>Impact on Training and Activation Functions:</strong> BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.</p>",
    "contentMarkdown": "*   Batch Normalization (BatchNorm) in deep learning does not directly lead to a standard normal distribution among layer outputs, but it does move the outputs closer to a normal distribution. The primary purpose of BatchNorm is to normalize the inputs of each layer, i.e., to shift and scale the inputs so that they have a mean of zero and a standard deviation of one. This is somewhat similar to a standard normal distribution.\n*   Here’s a breakdown of how BatchNorm works and its effects:\n    1.  **Normalizing Layer Inputs:** BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.\n        \n    2.  **Learnable Parameters:** After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (γγ\\\\gamma) and a shift factor (ββ\\\\beta). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.\n        \n    3.  **Improving Training Stability:** By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.\n        \n    4.  **Effect on Distribution:** While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.\n        \n    5.  **Impact on Training and Activation Functions:** BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.\n        \n*   In summary, BatchNorm helps in normalizing the inputs to each layer, making them have properties similar to a standard normal distribution, but it does not enforce a strict standard normal distribution. The learnable parameters in BatchNorm give the network flexibility to learn the most effective distribution of inputs for each layer.\n\n1.  **Normalizing Layer Inputs:** BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.\n    \n2.  **Learnable Parameters:** After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (γγ\\\\gamma) and a shift factor (ββ\\\\beta). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.\n    \n3.  **Improving Training Stability:** By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.\n    \n4.  **Effect on Distribution:** While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.\n    \n5.  **Impact on Training and Activation Functions:** BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.\n    \n\n**Normalizing Layer Inputs:** BatchNorm normalizes the inputs for each mini-batch. This normalization is done per feature (i.e., independently for each channel in the case of CNNs or each feature in fully connected layers). The normalization ensures that the mean of the inputs is close to 0 and the variance is close to 1.\n\n**Learnable Parameters:** After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor (γγ\\\\gamma) and a shift factor (ββ\\\\beta). These parameters allow the network to scale and shift the normalized feature and even to undo the normalization if that is what the learned behavior dictates, providing flexibility to learn if normalization is beneficial for that specific feature. This means the layer can learn the optimal scale and mean of the inputs for the activations.\n\n**Improving Training Stability:** By normalizing the inputs, BatchNorm helps in stabilizing the learning process and reduces the sensitivity of the network to the initial weights and learning rate.\n\n**Effect on Distribution:** While BatchNorm makes the inputs to a layer more normalized, it does not force them to strictly follow a standard normal distribution (mean 0, variance 1). The actual distribution of layer outputs can vary depending on the data, the network architecture, and the stage of training. BatchNorm ensures that the distribution of the inputs to each layer does not change drastically during training, which is known as reducing internal covariate shift.\n\n**Impact on Training and Activation Functions:** BatchNorm helps in stabilizing the learning process by ensuring that the distribution of inputs to each layer does not change drastically during training, a concept known as reducing internal covariate shift. BatchNorm can also make non-linear activation functions (like sigmoid or tanh) work more effectively by preventing the inputs from falling into the saturated regions of the function.",
    "contentLength": 15021,
    "wordCount": 1031,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#does-batchnorm-lead-to-a-standard-normal-distribution-among-layer-outputs?"
  },
  {
    "id": "ai-batchnorm-related-does-layernorm-seek-to-obtain-a-normal-dis-11",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "FAQs",
    "title": "Related: Does LayerNorm Seek to Obtain a Normal Distribution at the Output of a Layer?",
    "order": 11,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Layer Normalization (LayerNorm) does not specifically seek to obtain a normal distribution at the output of a layer. Instead, its primary goal is to normalize the inputs across the features for each data sample independently. This means that for each sample in a batch, LayerNorm computes the mean and variance used for normalization across the features (i.e., across the neurons in a layer).</li>\n  <li>The normalization process involves subtracting the mean and dividing by the standard deviation, which could make the data more normally distributed in a statistical sense. However, the main intent of LayerNorm is to stabilize the learning process and to help with faster convergence during training of deep neural networks, rather than enforcing a strict normal distribution of the layer outputs.</li>\n  <li>This stabilization is achieved by reducing the internal covariate shift, which refers to the change in the distribution of network activations due to the change in network parameters during training. LayerNorm, like other normalization techniques, makes the training process less sensitive to the learning rate and other hyper-parameters and can lead to improved generalization performance in deep learning models.</li>\n</ul>",
    "contentMarkdown": "*   Layer Normalization (LayerNorm) does not specifically seek to obtain a normal distribution at the output of a layer. Instead, its primary goal is to normalize the inputs across the features for each data sample independently. This means that for each sample in a batch, LayerNorm computes the mean and variance used for normalization across the features (i.e., across the neurons in a layer).\n*   The normalization process involves subtracting the mean and dividing by the standard deviation, which could make the data more normally distributed in a statistical sense. However, the main intent of LayerNorm is to stabilize the learning process and to help with faster convergence during training of deep neural networks, rather than enforcing a strict normal distribution of the layer outputs.\n*   This stabilization is achieved by reducing the internal covariate shift, which refers to the change in the distribution of network activations due to the change in network parameters during training. LayerNorm, like other normalization techniques, makes the training process less sensitive to the learning rate and other hyper-parameters and can lead to improved generalization performance in deep learning models.",
    "contentLength": 1248,
    "wordCount": 185,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#related:-does-layernorm-seek-to-obtain-a-normal-distribution-at-the-output-of-a-layer?"
  },
  {
    "id": "ai-batchnorm-does-batchnorm-normalize-at-a-per-feature-level-12",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "FAQs",
    "title": "Does BatchNorm Normalize at a “per-feature” Level?",
    "order": 12,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>BatchNorm normalizes is indeed applied normalization is done per feature” in the context of Batch Normalization (BatchNorm) in deep learning, it refers to how the normalization process is applied independently to each feature within a batch of data.</li>\n  <li>To elaborate:\n    <ol>\n      <li><strong>Definition of a Feature:</strong> In deep learning, a “feature” typically refers to a single measurable property or characteristic of the data. For instance:\n        <ul>\n          <li>In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.</li>\n          <li>In a fully connected layer, a feature refers to an individual neuron’s input or output.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Treating Channels as Features in Image Data:</strong> In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.</p>\n      </li>\n      <li><strong>Normalization Process:</strong> BatchNorm normalizes the data for each feature (or channel) separately. This means that for each filter output (channel at that layer), BatchNorm calculates the mean and variance across the mini-batch. The steps are:\n        <ul>\n          <li><strong>Compute Mean and Variance:</strong> For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.</li>\n          <li><strong>Normalize:</strong> Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Per Feature Processing:</strong> This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.</p>\n      </li>\n      <li>\n        <p><strong>Batch Dependent:</strong> The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.</p>\n      </li>\n      <li><strong>Learnable Parameters:</strong> After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor and a shift factor. These parameters allow the network to scale and shift the normalized feature, thus providing the flexibility for the network to learn if it actually benefits from the normalization or not.</li>\n    </ol>\n  </li>\n  <li>This per-feature normalization is a key aspect of BatchNorm and is instrumental in stabilizing the training process, accelerating convergence, and improving the overall performance of deep neural networks.</li>\n</ul>\n<ol>\n      <li><strong>Definition of a Feature:</strong> In deep learning, a “feature” typically refers to a single measurable property or characteristic of the data. For instance:\n        <ul>\n          <li>In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.</li>\n          <li>In a fully connected layer, a feature refers to an individual neuron’s input or output.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Treating Channels as Features in Image Data:</strong> In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.</p>\n      </li>\n      <li><strong>Normalization Process:</strong> BatchNorm normalizes the data for each feature (or channel) separately. This means that for each filter output (channel at that layer), BatchNorm calculates the mean and variance across the mini-batch. The steps are:\n        <ul>\n          <li><strong>Compute Mean and Variance:</strong> For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.</li>\n          <li><strong>Normalize:</strong> Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Per Feature Processing:</strong> This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.</p>\n      </li>\n      <li>\n        <p><strong>Batch Dependent:</strong> The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.</p>\n      </li>\n      <li><strong>Learnable Parameters:</strong> After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor and a shift factor. These parameters allow the network to scale and shift the normalized feature, thus providing the flexibility for the network to learn if it actually benefits from the normalization or not.</li>\n    </ol>\n<ul>\n          <li>In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.</li>\n          <li>In a fully connected layer, a feature refers to an individual neuron’s input or output.</li>\n        </ul>\n<p><strong>Treating Channels as Features in Image Data:</strong> In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.</p>\n<ul>\n          <li><strong>Compute Mean and Variance:</strong> For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.</li>\n          <li><strong>Normalize:</strong> Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.</li>\n        </ul>\n<p><strong>Per Feature Processing:</strong> This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.</p>\n<p><strong>Batch Dependent:</strong> The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.</p>",
    "contentMarkdown": "*   BatchNorm normalizes is indeed applied normalization is done per feature” in the context of Batch Normalization (BatchNorm) in deep learning, it refers to how the normalization process is applied independently to each feature within a batch of data.\n*   To elaborate:\n    1.  **Definition of a Feature:** In deep learning, a “feature” typically refers to a single measurable property or characteristic of the data. For instance:\n        *   In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.\n        *   In a fully connected layer, a feature refers to an individual neuron’s input or output.\n    2.  **Treating Channels as Features in Image Data:** In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.\n        \n    3.  **Normalization Process:** BatchNorm normalizes the data for each feature (or channel) separately. This means that for each filter output (channel at that layer), BatchNorm calculates the mean and variance across the mini-batch. The steps are:\n        *   **Compute Mean and Variance:** For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.\n        *   **Normalize:** Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.\n    4.  **Per Feature Processing:** This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.\n        \n    5.  **Batch Dependent:** The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.\n        \n    6.  **Learnable Parameters:** After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor and a shift factor. These parameters allow the network to scale and shift the normalized feature, thus providing the flexibility for the network to learn if it actually benefits from the normalization or not.\n*   This per-feature normalization is a key aspect of BatchNorm and is instrumental in stabilizing the training process, accelerating convergence, and improving the overall performance of deep neural networks.\n\n1.  **Definition of a Feature:** In deep learning, a “feature” typically refers to a single measurable property or characteristic of the data. For instance:\n    *   In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.\n    *   In a fully connected layer, a feature refers to an individual neuron’s input or output.\n2.  **Treating Channels as Features in Image Data:** In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.\n    \n3.  **Normalization Process:** BatchNorm normalizes the data for each feature (or channel) separately. This means that for each filter output (channel at that layer), BatchNorm calculates the mean and variance across the mini-batch. The steps are:\n    *   **Compute Mean and Variance:** For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.\n    *   **Normalize:** Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.\n4.  **Per Feature Processing:** This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.\n    \n5.  **Batch Dependent:** The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.\n    \n6.  **Learnable Parameters:** After normalization, BatchNorm introduces two learnable parameters for each feature: a scale factor and a shift factor. These parameters allow the network to scale and shift the normalized feature, thus providing the flexibility for the network to learn if it actually benefits from the normalization or not.\n\n*   In a Convolutional Neural Network (CNN), a “feature” typically refers to the output of a filter applied to the input. When working with images, these features correspond to different aspects of the image, such as edges, textures, or colors. Specifically, in the context of image data, a feature often corresponds to a specific channel at a particular layer. For example, in an RGB image, there are three primary channels: Red, Green, and Blue.\n*   In a fully connected layer, a feature refers to an individual neuron’s input or output.\n\n**Treating Channels as Features in Image Data:** In the case of image data in CNNs, treating each channel output of a filter as a feature for BatchNorm is more effective than treating each individual pixel as a feature. This approach maintains the spatial structure of image data and recognizes the importance of spatial correlations.\n\n*   **Compute Mean and Variance:** For a given feature, calculate the mean and variance across all the samples in the mini-batch. This calculation is not across the entire dataset but just the current batch of data being processed.\n*   **Normalize:** Subtract the mean and divide by the standard deviation (derived from the variance) for each feature. This step ensures that this particular feature (across all the samples in the batch) now has a mean close to zero and a standard deviation close to one.\n\n**Per Feature Processing:** This per-feature processing means that each feature (like each filter/channel output in an image or each neuron in a layer) is normalized independently of other features. This is crucial because different features can have different scales and ranges. Normalizing them individually allows the model to treat each feature on a comparable scale.\n\n**Batch Dependent:** The normalization is dependent on the batch, which means it can vary from one batch of data to the next. During training, this can add a form of noise to the learning process, which can actually help with generalization.",
    "contentLength": 8962,
    "wordCount": 1269,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#does-batchnorm-normalize-at-a-“per-feature”-level?"
  },
  {
    "id": "ai-batchnorm-books-13",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Books",
    "order": 13,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Section – 8.7.1 Batch Normalization, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016</li>\n  <li>Section 7.3.1. Advanced architecture patterns, <a href=\"https://amzn.to/2wVqZDq\">Deep Learning With Python</a>, 2017</li>\n</ul>",
    "contentMarkdown": "*   Section – 8.7.1 Batch Normalization, [Deep Learning](https://amzn.to/2NJW3gE), 2016\n*   Section 7.3.1. Advanced architecture patterns, [Deep Learning With Python](https://amzn.to/2wVqZDq), 2017",
    "contentLength": 244,
    "wordCount": 20,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#books"
  },
  {
    "id": "ai-batchnorm-papers-14",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Papers",
    "order": 14,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, 2015</li>\n  <li><a href=\"https://arxiv.org/abs/1702.03275\">Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models</a>, 2017</li>\n  <li><a href=\"https://arxiv.org/abs/1805.11604\">How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)</a>, 2018</li>\n</ul>",
    "contentMarkdown": "*   [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), 2015\n*   [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models](https://arxiv.org/abs/1702.03275), 2017\n*   [How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)](https://arxiv.org/abs/1805.11604), 2018",
    "contentLength": 478,
    "wordCount": 40,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#papers"
  },
  {
    "id": "ai-batchnorm-articles-15",
    "articleSlug": "batchnorm",
    "articleTitle": "Batchnorm",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Articles",
    "order": 15,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><a href=\"https://en.wikipedia.org/wiki/Batch_normalization\">Batch normalization, Wikipedia</a></li>\n  <li><a href=\"https://www.youtube.com/watch?v=nUUqwaxLnWs\">Why Does Batch Norm Work?, deeplearning.ai</a>, Video</li>\n  <li><a href=\"https://www.youtube.com/watch?v=Xogn6veSyxA\">Batch Normalization</a>, OpenAI, 2016</li>\n  <li><a href=\"https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\">Batch Normalization before or after ReLU?, Reddit</a></li>\n</ul>",
    "contentMarkdown": "*   [Batch normalization, Wikipedia](https://en.wikipedia.org/wiki/Batch_normalization)\n*   [Why Does Batch Norm Work?, deeplearning.ai](https://www.youtube.com/watch?v=nUUqwaxLnWs), Video\n*   [Batch Normalization](https://www.youtube.com/watch?v=Xogn6veSyxA), OpenAI, 2016\n*   [Batch Normalization before or after ReLU?, Reddit](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)",
    "contentLength": 514,
    "wordCount": 25,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/batchnorm/#articles"
  }
]