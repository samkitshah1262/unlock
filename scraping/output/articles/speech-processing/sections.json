[
  {
    "id": "ai-speech-processing-phoneme-1",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Phoneme",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>A phoneme is the smallest unit that distinguishes meaning between sounds in a given language.” What does that mean? Let’s look at a word using IPA, a transcription system created by the International Phonetic Association.</li>\n  <li>Let’s look at the word puff. We use broad transcription when describing phonemes. When we are using broad transcription we use slashes (<code class=\"language-plaintext highlighter-rouge\">/ /</code>). So the word puff in broad transcription is:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"texatom\" id=\"MathJax-Span-3\"><span class=\"mrow\" id=\"MathJax-Span-4\"><span class=\"mo\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-7\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-11\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"mo\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow></math></span></span></div>\n<ul>\n  <li>Here we see that puff has three phonemes <code class=\"language-plaintext highlighter-rouge\">/p/</code>, <code class=\"language-plaintext highlighter-rouge\">/ʌ/</code>,  and <code class=\"language-plaintext highlighter-rouge\">/f/</code>. When we store the pronunciation of the word puff in our head, this is how we remember it. What happens if we change one phoneme in the word puff? If we change the phoneme (not the letters) <code class=\"language-plaintext highlighter-rouge\">/f/</code> to the phoneme <code class=\"language-plaintext highlighter-rouge\">/k/</code> we get another word. We get the word puck which looks like this in broad transcription:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-14\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-15\"><span class=\"texatom\" id=\"MathJax-Span-16\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-20\"><span class=\"mrow\" id=\"MathJax-Span-21\"><span class=\"mo\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-24\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow></math></span></span></div>\n<ul>\n  <li>This is a type of test that we can do to see if <code class=\"language-plaintext highlighter-rouge\">/f/</code> and <code class=\"language-plaintext highlighter-rouge\">/k/</code> are different phonemes. If we swap these two phonemes we get a new word so we can say that in English <code class=\"language-plaintext highlighter-rouge\">/f/</code> and <code class=\"language-plaintext highlighter-rouge\">/k/</code> are different phonemes. We’re going to discuss phones now, but keep this in the back of your head, because we are going to come back to it.</li>\n</ul>",
    "contentMarkdown": "*   A phoneme is the smallest unit that distinguishes meaning between sounds in a given language.” What does that mean? Let’s look at a word using IPA, a transcription system created by the International Phonetic Association.\n*   Let’s look at the word puff. We use broad transcription when describing phonemes. When we are using broad transcription we use slashes (`/ /`). So the word puff in broad transcription is:\n\n/pʌf//pʌf/\n\n*   Here we see that puff has three phonemes `/p/`, `/ʌ/`, and `/f/`. When we store the pronunciation of the word puff in our head, this is how we remember it. What happens if we change one phoneme in the word puff? If we change the phoneme (not the letters) `/f/` to the phoneme `/k/` we get another word. We get the word puck which looks like this in broad transcription:\n\n/pʌk//pʌk/\n\n*   This is a type of test that we can do to see if `/f/` and `/k/` are different phonemes. If we swap these two phonemes we get a new word so we can say that in English `/f/` and `/k/` are different phonemes. We’re going to discuss phones now, but keep this in the back of your head, because we are going to come back to it.",
    "contentLength": 6268,
    "wordCount": 210,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#phoneme"
  },
  {
    "id": "ai-speech-processing-phone-2",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Phone",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Now that we’ve covered what a phoneme is, we can discuss phones. Remember that we defined a phoneme as “the smallest unit that distinguishes meaning between sounds in a given language.” However, a phoneme is really the mental representation of a sound, not the sound itself. The phoneme is the part that is stored in your brain. When you actually produce a sound you are producing a phone.\nTo give an example, let’s say you want to say the word for a small four-legged animal that meows, a cat. Your brain searches for the word in your lexicon to see if you know the word. You find the lexical entry. You see that phonemic representation of the word is <code class=\"language-plaintext highlighter-rouge\">/kæt/</code>. Then you use your vocal tract to produce the sounds <code class=\"language-plaintext highlighter-rouge\">[k]</code>, <code class=\"language-plaintext highlighter-rouge\">[æ]</code>, and <code class=\"language-plaintext highlighter-rouge\">[t]</code> and you get the word <code class=\"language-plaintext highlighter-rouge\">[kæt]</code>.</li>\n  <li>Phones, the actual sound part that you can hear, are marked with brackets (<code class=\"language-plaintext highlighter-rouge\">[]</code>) and the phonemes, the mental representation of the sound, are marked with slashes (<code class=\"language-plaintext highlighter-rouge\">/ /</code>).</li>\n</ul>",
    "contentMarkdown": "*   Now that we’ve covered what a phoneme is, we can discuss phones. Remember that we defined a phoneme as “the smallest unit that distinguishes meaning between sounds in a given language.” However, a phoneme is really the mental representation of a sound, not the sound itself. The phoneme is the part that is stored in your brain. When you actually produce a sound you are producing a phone. To give an example, let’s say you want to say the word for a small four-legged animal that meows, a cat. Your brain searches for the word in your lexicon to see if you know the word. You find the lexical entry. You see that phonemic representation of the word is `/kæt/`. Then you use your vocal tract to produce the sounds `[k]`, `[æ]`, and `[t]` and you get the word `[kæt]`.\n*   Phones, the actual sound part that you can hear, are marked with brackets (`[]`) and the phonemes, the mental representation of the sound, are marked with slashes (`/ /`).",
    "contentLength": 1364,
    "wordCount": 171,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#phone"
  },
  {
    "id": "ai-speech-processing-why-do-we-need-phonemes-and-phones-3",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Why Do We Need Phonemes and Phones?",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Recap: Phonemes are the mental representation of the how a word sounds and phones are the actual sounds themselves. If we take an example from above, the word puff we can write out the phonemic representation (with phonemes using slashes) and the phonetic representation (with phones using brackets).</li>\n</ul>\n<div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mspace linebreak=&quot;newline&quot; /><mo stretchy=&quot;false&quot;>[</mo><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 100%; display: inline-block; min-width: 2.503em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.03em, 5.732em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.98em, 4.378em, -999.997em); top: -4.008em; left: 50%; margin-left: -0.987em;\"><span class=\"texatom\" id=\"MathJax-Span-29\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"mo\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-33\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-37\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mo\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.03em, 4.378em, -999.997em); top: -2.654em; left: 50%; margin-left: -1.039em;\"><span class=\"mspace\" id=\"MathJax-Span-40\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-43\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular;\">]</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.934em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mspace linebreak=\"newline\"></mspace><mo stretchy=\"false\">[</mo><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>What does the above show us? Not a whole lot really. So why do we need two different versions? Recall that the transcription that uses phonemes is called broad transcription while the transcription that uses phones is called narrow transcription. These names can give us a clue about the differences.</p>\n  </li>\n  <li>\n    <p>By looking at the broad transcription, <code class=\"language-plaintext highlighter-rouge\">/pʌf/</code>, we can know how to pronounce the word puff. Not only we (as native English speakers) can pronounce the word, but also a non-native English speaker can  pronounce it as well. We should all be able to understand what we are saying. However, what if we wanted more information about how the word actually sounds? Narrow transcription can help us with that.</p>\n  </li>\n  <li>\n    <p>Narrow transcription just gives us extra information about how a word sounds. So the word puff can be written like this in narrow transcription:</p>\n  </li>\n</ul>\n<p>What does the above show us? Not a whole lot really. So why do we need two different versions? Recall that the transcription that uses phonemes is called broad transcription while the transcription that uses phones is called narrow transcription. These names can give us a clue about the differences.</p>\n<p>By looking at the broad transcription, <code class=\"language-plaintext highlighter-rouge\">/pʌf/</code>, we can know how to pronounce the word puff. Not only we (as native English speakers) can pronounce the word, but also a non-native English speaker can  pronounce it as well. We should all be able to understand what we are saying. However, what if we wanted more information about how the word actually sounds? Narrow transcription can help us with that.</p>\n<p>Narrow transcription just gives us extra information about how a word sounds. So the word puff can be written like this in narrow transcription:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2B0;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-48\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.4em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"mo\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-52\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mo\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Regular;\">ʰ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-55\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʰ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>Well, that’s new. This narrow transcription of the word puff gives us a little more information about how the word sounds. Here, we see that the <code class=\"language-plaintext highlighter-rouge\">[p]</code> is aspirated. This means that when pronouncing the sound <code class=\"language-plaintext highlighter-rouge\">[p]</code>,  we have an extra puff of air that comes out. We notate this by using the superscript <code class=\"language-plaintext highlighter-rouge\">ʰ</code>.\nSo you are probably asking yourself, why don’t we just put the <code class=\"language-plaintext highlighter-rouge\">ʰ</code> in the broad transcription? Remember that broad transcription uses phonemes and by definition, if we change a phoneme in a word, we will get a different word. Look at the following:</li>\n</ul>\n<div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2B0;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mo>&amp;#x2217;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-60\" style=\"width: 100%; display: inline-block; min-width: 3.44em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.82em, 5.94em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.98em, 4.378em, -999.997em); top: -4.008em; left: 50%; margin-left: -0.987em;\"><span class=\"texatom\" id=\"MathJax-Span-62\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-66\"><span class=\"mrow\" id=\"MathJax-Span-67\"><span class=\"mo\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-70\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"mo\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1002.82em, 4.378em, -999.997em); top: -2.445em; left: 50%; margin-left: -1.456em;\"><span class=\"mspace\" id=\"MathJax-Span-73\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mo\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-78\"><span class=\"mrow\" id=\"MathJax-Span-79\"><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular;\">ʰ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-81\"><span class=\"mrow\" id=\"MathJax-Span-82\"><span class=\"mo\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-85\"><span class=\"mrow\" id=\"MathJax-Span-86\"><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mo\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.184em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mspace linebreak=\"newline\"></mspace><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʰ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mo>∗</mo></math></span></span></div>\n<ul>\n  <li>An asterisk denotes that the above is incorrect. But why? Because in English, an aspirated p and an unaspirated p don’t change the meaning of a word. That is, you can pronounce the same sound two different ways, but it wouldn’t change the meaning. And by definition, if we change a phoneme, we change the meaning of a word. That means there’s only one <code class=\"language-plaintext highlighter-rouge\">/p/</code> phoneme in English. If we were speaking a language where aspiration does change the meaning of a word, then that language could have two phonemes, <code class=\"language-plaintext highlighter-rouge\">/p/</code> and <code class=\"language-plaintext highlighter-rouge\">/pʰ/</code>. Since it doesn’t change the meaning in English, we just mark it in narrow transcription.</li>\n</ul>",
    "contentMarkdown": "*   Recap: Phonemes are the mental representation of the how a word sounds and phones are the actual sounds themselves. If we take an example from above, the word puff we can write out the phonemic representation (with phonemes using slashes) and the phonetic representation (with phones using brackets).\n\n/pʌf/\\[pʌf\\]/pʌf/\\[pʌf\\]\n\n*   What does the above show us? Not a whole lot really. So why do we need two different versions? Recall that the transcription that uses phonemes is called broad transcription while the transcription that uses phones is called narrow transcription. These names can give us a clue about the differences.\n    \n*   By looking at the broad transcription, `/pʌf/`, we can know how to pronounce the word puff. Not only we (as native English speakers) can pronounce the word, but also a non-native English speaker can pronounce it as well. We should all be able to understand what we are saying. However, what if we wanted more information about how the word actually sounds? Narrow transcription can help us with that.\n    \n*   Narrow transcription just gives us extra information about how a word sounds. So the word puff can be written like this in narrow transcription:\n    \n\nWhat does the above show us? Not a whole lot really. So why do we need two different versions? Recall that the transcription that uses phonemes is called broad transcription while the transcription that uses phones is called narrow transcription. These names can give us a clue about the differences.\n\nBy looking at the broad transcription, `/pʌf/`, we can know how to pronounce the word puff. Not only we (as native English speakers) can pronounce the word, but also a non-native English speaker can pronounce it as well. We should all be able to understand what we are saying. However, what if we wanted more information about how the word actually sounds? Narrow transcription can help us with that.\n\nNarrow transcription just gives us extra information about how a word sounds. So the word puff can be written like this in narrow transcription:\n\n\\[pʰʌf\\]\\[pʰʌf\\]\n\n*   Well, that’s new. This narrow transcription of the word puff gives us a little more information about how the word sounds. Here, we see that the `[p]` is aspirated. This means that when pronouncing the sound `[p]`, we have an extra puff of air that comes out. We notate this by using the superscript `ʰ`. So you are probably asking yourself, why don’t we just put the `ʰ` in the broad transcription? Remember that broad transcription uses phonemes and by definition, if we change a phoneme in a word, we will get a different word. Look at the following:\n\n/pʌf//pʰʌf/∗/pʌf//pʰʌf/∗\n\n*   An asterisk denotes that the above is incorrect. But why? Because in English, an aspirated p and an unaspirated p don’t change the meaning of a word. That is, you can pronounce the same sound two different ways, but it wouldn’t change the meaning. And by definition, if we change a phoneme, we change the meaning of a word. That means there’s only one `/p/` phoneme in English. If we were speaking a language where aspiration does change the meaning of a word, then that language could have two phonemes, `/p/` and `/pʰ/`. Since it doesn’t change the meaning in English, we just mark it in narrow transcription.",
    "contentLength": 14657,
    "wordCount": 550,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#why-do-we-need-phonemes-and-phones?"
  },
  {
    "id": "ai-speech-processing-allophones-4",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Allophones",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>We can pronounce the <code class=\"language-plaintext highlighter-rouge\">/p/</code> phoneme in at least two different ways: <code class=\"language-plaintext highlighter-rouge\">[p]</code> and <code class=\"language-plaintext highlighter-rouge\">[pʰ]</code>.  This means that <code class=\"language-plaintext highlighter-rouge\">[p]</code> and <code class=\"language-plaintext highlighter-rouge\">[pʰ]</code> are allophones of the phoneme <code class=\"language-plaintext highlighter-rouge\">/p/</code>. The prefix “-allo” comes from the Greek állos meaning “other,” so you can think of allopones are just “another way to pronounce a phoneme.”</p>\n  </li>\n  <li>\n    <p>This really helps us when we talk about different accents. Take the word water for example. I’m American, so the phonemic representation that I have for the word water is:</p>\n  </li>\n</ul>\n<p>We can pronounce the <code class=\"language-plaintext highlighter-rouge\">/p/</code> phoneme in at least two different ways: <code class=\"language-plaintext highlighter-rouge\">[p]</code> and <code class=\"language-plaintext highlighter-rouge\">[pʰ]</code>.  This means that <code class=\"language-plaintext highlighter-rouge\">[p]</code> and <code class=\"language-plaintext highlighter-rouge\">[pʰ]</code> are allophones of the phoneme <code class=\"language-plaintext highlighter-rouge\">/p/</code>. The prefix “-allo” comes from the Greek állos meaning “other,” so you can think of allopones are just “another way to pronounce a phoneme.”</p>\n<p>This really helps us when we talk about different accents. Take the word water for example. I’m American, so the phonemic representation that I have for the word water is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x259;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x279;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-89\" style=\"width: 3.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.87em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-90\"><span class=\"texatom\" id=\"MathJax-Span-91\"><span class=\"mrow\" id=\"MathJax-Span-92\"><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-99\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular;\">ə</span></span></span><span class=\"texatom\" id=\"MathJax-Span-102\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">ɹ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-105\"><span class=\"mrow\" id=\"MathJax-Span-106\"><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ə</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɹ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow></math></span></span></div>\n<ul>\n  <li>But if you’ve ever heard an American pronounce the word water before, you know that many Americans don’t pronounce a <code class=\"language-plaintext highlighter-rouge\">[t]</code> sound. Instead, most Americans will pronounce that <code class=\"language-plaintext highlighter-rouge\">[t]</code> similar to a <code class=\"language-plaintext highlighter-rouge\">[d]</code> sound. It’s not pronounced the same was as a <code class=\"language-plaintext highlighter-rouge\">[d]</code> sound is pronounced, though. It’s actually a “flap” and written like this:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x27E;</mo></mrow><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.89em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mo\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"texatom\" id=\"MathJax-Span-111\"><span class=\"mrow\" id=\"MathJax-Span-112\"><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular;\">ɾ</span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɾ</mo></mrow><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>So the actual phonetic representation of the word water for many Americans is:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x27E;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x25A;</mo></mrow><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 3.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.76em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"mo\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"texatom\" id=\"MathJax-Span-119\"><span class=\"mrow\" id=\"MathJax-Span-120\"><span class=\"mo\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-122\"><span class=\"mrow\" id=\"MathJax-Span-123\"><span class=\"mo\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Regular;\">ɾ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular;\">ɚ</span></span></span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɾ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɚ</mo></mrow><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>So, in the phonemic representation (broad transcription), we have the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme, but most Americans will produce a <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> here. That means we can say that in American English <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> is an allophone of the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code>.</p>\n  </li>\n  <li>\n    <p>But sometimes the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme does use a <code class=\"language-plaintext highlighter-rouge\">[t]</code> sound like in the name Todd:</p>\n  </li>\n</ul>\n<p>So, in the phonemic representation (broad transcription), we have the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme, but most Americans will produce a <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> here. That means we can say that in American English <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> is an allophone of the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code>.</p>\n<p>But sometimes the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme does use a <code class=\"language-plaintext highlighter-rouge\">[t]</code> sound like in the name Todd:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-129\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.93em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-130\"><span class=\"texatom\" id=\"MathJax-Span-131\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mo\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-135\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mo\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-139\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow></math></span></span></div>\n<ul>\n  <li>and in narrow transcription:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2B0;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mi>d</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-146\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">ʰ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-149\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʰ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mi>d</mi><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>With these examples, we can see that the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code> has at least two allophones: <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code> and <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code>. We can even look at the word putt and see that the <code class=\"language-plaintext highlighter-rouge\">[t]</code> can be pronounced as a “regular” <code class=\"language-plaintext highlighter-rouge\">[t]</code> sound:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x28C;</mo></mrow><mi>t</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mo\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"texatom\" id=\"MathJax-Span-158\"><span class=\"mrow\" id=\"MathJax-Span-159\"><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">ʌ</span></span></span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʌ</mo></mrow><mi>t</mi><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>Fantastic! So now we know that the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code> has at least three allophones, <code class=\"language-plaintext highlighter-rouge\">[t]</code>,  <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code>,  and <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code>.  But how do we know when to say each one?</li>\n</ul>\n<h4 id=\"environments-of-phonemes\">Environments of Phonemes</h4>\n<ul>\n  <li>When we talk about the environments of a phoneme we are talking about where the phoneme occurs, usually in relation to other phonemes in a word. We can use this information to predict how a phoneme will be pronounced. Take for example the name Todd:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-163\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.93em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"texatom\" id=\"MathJax-Span-165\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-169\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-173\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow></math></span></span></div>\n<ul>\n  <li>\n    <p>If this was a word that we had never heard before, how would we know how to pronounce the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme? Well, we can already narrow it down to <code class=\"language-plaintext highlighter-rouge\">[t]</code>,  <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code>,  or <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> because we’ve seen in past examples that we can pronounce these when we have <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme. But how do we know which phone is the correct one?</p>\n  </li>\n  <li>\n    <p>If you combed through many many words in English, you would find out that the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code> is often aspirated when it’s at the beginning of a word. By looking at other words that start with <code class=\"language-plaintext highlighter-rouge\">/t/</code> like tap, take, tack, etc. you’ll find that <code class=\"language-plaintext highlighter-rouge\">/t/</code> becomes <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code> when at the beginning of a word and that the narrow transcription of the name Todd would be:</p>\n  </li>\n</ul>\n<p>If this was a word that we had never heard before, how would we know how to pronounce the <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme? Well, we can already narrow it down to <code class=\"language-plaintext highlighter-rouge\">[t]</code>,  <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code>,  or <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> because we’ve seen in past examples that we can pronounce these when we have <code class=\"language-plaintext highlighter-rouge\">/t/</code> phoneme. But how do we know which phone is the correct one?</p>\n<p>If you combed through many many words in English, you would find out that the phoneme <code class=\"language-plaintext highlighter-rouge\">/t/</code> is often aspirated when it’s at the beginning of a word. By looking at other words that start with <code class=\"language-plaintext highlighter-rouge\">/t/</code> like tap, take, tack, etc. you’ll find that <code class=\"language-plaintext highlighter-rouge\">/t/</code> becomes <code class=\"language-plaintext highlighter-rouge\">[tʰ]</code> when at the beginning of a word and that the narrow transcription of the name Todd would be:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>[</mo><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2B0;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x251;</mo></mrow><mi>d</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-180\"><span class=\"mrow\" id=\"MathJax-Span-181\"><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular;\">ʰ</span></span></span><span class=\"texatom\" id=\"MathJax-Span-183\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mo\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Regular;\">ɑ</span></span></span><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">[</mo><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>ʰ</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>ɑ</mo></mrow><mi>d</mi><mo stretchy=\"false\">]</mo></math></span></span></div>\n<ul>\n  <li>We can use the same process to find out how to pronounce other words in an American accent. If we look at the words eating, little, latter, etc… we can see that in American English all of <code class=\"language-plaintext highlighter-rouge\">/t/</code> are pronounced as <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code>.  Deciding all of the requirements for realizing <code class=\"language-plaintext highlighter-rouge\">/t/</code> as <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code> is beyond the scope of this post, but you can see that in similar environments the <code class=\"language-plaintext highlighter-rouge\">/t/</code> becomes a <code class=\"language-plaintext highlighter-rouge\">[ɾ]</code>.</li>\n</ul>",
    "contentMarkdown": "*   We can pronounce the `/p/` phoneme in at least two different ways: `[p]` and `[pʰ]`. This means that `[p]` and `[pʰ]` are allophones of the phoneme `/p/`. The prefix “-allo” comes from the Greek állos meaning “other,” so you can think of allopones are just “another way to pronounce a phoneme.”\n    \n*   This really helps us when we talk about different accents. Take the word water for example. I’m American, so the phonemic representation that I have for the word water is:\n    \n\nWe can pronounce the `/p/` phoneme in at least two different ways: `[p]` and `[pʰ]`. This means that `[p]` and `[pʰ]` are allophones of the phoneme `/p/`. The prefix “-allo” comes from the Greek állos meaning “other,” so you can think of allopones are just “another way to pronounce a phoneme.”\n\nThis really helps us when we talk about different accents. Take the word water for example. I’m American, so the phonemic representation that I have for the word water is:\n\n/wɑtəɹ//wɑtəɹ/\n\n*   But if you’ve ever heard an American pronounce the word water before, you know that many Americans don’t pronounce a `[t]` sound. Instead, most Americans will pronounce that `[t]` similar to a `[d]` sound. It’s not pronounced the same was as a `[d]` sound is pronounced, though. It’s actually a “flap” and written like this:\n\n\\[ɾ\\]\\[ɾ\\]\n\n*   So the actual phonetic representation of the word water for many Americans is:\n\n\\[wɑɾɚ\\]\\[wɑɾɚ\\]\n\n*   So, in the phonemic representation (broad transcription), we have the `/t/` phoneme, but most Americans will produce a `[ɾ]` here. That means we can say that in American English `[ɾ]` is an allophone of the phoneme `/t/`.\n    \n*   But sometimes the `/t/` phoneme does use a `[t]` sound like in the name Todd:\n    \n\nSo, in the phonemic representation (broad transcription), we have the `/t/` phoneme, but most Americans will produce a `[ɾ]` here. That means we can say that in American English `[ɾ]` is an allophone of the phoneme `/t/`.\n\nBut sometimes the `/t/` phoneme does use a `[t]` sound like in the name Todd:\n\n/tɑd//tɑd/\n\n*   and in narrow transcription:\n\n\\[tʰɑd\\]\\[tʰɑd\\]\n\n*   With these examples, we can see that the phoneme `/t/` has at least two allophones: `[tʰ]` and `[ɾ]`. We can even look at the word putt and see that the `[t]` can be pronounced as a “regular” `[t]` sound:\n\n\\[pʌt\\]\\[pʌt\\]\n\n*   Fantastic! So now we know that the phoneme `/t/` has at least three allophones, `[t]`, `[tʰ]`, and `[ɾ]`. But how do we know when to say each one?\n\n#### Environments of Phonemes\n\n*   When we talk about the environments of a phoneme we are talking about where the phoneme occurs, usually in relation to other phonemes in a word. We can use this information to predict how a phoneme will be pronounced. Take for example the name Todd:\n\n/tɑd//tɑd/\n\n*   If this was a word that we had never heard before, how would we know how to pronounce the `/t/` phoneme? Well, we can already narrow it down to `[t]`, `[tʰ]`, or `[ɾ]` because we’ve seen in past examples that we can pronounce these when we have `/t/` phoneme. But how do we know which phone is the correct one?\n    \n*   If you combed through many many words in English, you would find out that the phoneme `/t/` is often aspirated when it’s at the beginning of a word. By looking at other words that start with `/t/` like tap, take, tack, etc. you’ll find that `/t/` becomes `[tʰ]` when at the beginning of a word and that the narrow transcription of the name Todd would be:\n    \n\nIf this was a word that we had never heard before, how would we know how to pronounce the `/t/` phoneme? Well, we can already narrow it down to `[t]`, `[tʰ]`, or `[ɾ]` because we’ve seen in past examples that we can pronounce these when we have `/t/` phoneme. But how do we know which phone is the correct one?\n\nIf you combed through many many words in English, you would find out that the phoneme `/t/` is often aspirated when it’s at the beginning of a word. By looking at other words that start with `/t/` like tap, take, tack, etc. you’ll find that `/t/` becomes `[tʰ]` when at the beginning of a word and that the narrow transcription of the name Todd would be:\n\n\\[tʰɑd\\]\\[tʰɑd\\]\n\n*   We can use the same process to find out how to pronounce other words in an American accent. If we look at the words eating, little, latter, etc… we can see that in American English all of `/t/` are pronounced as `[ɾ]`. Deciding all of the requirements for realizing `/t/` as `[ɾ]` is beyond the scope of this post, but you can see that in similar environments the `/t/` becomes a `[ɾ]`.",
    "contentLength": 26686,
    "wordCount": 799,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#allophones"
  },
  {
    "id": "ai-speech-processing-graphemes-5",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Graphemes",
    "order": 5,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>In linguistics, a grapheme is the smallest functional unit of a <strong>writing system</strong>.</li>\n  <li>The name grapheme is given to the letter or combination of letters that represents a phoneme. For example, the word ‘ghost’ contains five letters and four graphemes (<code class=\"language-plaintext highlighter-rouge\">gh</code>, <code class=\"language-plaintext highlighter-rouge\">o</code>, <code class=\"language-plaintext highlighter-rouge\">s</code>, and <code class=\"language-plaintext highlighter-rouge\">t</code>), representing four phonemes.</li>\n</ul>",
    "contentMarkdown": "*   In linguistics, a grapheme is the smallest functional unit of a **writing system**.\n*   The name grapheme is given to the letter or combination of letters that represents a phoneme. For example, the word ‘ghost’ contains five letters and four graphemes (`gh`, `o`, `s`, and `t`), representing four phonemes.",
    "contentLength": 573,
    "wordCount": 50,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#graphemes"
  },
  {
    "id": "ai-speech-processing-mono-vs-stereo-sound-6",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Mono vs. Stereo Sound",
    "order": 6,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>The difference between monophonic (mono) and stereophonic (stereo) sound is the number of channels used to record and playback audio.</li>\n  <li>Mono signals are recorded and played back using a single audio channel, while stereo sounds are recorded and played back using two audio channels.</li>\n  <li>As a listener, the most noticeable difference is that stereo sounds are capable of producing the perception of width, whereas mono sounds are not.</li>\n  <li>Using Mono instead of Stereo playback is most useful for users with certain types of hearing loss or for safety reasons, for example when you need to listen to your surroundings.</li>\n</ul>",
    "contentMarkdown": "*   The difference between monophonic (mono) and stereophonic (stereo) sound is the number of channels used to record and playback audio.\n*   Mono signals are recorded and played back using a single audio channel, while stereo sounds are recorded and played back using two audio channels.\n*   As a listener, the most noticeable difference is that stereo sounds are capable of producing the perception of width, whereas mono sounds are not.\n*   Using Mono instead of Stereo playback is most useful for users with certain types of hearing loss or for safety reasons, for example when you need to listen to your surroundings.",
    "contentLength": 661,
    "wordCount": 103,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#mono-vs.-stereo-sound"
  },
  {
    "id": "ai-speech-processing-key-takeaways-7",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Terminology",
    "title": "Key Takeaways",
    "order": 7,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>A phoneme is a mental representation of a sound, not necessarily a letter. Also, when we swap a phoneme we change the word.</li>\n  <li>A phone is the phonetic representation of a phoneme (the actual sound).</li>\n  <li>Allophones are different ways to pronounce the same phoneme while keeping the same meaning.</li>\n  <li>Sometimes allophones are predictable depending on their environment and who is speaking.</li>\n  <li>With Mono audio, both left and right audio channels get played back simultaneously via a single channel when playing audio (as opposed to Stereo audio where individual channels retain their presence and are played back separately).</li>\n</ul>",
    "contentMarkdown": "*   A phoneme is a mental representation of a sound, not necessarily a letter. Also, when we swap a phoneme we change the word.\n*   A phone is the phonetic representation of a phoneme (the actual sound).\n*   Allophones are different ways to pronounce the same phoneme while keeping the same meaning.\n*   Sometimes allophones are predictable depending on their environment and who is speaking.\n*   With Mono audio, both left and right audio channels get played back simultaneously via a single channel when playing audio (as opposed to Stereo audio where individual channels retain their presence and are played back separately).",
    "contentLength": 674,
    "wordCount": 102,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#key-takeaways"
  },
  {
    "id": "ai-speech-processing-oscillogram-8",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Feature Representations",
    "title": "Oscillogram",
    "order": 8,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>An oscillogram (also called a “time-domain waveform” or simply, a “waveform” in speech context) is a plot of amplitude vs. time. It is a record produced by an oscillograph or oscilloscope.</li>\n</ul>",
    "contentMarkdown": "*   An oscillogram (also called a “time-domain waveform” or simply, a “waveform” in speech context) is a plot of amplitude vs. time. It is a record produced by an oscillograph or oscilloscope.",
    "contentLength": 210,
    "wordCount": 32,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#oscillogram"
  },
  {
    "id": "ai-speech-processing-spectrum-9",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Feature Representations",
    "title": "Spectrum",
    "order": 9,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Taking the Fourier transform of a slice of a speech signal yields the spectrum/spectral vector for that slice. A sequence of these spectral vectors yields the plot of frequency vs. time as shown below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/Spectrum.jpg\" alt=\"\"></p>",
    "contentMarkdown": "*   Taking the Fourier transform of a slice of a speech signal yields the spectrum/spectral vector for that slice. A sequence of these spectral vectors yields the plot of frequency vs. time as shown below.\n\n![](/primers/ai/assets/speech/Spectrum.jpg)",
    "contentLength": 288,
    "wordCount": 36,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#spectrum"
  },
  {
    "id": "ai-speech-processing-spectrogram-10",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Feature Representations",
    "title": "Spectrogram",
    "order": 10,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Frequency vs. time representation of a speech signal is referred to as a spectrogram.</li>\n  <li>To obtain a spectrogram, first obtain the spectrum (image source: <a href=\"http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf\">Speech Technology - Kishore Prahallad</a>):</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/Spectrogram1.jpg\" alt=\"\"></p>\n<ul>\n  <li>Rotate it by 90 degrees:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/Spectrogram2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Color-code the amplitude:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/Spectrogram3.jpg\" alt=\"\"></p>\n<ul>\n  <li>Horizontally tiling the color-coded spectrums yields a spectrogram as shown below. A spectrogram is thus formally defined as a plot of frequency vs. time.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/Spectrogram5.jpg\" alt=\"\"></p>\n<ul>\n  <li>An example spectrogram from <a href=\"https://en.wikipedia.org/wiki/Spectrogram\">Wikipedia: Spectrogram</a> is as shown below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/spectrogram.png\" alt=\"\"></p>\n<ul>\n  <li>An example 3D spectrogram is as shown below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/spectrogram3d.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "*   Frequency vs. time representation of a speech signal is referred to as a spectrogram.\n*   To obtain a spectrogram, first obtain the spectrum (image source: [Speech Technology - Kishore Prahallad](http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf)):\n\n![](/primers/ai/assets/speech/Spectrogram1.jpg)\n\n*   Rotate it by 90 degrees:\n\n![](/primers/ai/assets/speech/Spectrogram2.jpg)\n\n*   Color-code the amplitude:\n\n![](/primers/ai/assets/speech/Spectrogram3.jpg)\n\n*   Horizontally tiling the color-coded spectrums yields a spectrogram as shown below. A spectrogram is thus formally defined as a plot of frequency vs. time.\n\n![](/primers/ai/assets/speech/Spectrogram5.jpg)\n\n*   An example spectrogram from [Wikipedia: Spectrogram](https://en.wikipedia.org/wiki/Spectrogram) is as shown below:\n\n![](/primers/ai/assets/speech/spectrogram.png)\n\n*   An example 3D spectrogram is as shown below:\n\n![](/primers/ai/assets/speech/spectrogram3d.jpeg)",
    "contentLength": 1195,
    "wordCount": 92,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#spectrogram"
  },
  {
    "id": "ai-speech-processing-why-spectrograms-11",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Feature Representations",
    "title": "Why Spectrograms?",
    "order": 11,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Dark regions indicate peaks (formants) in the spectrum:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/spec1.jpg\" alt=\"\"></p>\n<ul>\n  <li>Phones and their properties are better observed in spectrogram:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/spec2.jpg\" alt=\"\"></p>\n<ul>\n  <li>Sounds can be identified much better by the formants and their transitions. Hidden Markov Models implicitly model these spectrograms to perform speech recognition.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/spec3.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Key takeaways</strong>\n    <ul>\n      <li>A spectrogram is a time-frequency representation of the speech signal.</li>\n      <li>A spectrogram is a tool to study speech sounds (phones).</li>\n      <li>Phones and their properties are visually studied by phoneticians using a spectrogram.</li>\n      <li>Hidden Markov Models implicitly model spectrograms for speech to text systems.</li>\n      <li>Useful for evaluation of text to speech systems: A high quality text to speech system should produce synthesized speech whose spectrograms nearly match those with with spoken language.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>A spectrogram is a time-frequency representation of the speech signal.</li>\n      <li>A spectrogram is a tool to study speech sounds (phones).</li>\n      <li>Phones and their properties are visually studied by phoneticians using a spectrogram.</li>\n      <li>Hidden Markov Models implicitly model spectrograms for speech to text systems.</li>\n      <li>Useful for evaluation of text to speech systems: A high quality text to speech system should produce synthesized speech whose spectrograms nearly match those with with spoken language.</li>\n    </ul>",
    "contentMarkdown": "*   Dark regions indicate peaks (formants) in the spectrum:\n\n![](/primers/ai/assets/speech/spec1.jpg)\n\n*   Phones and their properties are better observed in spectrogram:\n\n![](/primers/ai/assets/speech/spec2.jpg)\n\n*   Sounds can be identified much better by the formants and their transitions. Hidden Markov Models implicitly model these spectrograms to perform speech recognition.\n\n![](/primers/ai/assets/speech/spec3.jpg)\n\n*   **Key takeaways**\n    *   A spectrogram is a time-frequency representation of the speech signal.\n    *   A spectrogram is a tool to study speech sounds (phones).\n    *   Phones and their properties are visually studied by phoneticians using a spectrogram.\n    *   Hidden Markov Models implicitly model spectrograms for speech to text systems.\n    *   Useful for evaluation of text to speech systems: A high quality text to speech system should produce synthesized speech whose spectrograms nearly match those with with spoken language.\n\n*   A spectrogram is a time-frequency representation of the speech signal.\n*   A spectrogram is a tool to study speech sounds (phones).\n*   Phones and their properties are visually studied by phoneticians using a spectrogram.\n*   Hidden Markov Models implicitly model spectrograms for speech to text systems.\n*   Useful for evaluation of text to speech systems: A high quality text to speech system should produce synthesized speech whose spectrograms nearly match those with with spoken language.",
    "contentLength": 1731,
    "wordCount": 201,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#why-spectrograms?"
  },
  {
    "id": "ai-speech-processing-mel-filterbanks-and-mfccs-12",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "What are “tokens” When Embedding an Audio Sample Using an Encoder? How are They Different Compared to Word/sub-word Tokens in NLP?",
    "title": "Mel-Filterbanks and MFCCs",
    "order": 12,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Empirical studies have shown that the human auditory system resolves frequencies non-linearly, and the non-linear resolution can be approximated using the Mel-scale which is given by</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>M</mi><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>1127.01048</mn><mo>&amp;#x2219;</mo><msub><mi>log</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>e</mi></mrow></msub><mo>&amp;#x2061;</mo><mi>f</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-188\" style=\"width: 13.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.15em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-193\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1127.01048</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∙</span><span class=\"msubsup\" id=\"MathJax-Span-197\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">log</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 1.305em;\"><span class=\"texatom\" id=\"MathJax-Span-199\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-202\"></span><span class=\"mi\" id=\"MathJax-Span-203\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>M</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>1127.01048</mn><mo>∙</mo><msub><mi>log</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>e</mi></mrow></msub><mo>⁡</mo><mi>f</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-14\">M(f)=1127.01048 \\bullet \\log _{e} f</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-204\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-205\"><span class=\"mi\" id=\"MathJax-Span-206\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">f</script> is a frequency (Volkman, Stevens, &amp; Newman, 1937).</li>\n    </ul>\n  </li>\n  <li>This indicates that the human auditory system is more sensitive to frequency difference in lower frequency band than in higher frequency band.</li>\n  <li>The figure below illustrates the process of extracting Mel-frequency cepstrum coefficients (MFCCs) with triangular filters that are equally-spaced in Mel-scale. In the linear scale, note that as the frequency increases, the width of the filters increases.</li>\n</ul>\n<p>Empirical studies have shown that the human auditory system resolves frequencies non-linearly, and the non-linear resolution can be approximated using the Mel-scale which is given by</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-204\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-205\"><span class=\"mi\" id=\"MathJax-Span-206\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">f</script> is a frequency (Volkman, Stevens, &amp; Newman, 1937).</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/speech/mfcc.jpg\" alt=\"\"></p>\n<ul>\n  <li>An input speech is transformed using the Discrete Fourier Transform, and the filter-bank energies (also called the Mel filter-bank energies or Mel spectrogram) are computed using triangular filters mentioned above. The log-values of the filter-bank energies (also called the log-Mel filterbanks or log-Mel spectrogram) are then decorrelated using the discrete cosine transform (DCT). Finally,  M-dimensional MFCCs are extracted by taking M-DCT coefficients, and passed on as speech features for downstream processing.</li>\n  <li>However, deep learning models are able to exploit spectro-temporal correlations, enabling the use of the log-Mel spectrogram (instead of MFCCs) as equivalent or better options, especially as seen with tasks that involve automatic speech recognition (ASR) and keyword spotting (KWS). As a result, a good number of ASR and KWS papers utilize log-Mel or Mel filterbank speech features with temporal context.</li>\n  <li>Research has reported that using MFCCs is beneficial for both speaker and speech recognition. While MFCCs are still commonly fed as input speech features to neural nets. However, newer neural net architectures typically rely on the log-Mel filterbank energies (LMFBE) (which can be used to generate MFCCs after DCT-based decorrelation as indicated above).</li>\n</ul>",
    "contentMarkdown": "*   Empirical studies have shown that the human auditory system resolves frequencies non-linearly, and the non-linear resolution can be approximated using the Mel-scale which is given by\n    \n    M(f)\\=1127.01048∙logefM(f)\\=1127.01048∙loge⁡f\n    \n    M(f)=1127.01048 \\\\bullet \\\\log \\_{e} f\n    *   where fff is a frequency (Volkman, Stevens, & Newman, 1937).\n*   This indicates that the human auditory system is more sensitive to frequency difference in lower frequency band than in higher frequency band.\n*   The figure below illustrates the process of extracting Mel-frequency cepstrum coefficients (MFCCs) with triangular filters that are equally-spaced in Mel-scale. In the linear scale, note that as the frequency increases, the width of the filters increases.\n\nEmpirical studies have shown that the human auditory system resolves frequencies non-linearly, and the non-linear resolution can be approximated using the Mel-scale which is given by\n\n*   where fff is a frequency (Volkman, Stevens, & Newman, 1937).\n\n![](/primers/ai/assets/speech/mfcc.jpg)\n\n*   An input speech is transformed using the Discrete Fourier Transform, and the filter-bank energies (also called the Mel filter-bank energies or Mel spectrogram) are computed using triangular filters mentioned above. The log-values of the filter-bank energies (also called the log-Mel filterbanks or log-Mel spectrogram) are then decorrelated using the discrete cosine transform (DCT). Finally, M-dimensional MFCCs are extracted by taking M-DCT coefficients, and passed on as speech features for downstream processing.\n*   However, deep learning models are able to exploit spectro-temporal correlations, enabling the use of the log-Mel spectrogram (instead of MFCCs) as equivalent or better options, especially as seen with tasks that involve automatic speech recognition (ASR) and keyword spotting (KWS). As a result, a good number of ASR and KWS papers utilize log-Mel or Mel filterbank speech features with temporal context.\n*   Research has reported that using MFCCs is beneficial for both speaker and speech recognition. While MFCCs are still commonly fed as input speech features to neural nets. However, newer neural net architectures typically rely on the log-Mel filterbank energies (LMFBE) (which can be used to generate MFCCs after DCT-based decorrelation as indicated above).",
    "contentLength": 8616,
    "wordCount": 330,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#mel-filterbanks-and-mfccs"
  },
  {
    "id": "ai-speech-processing-example-spectrogram-and-oscillogram-13",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "What are “tokens” When Embedding an Audio Sample Using an Encoder? How are They Different Compared to Word/sub-word Tokens in NLP?",
    "title": "Example Spectrogram and Oscillogram",
    "order": 13,
    "orderInChapter": 2,
    "contentHtml": "<p><img src=\"/primers/ai/assets/speech/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png\" alt=\"\"></p>\n<ul>\n  <li>This is an oscillogram and spectrogram of the boatwhistle call of the toadfish Sanopus astrifer. The upper blue-colored plot is an oscillogram presenting the waveform and amplitude of the sound over time, X-axis is Time (sec) and the Y-axis is Amplitude. The lower figure is a plot of the sounds’ frequency over time, X-axis is Time (sec) and the Y-axis is Frequency (kHz). The amount of energy present in each frequency is represented by the intensity of the color. The brighter the color, the more energy is present in the sound at that frequency.&nbsp;</li>\n</ul>",
    "contentMarkdown": "![](/primers/ai/assets/speech/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png)\n\n*   This is an oscillogram and spectrogram of the boatwhistle call of the toadfish Sanopus astrifer. The upper blue-colored plot is an oscillogram presenting the waveform and amplitude of the sound over time, X-axis is Time (sec) and the Y-axis is Amplitude. The lower figure is a plot of the sounds’ frequency over time, X-axis is Time (sec) and the Y-axis is Frequency (kHz). The amount of energy present in each frequency is represented by the intensity of the color. The brighter the color, the more energy is present in the sound at that frequency.",
    "contentLength": 721,
    "wordCount": 96,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#example-spectrogram-and-oscillogram"
  },
  {
    "id": "ai-speech-processing-perceptual-linear-prediction-plp-14",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "What are “tokens” When Embedding an Audio Sample Using an Encoder? How are They Different Compared to Word/sub-word Tokens in NLP?",
    "title": "Perceptual Linear Prediction (PLP)",
    "order": 14,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Similar from MFCCs, perceptual linear prediction (PLP) features can also be fed into neural nets as input. PLP is a combination of spectral analysis and linear prediction analysis. It uses concepts from the psychophysics of hearing to compute a simple auditory spectrum.</li>\n  <li>Read more in <a href=\"https://pubmed.ncbi.nlm.nih.gov/2341679/\">H. Hermansky (1990)</a>.</li>\n</ul>",
    "contentMarkdown": "*   Similar from MFCCs, perceptual linear prediction (PLP) features can also be fed into neural nets as input. PLP is a combination of spectral analysis and linear prediction analysis. It uses concepts from the psychophysics of hearing to compute a simple auditory spectrum.\n*   Read more in [H. Hermansky (1990)](https://pubmed.ncbi.nlm.nih.gov/2341679/).",
    "contentLength": 392,
    "wordCount": 50,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#perceptual-linear-prediction-(plp)"
  },
  {
    "id": "ai-speech-processing-prosodic-features-15",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "What are “tokens” When Embedding an Audio Sample Using an Encoder? How are They Different Compared to Word/sub-word Tokens in NLP?",
    "title": "Prosodic Features",
    "order": 15,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Prosodic features include pitch and its dynamic variations, inter-pause statistics, phone duration, etc. (Shriberg, 2007)</li>\n  <li>Very often, prosodic features are extracted with larger frame size than acoustical features since prosodic features exist over a long speech segment such as syllables. The pitch and energy-contours change slowly compared to the spectrum, which implies that the variation can be captured over a long speech segment. Many pieces of literature have reported that prosodic features usually do not outperform acoustical features but incorporating prosodic features in addition to acoustical features can improve speaker recognition performance (Shriberg, 2007; Sönmez, Shriberg, Heck, &amp; Weintraub, 1998; Peskin et al., 2003; Campbell, Reynolds, &amp; Dunn, 2003; Reynolds et al., 2003).</li>\n</ul>",
    "contentMarkdown": "*   Prosodic features include pitch and its dynamic variations, inter-pause statistics, phone duration, etc. (Shriberg, 2007)\n*   Very often, prosodic features are extracted with larger frame size than acoustical features since prosodic features exist over a long speech segment such as syllables. The pitch and energy-contours change slowly compared to the spectrum, which implies that the variation can be captured over a long speech segment. Many pieces of literature have reported that prosodic features usually do not outperform acoustical features but incorporating prosodic features in addition to acoustical features can improve speaker recognition performance (Shriberg, 2007; Sönmez, Shriberg, Heck, & Weintraub, 1998; Peskin et al., 2003; Campbell, Reynolds, & Dunn, 2003; Reynolds et al., 2003).",
    "contentLength": 840,
    "wordCount": 115,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#prosodic-features"
  },
  {
    "id": "ai-speech-processing-idiolectal-features-16",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "What are “tokens” When Embedding an Audio Sample Using an Encoder? How are They Different Compared to Word/sub-word Tokens in NLP?",
    "title": "Idiolectal Features",
    "order": 16,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>The idiolectal features are motivated by the fact that people usually use idiolectal information to recognize speakers.</li>\n  <li>In telephone conversation corpus, Doddington (2001) reported enrolled speakers can be verified not using acoustic features that are extracted from a speech signal but using idiolectal features that are observed in true-underlying transcription of speech. The phonetic speaker verification, motivated by Doddington (2001)’s work, creates a speaker using his/her phone n-gram probabilities that are obtained using multiple-language speech recognizers (Andrews, Kohler, &amp; Campbell, 2001).</li>\n</ul>",
    "contentMarkdown": "*   The idiolectal features are motivated by the fact that people usually use idiolectal information to recognize speakers.\n*   In telephone conversation corpus, Doddington (2001) reported enrolled speakers can be verified not using acoustic features that are extracted from a speech signal but using idiolectal features that are observed in true-underlying transcription of speech. The phonetic speaker verification, motivated by Doddington (2001)’s work, creates a speaker using his/her phone n-gram probabilities that are obtained using multiple-language speech recognizers (Andrews, Kohler, & Campbell, 2001).",
    "contentLength": 642,
    "wordCount": 83,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#idiolectal-features"
  },
  {
    "id": "ai-speech-processing-architectural-overview-17",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Architectural Overview",
    "order": 17,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>A top-level overview of Siri architecture from <a href=\"https://www.youtube.com/watch?v=RBgfLvAOrss\">Deep Learning in Speech Recognition by Alex Acero, Apple Inc.</a> is shown below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/siritechstack.jpg\" alt=\"\"></p>\n<ul>\n  <li>Note that this is an oversimplification of a complex system. Usually, the input data (which is pre-processed into speech features such as Mel-Filterbanks or MFCCs) gets fed as an input to the keyword spotting module which further gates two modules: (i) speaker recognition module, and (ii) automatic speech recognition (ASR) module. The encoder part of the ASR module can run in parallel with a language identification (LID) module which can select the language model correspond to the detected language which feeds the ASR decoder. The ASR output further feeds the natural language understanding (NLU) block which performs intent classification and slot filling. Dialog acts are generated next. Finally, the text-to-speech synthesis (TTS) block yields the end result: a response from the voice assistant. Note that the detected language also helps select which NLU and TTS block to use.</li>\n  <li>Also note that the first phase of the above flow, keyword spotting, can be gated by a voice activity detector (VAD) which is usually a simple neural network with a couple of layers whose job is to just figure out if there’s speech or not in the audio it is listening to. This is usually an always-on on-device block. This helps save power by not having a more complex model like the keyword spotter run as an always-on system.</li>\n</ul>",
    "contentMarkdown": "*   A top-level overview of Siri architecture from [Deep Learning in Speech Recognition by Alex Acero, Apple Inc.](https://www.youtube.com/watch?v=RBgfLvAOrss) is shown below.\n\n![](/primers/ai/assets/speech/siritechstack.jpg)\n\n*   Note that this is an oversimplification of a complex system. Usually, the input data (which is pre-processed into speech features such as Mel-Filterbanks or MFCCs) gets fed as an input to the keyword spotting module which further gates two modules: (i) speaker recognition module, and (ii) automatic speech recognition (ASR) module. The encoder part of the ASR module can run in parallel with a language identification (LID) module which can select the language model correspond to the detected language which feeds the ASR decoder. The ASR output further feeds the natural language understanding (NLU) block which performs intent classification and slot filling. Dialog acts are generated next. Finally, the text-to-speech synthesis (TTS) block yields the end result: a response from the voice assistant. Note that the detected language also helps select which NLU and TTS block to use.\n*   Also note that the first phase of the above flow, keyword spotting, can be gated by a voice activity detector (VAD) which is usually a simple neural network with a couple of layers whose job is to just figure out if there’s speech or not in the audio it is listening to. This is usually an always-on on-device block. This helps save power by not having a more complex model like the keyword spotter run as an always-on system.",
    "contentLength": 1622,
    "wordCount": 241,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#architectural-overview"
  },
  {
    "id": "ai-speech-processing-fundamental-speech-tasks-18",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Fundamental Speech Tasks",
    "order": 18,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>There four fundamental speech tasks, as applied to digital voice assistants:\n    <ul>\n      <li><strong>Wake word detection/keyword spotting (KWS):</strong> On the device, detect the wakeword/trigger keyword to get the device’s attention;</li>\n      <li><strong>Automatic speech recognition (ASR):</strong> Upon detecting the wake word, convert audio streamed to the cloud into words;</li>\n      <li><strong>Natural-language understanding (NLU):</strong> Extract the meaning of the recognized words so that the assistant can take the appropriate action in response to the customer’s request; and</li>\n      <li><strong>Text-to-speech synthesis (TTS):</strong> Convert the assistant’s textual response to the customer’s request into spoken audio.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Wake word detection/keyword spotting (KWS):</strong> On the device, detect the wakeword/trigger keyword to get the device’s attention;</li>\n      <li><strong>Automatic speech recognition (ASR):</strong> Upon detecting the wake word, convert audio streamed to the cloud into words;</li>\n      <li><strong>Natural-language understanding (NLU):</strong> Extract the meaning of the recognized words so that the assistant can take the appropriate action in response to the customer’s request; and</li>\n      <li><strong>Text-to-speech synthesis (TTS):</strong> Convert the assistant’s textual response to the customer’s request into spoken audio.</li>\n    </ul>",
    "contentMarkdown": "*   There four fundamental speech tasks, as applied to digital voice assistants:\n    *   **Wake word detection/keyword spotting (KWS):** On the device, detect the wakeword/trigger keyword to get the device’s attention;\n    *   **Automatic speech recognition (ASR):** Upon detecting the wake word, convert audio streamed to the cloud into words;\n    *   **Natural-language understanding (NLU):** Extract the meaning of the recognized words so that the assistant can take the appropriate action in response to the customer’s request; and\n    *   **Text-to-speech synthesis (TTS):** Convert the assistant’s textual response to the customer’s request into spoken audio.\n\n*   **Wake word detection/keyword spotting (KWS):** On the device, detect the wakeword/trigger keyword to get the device’s attention;\n*   **Automatic speech recognition (ASR):** Upon detecting the wake word, convert audio streamed to the cloud into words;\n*   **Natural-language understanding (NLU):** Extract the meaning of the recognized words so that the assistant can take the appropriate action in response to the customer’s request; and\n*   **Text-to-speech synthesis (TTS):** Convert the assistant’s textual response to the customer’s request into spoken audio.",
    "contentLength": 1465,
    "wordCount": 170,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#fundamental-speech-tasks"
  },
  {
    "id": "ai-speech-processing-hierarchy-of-phones-words-and-sentences-19",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Hierarchy of Phones, Words, and Sentences",
    "order": 19,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>The input audio (typically stored as a wave-file, but could be another audio format as well, such as FLACC, OGG, etc.) is converted to a feature sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mo\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-210\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mn\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-216\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">(f_1, \\ldots, f_T)</script>. The goal is to find a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>w</mi><mi>N</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-220\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.11em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-221\"><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-223\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mn\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-229\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">(w_1, \\ldots, w_N)</script> that matches the feature sequence best.</p>\n  </li>\n  <li>\n    <p>An HMM is used to model basic units (phones) in speech. To bridge the gap between phone-level HMMs and word decoding, let’s look at the hierarchical structure of HMM states, phones, words and sentences.</p>\n    <ul>\n      <li><strong>A phone is usually modeled by a HMM with three states.</strong> The three states correspond to three subphones, which are <strong>transition-in, steady-state, transition-out regions of the phone</strong>.</li>\n      <li><strong>A word is consisted by a sequence of phones.</strong> For example, “one” is composed of three phones: <code class=\"language-plaintext highlighter-rouge\">w</code>, <code class=\"language-plaintext highlighter-rouge\">ah</code>, and <code class=\"language-plaintext highlighter-rouge\">n</code>. The word model is just the sequential concatenation of the phone models.</li>\n      <li><strong>Lexicon contains information about phone sequence of words.</strong> There is a dictionary (lexicon) which includes phone sequence for each of word. Therefore once all phones are represented by pre-trained HMM models, the word models become available by searching in the dictionary and concatenating phone level models belonging to that word. Compared with fitting a HMM for individual word, this strategy greatly reduced the complexity.</li>\n      <li><strong>A sentence is a grammatically valid sequence of words.</strong> Words do not randomly connect to form a sentence. The transition between adjacent words can be estimated from a large text corpus.</li>\n    </ul>\n  </li>\n</ul>\n<p>The input audio (typically stored as a wave-file, but could be another audio format as well, such as FLACC, OGG, etc.) is converted to a feature sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 5.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.27em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mo\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-210\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mn\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-216\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">(f_1, \\ldots, f_T)</script>. The goal is to find a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>w</mi><mi>N</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-220\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.11em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-221\"><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-223\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mn\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-229\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">(w_1, \\ldots, w_N)</script> that matches the feature sequence best.</p>\n<p>An HMM is used to model basic units (phones) in speech. To bridge the gap between phone-level HMMs and word decoding, let’s look at the hierarchical structure of HMM states, phones, words and sentences.</p>\n<ul>\n      <li><strong>A phone is usually modeled by a HMM with three states.</strong> The three states correspond to three subphones, which are <strong>transition-in, steady-state, transition-out regions of the phone</strong>.</li>\n      <li><strong>A word is consisted by a sequence of phones.</strong> For example, “one” is composed of three phones: <code class=\"language-plaintext highlighter-rouge\">w</code>, <code class=\"language-plaintext highlighter-rouge\">ah</code>, and <code class=\"language-plaintext highlighter-rouge\">n</code>. The word model is just the sequential concatenation of the phone models.</li>\n      <li><strong>Lexicon contains information about phone sequence of words.</strong> There is a dictionary (lexicon) which includes phone sequence for each of word. Therefore once all phones are represented by pre-trained HMM models, the word models become available by searching in the dictionary and concatenating phone level models belonging to that word. Compared with fitting a HMM for individual word, this strategy greatly reduced the complexity.</li>\n      <li><strong>A sentence is a grammatically valid sequence of words.</strong> Words do not randomly connect to form a sentence. The transition between adjacent words can be estimated from a large text corpus.</li>\n    </ul>",
    "contentMarkdown": "*   The input audio (typically stored as a wave-file, but could be another audio format as well, such as FLACC, OGG, etc.) is converted to a feature sequence (f1,…,fT)(f1,…,fT)(f\\_1, \\\\ldots, f\\_T). The goal is to find a sequence of words (w1,…,wN)(w1,…,wN)(w\\_1, \\\\ldots, w\\_N) that matches the feature sequence best.\n    \n*   An HMM is used to model basic units (phones) in speech. To bridge the gap between phone-level HMMs and word decoding, let’s look at the hierarchical structure of HMM states, phones, words and sentences.\n    \n    *   **A phone is usually modeled by a HMM with three states.** The three states correspond to three subphones, which are **transition-in, steady-state, transition-out regions of the phone**.\n    *   **A word is consisted by a sequence of phones.** For example, “one” is composed of three phones: `w`, `ah`, and `n`. The word model is just the sequential concatenation of the phone models.\n    *   **Lexicon contains information about phone sequence of words.** There is a dictionary (lexicon) which includes phone sequence for each of word. Therefore once all phones are represented by pre-trained HMM models, the word models become available by searching in the dictionary and concatenating phone level models belonging to that word. Compared with fitting a HMM for individual word, this strategy greatly reduced the complexity.\n    *   **A sentence is a grammatically valid sequence of words.** Words do not randomly connect to form a sentence. The transition between adjacent words can be estimated from a large text corpus.\n\nThe input audio (typically stored as a wave-file, but could be another audio format as well, such as FLACC, OGG, etc.) is converted to a feature sequence (f1,…,fT)(f1,…,fT)(f\\_1, \\\\ldots, f\\_T). The goal is to find a sequence of words (w1,…,wN)(w1,…,wN)(w\\_1, \\\\ldots, w\\_N) that matches the feature sequence best.\n\nAn HMM is used to model basic units (phones) in speech. To bridge the gap between phone-level HMMs and word decoding, let’s look at the hierarchical structure of HMM states, phones, words and sentences.\n\n*   **A phone is usually modeled by a HMM with three states.** The three states correspond to three subphones, which are **transition-in, steady-state, transition-out regions of the phone**.\n*   **A word is consisted by a sequence of phones.** For example, “one” is composed of three phones: `w`, `ah`, and `n`. The word model is just the sequential concatenation of the phone models.\n*   **Lexicon contains information about phone sequence of words.** There is a dictionary (lexicon) which includes phone sequence for each of word. Therefore once all phones are represented by pre-trained HMM models, the word models become available by searching in the dictionary and concatenating phone level models belonging to that word. Compared with fitting a HMM for individual word, this strategy greatly reduced the complexity.\n*   **A sentence is a grammatically valid sequence of words.** Words do not randomly connect to form a sentence. The transition between adjacent words can be estimated from a large text corpus.",
    "contentLength": 17460,
    "wordCount": 484,
    "hasCode": true,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#hierarchy-of-phones,-words,-and-sentences"
  },
  {
    "id": "ai-speech-processing-automatic-speech-recognition-20",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Automatic Speech Recognition",
    "order": 20,
    "orderInChapter": 4,
    "contentHtml": "<h4 id=\"what-is-automatic-speech-recognition\">What is Automatic Speech Recognition?</h4>\n<ul>\n  <li>Research in ASR (Automatic Speech Recognition) aims to enable computers to “understand” human speech and convert it into text. ASR is the next frontier in intelligent human-machine interaction and also a precondition for perfecting machine translation and natural language understanding. Research into ASR can be traced back to the 1950s in its initial isolated word speech recognition system. Since then, with persistent efforts of numerous scholars, ASR has made significant progress and can now power large-vocabulary continuous speech recognition systems.</li>\n  <li>Especially in the emerging era of big data and application of deep neural networks, ASR systems have achieved notable performance improvements. ASR technology has also been gradually gaining practical use, becoming more product-oriented. Smart speech recognition software and applications based on ASR are increasingly entering our daily lives, in form of voice input methods, intelligent voice assistants, and interactive voice recognition systems for vehicles.</li>\n</ul>\n<h4 id=\"framework-of-an-asr-system\">Framework of an ASR System</h4>\n<ul>\n  <li>\n    <p>The purpose of ASR is to map input waveform sequences to their corresponding word or character sequences. Therefore, implementing ASR can be considered a channel decoding or pattern classification problem. Statistical modeling is a core ASR method, in which, for a given speech waveform sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-233\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">O</script>, we can use a “maximum a posteriori” (MAP) estimator, based on the mode of a posterior Bayesian distribution, to estimate the most likely output sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo>&amp;#x2217;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 1.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1001.46em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Regular;\">∗</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo>∗</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">W*</script>, with the formula shown in the figure below.</p>\n\n    <p><img src=\"/primers/ai/assets/speech/asr_math.png\" alt=\"\"></p>\n\n    <ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>O</mi><mo>&amp;#x2223;</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-240\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.75em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-241\"><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>O</mi><mo>∣</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">P(O \\mid W)</script> is the probability of generating the correct observation sequence, i.e. corresponding to the acoustic model (AM) of the ASR system, conditional on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">W</script>. Likelihood <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.19em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">P(W)</script> is the ‘a priori probability’ of the exact sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">W</script> occurring. It is called the language model (LM).</li>\n    </ul>\n  </li>\n  <li>\n    <p>The figure below shows the structure diagram of a marked ASR system, which mainly comprises a front-end processing module, acoustic model, language model, and decoder. The decoding process is primarily to use the trained acoustic model and language model to obtain the optimal output sequence.</p>\n  </li>\n</ul>\n<p>The purpose of ASR is to map input waveform sequences to their corresponding word or character sequences. Therefore, implementing ASR can be considered a channel decoding or pattern classification problem. Statistical modeling is a core ASR method, in which, for a given speech waveform sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-233\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">O</script>, we can use a “maximum a posteriori” (MAP) estimator, based on the mode of a posterior Bayesian distribution, to estimate the most likely output sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo>&amp;#x2217;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 1.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1001.46em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Regular;\">∗</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo>∗</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">W*</script>, with the formula shown in the figure below.</p>\n<p><img src=\"/primers/ai/assets/speech/asr_math.png\" alt=\"\"></p>\n<ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>O</mi><mo>&amp;#x2223;</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-240\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.75em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-241\"><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>O</mi><mo>∣</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">P(O \\mid W)</script> is the probability of generating the correct observation sequence, i.e. corresponding to the acoustic model (AM) of the ASR system, conditional on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-248\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">W</script>. Likelihood <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.19em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-256\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">P(W)</script> is the ‘a priori probability’ of the exact sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">W</script> occurring. It is called the language model (LM).</li>\n    </ul>\n<p>The figure below shows the structure diagram of a marked ASR system, which mainly comprises a front-end processing module, acoustic model, language model, and decoder. The decoding process is primarily to use the trained acoustic model and language model to obtain the optimal output sequence.</p>\n<p><img src=\"/primers/ai/assets/speech/asrsys.png\" alt=\"\"></p>\n<ul>\n  <li>A version of the ASR system (with components such as pronunciation models, language models for re-scoring) from <a href=\"https://www.youtube.com/watch?v=3MjIkWxXigM\">End-to-End Models for Speech Processing at Stanford by Navdeep Jaitly, NVIDIA</a> is as below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/asr_block.jpg\" alt=\"\"></p>\n<h4 id=\"acoustic-model-encoder\">Acoustic Model (Encoder)</h4>\n<ul>\n  <li>An acoustic model’s task is to compute <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>O</mi><mo>&amp;#x2223;</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-260\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.75em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"mi\" id=\"MathJax-Span-262\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-265\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>O</mi><mo>∣</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">P(O \\mid W)</script>, i.e. the probability of generating a speech waveform for the mode. An acoustic model, as an important part of the ASR system, accounts for a large part of the computational overhead and also determines the system’s performance. GMM-HMM-based acoustic models are widely used in traditional speech recognition systems.</li>\n  <li>In this model, GMM is used to model the distribution of the acoustic characteristics of speech and HMM is used to model the time sequence of speech signals. Since the rise of deep learning in 2006, deep neural networks (DNNs) have been applied in speech acoustic models. In <a href=\"https://www.cs.utoronto.ca/~gdahl/papers/dbnPhoneRec.pdf\">Mohamed et al. (2009)</a>, Hinton and his students used feedforward fully-connected deep neural networks in speech recognition acoustic modeling.</li>\n  <li>Typical pre-processing involves converting the audio to mono (similar to how computer vision models flatten the input image across the three channels, viz., R, G, B) and resampling it to 16kHz before it is input to the model.</li>\n</ul>\n<h4 id=\"encoder-decoder-architectures-past-vs-present\">Encoder-Decoder Architectures: Past vs. Present</h4>\n<ul>\n  <li>Traditional speech recognition systems adopt a GMM-HMM architecture where the GMM is the acoustic model that computes the state/observation probabilities and the HMM decoder combines these probabilities using dynamic programming (DP). With the advent of deep learning, the role played by the GMM as the acoustic model is now replaced by a DNN. Rather than the GMM modeling the observation probabilities, a DNN is trained to output them. The HMM still acts as a decoder. The figure below illustrates both approaches side-by-side.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/gmmhmm_dnnhmm.png\" alt=\"\"></p>\n<ul>\n  <li>The DNN computes the observation probabilities and outputs a probability distribution over as many classes as the HMM states for each speech frame using a softmax layer. The number of HMM states depend of the number of phones, with a typical setup of 3 target labels for each phone for the beginning, middle and end of the segment, 1 state for silence, and 1 state for background.  The DNN is typically trained to minimize the average cross-entropy loss over all frames between the predicted and the ground-truth distributions. The HMM decoder computes the word detection score using the observation, the state transition, and the prior probabilities. An architectural overview of a DNN-HMM is shown in the diagram below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/dnnhmm.jpg\" alt=\"\"></p>\n<ul>\n  <li>Compared to traditional GMM-HMM acoustic models, DNN-HMM-based acoustic models perform better in terms of TIMIT database. When compared with GMM, DNN is advantageous in the following ways:\n    <ul>\n      <li>De-distribution hypothesis is not required for characteristic distribution when DNN models the posterior probability of the acoustic characteristics of speech.</li>\n      <li>GMM requires de-correlation processing for input characteristics, but DNN is capable of using various forms of input characteristics.</li>\n      <li>GMM can only use single-frame speech as inputs, but DNN is capable of capturing valid context information by means of splicing adjoining frames.</li>\n    </ul>\n  </li>\n  <li>Once the HMM model is built, to figure out the maximum a posteriori probability estimate of the most likely sequence of hidden states, i.e., the Viterbi path, a graph-traversal algorithm like the <a href=\"https://en.wikipedia.org/wiki/Viterbi_algorithm\">Viterbi decoder algorithm</a> (typically implemented using the concept of dynamic programming in algorithms) can be applied.</li>\n  <li>Given that speech comprises of sequential data, RNNs (and it’s variants, GRUs and LSTMs) are natural choices for DNNs. However, for tasks such as speech recognition, where the alignment between the inputs and the labels is unknown, RNNs have so far been limited to an auxiliary role. The problem is that the standard training methods require a separate target for every input, which is usually not available. The traditional solution — the so-called hybrid approach — is to use Hidden Markov Models (HMMs) to generate targets for the RNN, then invert the RNN outputs to provide observation probabilities <a href=\"https://www.amazon.com/Connectionist-Speech-Recognition-International-Engineering/dp/0792393961\">(Bourlard and Morgan, 1994)</a>. However the hybrid approach does not exploit the full potential of RNNs for sequence processing, and it also leads to an awkward combination of discriminative and generative training.</li>\n  <li>The connectionist temporal classification (CTC) output layer <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\">(Graves et al., 2006)</a> removes the need for HMMs for providing alignment altogether by directly training RNNs to label sequences with unknown alignments, using a single discriminative loss function. CTC can also be combined with probabilistic language models for word-level speech and handwriting recognition. Note that the HMM-based decoder is still a good idea for cases where the task at hand involves a limited vocabulary and as a result, a smaller set of pronunciations, such as keyword spotting (vs. speech recognition).</li>\n  <li>Using the CTC loss enables all-neural encoder-decoder seq2seq architectures that utilize an end-to-end neural architecture which generates phonemes at the output of the encoder as an intermediate step, which are consumed by a decoder which utilizes a language model and pronunciation model to generate transcriptions. The language model helps with secondary aspects of ASR such as punctuation, capitalization, and suggesting the right spellings based on the best word match.</li>\n  <li>Recent models in this area such as <a href=\"https://arxiv.org/abs/1508.01211\">Listen Attend Spell (LAS)</a> forgo the intermediate phoneme labels altogether and train an end-to-end architecture that directly emits transcriptions at its output.</li>\n  <li>For more in this area, Alex Graves’s book on <a href=\"https://www.cs.toronto.edu/~graves/preprint.pdf\">Supervised Sequence Labelling with Recurrent Neural Networks</a> is a great reference.</li>\n</ul>\n<ul>\n      <li>De-distribution hypothesis is not required for characteristic distribution when DNN models the posterior probability of the acoustic characteristics of speech.</li>\n      <li>GMM requires de-correlation processing for input characteristics, but DNN is capable of using various forms of input characteristics.</li>\n      <li>GMM can only use single-frame speech as inputs, but DNN is capable of capturing valid context information by means of splicing adjoining frames.</li>\n    </ul>\n<h4 id=\"putting-it-all-together\">Putting It All Together</h4>\n<ul>\n  <li>Let’s take an example of a simple digit recognition task. The words are the digits from one to nine. THe following hierarchical graph from <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Speech and Language Processing by Jurafsky and Martin, 2008</a> shows the hierarchical transitions of this task. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>o</mi><mi>n</mi><mi>e</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>t</mi><mi>w</mi><mi>o</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 5.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.482em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.43em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mo\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-277\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>o</mi><mi>n</mi><mi>e</mi><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>t</mi><mi>w</mi><mi>o</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">P(one \\| two)</script> represents the transition probability from digit two to one.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/phone-word-sentence-hierarchy.png\" alt=\"\"></p>\n<ul>\n  <li>Given this hierarchical transition matrix, a Viterbi trellis decoding method is used. The following figure from <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Speech and Language Processing by Jurafsky and Martin, 2008</a> shows the scheme of this decoding process. The words (digits) are stacked vertically and the feature sequence is shown horizontally.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/viterbi_trellis.png\" alt=\"\"></p>",
    "contentMarkdown": "#### What is Automatic Speech Recognition?\n\n*   Research in ASR (Automatic Speech Recognition) aims to enable computers to “understand” human speech and convert it into text. ASR is the next frontier in intelligent human-machine interaction and also a precondition for perfecting machine translation and natural language understanding. Research into ASR can be traced back to the 1950s in its initial isolated word speech recognition system. Since then, with persistent efforts of numerous scholars, ASR has made significant progress and can now power large-vocabulary continuous speech recognition systems.\n*   Especially in the emerging era of big data and application of deep neural networks, ASR systems have achieved notable performance improvements. ASR technology has also been gradually gaining practical use, becoming more product-oriented. Smart speech recognition software and applications based on ASR are increasingly entering our daily lives, in form of voice input methods, intelligent voice assistants, and interactive voice recognition systems for vehicles.\n\n#### Framework of an ASR System\n\n*   The purpose of ASR is to map input waveform sequences to their corresponding word or character sequences. Therefore, implementing ASR can be considered a channel decoding or pattern classification problem. Statistical modeling is a core ASR method, in which, for a given speech waveform sequence OOO, we can use a “maximum a posteriori” (MAP) estimator, based on the mode of a posterior Bayesian distribution, to estimate the most likely output sequence W∗W∗W\\*, with the formula shown in the figure below.\n    \n    ![](/primers/ai/assets/speech/asr_math.png)\n    \n    *   where, P(O∣W)P(O∣W)P(O \\\\mid W) is the probability of generating the correct observation sequence, i.e. corresponding to the acoustic model (AM) of the ASR system, conditional on WWW. Likelihood P(W)P(W)P(W) is the ‘a priori probability’ of the exact sequence WWW occurring. It is called the language model (LM).\n*   The figure below shows the structure diagram of a marked ASR system, which mainly comprises a front-end processing module, acoustic model, language model, and decoder. The decoding process is primarily to use the trained acoustic model and language model to obtain the optimal output sequence.\n    \n\nThe purpose of ASR is to map input waveform sequences to their corresponding word or character sequences. Therefore, implementing ASR can be considered a channel decoding or pattern classification problem. Statistical modeling is a core ASR method, in which, for a given speech waveform sequence OOO, we can use a “maximum a posteriori” (MAP) estimator, based on the mode of a posterior Bayesian distribution, to estimate the most likely output sequence W∗W∗W\\*, with the formula shown in the figure below.\n\n![](/primers/ai/assets/speech/asr_math.png)\n\n*   where, P(O∣W)P(O∣W)P(O \\\\mid W) is the probability of generating the correct observation sequence, i.e. corresponding to the acoustic model (AM) of the ASR system, conditional on WWW. Likelihood P(W)P(W)P(W) is the ‘a priori probability’ of the exact sequence WWW occurring. It is called the language model (LM).\n\nThe figure below shows the structure diagram of a marked ASR system, which mainly comprises a front-end processing module, acoustic model, language model, and decoder. The decoding process is primarily to use the trained acoustic model and language model to obtain the optimal output sequence.\n\n![](/primers/ai/assets/speech/asrsys.png)\n\n*   A version of the ASR system (with components such as pronunciation models, language models for re-scoring) from [End-to-End Models for Speech Processing at Stanford by Navdeep Jaitly, NVIDIA](https://www.youtube.com/watch?v=3MjIkWxXigM) is as below:\n\n![](/primers/ai/assets/speech/asr_block.jpg)\n\n#### Acoustic Model (Encoder)\n\n*   An acoustic model’s task is to compute P(O∣W)P(O∣W)P(O \\\\mid W), i.e. the probability of generating a speech waveform for the mode. An acoustic model, as an important part of the ASR system, accounts for a large part of the computational overhead and also determines the system’s performance. GMM-HMM-based acoustic models are widely used in traditional speech recognition systems.\n*   In this model, GMM is used to model the distribution of the acoustic characteristics of speech and HMM is used to model the time sequence of speech signals. Since the rise of deep learning in 2006, deep neural networks (DNNs) have been applied in speech acoustic models. In [Mohamed et al. (2009)](https://www.cs.utoronto.ca/~gdahl/papers/dbnPhoneRec.pdf), Hinton and his students used feedforward fully-connected deep neural networks in speech recognition acoustic modeling.\n*   Typical pre-processing involves converting the audio to mono (similar to how computer vision models flatten the input image across the three channels, viz., R, G, B) and resampling it to 16kHz before it is input to the model.\n\n#### Encoder-Decoder Architectures: Past vs. Present\n\n*   Traditional speech recognition systems adopt a GMM-HMM architecture where the GMM is the acoustic model that computes the state/observation probabilities and the HMM decoder combines these probabilities using dynamic programming (DP). With the advent of deep learning, the role played by the GMM as the acoustic model is now replaced by a DNN. Rather than the GMM modeling the observation probabilities, a DNN is trained to output them. The HMM still acts as a decoder. The figure below illustrates both approaches side-by-side.\n\n![](/primers/ai/assets/speech/gmmhmm_dnnhmm.png)\n\n*   The DNN computes the observation probabilities and outputs a probability distribution over as many classes as the HMM states for each speech frame using a softmax layer. The number of HMM states depend of the number of phones, with a typical setup of 3 target labels for each phone for the beginning, middle and end of the segment, 1 state for silence, and 1 state for background. The DNN is typically trained to minimize the average cross-entropy loss over all frames between the predicted and the ground-truth distributions. The HMM decoder computes the word detection score using the observation, the state transition, and the prior probabilities. An architectural overview of a DNN-HMM is shown in the diagram below.\n\n![](/primers/ai/assets/speech/dnnhmm.jpg)\n\n*   Compared to traditional GMM-HMM acoustic models, DNN-HMM-based acoustic models perform better in terms of TIMIT database. When compared with GMM, DNN is advantageous in the following ways:\n    *   De-distribution hypothesis is not required for characteristic distribution when DNN models the posterior probability of the acoustic characteristics of speech.\n    *   GMM requires de-correlation processing for input characteristics, but DNN is capable of using various forms of input characteristics.\n    *   GMM can only use single-frame speech as inputs, but DNN is capable of capturing valid context information by means of splicing adjoining frames.\n*   Once the HMM model is built, to figure out the maximum a posteriori probability estimate of the most likely sequence of hidden states, i.e., the Viterbi path, a graph-traversal algorithm like the [Viterbi decoder algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) (typically implemented using the concept of dynamic programming in algorithms) can be applied.\n*   Given that speech comprises of sequential data, RNNs (and it’s variants, GRUs and LSTMs) are natural choices for DNNs. However, for tasks such as speech recognition, where the alignment between the inputs and the labels is unknown, RNNs have so far been limited to an auxiliary role. The problem is that the standard training methods require a separate target for every input, which is usually not available. The traditional solution — the so-called hybrid approach — is to use Hidden Markov Models (HMMs) to generate targets for the RNN, then invert the RNN outputs to provide observation probabilities [(Bourlard and Morgan, 1994)](https://www.amazon.com/Connectionist-Speech-Recognition-International-Engineering/dp/0792393961). However the hybrid approach does not exploit the full potential of RNNs for sequence processing, and it also leads to an awkward combination of discriminative and generative training.\n*   The connectionist temporal classification (CTC) output layer [(Graves et al., 2006)](https://www.cs.toronto.edu/~graves/icml_2006.pdf) removes the need for HMMs for providing alignment altogether by directly training RNNs to label sequences with unknown alignments, using a single discriminative loss function. CTC can also be combined with probabilistic language models for word-level speech and handwriting recognition. Note that the HMM-based decoder is still a good idea for cases where the task at hand involves a limited vocabulary and as a result, a smaller set of pronunciations, such as keyword spotting (vs. speech recognition).\n*   Using the CTC loss enables all-neural encoder-decoder seq2seq architectures that utilize an end-to-end neural architecture which generates phonemes at the output of the encoder as an intermediate step, which are consumed by a decoder which utilizes a language model and pronunciation model to generate transcriptions. The language model helps with secondary aspects of ASR such as punctuation, capitalization, and suggesting the right spellings based on the best word match.\n*   Recent models in this area such as [Listen Attend Spell (LAS)](https://arxiv.org/abs/1508.01211) forgo the intermediate phoneme labels altogether and train an end-to-end architecture that directly emits transcriptions at its output.\n*   For more in this area, Alex Graves’s book on [Supervised Sequence Labelling with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf) is a great reference.\n\n*   De-distribution hypothesis is not required for characteristic distribution when DNN models the posterior probability of the acoustic characteristics of speech.\n*   GMM requires de-correlation processing for input characteristics, but DNN is capable of using various forms of input characteristics.\n*   GMM can only use single-frame speech as inputs, but DNN is capable of capturing valid context information by means of splicing adjoining frames.\n\n#### Putting It All Together\n\n*   Let’s take an example of a simple digit recognition task. The words are the digits from one to nine. THe following hierarchical graph from [Speech and Language Processing by Jurafsky and Martin, 2008](https://web.stanford.edu/~jurafsky/slp3/) shows the hierarchical transitions of this task. P(one‖two)P(one‖two)P(one \\\\| two) represents the transition probability from digit two to one.\n\n![](/primers/ai/assets/speech/phone-word-sentence-hierarchy.png)\n\n*   Given this hierarchical transition matrix, a Viterbi trellis decoding method is used. The following figure from [Speech and Language Processing by Jurafsky and Martin, 2008](https://web.stanford.edu/~jurafsky/slp3/) shows the scheme of this decoding process. The words (digits) are stacked vertically and the feature sequence is shown horizontally.\n\n![](/primers/ai/assets/speech/viterbi_trellis.png)",
    "contentLength": 33815,
    "wordCount": 1574,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#automatic-speech-recognition"
  },
  {
    "id": "ai-speech-processing-keyword-spotting-wakeword-detection-21",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Keyword Spotting / Wakeword Detection",
    "order": 21,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>To achieve always-on keyword spotting, running a complex neural network all the time is a recipe for battery drain.</li>\n  <li>The trick is to use a two-pass system by breaking down the problem into two pieces: (i) low-compute (running on an always-on processor), and (ii) more accurate (running on the main processor) as shown below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/aon-kws.jpg\" alt=\"\"></p>\n<ul>\n  <li>The first low-compute piece typically runs on a special always-on processor (AOP) which is named so because it does not enter sleep states (unlike the main processor) and is always-on.\nThe trade-off is that it is much less powerful than the main processor so you can only do small number of computations if you don’t want to impact the battery life.</li>\n  <li>The second more accurate piece running on the main processor consumes high power but only uses that power for a very limited amount of time when you actually want it to compute something, i.e., when it is not sleeping. The rest of the time it goes to several levels of sleep and most of the time the main processor is completely asleep.</li>\n  <li>Thus, we have a two layer system similar to a processor cache hierarchy. The first level is running all the time with a very simple DNN and then if it is confident with its predictions, it does not wake up the main system. However, if it is unsure of its predictions, it wakes up the main processor which runs a bigger, fancier DNN that is more accurate.</li>\n  <li>Note that both passes share the same set of computed speech features which are typically generated on the AOP.</li>\n  <li>The level-1/first-stage thresholds are tuned to trade-off false-reject rates for high false-accept rates so we don’t easily miss an event where the user was looking to interact with the device due to it not waking up the second-stage (negatives/rejects do not wake up stage-2). Since the level-2/second-stage model is a beefier model, it can mitigate false-accepts easily. It is also important to make sure the precision is not too low as to wake up the level-2/second-stage model (also known as the “checker” model) frequently, and eventually lead to a battery drain.\n    <ul>\n      <li>This requires the level-1 stage to have high recall with modest precision to ensure that the voice assistant doesn’t miss a true positive. At the same time, the level-1 stage needs to optimize for speed to ensure we deliver a great set of candidates and satisfy the strict latency requirements of the system.</li>\n      <li>The level-2 stage, on the other hand, has high precision with modest recall to ensure a smooth user-experience by ensuring that the voice-assistant minimizes false positives (and avoids waking up frequently due to excessive false positives). The level-2 stage can also incorporate a more complex architecture; since the level-2 stage is only invoked on a handful of instances compared to level-1, the latencies involved with such complex architectures are usually manageable.</li>\n      <li>This paradigm is similar to how recommender systems optimize for recall during their retrieval stage and precision during their ranking stage.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>This requires the level-1 stage to have high recall with modest precision to ensure that the voice assistant doesn’t miss a true positive. At the same time, the level-1 stage needs to optimize for speed to ensure we deliver a great set of candidates and satisfy the strict latency requirements of the system.</li>\n      <li>The level-2 stage, on the other hand, has high precision with modest recall to ensure a smooth user-experience by ensuring that the voice-assistant minimizes false positives (and avoids waking up frequently due to excessive false positives). The level-2 stage can also incorporate a more complex architecture; since the level-2 stage is only invoked on a handful of instances compared to level-1, the latencies involved with such complex architectures are usually manageable.</li>\n      <li>This paradigm is similar to how recommender systems optimize for recall during their retrieval stage and precision during their ranking stage.</li>\n    </ul>",
    "contentMarkdown": "*   To achieve always-on keyword spotting, running a complex neural network all the time is a recipe for battery drain.\n*   The trick is to use a two-pass system by breaking down the problem into two pieces: (i) low-compute (running on an always-on processor), and (ii) more accurate (running on the main processor) as shown below:\n\n![](/primers/ai/assets/speech/aon-kws.jpg)\n\n*   The first low-compute piece typically runs on a special always-on processor (AOP) which is named so because it does not enter sleep states (unlike the main processor) and is always-on. The trade-off is that it is much less powerful than the main processor so you can only do small number of computations if you don’t want to impact the battery life.\n*   The second more accurate piece running on the main processor consumes high power but only uses that power for a very limited amount of time when you actually want it to compute something, i.e., when it is not sleeping. The rest of the time it goes to several levels of sleep and most of the time the main processor is completely asleep.\n*   Thus, we have a two layer system similar to a processor cache hierarchy. The first level is running all the time with a very simple DNN and then if it is confident with its predictions, it does not wake up the main system. However, if it is unsure of its predictions, it wakes up the main processor which runs a bigger, fancier DNN that is more accurate.\n*   Note that both passes share the same set of computed speech features which are typically generated on the AOP.\n*   The level-1/first-stage thresholds are tuned to trade-off false-reject rates for high false-accept rates so we don’t easily miss an event where the user was looking to interact with the device due to it not waking up the second-stage (negatives/rejects do not wake up stage-2). Since the level-2/second-stage model is a beefier model, it can mitigate false-accepts easily. It is also important to make sure the precision is not too low as to wake up the level-2/second-stage model (also known as the “checker” model) frequently, and eventually lead to a battery drain.\n    *   This requires the level-1 stage to have high recall with modest precision to ensure that the voice assistant doesn’t miss a true positive. At the same time, the level-1 stage needs to optimize for speed to ensure we deliver a great set of candidates and satisfy the strict latency requirements of the system.\n    *   The level-2 stage, on the other hand, has high precision with modest recall to ensure a smooth user-experience by ensuring that the voice-assistant minimizes false positives (and avoids waking up frequently due to excessive false positives). The level-2 stage can also incorporate a more complex architecture; since the level-2 stage is only invoked on a handful of instances compared to level-1, the latencies involved with such complex architectures are usually manageable.\n    *   This paradigm is similar to how recommender systems optimize for recall during their retrieval stage and precision during their ranking stage.\n\n*   This requires the level-1 stage to have high recall with modest precision to ensure that the voice assistant doesn’t miss a true positive. At the same time, the level-1 stage needs to optimize for speed to ensure we deliver a great set of candidates and satisfy the strict latency requirements of the system.\n*   The level-2 stage, on the other hand, has high precision with modest recall to ensure a smooth user-experience by ensuring that the voice-assistant minimizes false positives (and avoids waking up frequently due to excessive false positives). The level-2 stage can also incorporate a more complex architecture; since the level-2 stage is only invoked on a handful of instances compared to level-1, the latencies involved with such complex architectures are usually manageable.\n*   This paradigm is similar to how recommender systems optimize for recall during their retrieval stage and precision during their ranking stage.",
    "contentLength": 4191,
    "wordCount": 658,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#keyword-spotting-/-wakeword-detection"
  },
  {
    "id": "ai-speech-processing-measuring-performance-22",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Measuring Performance",
    "order": 22,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Evaluating performance in keyword spotting involves several key metrics, primarily focusing on Precision, Recall, F1-score, and Detection Error Trade-off (DET) curves. Each metric provides a distinct perspective on the system’s performance and effectiveness in different operational scenarios.</li>\n</ul>\n<h4 id=\"precision\">Precision</h4>\n<ul>\n  <li>Precision measures the accuracy of the model in identifying wakewords. It indicates how many of the model’s wakeword detections are correct, calculated as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1009.01em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mtext\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-283\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-284\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-285\"><span class=\"mi\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.7em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.872em;\"><span class=\"mrow\" id=\"MathJax-Span-288\"><span class=\"mi\" id=\"MathJax-Span-289\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-292\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.86em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.857em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>A high precision value ensures fewer false activations, conserving device power and enhancing user experience by reducing unintended interactions.</li>\n</ul>\n<h4 id=\"recall\">Recall</h4>\n<ul>\n  <li>Recall assesses the reliability of the model in detecting actual wakeword occurrences. It shows the proportion of actual wakeword instances the model successfully identifies, calculated as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-294\" style=\"width: 9.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1007.92em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mtext\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-298\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>High recall ensures the voice assistant is consistently responsive, minimizing missed interactions and improving user satisfaction.</li>\n</ul>\n<h4 id=\"precision-recall-pr-curves\">Precision-Recall (PR) Curves</h4>\n<ul>\n  <li>Precision-Recall curves visually illustrate the trade-off between precision and recall at various threshold settings. By plotting precision against recall, developers can analyze system behavior and select optimal operational thresholds that suit specific precision and recall requirements.</li>\n  <li>However, PR curves have limitations in keyword spotting scenarios, especially when evaluating systems where false positives and false negatives must both be strictly managed.</li>\n</ul>\n<h4 id=\"f1-score\">F1-score</h4>\n<ul>\n  <li>The F1-score provides a balanced evaluation by combining precision and recall into a single metric. It is especially useful in scenarios where balancing false positives and false negatives is critical. The F1-score is the harmonic mean of precision and recall:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>F</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn><mo>&amp;#x00D7;</mo><mfrac><mrow><mtext>Precision</mtext><mo>&amp;#x00D7;</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-308\" style=\"width: 13.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1011.41em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-309\"><span class=\"msubsup\" id=\"MathJax-Span-310\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mn\" id=\"MathJax-Span-312\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-315\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mfrac\" id=\"MathJax-Span-316\"><span style=\"display: inline-block; position: relative; width: 7.607em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1007.45em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -3.695em;\"><span class=\"mrow\" id=\"MathJax-Span-317\"><span class=\"mtext\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mtext\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1007.45em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -3.747em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mtext\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mtext\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Recall</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.61em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.607em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>F</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>Optimizing the F1-score helps maintain a balanced performance between accuracy and reliability.</li>\n</ul>\n<h4 id=\"detection-error-trade-off-det-curves\">Detection Error Trade-off (DET) Curves</h4>\n<ul>\n  <li>\n    <p>DET curves offer a comprehensive visual assessment of keyword spotting system performance, particularly highlighting the trade-offs between false alarm rates and missed detection rates across varying thresholds. Unlike precision-recall curves, DET curves specifically plot the False Negative Rate (FNR) against the False Positive Rate (FPR) on a normal deviate scale.</p>\n  </li>\n  <li>\n    <p><strong>False Negative Rate (FNR)</strong>: Probability of missing the wakeword (i.e., not waking the device when the wakeword is spoken), calculated as:</p>\n  </li>\n</ul>\n<p>DET curves offer a comprehensive visual assessment of keyword spotting system performance, particularly highlighting the trade-offs between false alarm rates and missed detection rates across varying thresholds. Unlike precision-recall curves, DET curves specifically plot the False Negative Rate (FNR) against the False Positive Rate (FPR) on a normal deviate scale.</p>\n<p><strong>False Negative Rate (FNR)</strong>: Probability of missing the wakeword (i.e., not waking the device when the wakeword is spoken), calculated as:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>FNR</mtext><mo>=</mo><mfrac><mrow><mi>F</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 8.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1007.29em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mtext\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular;\">FNR</span><span class=\"mo\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-329\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.36em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.674em;\"><span class=\"mrow\" id=\"MathJax-Span-330\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>FNR</mtext><mo>=</mo><mfrac><mrow><mi>F</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div>\n<ul>\n  <li><strong>False Positive Rate (FPR)</strong>: Probability of falsely detecting a wakeword when it is not present, calculated as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>FPR</mtext><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1007.14em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mtext\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular;\">FPR</span><span class=\"mo\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-343\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-344\"><span class=\"mi\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-346\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-347\"><span class=\"mi\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>FPR</mtext><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>\n    <p>DET curves are generally preferred for keyword spotting tasks due to several advantages:</p>\n\n    <ul>\n      <li><strong>Symmetric Visualization</strong>: DET curves equally emphasize both false positives and false negatives, enabling balanced optimization.</li>\n      <li><strong>Clear Operational Trade-offs</strong>: DET curves provide a clearer depiction of trade-offs critical for user experience and battery efficiency.</li>\n      <li><strong>Direct Threshold Selection</strong>: They facilitate intuitive threshold tuning aligned with real-world performance requirements, directly addressing operational priorities.</li>\n    </ul>\n  </li>\n  <li>\n    <p>By employing DET curves alongside precision, recall, and F1-score, developers can holistically assess and optimize keyword spotting systems, ensuring effective balance across responsiveness, accuracy, and power efficiency.</p>\n  </li>\n</ul>\n<p>DET curves are generally preferred for keyword spotting tasks due to several advantages:</p>\n<ul>\n      <li><strong>Symmetric Visualization</strong>: DET curves equally emphasize both false positives and false negatives, enabling balanced optimization.</li>\n      <li><strong>Clear Operational Trade-offs</strong>: DET curves provide a clearer depiction of trade-offs critical for user experience and battery efficiency.</li>\n      <li><strong>Direct Threshold Selection</strong>: They facilitate intuitive threshold tuning aligned with real-world performance requirements, directly addressing operational priorities.</li>\n    </ul>\n<p>By employing DET curves alongside precision, recall, and F1-score, developers can holistically assess and optimize keyword spotting systems, ensuring effective balance across responsiveness, accuracy, and power efficiency.</p>\n<h4 id=\"online-learning-with-live-data-to-improve-robustness\">Online Learning with Live Data to Improve Robustness</h4>\n<ul>\n  <li>To improve the robustness of a speech model such as ASR, Keyword Spotting etc., we can train it on online data gathered from the field. To do so, we run the classifier with a very low threshold, for instance, in case of keyword spotting, if the usual detection threshold is 0.8, we run it at a threshold of 0.5. This leads to the keyword spotter firing on a lot of keywords – most of them being false positives and the others being true positives. This is because for a classifier, lower the classification threshold, more the number of false positives, while higher the classification threshold, more the number of false negatives.</li>\n  <li>Next, we manually sift through the results and identify the true positives and use them for re-training the classifier. The false positives, in turn, are used as hard negatives to also re-train the classifier and thus, make it more robust to erroneous scenarios involving false accepts.</li>\n</ul>",
    "contentMarkdown": "*   Evaluating performance in keyword spotting involves several key metrics, primarily focusing on Precision, Recall, F1-score, and Detection Error Trade-off (DET) curves. Each metric provides a distinct perspective on the system’s performance and effectiveness in different operational scenarios.\n\n#### Precision\n\n*   Precision measures the accuracy of the model in identifying wakewords. It indicates how many of the model’s wakeword detections are correct, calculated as:\n\nPrecision\\=TPTP+FPPrecision\\=TPTP+FP\n\n*   A high precision value ensures fewer false activations, conserving device power and enhancing user experience by reducing unintended interactions.\n\n#### Recall\n\n*   Recall assesses the reliability of the model in detecting actual wakeword occurrences. It shows the proportion of actual wakeword instances the model successfully identifies, calculated as:\n\nRecall\\=TPTP+FNRecall\\=TPTP+FN\n\n*   High recall ensures the voice assistant is consistently responsive, minimizing missed interactions and improving user satisfaction.\n\n#### Precision-Recall (PR) Curves\n\n*   Precision-Recall curves visually illustrate the trade-off between precision and recall at various threshold settings. By plotting precision against recall, developers can analyze system behavior and select optimal operational thresholds that suit specific precision and recall requirements.\n*   However, PR curves have limitations in keyword spotting scenarios, especially when evaluating systems where false positives and false negatives must both be strictly managed.\n\n#### F1-score\n\n*   The F1-score provides a balanced evaluation by combining precision and recall into a single metric. It is especially useful in scenarios where balancing false positives and false negatives is critical. The F1-score is the harmonic mean of precision and recall:\n\nF1\\=2×Precision×RecallPrecision+RecallF1\\=2×Precision×RecallPrecision+Recall\n\n*   Optimizing the F1-score helps maintain a balanced performance between accuracy and reliability.\n\n#### Detection Error Trade-off (DET) Curves\n\n*   DET curves offer a comprehensive visual assessment of keyword spotting system performance, particularly highlighting the trade-offs between false alarm rates and missed detection rates across varying thresholds. Unlike precision-recall curves, DET curves specifically plot the False Negative Rate (FNR) against the False Positive Rate (FPR) on a normal deviate scale.\n    \n*   **False Negative Rate (FNR)**: Probability of missing the wakeword (i.e., not waking the device when the wakeword is spoken), calculated as:\n    \n\nDET curves offer a comprehensive visual assessment of keyword spotting system performance, particularly highlighting the trade-offs between false alarm rates and missed detection rates across varying thresholds. Unlike precision-recall curves, DET curves specifically plot the False Negative Rate (FNR) against the False Positive Rate (FPR) on a normal deviate scale.\n\n**False Negative Rate (FNR)**: Probability of missing the wakeword (i.e., not waking the device when the wakeword is spoken), calculated as:\n\nFNR\\=FNTP+FNFNR\\=FNTP+FN\n\n*   **False Positive Rate (FPR)**: Probability of falsely detecting a wakeword when it is not present, calculated as:\n\nFPR\\=FPFP+TNFPR\\=FPFP+TN\n\n*   DET curves are generally preferred for keyword spotting tasks due to several advantages:\n    \n    *   **Symmetric Visualization**: DET curves equally emphasize both false positives and false negatives, enabling balanced optimization.\n    *   **Clear Operational Trade-offs**: DET curves provide a clearer depiction of trade-offs critical for user experience and battery efficiency.\n    *   **Direct Threshold Selection**: They facilitate intuitive threshold tuning aligned with real-world performance requirements, directly addressing operational priorities.\n*   By employing DET curves alongside precision, recall, and F1-score, developers can holistically assess and optimize keyword spotting systems, ensuring effective balance across responsiveness, accuracy, and power efficiency.\n    \n\nDET curves are generally preferred for keyword spotting tasks due to several advantages:\n\n*   **Symmetric Visualization**: DET curves equally emphasize both false positives and false negatives, enabling balanced optimization.\n*   **Clear Operational Trade-offs**: DET curves provide a clearer depiction of trade-offs critical for user experience and battery efficiency.\n*   **Direct Threshold Selection**: They facilitate intuitive threshold tuning aligned with real-world performance requirements, directly addressing operational priorities.\n\nBy employing DET curves alongside precision, recall, and F1-score, developers can holistically assess and optimize keyword spotting systems, ensuring effective balance across responsiveness, accuracy, and power efficiency.\n\n#### Online Learning with Live Data to Improve Robustness\n\n*   To improve the robustness of a speech model such as ASR, Keyword Spotting etc., we can train it on online data gathered from the field. To do so, we run the classifier with a very low threshold, for instance, in case of keyword spotting, if the usual detection threshold is 0.8, we run it at a threshold of 0.5. This leads to the keyword spotter firing on a lot of keywords – most of them being false positives and the others being true positives. This is because for a classifier, lower the classification threshold, more the number of false positives, while higher the classification threshold, more the number of false negatives.\n*   Next, we manually sift through the results and identify the true positives and use them for re-training the classifier. The false positives, in turn, are used as hard negatives to also re-train the classifier and thus, make it more robust to erroneous scenarios involving false accepts.",
    "contentLength": 25381,
    "wordCount": 781,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#measuring-performance"
  },
  {
    "id": "ai-speech-processing-handling-dynamic-language-switching-and-code-switc-23",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Handling Dynamic Language Switching and Code Switching",
    "order": 23,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>In order to support such dynamic language switching from one interaction to the next (which is different from code switching, where words from more than one language are used within the same interaction or utterance), the backend intelligence must perform language identification (LID) along with automatic speech recognition (ASR) so that the detected language can be used to trigger appropriate downstream systems such as natural language understanding, text-to-speech synthesis, etc., which usually are language specific.</li>\n  <li>To enable dynamic language switching between two or more pre-specified languages, a commonly adopted strategy is to run several monolingual ASR systems in parallel along with a standalone acoustic LID module, and pass only one of the recognized transcripts downstream depending on the outcome of language detection. While this approach works well in practice, it is neither cost-effective for more than two languages, nor suitable for on-device scenarios where compute resources and memory are limited. To this end, <a href=\"https://assets.amazon.science/13/73/af2e3c504c9ca787e1604ed9b687/joint-asr-and-language-identification-using-rnn-t-an-efficient-approach-to-dynamic-language-switching.pdf\">Joint ASR and Language Identification Using RNN-T: An Efficient Approach to Dynamic Language Switching (2021)</a> proposed all-neural architectures that can jointly perform ASR and LID for a group of pre-specified languages and thereby significantly improve the cost effectiveness and efficiency of dynamic language switching. Joint ASR-LID modeling, by definition, involves multilingual speech recognition. Multilingual ASR models are trained by pooling data from the languages of interest, and it is often observed that languages with smaller amounts of data (i.e., low-resource languages) benefit more from this.</li>\n</ul>",
    "contentMarkdown": "*   In order to support such dynamic language switching from one interaction to the next (which is different from code switching, where words from more than one language are used within the same interaction or utterance), the backend intelligence must perform language identification (LID) along with automatic speech recognition (ASR) so that the detected language can be used to trigger appropriate downstream systems such as natural language understanding, text-to-speech synthesis, etc., which usually are language specific.\n*   To enable dynamic language switching between two or more pre-specified languages, a commonly adopted strategy is to run several monolingual ASR systems in parallel along with a standalone acoustic LID module, and pass only one of the recognized transcripts downstream depending on the outcome of language detection. While this approach works well in practice, it is neither cost-effective for more than two languages, nor suitable for on-device scenarios where compute resources and memory are limited. To this end, [Joint ASR and Language Identification Using RNN-T: An Efficient Approach to Dynamic Language Switching (2021)](https://assets.amazon.science/13/73/af2e3c504c9ca787e1604ed9b687/joint-asr-and-language-identification-using-rnn-t-an-efficient-approach-to-dynamic-language-switching.pdf) proposed all-neural architectures that can jointly perform ASR and LID for a group of pre-specified languages and thereby significantly improve the cost effectiveness and efficiency of dynamic language switching. Joint ASR-LID modeling, by definition, involves multilingual speech recognition. Multilingual ASR models are trained by pooling data from the languages of interest, and it is often observed that languages with smaller amounts of data (i.e., low-resource languages) benefit more from this.",
    "contentLength": 1870,
    "wordCount": 240,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#handling-dynamic-language-switching-and-code-switching"
  },
  {
    "id": "ai-speech-processing-speaker-recognition-24",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Speech Processing Tasks",
    "title": "Speaker Recognition",
    "order": 24,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>Speaker recognition can be classified into (i) speaker verification, and (ii) speaker identification. Speaker verification aims to verify whether an input speech corresponds to the claimed identity. Speaker identification aims to identify an input speech by selecting one model from a set of enrolled speaker models. In some cases, speaker verification follows speaker identification in order to validate the identification result.</li>\n  <li>Speaker verification is gauged using the EER metric on the DET-curve while speaker identification is gauged using accuracy.</li>\n  <li><a href=\"http://groups.csail.mit.edu/sls/publications/2010/Dehak_IEEE_Transactions.pdf\">i-vectors</a> by Dehak et al. in 2010 were the leading technology behind speaker recognition, up until DNNs took over with <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41939.pdf\">d-vectors</a> by Variani et al. in 2014. The latest in speaker recognition is <a href=\"https://www.danielpovey.com/files/2018_icassp_xvectors.pdf\">x-vectors</a> by Snyder et al. in 2018 which proposed using data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The figure below shows the embedding extraction process with d-vectors:</li>\n</ul>\n<p><img src=\"../../../images/papers/dvectors.jpg\" alt=\"\"></p>\n<ul>\n  <li>Read <a href=\"https://slsp.kaist.ac.kr/paperdata/book_chapter.pdf\">more</a> on speaker recognition in this book chapter by Jin and Yoo from KAIST.</li>\n</ul>",
    "contentMarkdown": "*   Speaker recognition can be classified into (i) speaker verification, and (ii) speaker identification. Speaker verification aims to verify whether an input speech corresponds to the claimed identity. Speaker identification aims to identify an input speech by selecting one model from a set of enrolled speaker models. In some cases, speaker verification follows speaker identification in order to validate the identification result.\n*   Speaker verification is gauged using the EER metric on the DET-curve while speaker identification is gauged using accuracy.\n*   [i-vectors](http://groups.csail.mit.edu/sls/publications/2010/Dehak_IEEE_Transactions.pdf) by Dehak et al. in 2010 were the leading technology behind speaker recognition, up until DNNs took over with [d-vectors](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41939.pdf) by Variani et al. in 2014. The latest in speaker recognition is [x-vectors](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf) by Snyder et al. in 2018 which proposed using data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The figure below shows the embedding extraction process with d-vectors:\n\n![](../../../images/papers/dvectors.jpg)\n\n*   Read [more](https://slsp.kaist.ac.kr/paperdata/book_chapter.pdf) on speaker recognition in this book chapter by Jin and Yoo from KAIST.",
    "contentLength": 1573,
    "wordCount": 174,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#speaker-recognition"
  },
  {
    "id": "ai-speech-processing-time-domain-25",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Data Augmentation",
    "title": "Time Domain",
    "order": 25,
    "orderInChapter": 1,
    "contentHtml": "<h4 id=\"gain\">Gain</h4>\n<ul>\n  <li>Gain augmentation can be applied by simply multiplying the raw audio by a scalar.</li>\n</ul>\n<h4 id=\"types-of-noise\">Types of Noise</h4>\n<h5 id=\"reverberation-noise--room-impulse-response-rir\">Reverberation Noise / Room Impulse Response (RIR)</h5>\n<ul>\n  <li>Reverberation noise, also known as room impulse response (RIR), is produced when a sound source stops within an enclosed space. Sound waves continue to reflect off the ceiling, walls and floor surfaces until they eventually die out. These reflected sound waves are known as reverberation.</li>\n</ul>\n<h5 id=\"babble-noise\">Babble Noise</h5>\n<ul>\n  <li>Babble noise is considered as one of the best noises for masking speech. Refer <a href=\"https://personal.utdallas.edu/~jxh052100/Publications/JP-61-IEEE-ASLP-Babble-NitishK-JohnH-Sept09.pdf\">Babble Noise: Modeling, Analysis, and Applications</a>.</li>\n</ul>\n<h4 id=\"speed-and-pitch\">Speed and Pitch</h4>\n<ul>\n  <li>These augmentations, while slow to run, are mainly used to artificially increase the number of training speakers.</li>\n</ul>\n<h4 id=\"codec-augmentation\">Codec Augmentation</h4>\n<ul>\n  <li>Transcoding audio in a new codec can increase diversity in the training dataset, however it is extremely slow and we expect Alexa data to have limited variation in used codecs.</li>\n</ul>\n<h4 id=\"mixup\">Mixup</h4>\n<ul>\n  <li>Mixup has shown promising results on small datasets with a limited number of utterances per training speaker.</li>\n</ul>\n<h4 id=\"dynamic-range-compression\">Dynamic Range Compression</h4>\n<ul>\n  <li>We do expect most Alexa recordings to have a constant volume due to the audio preprocessing. Therefore we expect that introducing or suppressing volume variations within a recording to have a limited impact on real world data.</li>\n</ul>",
    "contentMarkdown": "#### Gain\n\n*   Gain augmentation can be applied by simply multiplying the raw audio by a scalar.\n\n#### Types of Noise\n\n##### Reverberation Noise / Room Impulse Response (RIR)\n\n*   Reverberation noise, also known as room impulse response (RIR), is produced when a sound source stops within an enclosed space. Sound waves continue to reflect off the ceiling, walls and floor surfaces until they eventually die out. These reflected sound waves are known as reverberation.\n\n##### Babble Noise\n\n*   Babble noise is considered as one of the best noises for masking speech. Refer [Babble Noise: Modeling, Analysis, and Applications](https://personal.utdallas.edu/~jxh052100/Publications/JP-61-IEEE-ASLP-Babble-NitishK-JohnH-Sept09.pdf).\n\n#### Speed and Pitch\n\n*   These augmentations, while slow to run, are mainly used to artificially increase the number of training speakers.\n\n#### Codec Augmentation\n\n*   Transcoding audio in a new codec can increase diversity in the training dataset, however it is extremely slow and we expect Alexa data to have limited variation in used codecs.\n\n#### Mixup\n\n*   Mixup has shown promising results on small datasets with a limited number of utterances per training speaker.\n\n#### Dynamic Range Compression\n\n*   We do expect most Alexa recordings to have a constant volume due to the audio preprocessing. Therefore we expect that introducing or suppressing volume variations within a recording to have a limited impact on real world data.",
    "contentLength": 1808,
    "wordCount": 217,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#time-domain"
  },
  {
    "id": "ai-speech-processing-frequency-domain-26",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Data Augmentation",
    "title": "Frequency Domain",
    "order": 26,
    "orderInChapter": 2,
    "contentHtml": "<h4 id=\"specaugment\">SpecAugment</h4>\n<ul>\n  <li>This paper by <a href=\"https://arxiv.org/abs/1904.08779\">Park et al. (2019)</a> from Google presents SpecAugment, a simple data augmentation method for speech recognition.</li>\n  <li>SpecAugment greatly improves the performance of ASR networks. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. They apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end speech recognition tasks.</li>\n  <li>They achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks on end-to-end LAS networks by augmenting the training set using simple handcrafted policies, surpassing the performance of hybrid systems even without the aid of a language model. SpecAugment converts ASR from an over-fitting to an under-fitting problem, and they are able to gain performance by using bigger networks and training longer. On LibriSpeech, they achieve 6.8% WER (more on WER in the section on <a href=\"#evaluation-metrics\">Evaluation metrics</a>) on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, they achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5’00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.</li>\n</ul>",
    "contentMarkdown": "#### SpecAugment\n\n*   This paper by [Park et al. (2019)](https://arxiv.org/abs/1904.08779) from Google presents SpecAugment, a simple data augmentation method for speech recognition.\n*   SpecAugment greatly improves the performance of ASR networks. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. They apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end speech recognition tasks.\n*   They achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks on end-to-end LAS networks by augmenting the training set using simple handcrafted policies, surpassing the performance of hybrid systems even without the aid of a language model. SpecAugment converts ASR from an over-fitting to an under-fitting problem, and they are able to gain performance by using bigger networks and training longer. On LibriSpeech, they achieve 6.8% WER (more on WER in the section on [Evaluation metrics](#evaluation-metrics)) on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, they achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5’00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
    "contentLength": 1650,
    "wordCount": 227,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#frequency-domain"
  },
  {
    "id": "ai-speech-processing-precision-and-recall-27",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Precision and Recall",
    "order": 27,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>In case of an imbalanced dataset scenario, precision and recall are appropriate performance metrics. Precision and recall are typically juxtaposed together when reported.</li>\n  <li>Precision is defined as the fraction of relevant instances among all retrieved instances.</li>\n  <li>Recall, sometimes referred to as <strong><a href=\"../evaluation-metrics/#sensitivity-and-specificity\">sensitivity</a></strong>, is the fraction of retrieved instances among all relevant instances.</li>\n  <li>Note that precision and recall are computed for each class. They are commonly used to evaluate the performance of classification or information retrieval systems.</li>\n</ul>\n<blockquote>\n  <p>A perfect classifier has precision and recall both equal to 1.</p>\n</blockquote>\n<p>A perfect classifier has precision and recall both equal to 1.</p>\n<ul>\n  <li>It is often possible to calibrate the number of results returned by a model and improve precision at the expense of recall, or vice versa.</li>\n  <li>Precision and recall should always be reported together and are not quoted individually. This is because it is easy to vary the sensitivity of a model to improve precision at the expense of recall, or vice versa.\n    <ul>\n      <li>As an example, imagine that the manufacturer of a pregnancy test needed to reach a certain level of precision, or of specificity, for FDA approval. The pregnancy test shows one line if it is moderately confident of the pregnancy, and a double line if it is very sure. If the manufacturer decides to only count the double lines as positives, the test will return far fewer positives overall, but the precision will improve, while the recall will go down. This shows why precision and recall should always be reported together.</li>\n    </ul>\n  </li>\n  <li>The figure below (taken from the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">precision and recall</a>) shows a graphical representation of precision and recall:</li>\n</ul>\n<ul>\n      <li>As an example, imagine that the manufacturer of a pregnancy test needed to reach a certain level of precision, or of specificity, for FDA approval. The pregnancy test shows one line if it is moderately confident of the pregnancy, and a double line if it is very sure. If the manufacturer decides to only count the double lines as positives, the test will return far fewer positives overall, but the precision will improve, while the recall will go down. This shows why precision and recall should always be reported together.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/precision-and-recall.png\" alt=\"\"></p>\n<ul>\n  <li>Formally, precision and recall can be defined as:\n    <ul>\n      <li><strong>Precision</strong>: Out of all the samples marked positive, how many were actually positive (i.e., the true positives)?</li>\n      <li><strong>Recall</strong>: Out of all the samples that are actually positive, how many were marked positive (i.e., the true positives)?</li>\n      <li>From the above definitions it is clear that with PR metrics, the focus is on the positive class (also called the relevant class).</li>\n    </ul>\n  </li>\n  <li>\n    <p>In the section on <a href=\"#precision-recall-curve\">Precision-Recall (PR) Curves</a>, we explore how to get the best out of these two metrics using PR curves.</p>\n  </li>\n  <li><strong>Key takeaways</strong>\n    <ul>\n      <li>Precision: how many <strong>selected</strong> items are <strong>relevant</strong>?</li>\n      <li>Recall: how many <strong>relevant</strong> items are <strong>selected</strong>?</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Precision</strong>: Out of all the samples marked positive, how many were actually positive (i.e., the true positives)?</li>\n      <li><strong>Recall</strong>: Out of all the samples that are actually positive, how many were marked positive (i.e., the true positives)?</li>\n      <li>From the above definitions it is clear that with PR metrics, the focus is on the positive class (also called the relevant class).</li>\n    </ul>\n<p>In the section on <a href=\"#precision-recall-curve\">Precision-Recall (PR) Curves</a>, we explore how to get the best out of these two metrics using PR curves.</p>\n<ul>\n      <li>Precision: how many <strong>selected</strong> items are <strong>relevant</strong>?</li>\n      <li>Recall: how many <strong>relevant</strong> items are <strong>selected</strong>?</li>\n    </ul>\n<h4 id=\"historical-background\">Historical Background</h4>\n<ul>\n  <li>This section is optional and offers a historical walk-through of how precision, recall and F1-score came about, so you may skip to the next section if so desired.</li>\n  <li>Precision and recall were first defined by the American scientist Allen Kent and his colleagues in their 1955 paper Machine literature searching VIII. Operational criteria for designing information retrieval systems.</li>\n  <li>Kent served in the US Army Air Corps in World War II, and was assigned after the war by the US military to a classified project at MIT in mechanized document encoding and search.</li>\n  <li>In 1955, Kent and his colleagues Madeline Berry, Fred Luehrs, and J.W. Perry were working on a project in information retrieval using punch cards and reel-to-reel tapes. The team found a need to be able to quantify the performance of an information retrieval system objectively, allowing improvements in a system to be measured consistently, and so they published their definition of precision and recall.</li>\n  <li>They described their ideas as a theory underlying the field of information retrieval, just as the second law of thermodynamics “underlies the design of a steam engine, regardless of its type or power rating”.</li>\n  <li>Since then, the definitions of precision and recall have remained fundamentally the same, although for search engines the definitions have been modified to take into account certain nuances of human behavior, giving rise to the modified metrics precision @ <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-353\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-354\"><span class=\"mi\" id=\"MathJax-Span-355\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">k</script> and mean average precision (mAP), which are the values normally quoted in information retrieval contexts today.</li>\n  <li>In 1979, the Dutch computer science professor Cornelis Joost van Rijsbergen recognized the problems of defining search engine performance in terms of two numbers and decided on a convenient scalar function that combines the two. He called this metric the Effectiveness function and assigned it the letter E. This was later modified to the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-356\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-357\"><span class=\"msubsup\" id=\"MathJax-Span-358\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-359\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mn\" id=\"MathJax-Span-360\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">F_1</script> score, or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B2;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"msubsup\" id=\"MathJax-Span-363\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-365\"><span class=\"mrow\" id=\"MathJax-Span-366\"><span class=\"mi\" id=\"MathJax-Span-367\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>β</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">F_{\\beta}</script> score, which is still used today to summarize precision and recall.</li>\n</ul>\n<h4 id=\"examples\">Examples</h4>\n<ul>\n  <li>Precision and recall can be best explained using examples. Consider the case of evaluating how well does a robot sifts good apples from rotten apples. A robot looks into the basket and picks out all the good apples, leaving the rotten apples behind, but is not perfect and could sometimes mistake a rotten apple for a good apple orange.</li>\n  <li>After the robot finishes picking the good apples, precision and recall can be calculated as:\n    <ul>\n      <li><strong>Precision</strong>: number of good apples picked out of all the picked apples.</li>\n      <li><strong>Recall</strong>: number of good apples picked out of all possible good apples.</li>\n    </ul>\n  </li>\n  <li><strong>Precision</strong> is about <strong>exactness</strong>, classifying only one instance correctly yields 100% precision, but a very low recall, it tells us how well the system identifies samples from a given class.</li>\n  <li><strong>Recall</strong> is about <strong>completeness</strong>, classifying all instances as positive yields 100% recall, but a very low precision, it tells how well the system does and identify all the samples from a given class.</li>\n  <li>As another example, consider the task of information retrieval. As such, precision and recall can be calculated as:\n    <ul>\n      <li><strong>Precision</strong>: number of relevant documents retrieved out of all retrieved documents.</li>\n      <li><strong>Recall</strong>: number of relevant documents retrieved out of all relevant documents.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Precision</strong>: number of good apples picked out of all the picked apples.</li>\n      <li><strong>Recall</strong>: number of good apples picked out of all possible good apples.</li>\n    </ul>\n<ul>\n      <li><strong>Precision</strong>: number of relevant documents retrieved out of all retrieved documents.</li>\n      <li><strong>Recall</strong>: number of relevant documents retrieved out of all relevant documents.</li>\n    </ul>\n<h4 id=\"applications\">Applications</h4>\n<ul>\n  <li>Precision and recall are measured for every possible class in your dataset. So, <strong>precision</strong> and <strong>recall</strong> metrics are relatively much more appropriate (especially compared to accuracy) when dealing with <strong>imbalanced classes</strong>.</li>\n</ul>\n<blockquote>\n  <p>An important point to note is that PR are able to handle class imbalance in scenarios where the positive class (also called the minority class) is rare. If, however, the dataset is imbalanced in such a way that the negative class is the one that’s rare, PR curves are sub-optimal and can be misleading. In these cases, ROC curves might be a better fit.</p>\n</blockquote>\n<p>An important point to note is that PR are able to handle class imbalance in scenarios where the positive class (also called the minority class) is rare. If, however, the dataset is imbalanced in such a way that the negative class is the one that’s rare, PR curves are sub-optimal and can be misleading. In these cases, ROC curves might be a better fit.</p>\n<ul>\n  <li>So when do we use PR metrics? Here’s the typical use-cases:\n    <ul>\n      <li><strong>When two classes are equally important</strong>: PR would be the metrics to use if the goal of the model is to perform equally well on both classes. Image classification between cats and dogs is a good example because the performance on cats is equally important on dogs.</li>\n      <li><strong>When minority class is more important</strong>: PR would be the metrics to use if the focus of the model is to identify correctly as many positive samples as possible. Take spam detectors for example, the goal is to find all the possible spam emails. Regular emails are not of interest at all — they overshadow the number of positives.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>When two classes are equally important</strong>: PR would be the metrics to use if the goal of the model is to perform equally well on both classes. Image classification between cats and dogs is a good example because the performance on cats is equally important on dogs.</li>\n      <li><strong>When minority class is more important</strong>: PR would be the metrics to use if the focus of the model is to identify correctly as many positive samples as possible. Take spam detectors for example, the goal is to find all the possible spam emails. Regular emails are not of interest at all — they overshadow the number of positives.</li>\n    </ul>\n<h4 id=\"formulae\">Formulae</h4>\n<ul>\n  <li>\n    <p>Mathematically, precision and recall are defined as,</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Precision</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mspace linebreak=&quot;newline&quot; /></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-368\" style=\"width: 100%; display: inline-block; min-width: 10.94em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1009.12em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-369\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1009.12em, 4.898em, -999.997em); top: -4.008em; left: 50%; margin-left: -4.529em;\"><span class=\"mi\" id=\"MathJax-Span-370\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-372\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-373\"><span class=\"mi\" id=\"MathJax-Span-374\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.7em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.872em;\"><span class=\"mrow\" id=\"MathJax-Span-376\"><span class=\"mi\" id=\"MathJax-Span-377\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-378\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-379\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-381\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.86em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.857em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 50%; margin-left: 0em;\"><span class=\"mspace\" id=\"MathJax-Span-382\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 3.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Precision</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><mspace linebreak=\"newline\"></mspace></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-34\">\\operatorname {Precision}=\\frac{TP}{TP + FP} \\\\</script>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Recall</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mspace linebreak=&quot;newline&quot; /></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-383\" style=\"width: 100%; display: inline-block; min-width: 9.69em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.503em, 1008.08em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-384\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1008.08em, 4.898em, -999.997em); top: -4.008em; left: 50%; margin-left: -4.008em;\"><span class=\"mi\" id=\"MathJax-Span-385\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-386\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-387\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-388\"><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-391\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 50%; margin-left: 0em;\"><span class=\"mspace\" id=\"MathJax-Span-397\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 3.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Recall</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><mspace linebreak=\"newline\"></mspace></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-35\">\\operatorname{Recall}=\\frac{TP}{TP + FN} \\\\</script>\n\n    <ul>\n      <li>where,\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-398\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.2em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-399\"><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">TP</script> is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">FP</script> is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-406\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">FN</script> is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Mathematically, precision and recall are defined as,</p>\n<ul>\n      <li>where,\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-398\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.2em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-399\"><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">TP</script> is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">FP</script> is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-406\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">FN</script> is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-398\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.2em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-399\"><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">TP</script> is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">FP</script> is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-406\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">FN</script> is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/precision-and-recall-formulae.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>In the content of the robot sifting good apples from the rotten ones,</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Precision</mi><mo>=</mo><mfrac><mtext>&amp;#xA0;# of picked good apples&amp;#xA0;</mtext><mtext>&amp;#xA0;# of picked apples&amp;#xA0;</mtext></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-410\" style=\"width: 18.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1015.32em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-411\"><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-414\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 10.159em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1009.79em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -5.049em;\"><span class=\"mtext\" id=\"MathJax-Span-415\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;# of picked good apples&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1007.55em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -3.904em;\"><span class=\"mtext\" id=\"MathJax-Span-416\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;# of picked apples&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1010.16em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 10.159em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Precision</mi><mo>=</mo><mfrac><mtext>&nbsp;# of picked good apples&nbsp;</mtext><mtext>&nbsp;# of picked apples&nbsp;</mtext></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-39\">\\operatorname {Precision}=\\frac{\\text { # of picked good apples }}{\\text { # of picked apples }}</script>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Recall</mi><mo>=</mo><mfrac><mtext>&amp;#xA0;# of picked good apples&amp;#xA0;</mtext><mtext>&amp;#xA0;# of good apples&amp;#xA0;</mtext></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-417\" style=\"width: 16.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1014.12em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-418\"><span class=\"mi\" id=\"MathJax-Span-419\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-421\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 10.159em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1009.79em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -5.049em;\"><span class=\"mtext\" id=\"MathJax-Span-422\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;# of picked good apples&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1006.88em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -3.591em;\"><span class=\"mtext\" id=\"MathJax-Span-423\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;# of good apples&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1010.16em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 10.159em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Recall</mi><mo>=</mo><mfrac><mtext>&nbsp;# of picked good apples&nbsp;</mtext><mtext>&nbsp;# of good apples&nbsp;</mtext></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-40\">\\operatorname{Recall}=\\frac{\\text { # of picked good apples }}{\\text { # of good apples }}</script>\n  </li>\n  <li>\n    <p>In the context of information retrieval,</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Precision</mi><mo>=</mo><mfrac><mtext>&amp;#xA0;retrieved relevant documents&amp;#xA0;</mtext><mtext>&amp;#xA0;all retrieved documents&amp;#xA0;</mtext></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-424\" style=\"width: 20.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1017.4em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-425\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Regular;\">Precision</span><span class=\"mo\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-428\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 12.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1011.88em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -6.039em;\"><span class=\"mtext\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;retrieved relevant documents&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1009.64em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -4.945em;\"><span class=\"mtext\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;all retrieved documents&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1012.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 12.242em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Precision</mi><mo>=</mo><mfrac><mtext>&nbsp;retrieved relevant documents&nbsp;</mtext><mtext>&nbsp;all retrieved documents&nbsp;</mtext></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-41\">\\operatorname {Precision}=\\frac{\\text { retrieved relevant documents }}{\\text { all retrieved documents }}</script>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Recall</mi><mo>=</mo><mfrac><mtext>&amp;#xA0;retrieved relevant documents&amp;#xA0;</mtext><mtext>&amp;#xA0;all relevant documents&amp;#xA0;</mtext></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-431\" style=\"width: 19.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1016.2em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-432\"><span class=\"mi\" id=\"MathJax-Span-433\" style=\"font-family: STIXGeneral-Regular;\">Recall</span><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-435\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 12.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1011.88em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -6.039em;\"><span class=\"mtext\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;retrieved relevant documents&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1009.33em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -4.789em;\"><span class=\"mtext\" id=\"MathJax-Span-437\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;all relevant documents&nbsp;</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1012.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 12.242em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Recall</mi><mo>=</mo><mfrac><mtext>&nbsp;retrieved relevant documents&nbsp;</mtext><mtext>&nbsp;all relevant documents&nbsp;</mtext></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-42\">\\operatorname{Recall}=\\frac{\\text { retrieved relevant documents }}{\\text { all relevant documents }}</script>\n  </li>\n</ul>\n<p>In the content of the robot sifting good apples from the rotten ones,</p>\n<p>In the context of information retrieval,</p>\n<h4 id=\"precisionrecall-tradeoff\">Precision/Recall Tradeoff</h4>\n<ul>\n  <li>Depending on the problem at hand, you either care about high precision or high recall.</li>\n  <li>Examples of high precision:\n    <ul>\n      <li>For a model that detects shop lifters, the focus should be on developing a high precision model by reducing false positives (note that precision is given by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-438\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1002.61em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-439\"><span class=\"mfrac\" id=\"MathJax-Span-440\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-441\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-444\"><span class=\"mi\" id=\"MathJax-Span-445\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-447\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-448\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">\\frac{TP}{TP+FP}</script> and since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-450\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"mi\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">FP</script> features in the denominator, reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-454\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-455\"><span class=\"mi\" id=\"MathJax-Span-456\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">FP</script> leads to high precision). This implies that if we tag someone as a shop lifter, we’d like to make sure we do so with high confidence.</li>\n    </ul>\n  </li>\n  <li>Examples of high recall:\n    <ul>\n      <li>In an adult content detection problem, the focus should be on developing a high recall model by reducing false negatives (note that recall is given by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-458\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1002.66em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-459\"><span class=\"mfrac\" id=\"MathJax-Span-460\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-461\"><span class=\"mi\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-464\"><span class=\"mi\" id=\"MathJax-Span-465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-466\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-467\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">\\frac{TP}{TP+FN}</script> and since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-470\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-471\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">FN</script> features in the denominator, reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-474\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-475\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-477\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">FN</script> leads to high recall). This implies that if the model classified a video as good for kids (i.e., not having adult content), it should be marked so with high confidence.</li>\n      <li>In a disease detection scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model classified a patient as not having the disease, it should be done do with high confidence else it can prove fatal.</li>\n      <li>In an autonomous car driving scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model determined that there was no obstacle in the car’s surrounding radius, it should be done do with high confidence else fatalities can occur.</li>\n    </ul>\n  </li>\n  <li>Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. This is called the precision/recall tradeoff. However, in some scenarios, it is important to strike the right balance between both:\n    <ul>\n      <li>As an example (from the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">Precision and Recall</a>), brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>For a model that detects shop lifters, the focus should be on developing a high precision model by reducing false positives (note that precision is given by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-438\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1002.61em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-439\"><span class=\"mfrac\" id=\"MathJax-Span-440\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-441\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-444\"><span class=\"mi\" id=\"MathJax-Span-445\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-447\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-448\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.4em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.398em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">\\frac{TP}{TP+FP}</script> and since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-450\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"mi\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">FP</script> features in the denominator, reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-454\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.15em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-455\"><span class=\"mi\" id=\"MathJax-Span-456\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">FP</script> leads to high precision). This implies that if we tag someone as a shop lifter, we’d like to make sure we do so with high confidence.</li>\n    </ul>\n<ul>\n      <li>In an adult content detection problem, the focus should be on developing a high recall model by reducing false negatives (note that recall is given by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-458\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1002.66em, 2.711em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-459\"><span class=\"mfrac\" id=\"MathJax-Span-460\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.414em;\"><span class=\"mrow\" id=\"MathJax-Span-461\"><span class=\"mi\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.35em, 4.221em, -999.997em); top: -3.643em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-464\"><span class=\"mi\" id=\"MathJax-Span-465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-466\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-467\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.45em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.451em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">\\frac{TP}{TP+FN}</script> and since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-470\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-471\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">FN</script> features in the denominator, reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-474\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-475\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-477\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">FN</script> leads to high recall). This implies that if the model classified a video as good for kids (i.e., not having adult content), it should be marked so with high confidence.</li>\n      <li>In a disease detection scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model classified a patient as not having the disease, it should be done do with high confidence else it can prove fatal.</li>\n      <li>In an autonomous car driving scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model determined that there was no obstacle in the car’s surrounding radius, it should be done do with high confidence else fatalities can occur.</li>\n    </ul>\n<ul>\n      <li>As an example (from the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">Precision and Recall</a>), brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).</li>\n    </ul>",
    "contentMarkdown": "*   In case of an imbalanced dataset scenario, precision and recall are appropriate performance metrics. Precision and recall are typically juxtaposed together when reported.\n*   Precision is defined as the fraction of relevant instances among all retrieved instances.\n*   Recall, sometimes referred to as **[sensitivity](../evaluation-metrics/#sensitivity-and-specificity)**, is the fraction of retrieved instances among all relevant instances.\n*   Note that precision and recall are computed for each class. They are commonly used to evaluate the performance of classification or information retrieval systems.\n\n> A perfect classifier has precision and recall both equal to 1.\n\nA perfect classifier has precision and recall both equal to 1.\n\n*   It is often possible to calibrate the number of results returned by a model and improve precision at the expense of recall, or vice versa.\n*   Precision and recall should always be reported together and are not quoted individually. This is because it is easy to vary the sensitivity of a model to improve precision at the expense of recall, or vice versa.\n    *   As an example, imagine that the manufacturer of a pregnancy test needed to reach a certain level of precision, or of specificity, for FDA approval. The pregnancy test shows one line if it is moderately confident of the pregnancy, and a double line if it is very sure. If the manufacturer decides to only count the double lines as positives, the test will return far fewer positives overall, but the precision will improve, while the recall will go down. This shows why precision and recall should always be reported together.\n*   The figure below (taken from the Wikipedia article on [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)) shows a graphical representation of precision and recall:\n\n*   As an example, imagine that the manufacturer of a pregnancy test needed to reach a certain level of precision, or of specificity, for FDA approval. The pregnancy test shows one line if it is moderately confident of the pregnancy, and a double line if it is very sure. If the manufacturer decides to only count the double lines as positives, the test will return far fewer positives overall, but the precision will improve, while the recall will go down. This shows why precision and recall should always be reported together.\n\n![](/primers/ai/assets/evaluation-metrics/precision-and-recall.png)\n\n*   Formally, precision and recall can be defined as:\n    *   **Precision**: Out of all the samples marked positive, how many were actually positive (i.e., the true positives)?\n    *   **Recall**: Out of all the samples that are actually positive, how many were marked positive (i.e., the true positives)?\n    *   From the above definitions it is clear that with PR metrics, the focus is on the positive class (also called the relevant class).\n*   In the section on [Precision-Recall (PR) Curves](#precision-recall-curve), we explore how to get the best out of these two metrics using PR curves.\n    \n*   **Key takeaways**\n    *   Precision: how many **selected** items are **relevant**?\n    *   Recall: how many **relevant** items are **selected**?\n\n*   **Precision**: Out of all the samples marked positive, how many were actually positive (i.e., the true positives)?\n*   **Recall**: Out of all the samples that are actually positive, how many were marked positive (i.e., the true positives)?\n*   From the above definitions it is clear that with PR metrics, the focus is on the positive class (also called the relevant class).\n\nIn the section on [Precision-Recall (PR) Curves](#precision-recall-curve), we explore how to get the best out of these two metrics using PR curves.\n\n*   Precision: how many **selected** items are **relevant**?\n*   Recall: how many **relevant** items are **selected**?\n\n#### Historical Background\n\n*   This section is optional and offers a historical walk-through of how precision, recall and F1-score came about, so you may skip to the next section if so desired.\n*   Precision and recall were first defined by the American scientist Allen Kent and his colleagues in their 1955 paper Machine literature searching VIII. Operational criteria for designing information retrieval systems.\n*   Kent served in the US Army Air Corps in World War II, and was assigned after the war by the US military to a classified project at MIT in mechanized document encoding and search.\n*   In 1955, Kent and his colleagues Madeline Berry, Fred Luehrs, and J.W. Perry were working on a project in information retrieval using punch cards and reel-to-reel tapes. The team found a need to be able to quantify the performance of an information retrieval system objectively, allowing improvements in a system to be measured consistently, and so they published their definition of precision and recall.\n*   They described their ideas as a theory underlying the field of information retrieval, just as the second law of thermodynamics “underlies the design of a steam engine, regardless of its type or power rating”.\n*   Since then, the definitions of precision and recall have remained fundamentally the same, although for search engines the definitions have been modified to take into account certain nuances of human behavior, giving rise to the modified metrics precision @ kkk and mean average precision (mAP), which are the values normally quoted in information retrieval contexts today.\n*   In 1979, the Dutch computer science professor Cornelis Joost van Rijsbergen recognized the problems of defining search engine performance in terms of two numbers and decided on a convenient scalar function that combines the two. He called this metric the Effectiveness function and assigned it the letter E. This was later modified to the F1F1F\\_1 score, or FβFβF\\_{\\\\beta} score, which is still used today to summarize precision and recall.\n\n#### Examples\n\n*   Precision and recall can be best explained using examples. Consider the case of evaluating how well does a robot sifts good apples from rotten apples. A robot looks into the basket and picks out all the good apples, leaving the rotten apples behind, but is not perfect and could sometimes mistake a rotten apple for a good apple orange.\n*   After the robot finishes picking the good apples, precision and recall can be calculated as:\n    *   **Precision**: number of good apples picked out of all the picked apples.\n    *   **Recall**: number of good apples picked out of all possible good apples.\n*   **Precision** is about **exactness**, classifying only one instance correctly yields 100% precision, but a very low recall, it tells us how well the system identifies samples from a given class.\n*   **Recall** is about **completeness**, classifying all instances as positive yields 100% recall, but a very low precision, it tells how well the system does and identify all the samples from a given class.\n*   As another example, consider the task of information retrieval. As such, precision and recall can be calculated as:\n    *   **Precision**: number of relevant documents retrieved out of all retrieved documents.\n    *   **Recall**: number of relevant documents retrieved out of all relevant documents.\n\n*   **Precision**: number of good apples picked out of all the picked apples.\n*   **Recall**: number of good apples picked out of all possible good apples.\n\n*   **Precision**: number of relevant documents retrieved out of all retrieved documents.\n*   **Recall**: number of relevant documents retrieved out of all relevant documents.\n\n#### Applications\n\n*   Precision and recall are measured for every possible class in your dataset. So, **precision** and **recall** metrics are relatively much more appropriate (especially compared to accuracy) when dealing with **imbalanced classes**.\n\n> An important point to note is that PR are able to handle class imbalance in scenarios where the positive class (also called the minority class) is rare. If, however, the dataset is imbalanced in such a way that the negative class is the one that’s rare, PR curves are sub-optimal and can be misleading. In these cases, ROC curves might be a better fit.\n\nAn important point to note is that PR are able to handle class imbalance in scenarios where the positive class (also called the minority class) is rare. If, however, the dataset is imbalanced in such a way that the negative class is the one that’s rare, PR curves are sub-optimal and can be misleading. In these cases, ROC curves might be a better fit.\n\n*   So when do we use PR metrics? Here’s the typical use-cases:\n    *   **When two classes are equally important**: PR would be the metrics to use if the goal of the model is to perform equally well on both classes. Image classification between cats and dogs is a good example because the performance on cats is equally important on dogs.\n    *   **When minority class is more important**: PR would be the metrics to use if the focus of the model is to identify correctly as many positive samples as possible. Take spam detectors for example, the goal is to find all the possible spam emails. Regular emails are not of interest at all — they overshadow the number of positives.\n\n*   **When two classes are equally important**: PR would be the metrics to use if the goal of the model is to perform equally well on both classes. Image classification between cats and dogs is a good example because the performance on cats is equally important on dogs.\n*   **When minority class is more important**: PR would be the metrics to use if the focus of the model is to identify correctly as many positive samples as possible. Take spam detectors for example, the goal is to find all the possible spam emails. Regular emails are not of interest at all — they overshadow the number of positives.\n\n#### Formulae\n\n*   Mathematically, precision and recall are defined as,\n    \n    Precision\\=TPTP+FPPrecision\\=TPTP+FP\n    \n    \\\\operatorname {Precision}=\\\\frac{TP}{TP + FP} \\\\\\\\\n    \n    Recall\\=TPTP+FNRecall\\=TPTP+FN\n    \n    \\\\operatorname{Recall}=\\\\frac{TP}{TP + FN} \\\\\\\\\n    *   where,\n        *   TPTPTP is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.\n        *   FPFPFP is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.\n        *   FNFNFN is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.\n\nMathematically, precision and recall are defined as,\n\n*   where,\n    *   TPTPTP is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.\n    *   FPFPFP is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.\n    *   FNFNFN is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.\n\n*   TPTPTP is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.\n*   FPFPFP is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.\n*   FNFNFN is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.\n\n![](/primers/ai/assets/evaluation-metrics/precision-and-recall-formulae.jpg)\n\n*   In the content of the robot sifting good apples from the rotten ones,\n    \n    Precision\\= # of picked good apples  # of picked apples Precision\\= # of picked good apples  # of picked apples \n    \n    \\\\operatorname {Precision}=\\\\frac{\\\\text { # of picked good apples }}{\\\\text { # of picked apples }}\n    \n    Recall\\= # of picked good apples  # of good apples Recall\\= # of picked good apples  # of good apples \n    \n    \\\\operatorname{Recall}=\\\\frac{\\\\text { # of picked good apples }}{\\\\text { # of good apples }}\n*   In the context of information retrieval,\n    \n    Precision\\= retrieved relevant documents  all retrieved documents Precision\\= retrieved relevant documents  all retrieved documents \n    \n    \\\\operatorname {Precision}=\\\\frac{\\\\text { retrieved relevant documents }}{\\\\text { all retrieved documents }}\n    \n    Recall\\= retrieved relevant documents  all relevant documents Recall\\= retrieved relevant documents  all relevant documents \n    \n    \\\\operatorname{Recall}=\\\\frac{\\\\text { retrieved relevant documents }}{\\\\text { all relevant documents }}\n\nIn the content of the robot sifting good apples from the rotten ones,\n\nIn the context of information retrieval,\n\n#### Precision/Recall Tradeoff\n\n*   Depending on the problem at hand, you either care about high precision or high recall.\n*   Examples of high precision:\n    *   For a model that detects shop lifters, the focus should be on developing a high precision model by reducing false positives (note that precision is given by TPTP+FPTPTP+FP\\\\frac{TP}{TP+FP} and since FPFPFP features in the denominator, reducing FPFPFP leads to high precision). This implies that if we tag someone as a shop lifter, we’d like to make sure we do so with high confidence.\n*   Examples of high recall:\n    *   In an adult content detection problem, the focus should be on developing a high recall model by reducing false negatives (note that recall is given by TPTP+FNTPTP+FN\\\\frac{TP}{TP+FN} and since FNFNFN features in the denominator, reducing FNFNFN leads to high recall). This implies that if the model classified a video as good for kids (i.e., not having adult content), it should be marked so with high confidence.\n    *   In a disease detection scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model classified a patient as not having the disease, it should be done do with high confidence else it can prove fatal.\n    *   In an autonomous car driving scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model determined that there was no obstacle in the car’s surrounding radius, it should be done do with high confidence else fatalities can occur.\n*   Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. This is called the precision/recall tradeoff. However, in some scenarios, it is important to strike the right balance between both:\n    *   As an example (from the Wikipedia article on [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)), brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\n\n*   For a model that detects shop lifters, the focus should be on developing a high precision model by reducing false positives (note that precision is given by TPTP+FPTPTP+FP\\\\frac{TP}{TP+FP} and since FPFPFP features in the denominator, reducing FPFPFP leads to high precision). This implies that if we tag someone as a shop lifter, we’d like to make sure we do so with high confidence.\n\n*   In an adult content detection problem, the focus should be on developing a high recall model by reducing false negatives (note that recall is given by TPTP+FNTPTP+FN\\\\frac{TP}{TP+FN} and since FNFNFN features in the denominator, reducing FNFNFN leads to high recall). This implies that if the model classified a video as good for kids (i.e., not having adult content), it should be marked so with high confidence.\n*   In a disease detection scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model classified a patient as not having the disease, it should be done do with high confidence else it can prove fatal.\n*   In an autonomous car driving scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model determined that there was no obstacle in the car’s surrounding radius, it should be done do with high confidence else fatalities can occur.\n\n*   As an example (from the Wikipedia article on [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)), brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).",
    "contentLength": 83450,
    "wordCount": 2856,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#precision-and-recall"
  },
  {
    "id": "ai-speech-processing-receiver-operating-characteristic-roc-curve-28",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Receiver Operating Characteristic (ROC) Curve",
    "order": 28,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Suppose we have the probability prediction for each class in a multiclass classification problem, and as the next step, we need to calibrate the threshold on how to interpret the probabilities. Do we predict a positive outcome if the probability prediction is greater than 0.5 or 0.3? The Receiver Operating Characteristic (ROC) curve ROC helps answer this question.</li>\n  <li>Adjusting threshold values like this enables us to improve either precision or recall at the expense of the other. For this reason, it is useful to have a clear view of how the False Positive Rate and True Positive Rate vary together.</li>\n  <li>The ROC curve shows the variation of the error rates for all values of the manually-defined threshold. The curve is a plot of the <strong>False Positive Rate (also called the False Acceptance Rate) on the X-axis</strong> versus the <strong>True Positive Rate on the Y-axis</strong> for a number of different candidate threshold values between 0.0 and 1.0. A data analyst may plot the ROC curve and choose a threshold that gives a desirable balance between the false positives and false negatives.\n    <ul>\n      <li><strong>False Positive Rate (also called the False Acceptance Rate) on the X-axis</strong>: the False Positive Rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>False Positive Rate</mtext></mrow><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of negatives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-478\" style=\"width: 27.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 22.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1022.71em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-479\"><span class=\"texatom\" id=\"MathJax-Span-480\"><span class=\"mrow\" id=\"MathJax-Span-481\"><span class=\"mtext\" id=\"MathJax-Span-482\" style=\"font-family: STIXGeneral-Regular;\">False Positive Rate</span></span></span><span class=\"mo\" id=\"MathJax-Span-483\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-484\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.284em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1008.13em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -4.06em;\"><span class=\"mtext\" id=\"MathJax-Span-488\" style=\"font-family: STIXGeneral-Regular;\">number of negatives</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.28em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.284em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-489\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-490\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-491\"><span class=\"mi\" id=\"MathJax-Span-492\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-494\"><span class=\"mi\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>False Positive Rate</mtext></mrow><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of negatives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-49\">\\textrm{False Positive Rate} = \\frac{FP}{\\text{number of negatives}} = \\frac{FP}{FP+TN}</script>\n\n    <ul>\n      <li><strong>True Positive Rate on the Y-axis</strong>: the True Positive Rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>True Positive Rate</mtext></mrow><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of positives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-500\" style=\"width: 26.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 22.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1022.24em, 3.232em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-501\"><span class=\"texatom\" id=\"MathJax-Span-502\"><span class=\"mrow\" id=\"MathJax-Span-503\"><span class=\"mtext\" id=\"MathJax-Span-504\" style=\"font-family: STIXGeneral-Regular;\">True Positive Rate</span></span></span><span class=\"mo\" id=\"MathJax-Span-505\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-506\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-507\"><span class=\"mi\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1007.92em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -3.956em;\"><span class=\"mtext\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Regular;\">number of positives</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-512\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-513\"><span class=\"mi\" id=\"MathJax-Span-514\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-516\"><span class=\"mi\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>True Positive Rate</mtext></mrow><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of positives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-50\">\\textrm{True Positive Rate} = \\frac{FP}{\\text{number of positives}} = \\frac{TP}{TP+FN}</script>\n  </li>\n  <li>Note that both the False Positive Rate and the True Positive Rate are calculated for different probability thresholds.</li>\n  <li>As another example, if a search engine assigns a score to all candidate documents that it has retrieved, we can set the search engine to display all documents with a score greater than 10, or 11, or 12. The freedom to set this threshold value generates a smooth curve as below. The figure below shows a ROC curve for a binary classifier with AUC = 0.93. The orange line shows the model’s false positive and false negative rates, and the dotted blue line is the baseline of a random classifier with zero predictive power, achieving AUC = 0.5.</li>\n</ul>\n<ul>\n      <li><strong>False Positive Rate (also called the False Acceptance Rate) on the X-axis</strong>: the False Positive Rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.</li>\n    </ul>\n<ul>\n      <li><strong>True Positive Rate on the Y-axis</strong>: the True Positive Rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/precision-and-recall/ROC.svg\" alt=\"\"></p>\n<ul>\n  <li>Note that another way to obtain FPR and TPR is through TNR and FNR respectively, as follows:</li>\n</ul>\n<div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>T</mi><mi>N</mi><mi>R</mi><mspace linebreak=&quot;newline&quot; /><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>F</mi><mi>N</mi><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-522\" style=\"width: 100%; display: inline-block; min-width: 8.284em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.83em, 5.628em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-523\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.83em, 4.326em, -999.997em); top: -4.008em; left: 50%; margin-left: -3.435em;\"><span class=\"mi\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mi\" id=\"MathJax-Span-526\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-528\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-530\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-531\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1006.83em, 4.326em, -999.997em); top: -2.706em; left: 50%; margin-left: -3.435em;\"><span class=\"mspace\" id=\"MathJax-Span-533\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-534\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mi\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-539\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-542\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.809em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi>N</mi><mi>R</mi><mspace linebreak=\"newline\"></mspace><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>F</mi><mi>N</mi><mi>R</mi></math></span></span></div>\n<ul>\n  <li>Note that the equal error rate (EER) in ROC curves is obtained as follows:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/roc_eer.png\" alt=\"\"></p>\n<ul>\n  <li>Also, the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-543\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-544\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-546\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi><mo>=</mo><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">y = x</script> line in the ROC curve signifies the performance of a random classifier (so we need our curve to nudge towards the top-left to yield better AUC and thus better performance than a random classifier):</li>\n</ul>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/roc_perf.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Suppose we have the probability prediction for each class in a multiclass classification problem, and as the next step, we need to calibrate the threshold on how to interpret the probabilities. Do we predict a positive outcome if the probability prediction is greater than 0.5 or 0.3? The Receiver Operating Characteristic (ROC) curve ROC helps answer this question.\n*   Adjusting threshold values like this enables us to improve either precision or recall at the expense of the other. For this reason, it is useful to have a clear view of how the False Positive Rate and True Positive Rate vary together.\n*   The ROC curve shows the variation of the error rates for all values of the manually-defined threshold. The curve is a plot of the **False Positive Rate (also called the False Acceptance Rate) on the X-axis** versus the **True Positive Rate on the Y-axis** for a number of different candidate threshold values between 0.0 and 1.0. A data analyst may plot the ROC curve and choose a threshold that gives a desirable balance between the false positives and false negatives.\n    \n    *   **False Positive Rate (also called the False Acceptance Rate) on the X-axis**: the False Positive Rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n    \n    False Positive Rate\\=FPnumber of negatives\\=FPFP+TNFalse Positive Rate\\=FPnumber of negatives\\=FPFP+TN\n    \n    \\\\textrm{False Positive Rate} = \\\\frac{FP}{\\\\text{number of negatives}} = \\\\frac{FP}{FP+TN}\n    \n    *   **True Positive Rate on the Y-axis**: the True Positive Rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n    \n    True Positive Rate\\=FPnumber of positives\\=TPTP+FNTrue Positive Rate\\=FPnumber of positives\\=TPTP+FN\n    \n    \\\\textrm{True Positive Rate} = \\\\frac{FP}{\\\\text{number of positives}} = \\\\frac{TP}{TP+FN}\n*   Note that both the False Positive Rate and the True Positive Rate are calculated for different probability thresholds.\n*   As another example, if a search engine assigns a score to all candidate documents that it has retrieved, we can set the search engine to display all documents with a score greater than 10, or 11, or 12. The freedom to set this threshold value generates a smooth curve as below. The figure below shows a ROC curve for a binary classifier with AUC = 0.93. The orange line shows the model’s false positive and false negative rates, and the dotted blue line is the baseline of a random classifier with zero predictive power, achieving AUC = 0.5.\n\n*   **False Positive Rate (also called the False Acceptance Rate) on the X-axis**: the False Positive Rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n\n*   **True Positive Rate on the Y-axis**: the True Positive Rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n\n![](/primers/ai/assets/precision-and-recall/ROC.svg)\n\n*   Note that another way to obtain FPR and TPR is through TNR and FNR respectively, as follows:\n\nFPR\\=1−TNRTPR\\=1−FNRFPR\\=1−TNRTPR\\=1−FNR\n\n*   Note that the equal error rate (EER) in ROC curves is obtained as follows:\n\n![](/primers/ai/assets/evaluation-metrics/roc_eer.png)\n\n*   Also, the y\\=xy\\=xy = x line in the ROC curve signifies the performance of a random classifier (so we need our curve to nudge towards the top-left to yield better AUC and thus better performance than a random classifier):\n\n![](/primers/ai/assets/evaluation-metrics/roc_perf.png)",
    "contentLength": 21575,
    "wordCount": 614,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#receiver-operating-characteristic-(roc)-curve"
  },
  {
    "id": "ai-speech-processing-detection-error-tradeoff-det-curve-29",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Detection Error Tradeoff (DET) Curve",
    "order": 29,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>A detection error tradeoff (DET) curve is a graphical plot of error rates for binary classification systems, plotting the false rejection rate (FRR) vs. false acceptance rate (FAR) for different probability thresholds.</li>\n  <li>The X- and Y-axes are scaled non-linearly by their standard normal deviates (or just by logarithmic transformation), yielding tradeoff curves that are more linear than ROC curves, and use most of the image area to highlight the differences of importance in the critical operating region.</li>\n</ul>\n<h4 id=\"comparing-roc-and-det-curves\">Comparing ROC and DET Curves</h4>\n<ul>\n  <li>\n    <p>Let’s compare receiver operating characteristic (ROC) and detection error tradeoff (DET) curves for different classification algorithms for the same classification task.</p>\n  </li>\n  <li>\n    <p>DET curves are commonly plotted in normal deviate scale. To achieve this the DET display transforms the error rates as returned by sklearn’s <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve\">det_curve</a> and the axis scale using <code class=\"language-plaintext highlighter-rouge\">scipy.stats.norm</code>.</p>\n  </li>\n  <li>\n    <p>The point of this example is to demonstrate two properties of DET curves, namely:</p>\n\n    <ul>\n      <li>\n        <p>It might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.</p>\n      </li>\n      <li>\n        <p>DET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>The plots in this example compare the ROC curve on the left with the corresponding DET curve on the right. There is no particular reason why these classifiers have been chosen for the example plot over other classifiers available in scikit-learn.</p>\n  </li>\n</ul>\n<p>Let’s compare receiver operating characteristic (ROC) and detection error tradeoff (DET) curves for different classification algorithms for the same classification task.</p>\n<p>DET curves are commonly plotted in normal deviate scale. To achieve this the DET display transforms the error rates as returned by sklearn’s <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve\">det_curve</a> and the axis scale using <code class=\"language-plaintext highlighter-rouge\">scipy.stats.norm</code>.</p>\n<p>The point of this example is to demonstrate two properties of DET curves, namely:</p>\n<ul>\n      <li>\n        <p>It might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.</p>\n      </li>\n      <li>\n        <p>DET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).</p>\n      </li>\n    </ul>\n<p>It might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.</p>\n<p>DET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).</p>\n<p>The plots in this example compare the ROC curve on the left with the corresponding DET curve on the right. There is no particular reason why these classifiers have been chosen for the example plot over other classifiers available in scikit-learn.</p>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/det.png\" alt=\"\"></p>\n<ul>\n  <li>Note that the equal error rate (EER) in DET curves is the intersection of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-548\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-549\"><span class=\"mi\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-552\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi><mo>=</mo><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">y = x</script> line with the DET curve:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/evaluation-metrics/det_eer.png\" alt=\"\"></p>\n<ul>\n  <li>To generate DET curves using scikit-learn:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">det_curve</span>\n\n<span class=\"n\">y_true</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n<span class=\"n\">y_scores</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.35</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">])</span>\n\n<span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">fnr</span><span class=\"p\">,</span> <span class=\"n\">thresholds</span> <span class=\"o\">=</span> <span class=\"n\">det_curve</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_scores</span><span class=\"p\">)</span>\n<span class=\"c1\"># fpr: \t\t\tarray([0.5, 0.5, 0. ])\n# fnr: \t\t\tarray([0. , 0.5, 0.5])\n# thresholds: \tarray([0.35, 0.4 , 0.8 ])\n</span></code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">det_curve</span>\n\n<span class=\"n\">y_true</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n<span class=\"n\">y_scores</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.35</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">])</span>\n\n<span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">fnr</span><span class=\"p\">,</span> <span class=\"n\">thresholds</span> <span class=\"o\">=</span> <span class=\"n\">det_curve</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_scores</span><span class=\"p\">)</span>\n<span class=\"c1\"># fpr: \t\t\tarray([0.5, 0.5, 0. ])\n# fnr: \t\t\tarray([0. , 0.5, 0.5])\n# thresholds: \tarray([0.35, 0.4 , 0.8 ])\n</span></code></pre>\n<ul>\n  <li>\n    <p>Note the formulae to obtain FAR (FPR) and FRR (FNR):</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>F</mi><mi>A</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of negatives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac><mspace linebreak=&quot;newline&quot; /><mi>F</mi><mi>R</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>N</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of positives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>N</mi></mrow><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-553\" style=\"width: 100%; display: inline-block; min-width: 24.534em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(4.43em, 1020.42em, 9.742em, -999.997em); top: -5.935em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(2.503em, 1020.42em, 5.055em, -999.997em); top: -4.008em; left: 50%; margin-left: -10.206em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-556\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-557\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-558\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-562\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-563\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.284em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-564\"><span class=\"mi\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-566\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1008.13em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -4.06em;\"><span class=\"mtext\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Regular;\">number of negatives</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.28em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.284em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-568\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-569\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-570\"><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-573\"><span class=\"mi\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-575\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-576\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-577\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.503em, 1020.26em, 5.055em, -999.997em); top: -1.299em; left: 50%; margin-left: -10.154em;\"><span class=\"mspace\" id=\"MathJax-Span-579\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-581\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-583\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-588\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-589\"><span class=\"mi\" id=\"MathJax-Span-590\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1007.92em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -3.956em;\"><span class=\"mtext\" id=\"MathJax-Span-592\" style=\"font-family: STIXGeneral-Regular;\">number of positives</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-594\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.36em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.674em;\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"mi\" id=\"MathJax-Span-596\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-598\"><span class=\"mi\" id=\"MathJax-Span-599\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 5.94em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -4.434em; border-left: 0px solid; width: 0px; height: 6.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>F</mi><mi>A</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of negatives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac><mspace linebreak=\"newline\"></mspace><mi>F</mi><mi>R</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>N</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mtext>number of positives</mtext></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>N</mi></mrow><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-54\">FAR = FPR = \\frac{FP}{\\text{number of negatives}} = \\frac{FP}{FP + TN} \\\\\n  FRR = FNR = \\frac{FP}{\\text{number of positives}} = \\frac{FN}{FN + TP}</script>\n\n    <ul>\n      <li>where, FP: False positive; FN: False Negative; TN: True Negative; TP: True Positive</li>\n    </ul>\n  </li>\n  <li>\n    <p>Another way to obtain FAR and FRR is through TNR and TPR respectively, as follows:</p>\n  </li>\n</ul>\n<p>Note the formulae to obtain FAR (FPR) and FRR (FNR):</p>\n<ul>\n      <li>where, FP: False positive; FN: False Negative; TN: True Negative; TP: True Positive</li>\n    </ul>\n<p>Another way to obtain FAR and FRR is through TNR and TPR respectively, as follows:</p>\n<div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>F</mi><mi>A</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>T</mi><mi>N</mi><mi>R</mi><mspace linebreak=&quot;newline&quot; /><mi>F</mi><mi>R</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>T</mi><mi>P</mi><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-604\" style=\"width: 100%; display: inline-block; min-width: 8.284em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.83em, 5.628em, -999.997em); top: -4.008em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-605\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.83em, 4.326em, -999.997em); top: -4.008em; left: 50%; margin-left: -3.435em;\"><span class=\"mi\" id=\"MathJax-Span-606\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-609\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1006.72em, 4.326em, -999.997em); top: -2.706em; left: 50%; margin-left: -3.383em;\"><span class=\"mspace\" id=\"MathJax-Span-615\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-617\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">R</span><span class=\"mo\" id=\"MathJax-Span-619\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-621\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-623\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.809em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>F</mi><mi>A</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi>N</mi><mi>R</mi><mspace linebreak=\"newline\"></mspace><mi>F</mi><mi>R</mi><mi>R</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi>P</mi><mi>R</mi></math></span></span></div>\n<h4 id=\"area-under-the-roc-curve-auc\">Area Under the ROC Curve (AUC)</h4>\n<ul>\n  <li>The area under the ROC curve (AUROC) is a good metric for measuring the classifier’s performance. This value is normally between 0.5 (for a bad classifier) and 1.0 (a perfect classifier). The better the classifier, the higher the AUC and the closer the ROC curve will be to the top left corner.</li>\n</ul>",
    "contentMarkdown": "*   A detection error tradeoff (DET) curve is a graphical plot of error rates for binary classification systems, plotting the false rejection rate (FRR) vs. false acceptance rate (FAR) for different probability thresholds.\n*   The X- and Y-axes are scaled non-linearly by their standard normal deviates (or just by logarithmic transformation), yielding tradeoff curves that are more linear than ROC curves, and use most of the image area to highlight the differences of importance in the critical operating region.\n\n#### Comparing ROC and DET Curves\n\n*   Let’s compare receiver operating characteristic (ROC) and detection error tradeoff (DET) curves for different classification algorithms for the same classification task.\n    \n*   DET curves are commonly plotted in normal deviate scale. To achieve this the DET display transforms the error rates as returned by sklearn’s [det\\_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve) and the axis scale using `scipy.stats.norm`.\n    \n*   The point of this example is to demonstrate two properties of DET curves, namely:\n    \n    *   It might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.\n        \n    *   DET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).\n        \n*   The plots in this example compare the ROC curve on the left with the corresponding DET curve on the right. There is no particular reason why these classifiers have been chosen for the example plot over other classifiers available in scikit-learn.\n    \n\nLet’s compare receiver operating characteristic (ROC) and detection error tradeoff (DET) curves for different classification algorithms for the same classification task.\n\nDET curves are commonly plotted in normal deviate scale. To achieve this the DET display transforms the error rates as returned by sklearn’s [det\\_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve) and the axis scale using `scipy.stats.norm`.\n\nThe point of this example is to demonstrate two properties of DET curves, namely:\n\n*   It might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.\n    \n*   DET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).\n    \n\nIt might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.\n\nDET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).\n\nThe plots in this example compare the ROC curve on the left with the corresponding DET curve on the right. There is no particular reason why these classifiers have been chosen for the example plot over other classifiers available in scikit-learn.\n\n![](/primers/ai/assets/evaluation-metrics/det.png)\n\n*   Note that the equal error rate (EER) in DET curves is the intersection of the y\\=xy\\=xy = x line with the DET curve:\n\n![](/primers/ai/assets/evaluation-metrics/det_eer.png)\n\n*   To generate DET curves using scikit-learn:\n\n![](https://aman.ai/images/copy.png)\n\n`import numpy as np from sklearn.metrics import det_curve  y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8])  fpr, fnr, thresholds = det_curve(y_true, y_scores) # fpr: \t\t\tarray([0.5, 0.5, 0. ]) # fnr: \t\t\tarray([0. , 0.5, 0.5]) # thresholds: \tarray([0.35, 0.4 , 0.8 ])`\n\n![](https://aman.ai/images/copy.png)\n\n`import numpy as np from sklearn.metrics import det_curve  y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8])  fpr, fnr, thresholds = det_curve(y_true, y_scores) # fpr: \t\t\tarray([0.5, 0.5, 0. ]) # fnr: \t\t\tarray([0. , 0.5, 0.5]) # thresholds: \tarray([0.35, 0.4 , 0.8 ])`\n\n*   Note the formulae to obtain FAR (FPR) and FRR (FNR):\n    \n    FAR\\=FPR\\=FPnumber of negatives\\=FPFP+TNFRR\\=FNR\\=FPnumber of positives\\=FNFN+TPFAR\\=FPR\\=FPnumber of negatives\\=FPFP+TNFRR\\=FNR\\=FPnumber of positives\\=FNFN+TP\n    \n    FAR = FPR = \\\\frac{FP}{\\\\text{number of negatives}} = \\\\frac{FP}{FP + TN} \\\\\\\\ FRR = FNR = \\\\frac{FP}{\\\\text{number of positives}} = \\\\frac{FN}{FN + TP}\n    *   where, FP: False positive; FN: False Negative; TN: True Negative; TP: True Positive\n*   Another way to obtain FAR and FRR is through TNR and TPR respectively, as follows:\n    \n\nNote the formulae to obtain FAR (FPR) and FRR (FNR):\n\n*   where, FP: False positive; FN: False Negative; TN: True Negative; TP: True Positive\n\nAnother way to obtain FAR and FRR is through TNR and TPR respectively, as follows:\n\nFAR\\=1−TNRFRR\\=1−TPRFAR\\=1−TNRFRR\\=1−TPR\n\n#### Area Under the ROC Curve (AUC)\n\n*   The area under the ROC curve (AUROC) is a good metric for measuring the classifier’s performance. This value is normally between 0.5 (for a bad classifier) and 1.0 (a perfect classifier). The better the classifier, the higher the AUC and the closer the ROC curve will be to the top left corner.",
    "contentLength": 28520,
    "wordCount": 1011,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#detection-error-tradeoff-(det)-curve"
  },
  {
    "id": "ai-speech-processing-precision-recall-curve-30",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Precision-Recall Curve",
    "order": 30,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>We saw <a href=\"#precision-and-recall\">earlier</a> that when a dataset has imbalanced classes, precision and recall are better metrics than accuracy. Similarly, for imbalanced classes, a Precision-Recall curve is more suitable than a ROC curve.</li>\n  <li>A Precision-Recall curve is a plot of the <strong>Precision</strong> (y-axis) and the <strong>Recall</strong> (x-axis) for different thresholds, much like the ROC curve. Note that in computing precision and recall there is never a use of the true negatives, these measures only consider correct predictions.</li>\n</ul>\n<h4 id=\"area-under-the-pr-curve-auc\">Area Under the PR Curve (AUC)</h4>\n<ul>\n  <li>Similar to the <a href=\"#area-under-the-roc-curve-auc\">AUROC</a>, the PR AUC summarizes the curve with a range of threshold values as a single score.</li>\n  <li>The score can then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill.</li>\n</ul>",
    "contentMarkdown": "*   We saw [earlier](#precision-and-recall) that when a dataset has imbalanced classes, precision and recall are better metrics than accuracy. Similarly, for imbalanced classes, a Precision-Recall curve is more suitable than a ROC curve.\n*   A Precision-Recall curve is a plot of the **Precision** (y-axis) and the **Recall** (x-axis) for different thresholds, much like the ROC curve. Note that in computing precision and recall there is never a use of the true negatives, these measures only consider correct predictions.\n\n#### Area Under the PR Curve (AUC)\n\n*   Similar to the [AUROC](#area-under-the-roc-curve-auc), the PR AUC summarizes the curve with a range of threshold values as a single score.\n*   The score can then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill.",
    "contentLength": 1012,
    "wordCount": 137,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#precision-recall-curve"
  },
  {
    "id": "ai-speech-processing-key-takeaways-precision-recall-and-rocpr-curves-31",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Key Takeaways: Precision, Recall and ROC/PR Curves",
    "order": 31,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li><strong>ROC Curve</strong>: summaries the trade-off between the True Positive Rate and False Positive Rate for a predictive model using different probability thresholds.</li>\n  <li><strong>Precision-Recall Curve</strong>: summaries the trade-off between the True Positive Rate and the positive predictive value for a predictive model using different probability thresholds.</li>\n  <li>In the same way it is better to rely on <strong>precision</strong> and <strong>recall</strong> rather than <strong>accuracy</strong> in an imbalanced dataset scenario (since it can offer you an incorrect picture of the classifier’s performance), a Precision-Recall curve is better to calibrate the probability threshold compared to the ROC curve. In other words, ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases, the area under the curve (AUC) can be used as a summary of the model performance.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n <thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Metric</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Formula</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Accuracy</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-625\" style=\"width: 10.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.11em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1009.11em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-626\"><span class=\"mfrac\" id=\"MathJax-Span-627\"><span style=\"display: inline-block; position: relative; width: 8.872em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1003.75em, 4.229em, -999.997em); top: -4.64em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-628\"><span class=\"mi\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-631\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1008.75em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -4.342em;\"><span class=\"mrow\" id=\"MathJax-Span-634\"><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-637\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-638\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-640\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-641\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.87em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.872em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-56\">\\frac{TP+TN}{TP+TN + FP+FN}</script></td>\n<td class=\"tg-tleft-valign-second\">Overall performance of model</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Precision</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-646\" style=\"width: 4.884em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.051em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.05em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-647\"><span class=\"mfrac\" id=\"MathJax-Span-648\"><span style=\"display: inline-block; position: relative; width: 3.812em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.25em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.592em;\"><span class=\"mrow\" id=\"MathJax-Span-649\"><span class=\"mi\" id=\"MathJax-Span-650\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.69em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.842em;\"><span class=\"mrow\" id=\"MathJax-Span-652\"><span class=\"mi\" id=\"MathJax-Span-653\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-656\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-657\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.81em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.812em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-57\">\\frac{TP}{TP + FP}</script>  </td>\n<td class=\"tg-tleft-valign-second\">How accurate the positive predictions are</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Recall/Sensitivity</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.17em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.17em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mfrac\" id=\"MathJax-Span-660\"><span style=\"display: inline-block; position: relative; width: 3.932em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.25em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.592em;\"><span class=\"mrow\" id=\"MathJax-Span-661\"><span class=\"mi\" id=\"MathJax-Span-662\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.81em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-664\"><span class=\"mi\" id=\"MathJax-Span-665\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-666\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.93em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.932em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-58\">\\frac{TP}{TP + FN}</script></td>\n<td class=\"tg-tleft-valign-second\">Coverage of actual positive sample</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specificity</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-670\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.17em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.17em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-671\"><span class=\"mfrac\" id=\"MathJax-Span-672\"><span style=\"display: inline-block; position: relative; width: 3.932em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.37em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.652em;\"><span class=\"mrow\" id=\"MathJax-Span-673\"><span class=\"mi\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.75em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-676\"><span class=\"mi\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-679\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.93em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.932em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-59\">\\frac{TN}{TN + FP}</script></td>\n<td class=\"tg-tleft-valign-second\">Coverage of actual negative sample</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">F1-score</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>2</mn><mo>&amp;#x00D7;</mo><mfrac><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Precision</mtext></mrow><mo>&amp;#x00D7;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Recall</mtext></mrow></mrow><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Precision</mtext></mrow><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Recall</mtext></mrow></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-682\" style=\"width: 11.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.348em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1009.35em, 3.158em, -999.997em); top: -2.259em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-683\"><span class=\"mn\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">×</span><span class=\"mfrac\" id=\"MathJax-Span-686\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 7.562em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1007.38em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -3.688em;\"><span class=\"mrow\" id=\"MathJax-Span-687\"><span class=\"texatom\" id=\"MathJax-Span-688\"><span class=\"mrow\" id=\"MathJax-Span-689\"><span class=\"mtext\" id=\"MathJax-Span-690\" style=\"font-family: STIXGeneral-Regular;\">Precision</span></span></span><span class=\"mo\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">×</span><span class=\"texatom\" id=\"MathJax-Span-692\" style=\"padding-left: 0.241em;\"><span class=\"mrow\" id=\"MathJax-Span-693\"><span class=\"mtext\" id=\"MathJax-Span-694\" style=\"font-family: STIXGeneral-Regular;\">Recall</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1007.44em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -3.747em;\"><span class=\"mrow\" id=\"MathJax-Span-695\"><span class=\"texatom\" id=\"MathJax-Span-696\"><span class=\"mrow\" id=\"MathJax-Span-697\"><span class=\"mtext\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Regular;\">Precision</span></span></span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"texatom\" id=\"MathJax-Span-700\" style=\"padding-left: 0.241em;\"><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"mtext\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Regular;\">Recall</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.56em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.562em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.265em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>2</mn><mo>×</mo><mfrac><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Precision</mtext></mrow><mo>×</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Recall</mtext></mrow></mrow><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Precision</mtext></mrow><mo>+</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Recall</mtext></mrow></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-60\">2 \\times\\frac{\\textrm{Precision} \\times \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}}</script></td>\n<td class=\"tg-tleft-valign-second\">Harmonic mean of Precision and Recall</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n <thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Metric</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Formula</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Accuracy</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-625\" style=\"width: 10.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.11em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1009.11em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-626\"><span class=\"mfrac\" id=\"MathJax-Span-627\"><span style=\"display: inline-block; position: relative; width: 8.872em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1003.75em, 4.229em, -999.997em); top: -4.64em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-628\"><span class=\"mi\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-631\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1008.75em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -4.342em;\"><span class=\"mrow\" id=\"MathJax-Span-634\"><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-637\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-638\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-640\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-641\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.87em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.872em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-56\">\\frac{TP+TN}{TP+TN + FP+FN}</script></td>\n<td class=\"tg-tleft-valign-second\">Overall performance of model</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Precision</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-646\" style=\"width: 4.884em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.051em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.05em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-647\"><span class=\"mfrac\" id=\"MathJax-Span-648\"><span style=\"display: inline-block; position: relative; width: 3.812em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.25em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.592em;\"><span class=\"mrow\" id=\"MathJax-Span-649\"><span class=\"mi\" id=\"MathJax-Span-650\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.69em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.842em;\"><span class=\"mrow\" id=\"MathJax-Span-652\"><span class=\"mi\" id=\"MathJax-Span-653\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-656\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-657\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.81em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.812em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-57\">\\frac{TP}{TP + FP}</script>  </td>\n<td class=\"tg-tleft-valign-second\">How accurate the positive predictions are</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Recall/Sensitivity</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.17em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.17em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mfrac\" id=\"MathJax-Span-660\"><span style=\"display: inline-block; position: relative; width: 3.932em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.25em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.592em;\"><span class=\"mrow\" id=\"MathJax-Span-661\"><span class=\"mi\" id=\"MathJax-Span-662\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.81em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-664\"><span class=\"mi\" id=\"MathJax-Span-665\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-666\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.93em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.932em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-58\">\\frac{TP}{TP + FN}</script></td>\n<td class=\"tg-tleft-valign-second\">Coverage of actual positive sample</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specificity</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-670\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.17em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1004.17em, 3.098em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-671\"><span class=\"mfrac\" id=\"MathJax-Span-672\"><span style=\"display: inline-block; position: relative; width: 3.932em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1001.37em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -0.652em;\"><span class=\"mrow\" id=\"MathJax-Span-673\"><span class=\"mi\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1003.75em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -1.902em;\"><span class=\"mrow\" id=\"MathJax-Span-676\"><span class=\"mi\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-679\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.93em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.932em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-59\">\\frac{TN}{TN + FP}</script></td>\n<td class=\"tg-tleft-valign-second\">Coverage of actual negative sample</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">F1-score</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>2</mn><mo>&amp;#x00D7;</mo><mfrac><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Precision</mtext></mrow><mo>&amp;#x00D7;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Recall</mtext></mrow></mrow><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Precision</mtext></mrow><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>Recall</mtext></mrow></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-682\" style=\"width: 11.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.348em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.717em, 1009.35em, 3.158em, -999.997em); top: -2.259em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-683\"><span class=\"mn\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">×</span><span class=\"mfrac\" id=\"MathJax-Span-686\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 7.562em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;\"><span style=\"position: absolute; clip: rect(3.158em, 1007.38em, 4.17em, -999.997em); top: -4.64em; left: 50%; margin-left: -3.688em;\"><span class=\"mrow\" id=\"MathJax-Span-687\"><span class=\"texatom\" id=\"MathJax-Span-688\"><span class=\"mrow\" id=\"MathJax-Span-689\"><span class=\"mtext\" id=\"MathJax-Span-690\" style=\"font-family: STIXGeneral-Regular;\">Precision</span></span></span><span class=\"mo\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">×</span><span class=\"texatom\" id=\"MathJax-Span-692\" style=\"padding-left: 0.241em;\"><span class=\"mrow\" id=\"MathJax-Span-693\"><span class=\"mtext\" id=\"MathJax-Span-694\" style=\"font-family: STIXGeneral-Regular;\">Recall</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.158em, 1007.44em, 4.229em, -999.997em); top: -3.271em; left: 50%; margin-left: -3.747em;\"><span class=\"mrow\" id=\"MathJax-Span-695\"><span class=\"texatom\" id=\"MathJax-Span-696\"><span class=\"mrow\" id=\"MathJax-Span-697\"><span class=\"mtext\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Regular;\">Precision</span></span></span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"texatom\" id=\"MathJax-Span-700\" style=\"padding-left: 0.241em;\"><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"mtext\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Regular;\">Recall</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.56em, 1.253em, -999.997em); top: -1.307em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.562em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.074em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.265em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.925em; border-left: 0px solid; width: 0px; height: 2.646em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>2</mn><mo>×</mo><mfrac><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Precision</mtext></mrow><mo>×</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Recall</mtext></mrow></mrow><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Precision</mtext></mrow><mo>+</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>Recall</mtext></mrow></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-60\">2 \\times\\frac{\\textrm{Precision} \\times \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}}</script></td>\n<td class=\"tg-tleft-valign-second\">Harmonic mean of Precision and Recall</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "*   **ROC Curve**: summaries the trade-off between the True Positive Rate and False Positive Rate for a predictive model using different probability thresholds.\n*   **Precision-Recall Curve**: summaries the trade-off between the True Positive Rate and the positive predictive value for a predictive model using different probability thresholds.\n*   In the same way it is better to rely on **precision** and **recall** rather than **accuracy** in an imbalanced dataset scenario (since it can offer you an incorrect picture of the classifier’s performance), a Precision-Recall curve is better to calibrate the probability threshold compared to the ROC curve. In other words, ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases, the area under the curve (AUC) can be used as a summary of the model performance.\n\n**Metric**\n\n**Formula**\n\n**Description**\n\nAccuracy\n\nTP+TNTP+TN+FP+FNTP+TNTP+TN+FP+FN\n\n\\\\frac{TP+TN}{TP+TN + FP+FN}\n\nOverall performance of model\n\nPrecision\n\nTPTP+FPTPTP+FP\n\n\\\\frac{TP}{TP + FP}\n\nHow accurate the positive predictions are\n\nRecall/Sensitivity\n\nTPTP+FNTPTP+FN\n\n\\\\frac{TP}{TP + FN}\n\nCoverage of actual positive sample\n\nSpecificity\n\nTNTN+FPTNTN+FP\n\n\\\\frac{TN}{TN + FP}\n\nCoverage of actual negative sample\n\nF1-score\n\n2×Precision×RecallPrecision+Recall2×Precision×RecallPrecision+Recall\n\n2 \\\\times\\\\frac{\\\\textrm{Precision} \\\\times \\\\textrm{Recall}}{\\\\textrm{Precision} + \\\\textrm{Recall}}\n\nHarmonic mean of Precision and Recall\n\n**Metric**\n\n**Formula**\n\n**Description**\n\nAccuracy\n\nTP+TNTP+TN+FP+FNTP+TNTP+TN+FP+FN\n\n\\\\frac{TP+TN}{TP+TN + FP+FN}\n\nOverall performance of model\n\nPrecision\n\nTPTP+FPTPTP+FP\n\n\\\\frac{TP}{TP + FP}\n\nHow accurate the positive predictions are\n\nRecall/Sensitivity\n\nTPTP+FNTPTP+FN\n\n\\\\frac{TP}{TP + FN}\n\nCoverage of actual positive sample\n\nSpecificity\n\nTNTN+FPTNTN+FP\n\n\\\\frac{TN}{TN + FP}\n\nCoverage of actual negative sample\n\nF1-score\n\n2×Precision×RecallPrecision+Recall2×Precision×RecallPrecision+Recall\n\n2 \\\\times\\\\frac{\\\\textrm{Precision} \\\\times \\\\textrm{Recall}}{\\\\textrm{Precision} + \\\\textrm{Recall}}\n\nHarmonic mean of Precision and Recall",
    "contentLength": 43675,
    "wordCount": 252,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#key-takeaways:-precision,-recall-and-roc/pr-curves"
  },
  {
    "id": "ai-speech-processing-speech-recognition-32",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Speech Recognition",
    "order": 32,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li><a href=\"https://en.wikipedia.org/wiki/Word_error_rate\">Word Error Rate (WER)</a></li>\n  <li><a href=\"https://huggingface.co/spaces/evaluate-metric/cer\">Character Error Rate (CER)</a></li>\n</ul>",
    "contentMarkdown": "*   [Word Error Rate (WER)](https://en.wikipedia.org/wiki/Word_error_rate)\n*   [Character Error Rate (CER)](https://huggingface.co/spaces/evaluate-metric/cer)",
    "contentLength": 205,
    "wordCount": 10,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#speech-recognition"
  },
  {
    "id": "ai-speech-processing-speech-to-speech-machine-translation-33",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Speech-to-Speech Machine Translation",
    "order": 33,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li><a href=\"https://en.wikipedia.org/wiki/BLEU\">BLEU (BiLingual Evaluation Understudy)</a></li>\n  <li><a href=\"https://en.wikipedia.org/wiki/METEOR\">METEOR (Metric for Evaluation of Translation with Explicit ORdering)</a></li>\n</ul>",
    "contentMarkdown": "*   [BLEU (BiLingual Evaluation Understudy)](https://en.wikipedia.org/wiki/BLEU)\n*   [METEOR (Metric for Evaluation of Translation with Explicit ORdering)](https://en.wikipedia.org/wiki/METEOR)",
    "contentLength": 240,
    "wordCount": 15,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#speech-to-speech-machine-translation"
  },
  {
    "id": "ai-speech-processing-manual-evaluation-by-humans-for-fluency-grammar-co-34",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Evaluation Metrics",
    "title": "Manual Evaluation by Humans for Fluency, Grammar, Comparative Ranking, Etc.",
    "order": 34,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li><a href=\"https://en.wikipedia.org/wiki/Mean_opinion_score\">Mean Opinion Score (MOS)</a></li>\n</ul>",
    "contentMarkdown": "*   [Mean Opinion Score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score)",
    "contentLength": 109,
    "wordCount": 5,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#manual-evaluation-by-humans-for-fluency,-grammar,-comparative-ranking,-etc."
  },
  {
    "id": "ai-speech-processing-distil-whisper-35",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Popular Models",
    "title": "Distil-Whisper",
    "order": 35,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>HuggingFace’s <a href=\"https://github.com/huggingface/distil-whisper/blob/main/Distil_Whisper.pdf\">distil-whisper</a>: Distilled OpenAI’s Whisper on 20,000 hours of open-sourced audio data.</li>\n  <li>Distil-Whisper is significantly smaller and faster (5.8 times faster, 51% fewer parameters) than the original Whisper model, maintaining similar performance (within 1% WER) on out-of-distribution test data in a zero-shot setting.</li>\n  <li>The authors use a large-scale pseudo-labelling approach to assemble an open-source dataset for training, selecting high-quality pseudo-labels based on a word error rate (WER) heuristic.</li>\n  <li>The motivation was the fact that OpenAI’s Whisper yields astonishing accuracy for most audio, but it’s too slow and expensive for most production use cases. In addition, it has a tendency to hallucinate.</li>\n  <li>Encoding takes <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-703\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-704\"><span class=\"mi\" id=\"MathJax-Span-705\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-707\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">O(1)</script> passes while decoding takes <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-709\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-712\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-713\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-714\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">O(N)</script>. This implies that reducing decoder layers is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-715\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-716\"><span class=\"mi\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">N</script> time more effective. They kept the whole encoder, but utilized only two decoder layers.</li>\n  <li>The encoder is frozen during distillation to ensure Whisper’s robustness to noise is kept.</li>\n  <li>The model demonstrates improved robustness against hallucination errors in long-form audio, and its design allows it to be paired with Whisper for speculative decoding, doubling the inference speed while maintaining output accuracy.</li>\n  <li>The paper highlights the utility of large-scale pseudo-labelling in speech recognition and the effectiveness of the WER threshold filter in distillation. The training and inference code, along with the models, are made publicly available by the authors.</li>\n  <li>To make sure Distil-Whisper does not inherit hallucinations, they filtered out all data samples below a certain WER threshold. By doing so, we were able to reduce hallucinations and actually beat the teacher on long-form audio evaluation.</li>\n  <li>The checkpoints are <a href=\"https://github.com/huggingface/distil-whisper\">here</a> and also directly <a href=\"https://huggingface.co/distil-whisper\">available</a> in 🤗 Transformers. All with MIT License.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/speech/distil-whisper.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "*   HuggingFace’s [distil-whisper](https://github.com/huggingface/distil-whisper/blob/main/Distil_Whisper.pdf): Distilled OpenAI’s Whisper on 20,000 hours of open-sourced audio data.\n*   Distil-Whisper is significantly smaller and faster (5.8 times faster, 51% fewer parameters) than the original Whisper model, maintaining similar performance (within 1% WER) on out-of-distribution test data in a zero-shot setting.\n*   The authors use a large-scale pseudo-labelling approach to assemble an open-source dataset for training, selecting high-quality pseudo-labels based on a word error rate (WER) heuristic.\n*   The motivation was the fact that OpenAI’s Whisper yields astonishing accuracy for most audio, but it’s too slow and expensive for most production use cases. In addition, it has a tendency to hallucinate.\n*   Encoding takes O(1)O(1)O(1) passes while decoding takes O(N)O(N)O(N). This implies that reducing decoder layers is NNN time more effective. They kept the whole encoder, but utilized only two decoder layers.\n*   The encoder is frozen during distillation to ensure Whisper’s robustness to noise is kept.\n*   The model demonstrates improved robustness against hallucination errors in long-form audio, and its design allows it to be paired with Whisper for speculative decoding, doubling the inference speed while maintaining output accuracy.\n*   The paper highlights the utility of large-scale pseudo-labelling in speech recognition and the effectiveness of the WER threshold filter in distillation. The training and inference code, along with the models, are made publicly available by the authors.\n*   To make sure Distil-Whisper does not inherit hallucinations, they filtered out all data samples below a certain WER threshold. By doing so, we were able to reduce hallucinations and actually beat the teacher on long-form audio evaluation.\n*   The checkpoints are [here](https://github.com/huggingface/distil-whisper) and also directly [available](https://huggingface.co/distil-whisper) in 🤗 Transformers. All with MIT License.\n\n![](/primers/ai/assets/speech/distil-whisper.jpeg)",
    "contentLength": 6758,
    "wordCount": 277,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/speech-processing/#distil-whisper"
  },
  {
    "id": "ai-speech-processing-parakeet-36",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Popular Models",
    "title": "Parakeet",
    "order": 36,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><a href=\"https://huggingface.co/nvidia/parakeet-rnnt-1.1b\">Parakeet</a> is jointly developed by NVIDIA NeMo and <a href=\"http://suno.ai/\">Suno.ai</a> teams. It is up to 10x faster and 30% more accurate than Whisper models!</li>\n  <li>Parakeet is an XXL version of FastConformer Transducer (around 1.1B parameters) model that only supports the English language. The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams using NeMo toolkit for over several hundred epochs.</li>\n</ul>",
    "contentMarkdown": "*   [Parakeet](https://huggingface.co/nvidia/parakeet-rnnt-1.1b) is jointly developed by NVIDIA NeMo and [Suno.ai](http://suno.ai/) teams. It is up to 10x faster and 30% more accurate than Whisper models!\n*   Parakeet is an XXL version of FastConformer Transducer (around 1.1B parameters) model that only supports the English language. The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams using NeMo toolkit for over several hundred epochs.",
    "contentLength": 540,
    "wordCount": 70,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#parakeet"
  },
  {
    "id": "ai-speech-processing-canary-37",
    "articleSlug": "speech-processing",
    "articleTitle": "Speech Processing",
    "category": "Speech",
    "chapter": "Popular Models",
    "title": "Canary",
    "order": 37,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Canary-1B, with 1 billion parameters, is trained to excel in automatic speech-to-text recognition (ASR) and translation tasks, supporting English, German, French, and Spanish.</li>\n  <li><strong>Model Highlights:</strong>\n    <ul>\n      <li><strong>Language Support:</strong> Fluent in 4 languages for ASR and capable of translating between English and German/French/Spanish, including punctuation and capitalization control.</li>\n      <li><strong>Architecture:</strong> An innovative encoder-decoder setup combining a FastConformer encoder with a Transformer Decoder, ensuring efficient and accurate text generation.</li>\n      <li><strong>Ease of Use:</strong> Leveraging concatenated SentencePiece tokenizers for scalability and simplicity in language expansion.</li>\n    </ul>\n  </li>\n  <li><strong>Getting Started with NeMo:</strong>\n    <ul>\n      <li>To harness the power of Canary-1B, NVIDIA recommends installing the NeMo toolkit post-Cython and the latest PyTorch version. This toolkit not only facilitates model training and fine-tuning but also simplifies the inference process for developers and researchers.</li>\n    </ul>\n  </li>\n  <li><strong>Usage and Performance:</strong>\n    <ul>\n      <li>Canary-1B shines in both ASR and AST (automatic speech-to-text translation), providing exceptional accuracy and versatility. Whether you’re translating speeches or transcribing multi-lingual audio files, Canary-1B delivers with precision. It’s trained on a diverse dataset totaling 85k hours of speech, ensuring robust performance across its supported languages.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://huggingface.co/spaces/nvidia/canary-1b\">Demo</a>; <a href=\"https://huggingface.co/nvidia/canary-1b\">Model</a></li>\n</ul>\n<ul>\n      <li><strong>Language Support:</strong> Fluent in 4 languages for ASR and capable of translating between English and German/French/Spanish, including punctuation and capitalization control.</li>\n      <li><strong>Architecture:</strong> An innovative encoder-decoder setup combining a FastConformer encoder with a Transformer Decoder, ensuring efficient and accurate text generation.</li>\n      <li><strong>Ease of Use:</strong> Leveraging concatenated SentencePiece tokenizers for scalability and simplicity in language expansion.</li>\n    </ul>\n<ul>\n      <li>To harness the power of Canary-1B, NVIDIA recommends installing the NeMo toolkit post-Cython and the latest PyTorch version. This toolkit not only facilitates model training and fine-tuning but also simplifies the inference process for developers and researchers.</li>\n    </ul>\n<ul>\n      <li>Canary-1B shines in both ASR and AST (automatic speech-to-text translation), providing exceptional accuracy and versatility. Whether you’re translating speeches or transcribing multi-lingual audio files, Canary-1B delivers with precision. It’s trained on a diverse dataset totaling 85k hours of speech, ensuring robust performance across its supported languages.</li>\n    </ul>",
    "contentMarkdown": "*   Canary-1B, with 1 billion parameters, is trained to excel in automatic speech-to-text recognition (ASR) and translation tasks, supporting English, German, French, and Spanish.\n*   **Model Highlights:**\n    *   **Language Support:** Fluent in 4 languages for ASR and capable of translating between English and German/French/Spanish, including punctuation and capitalization control.\n    *   **Architecture:** An innovative encoder-decoder setup combining a FastConformer encoder with a Transformer Decoder, ensuring efficient and accurate text generation.\n    *   **Ease of Use:** Leveraging concatenated SentencePiece tokenizers for scalability and simplicity in language expansion.\n*   **Getting Started with NeMo:**\n    *   To harness the power of Canary-1B, NVIDIA recommends installing the NeMo toolkit post-Cython and the latest PyTorch version. This toolkit not only facilitates model training and fine-tuning but also simplifies the inference process for developers and researchers.\n*   **Usage and Performance:**\n    *   Canary-1B shines in both ASR and AST (automatic speech-to-text translation), providing exceptional accuracy and versatility. Whether you’re translating speeches or transcribing multi-lingual audio files, Canary-1B delivers with precision. It’s trained on a diverse dataset totaling 85k hours of speech, ensuring robust performance across its supported languages.\n*   [Demo](https://huggingface.co/spaces/nvidia/canary-1b); [Model](https://huggingface.co/nvidia/canary-1b)\n\n*   **Language Support:** Fluent in 4 languages for ASR and capable of translating between English and German/French/Spanish, including punctuation and capitalization control.\n*   **Architecture:** An innovative encoder-decoder setup combining a FastConformer encoder with a Transformer Decoder, ensuring efficient and accurate text generation.\n*   **Ease of Use:** Leveraging concatenated SentencePiece tokenizers for scalability and simplicity in language expansion.\n\n*   To harness the power of Canary-1B, NVIDIA recommends installing the NeMo toolkit post-Cython and the latest PyTorch version. This toolkit not only facilitates model training and fine-tuning but also simplifies the inference process for developers and researchers.\n\n*   Canary-1B shines in both ASR and AST (automatic speech-to-text translation), providing exceptional accuracy and versatility. Whether you’re translating speeches or transcribing multi-lingual audio files, Canary-1B delivers with precision. It’s trained on a diverse dataset totaling 85k hours of speech, ensuring robust performance across its supported languages.",
    "contentLength": 2994,
    "wordCount": 323,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/speech-processing/#canary"
  }
]