[
  {
    "id": "ai-fine-tune-and-eval-BERT-install-transformers-1",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Setup",
    "title": "Install Transformers",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"err\">!</span><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">transformers</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"err\">!</span><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">transformers</span>\n</code></pre>",
    "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`!pip install transformers`\n\n![](https://aman.ai/images/copy.png)\n\n`!pip install transformers`",
    "contentLength": 895,
    "wordCount": 8,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#install-transformers"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-helper-functions-2",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Setup",
    "title": "Helper Functions",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>In many of our (long-running) <code class=\"language-plaintext highlighter-rouge\">for</code>-loops, we should print periodic progress updates. Typically people pick the update interval manually, but yoy may defined a helper function to make that choice as follows:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"k\">def</span> <span class=\"nf\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"p\">,</span> <span class=\"n\">num_desired_updates</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    This function will try to pick an intelligent progress update interval \n    based on the magnitude of the total iterations.\n\n    Parameters:\n      `total_iters` - The number of iterations in the for-loop.\n      `num_desired_updates` - How many times we want to see an update over the \n                              course of the for-loop.\n    '''</span>\n    <span class=\"c1\"># Divide the total iterations by the desired number of updates. Most likely\n</span>    <span class=\"c1\"># this will be some ugly number.\n</span>    <span class=\"n\">exact_interval</span> <span class=\"o\">=</span> <span class=\"n\">total_iters</span> <span class=\"o\">/</span> <span class=\"n\">num_desired_updates</span>\n\n    <span class=\"c1\"># The `round` function has the ability to round down a number to, e.g., the\n</span>    <span class=\"c1\"># nearest thousandth: round(exact_interval, -3)\n</span>    <span class=\"c1\">#\n</span>    <span class=\"c1\"># To determine the magnitude to round to, find the magnitude of the total,\n</span>    <span class=\"c1\"># and then go one magnitude below that.\n</span>\n    <span class=\"c1\"># Get the order of magnitude of the total.\n</span>    <span class=\"n\">order_of_mag</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Our update interval should be rounded to an order of magnitude smaller. \n</span>    <span class=\"n\">round_mag</span> <span class=\"o\">=</span> <span class=\"n\">order_of_mag</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Round down and cast to an int.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"n\">exact_interval</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">round_mag</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Don't allow the interval to be zero!\n</span>    <span class=\"k\">if</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">update_interval</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"k\">def</span> <span class=\"nf\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"p\">,</span> <span class=\"n\">num_desired_updates</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    This function will try to pick an intelligent progress update interval \n    based on the magnitude of the total iterations.\n\n    Parameters:\n      `total_iters` - The number of iterations in the for-loop.\n      `num_desired_updates` - How many times we want to see an update over the \n                              course of the for-loop.\n    '''</span>\n    <span class=\"c1\"># Divide the total iterations by the desired number of updates. Most likely\n</span>    <span class=\"c1\"># this will be some ugly number.\n</span>    <span class=\"n\">exact_interval</span> <span class=\"o\">=</span> <span class=\"n\">total_iters</span> <span class=\"o\">/</span> <span class=\"n\">num_desired_updates</span>\n\n    <span class=\"c1\"># The `round` function has the ability to round down a number to, e.g., the\n</span>    <span class=\"c1\"># nearest thousandth: round(exact_interval, -3)\n</span>    <span class=\"c1\">#\n</span>    <span class=\"c1\"># To determine the magnitude to round to, find the magnitude of the total,\n</span>    <span class=\"c1\"># and then go one magnitude below that.\n</span>\n    <span class=\"c1\"># Get the order of magnitude of the total.\n</span>    <span class=\"n\">order_of_mag</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Our update interval should be rounded to an order of magnitude smaller. \n</span>    <span class=\"n\">round_mag</span> <span class=\"o\">=</span> <span class=\"n\">order_of_mag</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Round down and cast to an int.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"n\">exact_interval</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">round_mag</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Don't allow the interval to be zero!\n</span>    <span class=\"k\">if</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">update_interval</span>\n</code></pre>\n<ul>\n  <li>Helper function for formatting elapsed times as <code class=\"language-plaintext highlighter-rouge\">hh:mm:ss</code>:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">format_time</span><span class=\"p\">(</span><span class=\"n\">elapsed</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''</span>\n    <span class=\"c1\"># Round to the nearest second.\n</span>    <span class=\"n\">elapsed_rounded</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">round</span><span class=\"p\">((</span><span class=\"n\">elapsed</span><span class=\"p\">)))</span>\n    \n    <span class=\"c1\"># Format as hh:mm:ss\n</span>    <span class=\"k\">return</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">timedelta</span><span class=\"p\">(</span><span class=\"n\">seconds</span><span class=\"o\">=</span><span class=\"n\">elapsed_rounded</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">format_time</span><span class=\"p\">(</span><span class=\"n\">elapsed</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''</span>\n    <span class=\"c1\"># Round to the nearest second.\n</span>    <span class=\"n\">elapsed_rounded</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">round</span><span class=\"p\">((</span><span class=\"n\">elapsed</span><span class=\"p\">)))</span>\n    \n    <span class=\"c1\"># Format as hh:mm:ss\n</span>    <span class=\"k\">return</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">timedelta</span><span class=\"p\">(</span><span class=\"n\">seconds</span><span class=\"o\">=</span><span class=\"n\">elapsed_rounded</span><span class=\"p\">))</span>\n</code></pre>",
    "contentMarkdown": "*   In many of our (long-running) `for`\\-loops, we should print periodic progress updates. Typically people pick the update interval manually, but yoy may defined a helper function to make that choice as follows:\n\n![](https://aman.ai/images/copy.png)\n\n``def good_update_interval(total_iters, num_desired_updates):     '''     This function will try to pick an intelligent progress update interval      based on the magnitude of the total iterations.      Parameters:       `total_iters` - The number of iterations in the for-loop.       `num_desired_updates` - How many times we want to see an update over the                                course of the for-loop.     '''     # Divide the total iterations by the desired number of updates. Most likely     # this will be some ugly number.     exact_interval = total_iters / num_desired_updates      # The `round` function has the ability to round down a number to, e.g., the     # nearest thousandth: round(exact_interval, -3)     #     # To determine the magnitude to round to, find the magnitude of the total,     # and then go one magnitude below that.     # Get the order of magnitude of the total.     order_of_mag = len(str(total_iters)) - 1      # Our update interval should be rounded to an order of magnitude smaller.      round_mag = order_of_mag - 1      # Round down and cast to an int.     update_interval = int(round(exact_interval, -round_mag))      # Don't allow the interval to be zero!     if update_interval == 0:         update_interval = 1      return update_interval``\n\n![](https://aman.ai/images/copy.png)\n\n``def good_update_interval(total_iters, num_desired_updates):     '''     This function will try to pick an intelligent progress update interval      based on the magnitude of the total iterations.      Parameters:       `total_iters` - The number of iterations in the for-loop.       `num_desired_updates` - How many times we want to see an update over the                                course of the for-loop.     '''     # Divide the total iterations by the desired number of updates. Most likely     # this will be some ugly number.     exact_interval = total_iters / num_desired_updates      # The `round` function has the ability to round down a number to, e.g., the     # nearest thousandth: round(exact_interval, -3)     #     # To determine the magnitude to round to, find the magnitude of the total,     # and then go one magnitude below that.     # Get the order of magnitude of the total.     order_of_mag = len(str(total_iters)) - 1      # Our update interval should be rounded to an order of magnitude smaller.      round_mag = order_of_mag - 1      # Round down and cast to an int.     update_interval = int(round(exact_interval, -round_mag))      # Don't allow the interval to be zero!     if update_interval == 0:         update_interval = 1      return update_interval``\n\n*   Helper function for formatting elapsed times as `hh:mm:ss`:\n\n![](https://aman.ai/images/copy.png)\n\n`import time import datetime  def format_time(elapsed):     '''     Takes a time in seconds and returns a string hh:mm:ss     '''     # Round to the nearest second.     elapsed_rounded = int(round((elapsed)))          # Format as hh:mm:ss     return str(datetime.timedelta(seconds=elapsed_rounded))`\n\n![](https://aman.ai/images/copy.png)\n\n`import time import datetime  def format_time(elapsed):     '''     Takes a time in seconds and returns a string hh:mm:ss     '''     # Round to the nearest second.     elapsed_rounded = int(round((elapsed)))          # Format as hh:mm:ss     return str(datetime.timedelta(seconds=elapsed_rounded))`",
    "contentLength": 9151,
    "wordCount": 472,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#helper-functions"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-download-dataset-files-3",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Retrieve Dataset",
    "title": "Download Dataset Files",
    "order": 3,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>I’m sure there are many ways to retrieve this dataset–I’m using the TensorFlow Datasets library here as one easy way to do it.</p>\n  </li>\n  <li>\n    <p>Check out the documentation <a href=\"https://www.tensorflow.org/datasets/catalog/imdb_reviews\">here</a></p>\n  </li>\n</ul>\n<p>I’m sure there are many ways to retrieve this dataset–I’m using the TensorFlow Datasets library here as one easy way to do it.</p>\n<p>Check out the documentation <a href=\"https://www.tensorflow.org/datasets/catalog/imdb_reviews\">here</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow_datasets</span> <span class=\"k\">as</span> <span class=\"n\">tfds</span>\n\n<span class=\"c1\"># Download the train and test portions of the dataset.\n</span><span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"n\">tfds</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"imdb_reviews/plain_text\"</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s\">\"train\"</span><span class=\"p\">)</span>\n<span class=\"n\">test_data</span> <span class=\"o\">=</span> <span class=\"n\">tfds</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"imdb_reviews/plain_text\"</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s\">\"test\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Overwrite</span> <span class=\"n\">dataset</span> <span class=\"n\">info</span> <span class=\"k\">from</span> <span class=\"n\">restored</span> <span class=\"n\">data</span> <span class=\"n\">version</span><span class=\"p\">.</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Reusing</span> <span class=\"n\">dataset</span> <span class=\"n\">imdb_reviews</span> <span class=\"p\">(</span><span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Constructing</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span> <span class=\"k\">for</span> <span class=\"n\">split</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"k\">from</span> <span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Overwrite</span> <span class=\"n\">dataset</span> <span class=\"n\">info</span> <span class=\"k\">from</span> <span class=\"n\">restored</span> <span class=\"n\">data</span> <span class=\"n\">version</span><span class=\"p\">.</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Reusing</span> <span class=\"n\">dataset</span> <span class=\"n\">imdb_reviews</span> <span class=\"p\">(</span><span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Constructing</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span> <span class=\"k\">for</span> <span class=\"n\">split</span> <span class=\"n\">test</span><span class=\"p\">,</span> <span class=\"k\">from</span> <span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span>\n<span class=\"n\">Let</span><span class=\"err\">’</span><span class=\"n\">s</span> <span class=\"n\">examine</span> <span class=\"n\">the</span> <span class=\"n\">contents</span> <span class=\"ow\">and</span> <span class=\"n\">datatypes</span> <span class=\"n\">contained</span> <span class=\"ow\">in</span> <span class=\"n\">the</span> <span class=\"n\">dataset</span><span class=\"p\">.</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow_datasets</span> <span class=\"k\">as</span> <span class=\"n\">tfds</span>\n\n<span class=\"c1\"># Download the train and test portions of the dataset.\n</span><span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"n\">tfds</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"imdb_reviews/plain_text\"</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s\">\"train\"</span><span class=\"p\">)</span>\n<span class=\"n\">test_data</span> <span class=\"o\">=</span> <span class=\"n\">tfds</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"imdb_reviews/plain_text\"</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s\">\"test\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Overwrite</span> <span class=\"n\">dataset</span> <span class=\"n\">info</span> <span class=\"k\">from</span> <span class=\"n\">restored</span> <span class=\"n\">data</span> <span class=\"n\">version</span><span class=\"p\">.</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Reusing</span> <span class=\"n\">dataset</span> <span class=\"n\">imdb_reviews</span> <span class=\"p\">(</span><span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Constructing</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span> <span class=\"k\">for</span> <span class=\"n\">split</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"k\">from</span> <span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Overwrite</span> <span class=\"n\">dataset</span> <span class=\"n\">info</span> <span class=\"k\">from</span> <span class=\"n\">restored</span> <span class=\"n\">data</span> <span class=\"n\">version</span><span class=\"p\">.</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Reusing</span> <span class=\"n\">dataset</span> <span class=\"n\">imdb_reviews</span> <span class=\"p\">(</span><span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">INFO</span><span class=\"p\">:</span><span class=\"n\">absl</span><span class=\"p\">:</span><span class=\"n\">Constructing</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span> <span class=\"k\">for</span> <span class=\"n\">split</span> <span class=\"n\">test</span><span class=\"p\">,</span> <span class=\"k\">from</span> <span class=\"o\">/</span><span class=\"n\">root</span><span class=\"o\">/</span><span class=\"n\">tensorflow_datasets</span><span class=\"o\">/</span><span class=\"n\">imdb_reviews</span><span class=\"o\">/</span><span class=\"n\">plain_text</span><span class=\"o\">/</span><span class=\"mf\">1.0</span><span class=\"p\">.</span><span class=\"mi\">0</span>\n<span class=\"n\">Let</span><span class=\"err\">’</span><span class=\"n\">s</span> <span class=\"n\">examine</span> <span class=\"n\">the</span> <span class=\"n\">contents</span> <span class=\"ow\">and</span> <span class=\"n\">datatypes</span> <span class=\"n\">contained</span> <span class=\"ow\">in</span> <span class=\"n\">the</span> <span class=\"n\">dataset</span><span class=\"p\">.</span>\n</code></pre>\n<ul>\n  <li>Each sample has a label and text field.</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">train_data\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">train_data\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">&lt;DatasetV1Adapter shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}&gt;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">&lt;DatasetV1Adapter shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}&gt;\n</code></pre>\n<ul>\n  <li>Let’s pull the data out of TensorFlow’s icy grip, so we just have plain Python types :)</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"n\">train_text</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">train_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Loop over the training set...\n</span><span class=\"k\">for</span> <span class=\"n\">ex</span> <span class=\"ow\">in</span> <span class=\"n\">train_data</span><span class=\"p\">.</span><span class=\"n\">as_numpy_iterator</span><span class=\"p\">():</span>\n\n    <span class=\"c1\"># The text is a `bytes` object, decode to string.\n</span>    <span class=\"n\">train_text</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">].</span><span class=\"n\">decode</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Cast the label from `np.int64` to `int`\n</span>    <span class=\"n\">train_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'label'</span><span class=\"p\">]))</span>\n\n<span class=\"n\">test_text</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">test_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Loop over the test set...\n</span><span class=\"k\">for</span> <span class=\"n\">ex</span> <span class=\"ow\">in</span> <span class=\"n\">test_data</span><span class=\"p\">.</span><span class=\"n\">as_numpy_iterator</span><span class=\"p\">():</span>\n\n    <span class=\"c1\"># The text is a `bytes` object, decode to string.\n</span>    <span class=\"n\">test_text</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">].</span><span class=\"n\">decode</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Cast the label from `np.int64` to `int`\n</span>    <span class=\"n\">test_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'label'</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># Print some stats.\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:,} Training Samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">)))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:,} Test Samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">)))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Labels:'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"n\">train_text</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">train_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Loop over the training set...\n</span><span class=\"k\">for</span> <span class=\"n\">ex</span> <span class=\"ow\">in</span> <span class=\"n\">train_data</span><span class=\"p\">.</span><span class=\"n\">as_numpy_iterator</span><span class=\"p\">():</span>\n\n    <span class=\"c1\"># The text is a `bytes` object, decode to string.\n</span>    <span class=\"n\">train_text</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">].</span><span class=\"n\">decode</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Cast the label from `np.int64` to `int`\n</span>    <span class=\"n\">train_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'label'</span><span class=\"p\">]))</span>\n\n<span class=\"n\">test_text</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">test_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Loop over the test set...\n</span><span class=\"k\">for</span> <span class=\"n\">ex</span> <span class=\"ow\">in</span> <span class=\"n\">test_data</span><span class=\"p\">.</span><span class=\"n\">as_numpy_iterator</span><span class=\"p\">():</span>\n\n    <span class=\"c1\"># The text is a `bytes` object, decode to string.\n</span>    <span class=\"n\">test_text</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">].</span><span class=\"n\">decode</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Cast the label from `np.int64` to `int`\n</span>    <span class=\"n\">test_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">ex</span><span class=\"p\">[</span><span class=\"s\">'label'</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># Print some stats.\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:,} Training Samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">)))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:,} Test Samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">)))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Labels:'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">25,000 Training Samples\n25,000 Test Samples\nLabels: [0 1]\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">25,000 Training Samples\n25,000 Test Samples\nLabels: [0 1]\n</code></pre>",
    "contentMarkdown": "*   I’m sure there are many ways to retrieve this dataset–I’m using the TensorFlow Datasets library here as one easy way to do it.\n    \n*   Check out the documentation [here](https://www.tensorflow.org/datasets/catalog/imdb_reviews)\n    \n\nI’m sure there are many ways to retrieve this dataset–I’m using the TensorFlow Datasets library here as one easy way to do it.\n\nCheck out the documentation [here](https://www.tensorflow.org/datasets/catalog/imdb_reviews)\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow_datasets as tfds  # Download the train and test portions of the dataset. train_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"train\") test_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"test\")  INFO:absl:Overwrite dataset info from restored data version. INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0) INFO:absl:Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 INFO:absl:Overwrite dataset info from restored data version. INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0) INFO:absl:Constructing tf.data.Dataset for split test, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 Let’s examine the contents and datatypes contained in the dataset.`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow_datasets as tfds  # Download the train and test portions of the dataset. train_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"train\") test_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"test\")  INFO:absl:Overwrite dataset info from restored data version. INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0) INFO:absl:Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 INFO:absl:Overwrite dataset info from restored data version. INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0) INFO:absl:Constructing tf.data.Dataset for split test, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 Let’s examine the contents and datatypes contained in the dataset.`\n\n*   Each sample has a label and text field.\n\n![](https://aman.ai/images/copy.png)\n\n`train_data`\n\n![](https://aman.ai/images/copy.png)\n\n`train_data`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`<DatasetV1Adapter shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>`\n\n![](https://aman.ai/images/copy.png)\n\n`<DatasetV1Adapter shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>`\n\n*   Let’s pull the data out of TensorFlow’s icy grip, so we just have plain Python types :)\n\n![](https://aman.ai/images/copy.png)\n\n``import numpy as np  train_text = [] train_labels = []  # Loop over the training set... for ex in train_data.as_numpy_iterator():      # The text is a `bytes` object, decode to string.     train_text.append(ex['text'].decode())      # Cast the label from `np.int64` to `int`     train_labels.append(int(ex['label']))  test_text = [] test_labels = []  # Loop over the test set... for ex in test_data.as_numpy_iterator():      # The text is a `bytes` object, decode to string.     test_text.append(ex['text'].decode())      # Cast the label from `np.int64` to `int`     test_labels.append(int(ex['label']))  # Print some stats. print('{:,} Training Samples'.format(len(train_labels))) print('{:,} Test Samples'.format(len(test_labels))) print('Labels:', np.unique(train_labels))``\n\n![](https://aman.ai/images/copy.png)\n\n``import numpy as np  train_text = [] train_labels = []  # Loop over the training set... for ex in train_data.as_numpy_iterator():      # The text is a `bytes` object, decode to string.     train_text.append(ex['text'].decode())      # Cast the label from `np.int64` to `int`     train_labels.append(int(ex['label']))  test_text = [] test_labels = []  # Loop over the test set... for ex in test_data.as_numpy_iterator():      # The text is a `bytes` object, decode to string.     test_text.append(ex['text'].decode())      # Cast the label from `np.int64` to `int`     test_labels.append(int(ex['label']))  # Print some stats. print('{:,} Training Samples'.format(len(train_labels))) print('{:,} Test Samples'.format(len(test_labels))) print('Labels:', np.unique(train_labels))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`25,000 Training Samples 25,000 Test Samples Labels: [0 1]`\n\n![](https://aman.ai/images/copy.png)\n\n`25,000 Training Samples 25,000 Test Samples Labels: [0 1]`",
    "contentLength": 20275,
    "wordCount": 455,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#download-dataset-files"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-inspect-training-samples-4",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Inspect Dataset",
    "title": "Inspect Training Samples",
    "order": 4,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Lets print out a handful of samples at random.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">textwrap</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n<span class=\"c1\"># Wrap text to 80 characters.\n</span><span class=\"n\">wrapper</span> <span class=\"o\">=</span> <span class=\"n\">textwrap</span><span class=\"p\">.</span><span class=\"n\">TextWrapper</span><span class=\"p\">(</span><span class=\"n\">width</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">)</span> \n\n<span class=\"c1\"># Randomly choose some examples.\n</span><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    \n    <span class=\"c1\"># Choose a random sample by index.\n</span>    <span class=\"n\">j</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n    \n    <span class=\"c1\"># Print out the label and the text. \n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'==== Label: {:} ===='</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">fill</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">''</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">textwrap</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n<span class=\"c1\"># Wrap text to 80 characters.\n</span><span class=\"n\">wrapper</span> <span class=\"o\">=</span> <span class=\"n\">textwrap</span><span class=\"p\">.</span><span class=\"n\">TextWrapper</span><span class=\"p\">(</span><span class=\"n\">width</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">)</span> \n\n<span class=\"c1\"># Randomly choose some examples.\n</span><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    \n    <span class=\"c1\"># Choose a random sample by index.\n</span>    <span class=\"n\">j</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n    \n    <span class=\"c1\"># Print out the label and the text. \n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'==== Label: {:} ===='</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">fill</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">''</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">==== Label: 0 ====\nSomewhere, on this site, someone wrote that to get the best version of the works\nof Jane Austen, one should simply read them. I agree with that. However, we love\nadaptations of great literature and the current writers' strike brings to mind\nthat without good writers, it's hard for actors to bring their roles to life.\nThe current version of Jane Austen's PERSUASION shows us what happens when you\ndon't have a good foundation in a well-written adaptation. This version does not\ncompare to the 1995 version with Amanda Root and Ciaran Hinds, which was well\nacted and kept the essence of the era and the constraints on the characters\n(with the exception of the bizarre parade &amp; kissing in the street scene in\nBath). The 2007 version shows a twitty Anne who seems angst-ridden. The other\ncharacters were not very developed which is a crime, considering how Austen\ncould paint such wonderful characters with some carefully chosen\nunderstatements. The sequence of events that made sense in the novel were\ncompletely tossed about, and Mrs. Smith, Anne's bedridden and impoverished\nschoolmate is walking around in Bath - - twittering away, as many of the\ncharacters seemed to do. The strength of character and the intelligence of\nCaptain Wentworth, which caused Anne to love him in the first place, didn't seem\nto be written into the Rupert Penry-Jones' Wentworth. Ciaran Hinds had more\nsubstance and was able to convey so much more with a look, than P-J was able to\ndo with his poses. All in all, the 2007 version was a disappointment. It seemed\nto reduce the novel into a hand- wringing, costumed melodrama of debatable\nworth. If they wanted to bring our modern emotional extravagances into Austen's\nwork, they should have done what they do with adaptations of Shakespeare: adapt\nit to the present. At least \"Bride &amp; Prejudice\" was taken out of the historical\n&amp; locational settings and was fun to watch, as was \"Clueless\". This wasn't\nPERSUASION, but they didn't know what else to call it.\n\n==== Label: 0 ====\nIt's difficult to put into words the almost seething hatred I have of this film.\nBut I'll try:&lt;br /&gt;&lt;br /&gt;Every other word was an expletive, the sex scenes were\nuncomfortable, drugs were rampant and stereotyping was beyond the norm, if not\noffensive to Italian-Americans.&lt;br /&gt;&lt;br /&gt;I'm not saying the acting was\nterrible, because Leguizamo, Sorvino, Brody, Espisito et. al, performed well.\nBut...almost every character in the film I despised. Not since The Bonfire of\nthe Vanities have I disliked every character on screen.\n\n==== Label: 0 ====\nTake one look at the cover of this movie, and you know right away that you are\nnot about to watch a landmark film. This is cheese filmmaking in every respect,\nbut it does have its moments. Despite the look of utter trash that the movie\ngives, the story is actually interesting at some points, although it is\nundeniably pulled along mainly by the cheerleading squads' shower scenes and sex\nscenes with numerous personality-free boyfriends. The acting is awful and the\ndirector did little more than point and shoot, which is why the extensive amount\nof nudity was needed to keep the audience's attention.&lt;br /&gt;&lt;br /&gt;In The Nutty\nProfessor, a hopelessly geeky professor discovers a potion that can turn him\ninto a cool and stylish womanizer, whereas in The Invisible Maniac, a mentally\ndamaged professor discovers a potion that can make him invisible, allowing him\nto spy on (and kill, for some reason) his students. Boring fodder. Don't expect\nany kind of mental stimulation from this, and prepare yourself for shrill and\nenormously overdone maniacal laughter which gets real annoying real quick...\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">==== Label: 0 ====\nSomewhere, on this site, someone wrote that to get the best version of the works\nof Jane Austen, one should simply read them. I agree with that. However, we love\nadaptations of great literature and the current writers' strike brings to mind\nthat without good writers, it's hard for actors to bring their roles to life.\nThe current version of Jane Austen's PERSUASION shows us what happens when you\ndon't have a good foundation in a well-written adaptation. This version does not\ncompare to the 1995 version with Amanda Root and Ciaran Hinds, which was well\nacted and kept the essence of the era and the constraints on the characters\n(with the exception of the bizarre parade &amp; kissing in the street scene in\nBath). The 2007 version shows a twitty Anne who seems angst-ridden. The other\ncharacters were not very developed which is a crime, considering how Austen\ncould paint such wonderful characters with some carefully chosen\nunderstatements. The sequence of events that made sense in the novel were\ncompletely tossed about, and Mrs. Smith, Anne's bedridden and impoverished\nschoolmate is walking around in Bath - - twittering away, as many of the\ncharacters seemed to do. The strength of character and the intelligence of\nCaptain Wentworth, which caused Anne to love him in the first place, didn't seem\nto be written into the Rupert Penry-Jones' Wentworth. Ciaran Hinds had more\nsubstance and was able to convey so much more with a look, than P-J was able to\ndo with his poses. All in all, the 2007 version was a disappointment. It seemed\nto reduce the novel into a hand- wringing, costumed melodrama of debatable\nworth. If they wanted to bring our modern emotional extravagances into Austen's\nwork, they should have done what they do with adaptations of Shakespeare: adapt\nit to the present. At least \"Bride &amp; Prejudice\" was taken out of the historical\n&amp; locational settings and was fun to watch, as was \"Clueless\". This wasn't\nPERSUASION, but they didn't know what else to call it.\n\n==== Label: 0 ====\nIt's difficult to put into words the almost seething hatred I have of this film.\nBut I'll try:&lt;br /&gt;&lt;br /&gt;Every other word was an expletive, the sex scenes were\nuncomfortable, drugs were rampant and stereotyping was beyond the norm, if not\noffensive to Italian-Americans.&lt;br /&gt;&lt;br /&gt;I'm not saying the acting was\nterrible, because Leguizamo, Sorvino, Brody, Espisito et. al, performed well.\nBut...almost every character in the film I despised. Not since The Bonfire of\nthe Vanities have I disliked every character on screen.\n\n==== Label: 0 ====\nTake one look at the cover of this movie, and you know right away that you are\nnot about to watch a landmark film. This is cheese filmmaking in every respect,\nbut it does have its moments. Despite the look of utter trash that the movie\ngives, the story is actually interesting at some points, although it is\nundeniably pulled along mainly by the cheerleading squads' shower scenes and sex\nscenes with numerous personality-free boyfriends. The acting is awful and the\ndirector did little more than point and shoot, which is why the extensive amount\nof nudity was needed to keep the audience's attention.&lt;br /&gt;&lt;br /&gt;In The Nutty\nProfessor, a hopelessly geeky professor discovers a potion that can turn him\ninto a cool and stylish womanizer, whereas in The Invisible Maniac, a mentally\ndamaged professor discovers a potion that can make him invisible, allowing him\nto spy on (and kill, for some reason) his students. Boring fodder. Don't expect\nany kind of mental stimulation from this, and prepare yourself for shrill and\nenormously overdone maniacal laughter which gets real annoying real quick...\n</code></pre>\n<ul>\n  <li>Let’s also check out the classes and their balance.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the number of tokens of each length.\n</span><span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"n\">countplot</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add labels\n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Class Distribution'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Category'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'# of Training Samples'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add thousands separators to the y-axis labels.\n</span><span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span> <span class=\"k\">as</span> <span class=\"n\">mpl</span>\n<span class=\"n\">ax</span><span class=\"p\">.</span><span class=\"n\">yaxis</span><span class=\"p\">.</span><span class=\"n\">set_major_formatter</span><span class=\"p\">(</span><span class=\"n\">mpl</span><span class=\"p\">.</span><span class=\"n\">ticker</span><span class=\"p\">.</span><span class=\"n\">StrMethodFormatter</span><span class=\"p\">(</span><span class=\"s\">'{x:,.0f}'</span><span class=\"p\">))</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the number of tokens of each length.\n</span><span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"n\">countplot</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add labels\n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Class Distribution'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Category'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'# of Training Samples'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add thousands separators to the y-axis labels.\n</span><span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span> <span class=\"k\">as</span> <span class=\"n\">mpl</span>\n<span class=\"n\">ax</span><span class=\"p\">.</span><span class=\"n\">yaxis</span><span class=\"p\">.</span><span class=\"n\">set_major_formatter</span><span class=\"p\">(</span><span class=\"n\">mpl</span><span class=\"p\">.</span><span class=\"n\">ticker</span><span class=\"p\">.</span><span class=\"n\">StrMethodFormatter</span><span class=\"p\">(</span><span class=\"s\">'{x:,.0f}'</span><span class=\"p\">))</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>The plot below shows the class balance in IMDB’s movie reviews:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/fine-tune-and-eval-BERT/imdb_reviews_class_distribution.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Lets print out a handful of samples at random.\n\n![](https://aman.ai/images/copy.png)\n\n`import textwrap import random  # Wrap text to 80 characters. wrapper = textwrap.TextWrapper(width=80)   # Randomly choose some examples. for i in range(3):          # Choose a random sample by index.     j = random.choice(range(len(train_text)))          # Print out the label and the text.      print('==== Label: {:} ===='.format(train_labels[j]))     print(wrapper.fill(train_text[j]))     print('')`\n\n![](https://aman.ai/images/copy.png)\n\n`import textwrap import random  # Wrap text to 80 characters. wrapper = textwrap.TextWrapper(width=80)   # Randomly choose some examples. for i in range(3):          # Choose a random sample by index.     j = random.choice(range(len(train_text)))          # Print out the label and the text.      print('==== Label: {:} ===='.format(train_labels[j]))     print(wrapper.fill(train_text[j]))     print('')`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`==== Label: 0 ==== Somewhere, on this site, someone wrote that to get the best version of the works of Jane Austen, one should simply read them. I agree with that. However, we love adaptations of great literature and the current writers' strike brings to mind that without good writers, it's hard for actors to bring their roles to life. The current version of Jane Austen's PERSUASION shows us what happens when you don't have a good foundation in a well-written adaptation. This version does not compare to the 1995 version with Amanda Root and Ciaran Hinds, which was well acted and kept the essence of the era and the constraints on the characters (with the exception of the bizarre parade & kissing in the street scene in Bath). The 2007 version shows a twitty Anne who seems angst-ridden. The other characters were not very developed which is a crime, considering how Austen could paint such wonderful characters with some carefully chosen understatements. The sequence of events that made sense in the novel were completely tossed about, and Mrs. Smith, Anne's bedridden and impoverished schoolmate is walking around in Bath - - twittering away, as many of the characters seemed to do. The strength of character and the intelligence of Captain Wentworth, which caused Anne to love him in the first place, didn't seem to be written into the Rupert Penry-Jones' Wentworth. Ciaran Hinds had more substance and was able to convey so much more with a look, than P-J was able to do with his poses. All in all, the 2007 version was a disappointment. It seemed to reduce the novel into a hand- wringing, costumed melodrama of debatable worth. If they wanted to bring our modern emotional extravagances into Austen's work, they should have done what they do with adaptations of Shakespeare: adapt it to the present. At least \"Bride & Prejudice\" was taken out of the historical & locational settings and was fun to watch, as was \"Clueless\". This wasn't PERSUASION, but they didn't know what else to call it.  ==== Label: 0 ==== It's difficult to put into words the almost seething hatred I have of this film. But I'll try:<br /><br />Every other word was an expletive, the sex scenes were uncomfortable, drugs were rampant and stereotyping was beyond the norm, if not offensive to Italian-Americans.<br /><br />I'm not saying the acting was terrible, because Leguizamo, Sorvino, Brody, Espisito et. al, performed well. But...almost every character in the film I despised. Not since The Bonfire of the Vanities have I disliked every character on screen.  ==== Label: 0 ==== Take one look at the cover of this movie, and you know right away that you are not about to watch a landmark film. This is cheese filmmaking in every respect, but it does have its moments. Despite the look of utter trash that the movie gives, the story is actually interesting at some points, although it is undeniably pulled along mainly by the cheerleading squads' shower scenes and sex scenes with numerous personality-free boyfriends. The acting is awful and the director did little more than point and shoot, which is why the extensive amount of nudity was needed to keep the audience's attention.<br /><br />In The Nutty Professor, a hopelessly geeky professor discovers a potion that can turn him into a cool and stylish womanizer, whereas in The Invisible Maniac, a mentally damaged professor discovers a potion that can make him invisible, allowing him to spy on (and kill, for some reason) his students. Boring fodder. Don't expect any kind of mental stimulation from this, and prepare yourself for shrill and enormously overdone maniacal laughter which gets real annoying real quick...`\n\n![](https://aman.ai/images/copy.png)\n\n`==== Label: 0 ==== Somewhere, on this site, someone wrote that to get the best version of the works of Jane Austen, one should simply read them. I agree with that. However, we love adaptations of great literature and the current writers' strike brings to mind that without good writers, it's hard for actors to bring their roles to life. The current version of Jane Austen's PERSUASION shows us what happens when you don't have a good foundation in a well-written adaptation. This version does not compare to the 1995 version with Amanda Root and Ciaran Hinds, which was well acted and kept the essence of the era and the constraints on the characters (with the exception of the bizarre parade & kissing in the street scene in Bath). The 2007 version shows a twitty Anne who seems angst-ridden. The other characters were not very developed which is a crime, considering how Austen could paint such wonderful characters with some carefully chosen understatements. The sequence of events that made sense in the novel were completely tossed about, and Mrs. Smith, Anne's bedridden and impoverished schoolmate is walking around in Bath - - twittering away, as many of the characters seemed to do. The strength of character and the intelligence of Captain Wentworth, which caused Anne to love him in the first place, didn't seem to be written into the Rupert Penry-Jones' Wentworth. Ciaran Hinds had more substance and was able to convey so much more with a look, than P-J was able to do with his poses. All in all, the 2007 version was a disappointment. It seemed to reduce the novel into a hand- wringing, costumed melodrama of debatable worth. If they wanted to bring our modern emotional extravagances into Austen's work, they should have done what they do with adaptations of Shakespeare: adapt it to the present. At least \"Bride & Prejudice\" was taken out of the historical & locational settings and was fun to watch, as was \"Clueless\". This wasn't PERSUASION, but they didn't know what else to call it.  ==== Label: 0 ==== It's difficult to put into words the almost seething hatred I have of this film. But I'll try:<br /><br />Every other word was an expletive, the sex scenes were uncomfortable, drugs were rampant and stereotyping was beyond the norm, if not offensive to Italian-Americans.<br /><br />I'm not saying the acting was terrible, because Leguizamo, Sorvino, Brody, Espisito et. al, performed well. But...almost every character in the film I despised. Not since The Bonfire of the Vanities have I disliked every character on screen.  ==== Label: 0 ==== Take one look at the cover of this movie, and you know right away that you are not about to watch a landmark film. This is cheese filmmaking in every respect, but it does have its moments. Despite the look of utter trash that the movie gives, the story is actually interesting at some points, although it is undeniably pulled along mainly by the cheerleading squads' shower scenes and sex scenes with numerous personality-free boyfriends. The acting is awful and the director did little more than point and shoot, which is why the extensive amount of nudity was needed to keep the audience's attention.<br /><br />In The Nutty Professor, a hopelessly geeky professor discovers a potion that can turn him into a cool and stylish womanizer, whereas in The Invisible Maniac, a mentally damaged professor discovers a potion that can make him invisible, allowing him to spy on (and kill, for some reason) his students. Boring fodder. Don't expect any kind of mental stimulation from this, and prepare yourself for shrill and enormously overdone maniacal laughter which gets real annoying real quick...`\n\n*   Let’s also check out the classes and their balance.\n\n![](https://aman.ai/images/copy.png)\n\n`import matplotlib.pyplot as plt import seaborn as sns import numpy as np  sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (10,5)  # Plot the number of tokens of each length. ax = sns.countplot(train_labels)  # Add labels plt.title('Class Distribution') plt.xlabel('Category') plt.ylabel('# of Training Samples')  # Add thousands separators to the y-axis labels. import matplotlib as mpl ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))  plt.show()`\n\n![](https://aman.ai/images/copy.png)\n\n`import matplotlib.pyplot as plt import seaborn as sns import numpy as np  sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (10,5)  # Plot the number of tokens of each length. ax = sns.countplot(train_labels)  # Add labels plt.title('Class Distribution') plt.xlabel('Category') plt.ylabel('# of Training Samples')  # Add thousands separators to the y-axis labels. import matplotlib as mpl ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))  plt.show()`\n\n*   The plot below shows the class balance in IMDB’s movie reviews:\n\n![](/primers/ai/assets/fine-tune-and-eval-BERT/imdb_reviews_class_distribution.png)",
    "contentLength": 18777,
    "wordCount": 1488,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#inspect-training-samples"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-load-tokenizer-5",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Load Tokenizer",
    "order": 5,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>We’ll use the uncased version of BERT-base.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertTokenizer</span>\n\n<span class=\"c1\"># Load the BERT tokenizer.\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Loading BERT tokenizer...'</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span> <span class=\"n\">do_lower_case</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertTokenizer</span>\n\n<span class=\"c1\"># Load the BERT tokenizer.\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Loading BERT tokenizer...'</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span> <span class=\"n\">do_lower_case</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">Loading BERT tokenizer...\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">Loading BERT tokenizer...\n</code></pre>",
    "contentMarkdown": "*   We’ll use the uncased version of BERT-base.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import BertTokenizer  # Load the BERT tokenizer. print('Loading BERT tokenizer...') tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import BertTokenizer  # Load the BERT tokenizer. print('Loading BERT tokenizer...') tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Loading BERT tokenizer...`\n\n![](https://aman.ai/images/copy.png)\n\n`Loading BERT tokenizer...`",
    "contentLength": 2823,
    "wordCount": 53,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#load-tokenizer"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-tokenize-without-padding-6",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Tokenize Without Padding",
    "order": 6,
    "orderInChapter": 2,
    "contentHtml": "<h4 id=\"peak-gpu-memory-use\">Peak GPU Memory Use</h4>\n<ul>\n  <li>\n    <p>Even when applying smart batching, we may still want to truncate our inputs to a certain maximum length. BERT requires a lot of GPU memory, and it’s quite possible for the GPU to not be able to process a batch with too many samples in it and / or too long of sequences.</p>\n  </li>\n  <li>\n    <p>Smart batching means most of our batches will naturally have shorter sequence lengths and not require too much memory. However, all it takes is one batch that’s too long to fit on the GPU, and our <strong>training will fail</strong>!</p>\n  </li>\n  <li>\n    <p>In other words, we still have to be concerned with our “peak” memory usage, and it still likely makes sense to truncate to something lower than 512, even with smart batching.</p>\n  </li>\n</ul>\n<p>Even when applying smart batching, we may still want to truncate our inputs to a certain maximum length. BERT requires a lot of GPU memory, and it’s quite possible for the GPU to not be able to process a batch with too many samples in it and / or too long of sequences.</p>\n<p>Smart batching means most of our batches will naturally have shorter sequence lengths and not require too much memory. However, all it takes is one batch that’s too long to fit on the GPU, and our <strong>training will fail</strong>!</p>\n<p>In other words, we still have to be concerned with our “peak” memory usage, and it still likely makes sense to truncate to something lower than 512, even with smart batching.</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\">max_len = 400\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\">max_len = 400\n</code></pre>\n<h4 id=\"tokenize-but-dont-pad\">Tokenize, but Don’t Pad</h4>\n<ul>\n  <li>\n    <p>We’re going to start by tokenizing all of the samples and mapping the tokens to their IDs.</p>\n  </li>\n  <li>\n    <p>We’re also going to truncate the sequences to our chosen <code class=\"language-plaintext highlighter-rouge\">max_len</code>, and we’re going to add the special tokens.</p>\n  </li>\n  <li>\n    <p>But we are not padding yet! We don’t know what lengths to pad the sequences too until after we’ve grouped them into batches.</p>\n  </li>\n</ul>\n<p>We’re going to start by tokenizing all of the samples and mapping the tokens to their IDs.</p>\n<p>We’re also going to truncate the sequences to our chosen <code class=\"language-plaintext highlighter-rouge\">max_len</code>, and we’re going to add the special tokens.</p>\n<p>But we are not padding yet! We don’t know what lengths to pad the sequences too until after we’ve grouped them into batches.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"n\">full_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Tokenize all training examples\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} training samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># Choose an interval on which to print progress updates.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># For each training example...\n</span><span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">train_text</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># Report progress.\n</span>    <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Tokenize the sentence.\n</span>    <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">text</span><span class=\"p\">,</span>           <span class=\"c1\"># Movie review text\n</span>                                 <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Do add specials.\n</span>                                 <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"n\">max_len</span><span class=\"p\">,</span>  <span class=\"c1\"># Do truncate to `max_len`\n</span>                                 <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>     <span class=\"c1\"># Do truncate!\n</span>                                 <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>       <span class=\"c1\"># Don't pad!\n</span>                                 \n    <span class=\"c1\"># Add the tokenized result to our list.\n</span>    <span class=\"n\">full_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'DONE.'</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"n\">full_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Tokenize all training examples\n</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} training samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># Choose an interval on which to print progress updates.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># For each training example...\n</span><span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">train_text</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># Report progress.\n</span>    <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Tokenize the sentence.\n</span>    <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">text</span><span class=\"p\">,</span>           <span class=\"c1\"># Movie review text\n</span>                                 <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Do add specials.\n</span>                                 <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"n\">max_len</span><span class=\"p\">,</span>  <span class=\"c1\"># Do truncate to `max_len`\n</span>                                 <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>     <span class=\"c1\"># Do truncate!\n</span>                                 <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>       <span class=\"c1\"># Don't pad!\n</span>                                 \n    <span class=\"c1\"># Add the tokenized result to our list.\n</span>    <span class=\"n\">full_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'DONE.'</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\">Tokenizing 25,000 training samples...\nTokenized 0 samples.\nTokenized 2,000 samples.\nTokenized 4,000 samples.\nTokenized 6,000 samples.\nTokenized 8,000 samples.\nTokenized 10,000 samples.\nTokenized 12,000 samples.\nTokenized 14,000 samples.\nTokenized 16,000 samples.\nTokenized 18,000 samples.\nTokenized 20,000 samples.\nTokenized 22,000 samples.\nTokenized 24,000 samples.\nDONE.\n25,000 samples\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\">Tokenizing 25,000 training samples...\nTokenized 0 samples.\nTokenized 2,000 samples.\nTokenized 4,000 samples.\nTokenized 6,000 samples.\nTokenized 8,000 samples.\nTokenized 10,000 samples.\nTokenized 12,000 samples.\nTokenized 14,000 samples.\nTokenized 16,000 samples.\nTokenized 18,000 samples.\nTokenized 20,000 samples.\nTokenized 22,000 samples.\nTokenized 24,000 samples.\nDONE.\n25,000 samples\n</code></pre>",
    "contentMarkdown": "#### Peak GPU Memory Use\n\n*   Even when applying smart batching, we may still want to truncate our inputs to a certain maximum length. BERT requires a lot of GPU memory, and it’s quite possible for the GPU to not be able to process a batch with too many samples in it and / or too long of sequences.\n    \n*   Smart batching means most of our batches will naturally have shorter sequence lengths and not require too much memory. However, all it takes is one batch that’s too long to fit on the GPU, and our **training will fail**!\n    \n*   In other words, we still have to be concerned with our “peak” memory usage, and it still likely makes sense to truncate to something lower than 512, even with smart batching.\n    \n\nEven when applying smart batching, we may still want to truncate our inputs to a certain maximum length. BERT requires a lot of GPU memory, and it’s quite possible for the GPU to not be able to process a batch with too many samples in it and / or too long of sequences.\n\nSmart batching means most of our batches will naturally have shorter sequence lengths and not require too much memory. However, all it takes is one batch that’s too long to fit on the GPU, and our **training will fail**!\n\nIn other words, we still have to be concerned with our “peak” memory usage, and it still likely makes sense to truncate to something lower than 512, even with smart batching.\n\n![](https://aman.ai/images/copy.png)\n\n`max_len = 400`\n\n![](https://aman.ai/images/copy.png)\n\n`max_len = 400`\n\n#### Tokenize, but Don’t Pad\n\n*   We’re going to start by tokenizing all of the samples and mapping the tokens to their IDs.\n    \n*   We’re also going to truncate the sequences to our chosen `max_len`, and we’re going to add the special tokens.\n    \n*   But we are not padding yet! We don’t know what lengths to pad the sequences too until after we’ve grouped them into batches.\n    \n\nWe’re going to start by tokenizing all of the samples and mapping the tokens to their IDs.\n\nWe’re also going to truncate the sequences to our chosen `max_len`, and we’re going to add the special tokens.\n\nBut we are not padding yet! We don’t know what lengths to pad the sequences too until after we’ve grouped them into batches.\n\n![](https://aman.ai/images/copy.png)\n\n``full_input_ids = [] labels = []  # Tokenize all training examples print('Tokenizing {:,} training samples...'.format(len(train_text)))  # Choose an interval on which to print progress updates. update_interval = good_update_interval(total_iters=len(train_text), num_desired_updates=10)  # For each training example... for text in train_text:          # Report progress.     if ((len(full_input_ids) % update_interval) == 0):         print('  Tokenized {:,} samples.'.format(len(full_input_ids)))      # Tokenize the sentence.     input_ids = tokenizer.encode(text=text,           # Movie review text                                  add_special_tokens=True, # Do add specials.                                  max_length=max_len,  # Do truncate to `max_len`                                  truncation=True,     # Do truncate!                                  padding=False)       # Don't pad!                                       # Add the tokenized result to our list.     full_input_ids.append(input_ids)      print('DONE.') print('{:>10,} samples'.format(len(full_input_ids)))``\n\n![](https://aman.ai/images/copy.png)\n\n``full_input_ids = [] labels = []  # Tokenize all training examples print('Tokenizing {:,} training samples...'.format(len(train_text)))  # Choose an interval on which to print progress updates. update_interval = good_update_interval(total_iters=len(train_text), num_desired_updates=10)  # For each training example... for text in train_text:          # Report progress.     if ((len(full_input_ids) % update_interval) == 0):         print('  Tokenized {:,} samples.'.format(len(full_input_ids)))      # Tokenize the sentence.     input_ids = tokenizer.encode(text=text,           # Movie review text                                  add_special_tokens=True, # Do add specials.                                  max_length=max_len,  # Do truncate to `max_len`                                  truncation=True,     # Do truncate!                                  padding=False)       # Don't pad!                                       # Add the tokenized result to our list.     full_input_ids.append(input_ids)      print('DONE.') print('{:>10,} samples'.format(len(full_input_ids)))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Tokenizing 25,000 training samples... Tokenized 0 samples. Tokenized 2,000 samples. Tokenized 4,000 samples. Tokenized 6,000 samples. Tokenized 8,000 samples. Tokenized 10,000 samples. Tokenized 12,000 samples. Tokenized 14,000 samples. Tokenized 16,000 samples. Tokenized 18,000 samples. Tokenized 20,000 samples. Tokenized 22,000 samples. Tokenized 24,000 samples. DONE. 25,000 samples`\n\n![](https://aman.ai/images/copy.png)\n\n`Tokenizing 25,000 training samples... Tokenized 0 samples. Tokenized 2,000 samples. Tokenized 4,000 samples. Tokenized 6,000 samples. Tokenized 8,000 samples. Tokenized 10,000 samples. Tokenized 12,000 samples. Tokenized 14,000 samples. Tokenized 16,000 samples. Tokenized 18,000 samples. Tokenized 20,000 samples. Tokenized 22,000 samples. Tokenized 24,000 samples. DONE. 25,000 samples`",
    "contentLength": 12746,
    "wordCount": 673,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#tokenize-without-padding"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-sort-by-length-7",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Sort by Length",
    "order": 7,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Before we sort the samples by length, let’s look at the lengths of the samples in their original, unsorted order.</p>\n  </li>\n  <li>\n    <p>The below plot simply confirms that the sample lengths do vary significantly, and that they are unsorted.</p>\n  </li>\n</ul>\n<p>Before we sort the samples by length, let’s look at the lengths of the samples in their original, unsorted order.</p>\n<p>The below plot simply confirms that the sample lengths do vary significantly, and that they are unsorted.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"c1\"># Get all of the lengths.\n</span><span class=\"n\">unsorted_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">full_input_ids</span><span class=\"p\">]</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n\n<span class=\"c1\"># Use plot styling from seaborn.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">unsorted_lengths</span><span class=\"p\">)),</span> <span class=\"n\">unsorted_lengths</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s\">\"|\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Sample Number'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'Sequence Length'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Samples BEFORE Sorting'</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"c1\"># Get all of the lengths.\n</span><span class=\"n\">unsorted_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">full_input_ids</span><span class=\"p\">]</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n\n<span class=\"c1\"># Use plot styling from seaborn.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">unsorted_lengths</span><span class=\"p\">)),</span> <span class=\"n\">unsorted_lengths</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s\">\"|\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Sample Number'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'Sequence Length'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Samples BEFORE Sorting'</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>The following plot shows samples before sorting:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/fine-tune-and-eval-BERT/samples_before_sorting.png\" alt=\"\"></p>\n<ul>\n  <li>Now we’ll sort the examples by length so that we can create batches with equal (or at least similar) lengths.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"c1\"># Sort the two lists together by the length of the input sequence.\n</span><span class=\"n\">train_samples</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">,</span> <span class=\"n\">train_labels</span><span class=\"p\">),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"c1\"># Sort the two lists together by the length of the input sequence.\n</span><span class=\"n\">train_samples</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">,</span> <span class=\"n\">train_labels</span><span class=\"p\">),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n</code></pre>\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">train_samples</code> is now a list of tuples of (input_ids, label):</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">([101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102],\n 0)\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">([101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102],\n 0)\n</code></pre>\n<ul>\n  <li>Now let’s check out the shortest/longest sample:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">print('Shortest sample:', len(train_samples[0][0]))\nprint('Longest sample:', len(train_samples[-1][0]))\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">print('Shortest sample:', len(train_samples[0][0]))\nprint('Longest sample:', len(train_samples[-1][0]))\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"n\">Shortest</span> <span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"mi\">13</span>\n<span class=\"n\">Longest</span> <span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"mi\">400</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"n\">Shortest</span> <span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"mi\">13</span>\n<span class=\"n\">Longest</span> <span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"mi\">400</span>\n</code></pre>\n<ul>\n  <li>Let’s generate the same plot again, now that the samples are sorted by length.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"c1\"># Get the new list of lengths after sorting.\n</span><span class=\"n\">sorted_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">train_samples</span><span class=\"p\">]</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n\n<span class=\"c1\"># Use plot styling from seaborn.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sorted_lengths</span><span class=\"p\">)),</span> <span class=\"n\">sorted_lengths</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Sample Number'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'Sequence Length'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Samples after Sorting'</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"c1\"># Get the new list of lengths after sorting.\n</span><span class=\"n\">sorted_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">train_samples</span><span class=\"p\">]</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"n\">sns</span>\n\n<span class=\"c1\"># Use plot styling from seaborn.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s\">'darkgrid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Increase the plot size and font size.\n</span><span class=\"n\">sns</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s\">\"figure.figsize\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sorted_lengths</span><span class=\"p\">)),</span> <span class=\"n\">sorted_lengths</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Sample Number'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'Sequence Length'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Samples after Sorting'</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>The following plot shows samples after sorting:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/fine-tune-and-eval-BERT/samples_after_sorting.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Before we sort the samples by length, let’s look at the lengths of the samples in their original, unsorted order.\n    \n*   The below plot simply confirms that the sample lengths do vary significantly, and that they are unsorted.\n    \n\nBefore we sort the samples by length, let’s look at the lengths of the samples in their original, unsorted order.\n\nThe below plot simply confirms that the sample lengths do vary significantly, and that they are unsorted.\n\n![](https://aman.ai/images/copy.png)\n\n`# Get all of the lengths. unsorted_lengths = [len(x) for x in full_input_ids]  import matplotlib.pyplot as plt import seaborn as sns  # Use plot styling from seaborn. sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6)  plt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")  plt.xlabel('Sample Number') plt.ylabel('Sequence Length') plt.title('Samples BEFORE Sorting')  plt.show()`\n\n![](https://aman.ai/images/copy.png)\n\n`# Get all of the lengths. unsorted_lengths = [len(x) for x in full_input_ids]  import matplotlib.pyplot as plt import seaborn as sns  # Use plot styling from seaborn. sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6)  plt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")  plt.xlabel('Sample Number') plt.ylabel('Sequence Length') plt.title('Samples BEFORE Sorting')  plt.show()`\n\n*   The following plot shows samples before sorting:\n\n![](/primers/ai/assets/fine-tune-and-eval-BERT/samples_before_sorting.png)\n\n*   Now we’ll sort the examples by length so that we can create batches with equal (or at least similar) lengths.\n\n![](https://aman.ai/images/copy.png)\n\n`# Sort the two lists together by the length of the input sequence. train_samples = sorted(zip(full_input_ids, train_labels), key=lambda x: len(x[0]))`\n\n![](https://aman.ai/images/copy.png)\n\n`# Sort the two lists together by the length of the input sequence. train_samples = sorted(zip(full_input_ids, train_labels), key=lambda x: len(x[0]))`\n\n*   `train_samples` is now a list of tuples of (input\\_ids, label):\n\n![](https://aman.ai/images/copy.png)\n\n`train_samples[0]`\n\n![](https://aman.ai/images/copy.png)\n\n`train_samples[0]`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`([101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102],  0)`\n\n![](https://aman.ai/images/copy.png)\n\n`([101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102],  0)`\n\n*   Now let’s check out the shortest/longest sample:\n\n![](https://aman.ai/images/copy.png)\n\n`print('Shortest sample:', len(train_samples[0][0])) print('Longest sample:', len(train_samples[-1][0]))`\n\n![](https://aman.ai/images/copy.png)\n\n`print('Shortest sample:', len(train_samples[0][0])) print('Longest sample:', len(train_samples[-1][0]))`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Shortest sample: 13 Longest sample: 400`\n\n![](https://aman.ai/images/copy.png)\n\n`Shortest sample: 13 Longest sample: 400`\n\n*   Let’s generate the same plot again, now that the samples are sorted by length.\n\n![](https://aman.ai/images/copy.png)\n\n`# Get the new list of lengths after sorting. sorted_lengths = [len(s[0]) for s in train_samples]  import matplotlib.pyplot as plt import seaborn as sns  # Use plot styling from seaborn. sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6)  plt.plot(range(0, len(sorted_lengths)), sorted_lengths)  plt.xlabel('Sample Number') plt.ylabel('Sequence Length') plt.title('Samples after Sorting')  plt.show()`\n\n![](https://aman.ai/images/copy.png)\n\n`# Get the new list of lengths after sorting. sorted_lengths = [len(s[0]) for s in train_samples]  import matplotlib.pyplot as plt import seaborn as sns  # Use plot styling from seaborn. sns.set(style='darkgrid')  # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6)  plt.plot(range(0, len(sorted_lengths)), sorted_lengths)  plt.xlabel('Sample Number') plt.ylabel('Sequence Length') plt.title('Samples after Sorting')  plt.show()`\n\n*   The following plot shows samples after sorting:\n\n![](/primers/ai/assets/fine-tune-and-eval-BERT/samples_after_sorting.png)",
    "contentLength": 18633,
    "wordCount": 475,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#sort-by-length"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-random-batch-selection-8",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Random Batch Selection",
    "order": 8,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Choose our batch size.</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">batch_size = 16\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">batch_size = 16\n</code></pre>\n<ul>\n  <li>\n    <p>Now we’re ready to select our batches.</p>\n  </li>\n  <li>\n    <p>The strategy used here comes from Michaël Benesty’s <a href=\"https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8\">code</a> (also, blog post is <a href=\"https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\">here</a>), in his <code class=\"language-plaintext highlighter-rouge\">build_batches</code> function.</p>\n  </li>\n  <li>\n    <p>Rather than dividing the batches up in order, we will still add a degree of <strong>randomness</strong> to our selection.</p>\n  </li>\n  <li>\n    <p>Here’s the process:</p>\n\n    <ul>\n      <li>Pick a random starting point in the (sorted!) list of samples.</li>\n      <li>Grab a contiguous batch of samples starting from that point.</li>\n      <li>Delete those samples from the list, and repeat until all of the samples have been grabbed.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This will result in some <strong>fragmentation</strong> of the list, which means it won’t be quite as efficient as if we just sliced up the batches in sorted order.</p>\n  </li>\n  <li>\n    <p>The benefit is that our path through the training set can still have a degree of randomness. Also, given the distribution of lengths that we saw in the previous section (lots of samples with similar lengths), the fragmentation problem should be pretty minor!</p>\n  </li>\n</ul>\n<p>Now we’re ready to select our batches.</p>\n<p>The strategy used here comes from Michaël Benesty’s <a href=\"https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8\">code</a> (also, blog post is <a href=\"https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\">here</a>), in his <code class=\"language-plaintext highlighter-rouge\">build_batches</code> function.</p>\n<p>Rather than dividing the batches up in order, we will still add a degree of <strong>randomness</strong> to our selection.</p>\n<p>Here’s the process:</p>\n<ul>\n      <li>Pick a random starting point in the (sorted!) list of samples.</li>\n      <li>Grab a contiguous batch of samples starting from that point.</li>\n      <li>Delete those samples from the list, and repeat until all of the samples have been grabbed.</li>\n    </ul>\n<p>This will result in some <strong>fragmentation</strong> of the list, which means it won’t be quite as efficient as if we just sliced up the batches in sorted order.</p>\n<p>The benefit is that our path through the training set can still have a degree of randomness. Also, given the distribution of lengths that we saw in the previous section (lots of samples with similar lengths), the fragmentation problem should be pretty minor!</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\"><span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n<span class=\"c1\"># List of batches that we'll construct.\n</span><span class=\"n\">batch_ordered_sentences</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">batch_ordered_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating training batches of size {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Loop over all of the input samples...    \n</span><span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># Report progress.\n</span>    <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"mi\">500</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Selected {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># `to_take` is our actual batch size. It will be `batch_size` until \n</span>    <span class=\"c1\"># we get to the last batch, which may be smaller. \n</span>    <span class=\"n\">to_take</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Pick a random index in the list of remaining samples to start\n</span>    <span class=\"c1\"># our batch at.\n</span>    <span class=\"n\">select</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">to_take</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Select a contiguous batch of samples starting at `select`.\n</span>    <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:(</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">)]</span>\n\n    <span class=\"c1\"># Each sample is a tuple--split them apart to create a separate list of \n</span>    <span class=\"c1\"># sequences and a list of labels for this batch.\n</span>    <span class=\"n\">batch_ordered_sentences</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n    <span class=\"n\">batch_ordered_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># Remove these samples from the list.\n</span>    <span class=\"k\">del</span> <span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">]</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">  DONE - {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\"><span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n<span class=\"c1\"># List of batches that we'll construct.\n</span><span class=\"n\">batch_ordered_sentences</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">batch_ordered_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating training batches of size {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Loop over all of the input samples...    \n</span><span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># Report progress.\n</span>    <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"mi\">500</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Selected {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># `to_take` is our actual batch size. It will be `batch_size` until \n</span>    <span class=\"c1\"># we get to the last batch, which may be smaller. \n</span>    <span class=\"n\">to_take</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Pick a random index in the list of remaining samples to start\n</span>    <span class=\"c1\"># our batch at.\n</span>    <span class=\"n\">select</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_samples</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">to_take</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Select a contiguous batch of samples starting at `select`.\n</span>    <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:(</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">)]</span>\n\n    <span class=\"c1\"># Each sample is a tuple--split them apart to create a separate list of \n</span>    <span class=\"c1\"># sequences and a list of labels for this batch.\n</span>    <span class=\"n\">batch_ordered_sentences</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n    <span class=\"n\">batch_ordered_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># Remove these samples from the list.\n</span>    <span class=\"k\">del</span> <span class=\"n\">train_samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">]</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">  DONE - {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code25\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code25\">Creating training batches of size 16\nSelected 0 batches.\nSelected 500 batches.\nSelected 1,000 batches.\nSelected 1,500 batches.\n\nDONE - 1,563 batches.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code25\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code25\">Creating training batches of size 16\nSelected 0 batches.\nSelected 500 batches.\nSelected 1,000 batches.\nSelected 1,500 batches.\n\nDONE - 1,563 batches.\n</code></pre>",
    "contentMarkdown": "*   Choose our batch size.\n\n![](https://aman.ai/images/copy.png)\n\n`batch_size = 16`\n\n![](https://aman.ai/images/copy.png)\n\n`batch_size = 16`\n\n*   Now we’re ready to select our batches.\n    \n*   The strategy used here comes from Michaël Benesty’s [code](https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8) (also, blog post is [here](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e)), in his `build_batches` function.\n    \n*   Rather than dividing the batches up in order, we will still add a degree of **randomness** to our selection.\n    \n*   Here’s the process:\n    \n    *   Pick a random starting point in the (sorted!) list of samples.\n    *   Grab a contiguous batch of samples starting from that point.\n    *   Delete those samples from the list, and repeat until all of the samples have been grabbed.\n*   This will result in some **fragmentation** of the list, which means it won’t be quite as efficient as if we just sliced up the batches in sorted order.\n    \n*   The benefit is that our path through the training set can still have a degree of randomness. Also, given the distribution of lengths that we saw in the previous section (lots of samples with similar lengths), the fragmentation problem should be pretty minor!\n    \n\nNow we’re ready to select our batches.\n\nThe strategy used here comes from Michaël Benesty’s [code](https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8) (also, blog post is [here](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e)), in his `build_batches` function.\n\nRather than dividing the batches up in order, we will still add a degree of **randomness** to our selection.\n\nHere’s the process:\n\n*   Pick a random starting point in the (sorted!) list of samples.\n*   Grab a contiguous batch of samples starting from that point.\n*   Delete those samples from the list, and repeat until all of the samples have been grabbed.\n\nThis will result in some **fragmentation** of the list, which means it won’t be quite as efficient as if we just sliced up the batches in sorted order.\n\nThe benefit is that our path through the training set can still have a degree of randomness. Also, given the distribution of lengths that we saw in the previous section (lots of samples with similar lengths), the fragmentation problem should be pretty minor!\n\n![](https://aman.ai/images/copy.png)\n\n``import random  # List of batches that we'll construct. batch_ordered_sentences = [] batch_ordered_labels = []  print('Creating training batches of size {:}'.format(batch_size))  # Loop over all of the input samples...     while len(train_samples) > 0:          # Report progress.     if ((len(batch_ordered_sentences) % 500) == 0):         print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))      # `to_take` is our actual batch size. It will be `batch_size` until      # we get to the last batch, which may be smaller.      to_take = min(batch_size, len(train_samples))      # Pick a random index in the list of remaining samples to start     # our batch at.     select = random.randint(0, len(train_samples) - to_take)      # Select a contiguous batch of samples starting at `select`.     batch = train_samples[select:(select + to_take)]      # Each sample is a tuple--split them apart to create a separate list of      # sequences and a list of labels for this batch.     batch_ordered_sentences.append([s[0] for s in batch])     batch_ordered_labels.append([s[1] for s in batch])      # Remove these samples from the list.     del train_samples[select:select + to_take]  print('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))``\n\n![](https://aman.ai/images/copy.png)\n\n``import random  # List of batches that we'll construct. batch_ordered_sentences = [] batch_ordered_labels = []  print('Creating training batches of size {:}'.format(batch_size))  # Loop over all of the input samples...     while len(train_samples) > 0:          # Report progress.     if ((len(batch_ordered_sentences) % 500) == 0):         print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))      # `to_take` is our actual batch size. It will be `batch_size` until      # we get to the last batch, which may be smaller.      to_take = min(batch_size, len(train_samples))      # Pick a random index in the list of remaining samples to start     # our batch at.     select = random.randint(0, len(train_samples) - to_take)      # Select a contiguous batch of samples starting at `select`.     batch = train_samples[select:(select + to_take)]      # Each sample is a tuple--split them apart to create a separate list of      # sequences and a list of labels for this batch.     batch_ordered_sentences.append([s[0] for s in batch])     batch_ordered_labels.append([s[1] for s in batch])      # Remove these samples from the list.     del train_samples[select:select + to_take]  print('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Creating training batches of size 16 Selected 0 batches. Selected 500 batches. Selected 1,000 batches. Selected 1,500 batches.  DONE - 1,563 batches.`\n\n![](https://aman.ai/images/copy.png)\n\n`Creating training batches of size 16 Selected 0 batches. Selected 500 batches. Selected 1,000 batches. Selected 1,500 batches.  DONE - 1,563 batches.`",
    "contentLength": 14155,
    "wordCount": 708,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#random-batch-selection"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-add-padding-9",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Add Padding",
    "order": 9,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>We’ve created our batches, but many of them will contain sequences of different lengths. In order to leverage the GPUs parallel processing of batches, all of the sequences within a batch need to be the same length.</p>\n  </li>\n  <li>\n    <p>This means we need to do some padding!</p>\n  </li>\n  <li>\n    <p>We’ll also create our <strong>attention masks</strong> here, and cast everything to <strong>PyTorch tensors</strong> in preparation for our fine-tuning step.</p>\n  </li>\n</ul>\n<p>We’ve created our batches, but many of them will contain sequences of different lengths. In order to leverage the GPUs parallel processing of batches, all of the sequences within a batch need to be the same length.</p>\n<p>This means we need to do some padding!</p>\n<p>We’ll also create our <strong>attention masks</strong> here, and cast everything to <strong>PyTorch tensors</strong> in preparation for our fine-tuning step.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code26\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code26\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># For each batch...\n</span><span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_labels</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">,</span> <span class=\"n\">batch_ordered_labels</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># New version of the batch, this time with padded sequences and now with\n</span>    <span class=\"c1\"># attention masks defined.\n</span>    <span class=\"n\">batch_padded_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batch_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    \n    <span class=\"c1\"># First, find the longest sample in the batch. \n</span>    <span class=\"c1\"># Note that the sequences do currently include the special tokens!\n</span>    <span class=\"n\">max_size</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">([</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">])</span>\n\n    <span class=\"c1\">#print('Max size:', max_size)\n</span>\n    <span class=\"c1\"># For each input in this batch...\n</span>    <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># How many pad tokens do we need to add?\n</span>        <span class=\"n\">num_pads</span> <span class=\"o\">=</span> <span class=\"n\">max_size</span> <span class=\"o\">-</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Add `num_pads` padding tokens to the end of the sequence.\n</span>        <span class=\"n\">padded_input</span> <span class=\"o\">=</span> <span class=\"n\">sen</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pad_token_id</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">num_pads</span>\n\n        <span class=\"c1\"># Define the attention mask--it's just a `1` for every real token\n</span>        <span class=\"c1\"># and a `0` for every padding token.\n</span>        <span class=\"n\">attn_mask</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">num_pads</span>\n\n        <span class=\"c1\"># Add the padded results to the batch.\n</span>        <span class=\"n\">batch_padded_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">padded_input</span><span class=\"p\">)</span>\n        <span class=\"n\">batch_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">attn_mask</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Our batch has been padded, so we need to save this updated batch.\n</span>    <span class=\"c1\"># We also need the inputs to be PyTorch tensors, so we'll do that here.\n</span>    <span class=\"n\">py_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_padded_inputs</span><span class=\"p\">))</span>\n    <span class=\"n\">py_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_attn_masks</span><span class=\"p\">))</span>\n    <span class=\"n\">py_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_labels</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code26\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code26\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># For each batch...\n</span><span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_labels</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">,</span> <span class=\"n\">batch_ordered_labels</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># New version of the batch, this time with padded sequences and now with\n</span>    <span class=\"c1\"># attention masks defined.\n</span>    <span class=\"n\">batch_padded_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batch_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    \n    <span class=\"c1\"># First, find the longest sample in the batch. \n</span>    <span class=\"c1\"># Note that the sequences do currently include the special tokens!\n</span>    <span class=\"n\">max_size</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">([</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">])</span>\n\n    <span class=\"c1\">#print('Max size:', max_size)\n</span>\n    <span class=\"c1\"># For each input in this batch...\n</span>    <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># How many pad tokens do we need to add?\n</span>        <span class=\"n\">num_pads</span> <span class=\"o\">=</span> <span class=\"n\">max_size</span> <span class=\"o\">-</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Add `num_pads` padding tokens to the end of the sequence.\n</span>        <span class=\"n\">padded_input</span> <span class=\"o\">=</span> <span class=\"n\">sen</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pad_token_id</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">num_pads</span>\n\n        <span class=\"c1\"># Define the attention mask--it's just a `1` for every real token\n</span>        <span class=\"c1\"># and a `0` for every padding token.\n</span>        <span class=\"n\">attn_mask</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">num_pads</span>\n\n        <span class=\"c1\"># Add the padded results to the batch.\n</span>        <span class=\"n\">batch_padded_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">padded_input</span><span class=\"p\">)</span>\n        <span class=\"n\">batch_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">attn_mask</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Our batch has been padded, so we need to save this updated batch.\n</span>    <span class=\"c1\"># We also need the inputs to be PyTorch tensors, so we'll do that here.\n</span>    <span class=\"n\">py_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_padded_inputs</span><span class=\"p\">))</span>\n    <span class=\"n\">py_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_attn_masks</span><span class=\"p\">))</span>\n    <span class=\"n\">py_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_labels</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>Now that our data is ready, we can calculate the total number of tokens in the training data after using smart batching.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code27\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code27\"><span class=\"c1\"># Get the new list of lengths after sorting.\n</span><span class=\"n\">padded_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># For each batch...\n</span><span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">py_inputs</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># For each sample...\n</span>    <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">:</span>\n    \n        <span class=\"c1\"># Record its length.\n</span>        <span class=\"n\">padded_lengths</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Sum up the lengths to the get the total number of tokens after smart batching.\n</span><span class=\"n\">smart_token_count</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">padded_lengths</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># To get the total number of tokens in the dataset using fixed padding, it's\n# as simple as the number of samples times our `max_len` parameter (that we\n# would pad everything to).\n</span><span class=\"n\">fixed_token_count</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">max_len</span>\n\n<span class=\"c1\"># Calculate the percentage reduction.\n</span><span class=\"n\">prcnt_reduced</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">fixed_token_count</span> <span class=\"o\">-</span> <span class=\"n\">smart_token_count</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">fixed_token_count</span><span class=\"p\">)</span> \n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Total tokens:'</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'   Fixed Padding: {:,}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">fixed_token_count</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Smart Batching: {:,}  ({:.1%} less)'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">smart_token_count</span><span class=\"p\">,</span> <span class=\"n\">prcnt_reduced</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code27\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code27\"><span class=\"c1\"># Get the new list of lengths after sorting.\n</span><span class=\"n\">padded_lengths</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># For each batch...\n</span><span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">py_inputs</span><span class=\"p\">:</span>\n    \n    <span class=\"c1\"># For each sample...\n</span>    <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">:</span>\n    \n        <span class=\"c1\"># Record its length.\n</span>        <span class=\"n\">padded_lengths</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Sum up the lengths to the get the total number of tokens after smart batching.\n</span><span class=\"n\">smart_token_count</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">padded_lengths</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># To get the total number of tokens in the dataset using fixed padding, it's\n# as simple as the number of samples times our `max_len` parameter (that we\n# would pad everything to).\n</span><span class=\"n\">fixed_token_count</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">max_len</span>\n\n<span class=\"c1\"># Calculate the percentage reduction.\n</span><span class=\"n\">prcnt_reduced</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">fixed_token_count</span> <span class=\"o\">-</span> <span class=\"n\">smart_token_count</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">fixed_token_count</span><span class=\"p\">)</span> \n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Total tokens:'</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'   Fixed Padding: {:,}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">fixed_token_count</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Smart Batching: {:,}  ({:.1%} less)'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">smart_token_count</span><span class=\"p\">,</span> <span class=\"n\">prcnt_reduced</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code28\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code28\">Total tokens:\nFixed Padding: 10,000,000\nSmart Batching: 6,381,424  (36.2% less)\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code28\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code28\">Total tokens:\nFixed Padding: 10,000,000\nSmart Batching: 6,381,424  (36.2% less)\n</code></pre>\n<ul>\n  <li>We’ll see at the end that this reduction in token count corresponds well to the reduction in training time!</li>\n</ul>",
    "contentMarkdown": "*   We’ve created our batches, but many of them will contain sequences of different lengths. In order to leverage the GPUs parallel processing of batches, all of the sequences within a batch need to be the same length.\n    \n*   This means we need to do some padding!\n    \n*   We’ll also create our **attention masks** here, and cast everything to **PyTorch tensors** in preparation for our fine-tuning step.\n    \n\nWe’ve created our batches, but many of them will contain sequences of different lengths. In order to leverage the GPUs parallel processing of batches, all of the sequences within a batch need to be the same length.\n\nThis means we need to do some padding!\n\nWe’ll also create our **attention masks** here, and cast everything to **PyTorch tensors** in preparation for our fine-tuning step.\n\n![](https://aman.ai/images/copy.png)\n\n``import torch  py_inputs = [] py_attn_masks = [] py_labels = []  # For each batch... for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):      # New version of the batch, this time with padded sequences and now with     # attention masks defined.     batch_padded_inputs = []     batch_attn_masks = []          # First, find the longest sample in the batch.      # Note that the sequences do currently include the special tokens!     max_size = max([len(sen) for sen in batch_inputs])      #print('Max size:', max_size)     # For each input in this batch...     for sen in batch_inputs:                  # How many pad tokens do we need to add?         num_pads = max_size - len(sen)          # Add `num_pads` padding tokens to the end of the sequence.         padded_input = sen + [tokenizer.pad_token_id]*num_pads          # Define the attention mask--it's just a `1` for every real token         # and a `0` for every padding token.         attn_mask = [1] * len(sen) + [0] * num_pads          # Add the padded results to the batch.         batch_padded_inputs.append(padded_input)         batch_attn_masks.append(attn_mask)      # Our batch has been padded, so we need to save this updated batch.     # We also need the inputs to be PyTorch tensors, so we'll do that here.     py_inputs.append(torch.tensor(batch_padded_inputs))     py_attn_masks.append(torch.tensor(batch_attn_masks))     py_labels.append(torch.tensor(batch_labels))``\n\n![](https://aman.ai/images/copy.png)\n\n``import torch  py_inputs = [] py_attn_masks = [] py_labels = []  # For each batch... for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):      # New version of the batch, this time with padded sequences and now with     # attention masks defined.     batch_padded_inputs = []     batch_attn_masks = []          # First, find the longest sample in the batch.      # Note that the sequences do currently include the special tokens!     max_size = max([len(sen) for sen in batch_inputs])      #print('Max size:', max_size)     # For each input in this batch...     for sen in batch_inputs:                  # How many pad tokens do we need to add?         num_pads = max_size - len(sen)          # Add `num_pads` padding tokens to the end of the sequence.         padded_input = sen + [tokenizer.pad_token_id]*num_pads          # Define the attention mask--it's just a `1` for every real token         # and a `0` for every padding token.         attn_mask = [1] * len(sen) + [0] * num_pads          # Add the padded results to the batch.         batch_padded_inputs.append(padded_input)         batch_attn_masks.append(attn_mask)      # Our batch has been padded, so we need to save this updated batch.     # We also need the inputs to be PyTorch tensors, so we'll do that here.     py_inputs.append(torch.tensor(batch_padded_inputs))     py_attn_masks.append(torch.tensor(batch_attn_masks))     py_labels.append(torch.tensor(batch_labels))``\n\n*   Now that our data is ready, we can calculate the total number of tokens in the training data after using smart batching.\n\n![](https://aman.ai/images/copy.png)\n\n``# Get the new list of lengths after sorting. padded_lengths = []  # For each batch... for batch in py_inputs:          # For each sample...     for s in batch:              # Record its length.         padded_lengths.append(len(s))  # Sum up the lengths to the get the total number of tokens after smart batching. smart_token_count = np.sum(padded_lengths)  # To get the total number of tokens in the dataset using fixed padding, it's # as simple as the number of samples times our `max_len` parameter (that we # would pad everything to). fixed_token_count = len(train_text) * max_len  # Calculate the percentage reduction. prcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count)   print('Total tokens:') print('   Fixed Padding: {:,}'.format(fixed_token_count)) print('  Smart Batching: {:,}  ({:.1%} less)'.format(smart_token_count, prcnt_reduced))``\n\n![](https://aman.ai/images/copy.png)\n\n``# Get the new list of lengths after sorting. padded_lengths = []  # For each batch... for batch in py_inputs:          # For each sample...     for s in batch:              # Record its length.         padded_lengths.append(len(s))  # Sum up the lengths to the get the total number of tokens after smart batching. smart_token_count = np.sum(padded_lengths)  # To get the total number of tokens in the dataset using fixed padding, it's # as simple as the number of samples times our `max_len` parameter (that we # would pad everything to). fixed_token_count = len(train_text) * max_len  # Calculate the percentage reduction. prcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count)   print('Total tokens:') print('   Fixed Padding: {:,}'.format(fixed_token_count)) print('  Smart Batching: {:,}  ({:.1%} less)'.format(smart_token_count, prcnt_reduced))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Total tokens: Fixed Padding: 10,000,000 Smart Batching: 6,381,424  (36.2% less)`\n\n![](https://aman.ai/images/copy.png)\n\n`Total tokens: Fixed Padding: 10,000,000 Smart Batching: 6,381,424  (36.2% less)`\n\n*   We’ll see at the end that this reduction in token count corresponds well to the reduction in training time!",
    "contentLength": 18154,
    "wordCount": 811,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#add-padding"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-old-approach---fixed-padding-10",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Smart Batching",
    "title": "Old Approach - Fixed Padding",
    "order": 10,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>To see how BERT does on the benchmark without smart batching, you can run the following code instead of the above sections: <a href=\"#tokenize-without-padding\">Tokenize Without Padding</a>, <a href=\"#sort-by-length\">Sort by length</a>, <a href=\"#random-batch-selection\">Random Batch Selection</a> and <a href=\"#add-padding\">Add Padding</a>.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code29\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code29\"><span class=\"n\">use_fixed_padding</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n\n<span class=\"k\">if</span> <span class=\"n\">use_fixed_padding</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Specify batch_size and truncation length.    \n</span>    <span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n    <span class=\"n\">max_len</span> <span class=\"o\">=</span> <span class=\"mi\">400</span>   \n\n    <span class=\"c1\"># Tokenize all training examples\n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} training samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Tokenize all of the sentences and map the tokens to thier word IDs.\n</span>    <span class=\"n\">batches_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batches_attention_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batches_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">batch_size</span> <span class=\"o\">*</span> <span class=\"mi\">150</span> \n\n    <span class=\"c1\"># For every sentence...\n</span>    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">):</span>\n\n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># `encode_plus` will:\n</span>        <span class=\"c1\">#   (1) Tokenize the sentence.\n</span>        <span class=\"c1\">#   (2) Prepend the `[CLS]` token to the start.\n</span>        <span class=\"c1\">#   (3) Append the `[SEP]` token to the end.\n</span>        <span class=\"c1\">#   (4) Map tokens to their IDs.\n</span>        <span class=\"c1\">#   (5) Pad or truncate the sentence to `max_length`\n</span>        <span class=\"c1\">#   (6) Create attention masks for `[PAD]` tokens.\n</span>        <span class=\"n\">encoded_dict</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">batch_encode_plus</span><span class=\"p\">(</span>\n                            <span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">],</span> <span class=\"c1\"># Batch of sentences to encode.\n</span>                            <span class=\"n\">add_special_tokens</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Add '[CLS]' and '[SEP]'\n</span>                            <span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"mi\">400</span><span class=\"p\">,</span>           <span class=\"c1\"># Pad &amp; truncate all sentences.\n</span>                            <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"s\">'max_length'</span><span class=\"p\">,</span>     <span class=\"c1\"># Pad all to the `max_length` parameter.\n</span>                            <span class=\"n\">truncation</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n                            <span class=\"n\">return_attention_mask</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>   <span class=\"c1\"># Construct attn. masks.\n</span>                            <span class=\"n\">return_tensors</span> <span class=\"o\">=</span> <span class=\"s\">'pt'</span><span class=\"p\">,</span>     <span class=\"c1\"># Return pytorch tensors.\n</span>                    <span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Add the encoded sentence to the list.    \n</span>        <span class=\"n\">batches_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">encoded_dict</span><span class=\"p\">[</span><span class=\"s\">'input_ids'</span><span class=\"p\">])</span>\n        \n        <span class=\"c1\"># And its attention mask (simply differentiates padding from non-padding).\n</span>        <span class=\"n\">batches_attention_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">encoded_dict</span><span class=\"p\">[</span><span class=\"s\">'attention_mask'</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># Add the labels for the batch\n</span>        <span class=\"n\">batches_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">]))</span>\n    \n    <span class=\"c1\"># Rename the final variable to match the rest of the code in this Notebook.\n</span>    <span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"n\">batches_input_ids</span>\n    <span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"n\">batches_attention_masks</span>\n    <span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"n\">batches_labels</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code29\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code29\"><span class=\"n\">use_fixed_padding</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n\n<span class=\"k\">if</span> <span class=\"n\">use_fixed_padding</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Specify batch_size and truncation length.    \n</span>    <span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n    <span class=\"n\">max_len</span> <span class=\"o\">=</span> <span class=\"mi\">400</span>   \n\n    <span class=\"c1\"># Tokenize all training examples\n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} training samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Tokenize all of the sentences and map the tokens to thier word IDs.\n</span>    <span class=\"n\">batches_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batches_attention_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batches_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">batch_size</span> <span class=\"o\">*</span> <span class=\"mi\">150</span> \n\n    <span class=\"c1\"># For every sentence...\n</span>    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">):</span>\n\n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># `encode_plus` will:\n</span>        <span class=\"c1\">#   (1) Tokenize the sentence.\n</span>        <span class=\"c1\">#   (2) Prepend the `[CLS]` token to the start.\n</span>        <span class=\"c1\">#   (3) Append the `[SEP]` token to the end.\n</span>        <span class=\"c1\">#   (4) Map tokens to their IDs.\n</span>        <span class=\"c1\">#   (5) Pad or truncate the sentence to `max_length`\n</span>        <span class=\"c1\">#   (6) Create attention masks for `[PAD]` tokens.\n</span>        <span class=\"n\">encoded_dict</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">batch_encode_plus</span><span class=\"p\">(</span>\n                            <span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">],</span> <span class=\"c1\"># Batch of sentences to encode.\n</span>                            <span class=\"n\">add_special_tokens</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Add '[CLS]' and '[SEP]'\n</span>                            <span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"mi\">400</span><span class=\"p\">,</span>           <span class=\"c1\"># Pad &amp; truncate all sentences.\n</span>                            <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"s\">'max_length'</span><span class=\"p\">,</span>     <span class=\"c1\"># Pad all to the `max_length` parameter.\n</span>                            <span class=\"n\">truncation</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n                            <span class=\"n\">return_attention_mask</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>   <span class=\"c1\"># Construct attn. masks.\n</span>                            <span class=\"n\">return_tensors</span> <span class=\"o\">=</span> <span class=\"s\">'pt'</span><span class=\"p\">,</span>     <span class=\"c1\"># Return pytorch tensors.\n</span>                    <span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Add the encoded sentence to the list.    \n</span>        <span class=\"n\">batches_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">encoded_dict</span><span class=\"p\">[</span><span class=\"s\">'input_ids'</span><span class=\"p\">])</span>\n        \n        <span class=\"c1\"># And its attention mask (simply differentiates padding from non-padding).\n</span>        <span class=\"n\">batches_attention_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">encoded_dict</span><span class=\"p\">[</span><span class=\"s\">'attention_mask'</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># Add the labels for the batch\n</span>        <span class=\"n\">batches_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">train_labels</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">]))</span>\n    \n    <span class=\"c1\"># Rename the final variable to match the rest of the code in this Notebook.\n</span>    <span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"n\">batches_input_ids</span>\n    <span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"n\">batches_attention_masks</span>\n    <span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"n\">batches_labels</span>\n</code></pre>",
    "contentMarkdown": "*   To see how BERT does on the benchmark without smart batching, you can run the following code instead of the above sections: [Tokenize Without Padding](#tokenize-without-padding), [Sort by length](#sort-by-length), [Random Batch Selection](#random-batch-selection) and [Add Padding](#add-padding).\n\n![](https://aman.ai/images/copy.png)\n\n``use_fixed_padding = False  if use_fixed_padding:      # Specify batch_size and truncation length.         batch_size = 16     max_len = 400         # Tokenize all training examples     print('Tokenizing {:,} training samples...'.format(len(train_text)))      # Tokenize all of the sentences and map the tokens to thier word IDs.     batches_input_ids = []     batches_attention_masks = []     batches_labels = []      update_interval = batch_size * 150       # For every sentence...     for i in range(0, len(train_text), batch_size):          # Report progress.         if ((i % update_interval) == 0):             print('  Tokenized {:,} samples.'.format(i))          # `encode_plus` will:         #   (1) Tokenize the sentence.         #   (2) Prepend the `[CLS]` token to the start.         #   (3) Append the `[SEP]` token to the end.         #   (4) Map tokens to their IDs.         #   (5) Pad or truncate the sentence to `max_length`         #   (6) Create attention masks for `[PAD]` tokens.         encoded_dict = tokenizer.batch_encode_plus(                             train_text[i:i+batch_size], # Batch of sentences to encode.                             add_special_tokens = True,  # Add '[CLS]' and '[SEP]'                             max_length = 400,           # Pad & truncate all sentences.                             padding = 'max_length',     # Pad all to the `max_length` parameter.                             truncation = True,                             return_attention_mask = True,   # Construct attn. masks.                             return_tensors = 'pt',     # Return pytorch tensors.                     )                  # Add the encoded sentence to the list.             batches_input_ids.append(encoded_dict['input_ids'])                  # And its attention mask (simply differentiates padding from non-padding).         batches_attention_masks.append(encoded_dict['attention_mask'])          # Add the labels for the batch         batches_labels.append(torch.tensor(train_labels[i:i+batch_size]))          # Rename the final variable to match the rest of the code in this Notebook.     py_inputs = batches_input_ids     py_attn_masks = batches_attention_masks     py_labels = batches_labels``\n\n![](https://aman.ai/images/copy.png)\n\n``use_fixed_padding = False  if use_fixed_padding:      # Specify batch_size and truncation length.         batch_size = 16     max_len = 400         # Tokenize all training examples     print('Tokenizing {:,} training samples...'.format(len(train_text)))      # Tokenize all of the sentences and map the tokens to thier word IDs.     batches_input_ids = []     batches_attention_masks = []     batches_labels = []      update_interval = batch_size * 150       # For every sentence...     for i in range(0, len(train_text), batch_size):          # Report progress.         if ((i % update_interval) == 0):             print('  Tokenized {:,} samples.'.format(i))          # `encode_plus` will:         #   (1) Tokenize the sentence.         #   (2) Prepend the `[CLS]` token to the start.         #   (3) Append the `[SEP]` token to the end.         #   (4) Map tokens to their IDs.         #   (5) Pad or truncate the sentence to `max_length`         #   (6) Create attention masks for `[PAD]` tokens.         encoded_dict = tokenizer.batch_encode_plus(                             train_text[i:i+batch_size], # Batch of sentences to encode.                             add_special_tokens = True,  # Add '[CLS]' and '[SEP]'                             max_length = 400,           # Pad & truncate all sentences.                             padding = 'max_length',     # Pad all to the `max_length` parameter.                             truncation = True,                             return_attention_mask = True,   # Construct attn. masks.                             return_tensors = 'pt',     # Return pytorch tensors.                     )                  # Add the encoded sentence to the list.             batches_input_ids.append(encoded_dict['input_ids'])                  # And its attention mask (simply differentiates padding from non-padding).         batches_attention_masks.append(encoded_dict['attention_mask'])          # Add the labels for the batch         batches_labels.append(torch.tensor(train_labels[i:i+batch_size]))          # Rename the final variable to match the rest of the code in this Notebook.     py_inputs = batches_input_ids     py_attn_masks = batches_attention_masks     py_labels = batches_labels``",
    "contentLength": 13304,
    "wordCount": 505,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#old-approach---fixed-padding"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-load-pre-trained-model-11",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Fine-Tune BERT",
    "title": "Load Pre-Trained Model",
    "order": 11,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>We’ll use BERT-base-uncased for this example.</p>\n  </li>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">transformers</code> defines these <code class=\"language-plaintext highlighter-rouge\">Auto</code> classes which will automatically select the correct class for the pre-trained model that you specified.</p>\n  </li>\n</ul>\n<p>We’ll use BERT-base-uncased for this example.</p>\n<p><code class=\"language-plaintext highlighter-rouge\">transformers</code> defines these <code class=\"language-plaintext highlighter-rouge\">Auto</code> classes which will automatically select the correct class for the pre-trained model that you specified.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code30\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code30\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoConfig</span>\n\n<span class=\"c1\"># Load the Config object, with an output configured for classification.\n</span><span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">AutoConfig</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_model_name_or_path</span><span class=\"o\">=</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span>\n                                    <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Config type:'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)),</span> <span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code30\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code30\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoConfig</span>\n\n<span class=\"c1\"># Load the Config object, with an output configured for classification.\n</span><span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">AutoConfig</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_model_name_or_path</span><span class=\"o\">=</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span>\n                                    <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Config type:'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)),</span> <span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code31\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code31\">HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…\n\nConfig type: &lt;class 'transformers.configuration_bert.BertConfig'&gt; \n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code31\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code31\">HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…\n\nConfig type: &lt;class 'transformers.configuration_bert.BertConfig'&gt; \n</code></pre>\n<ul>\n  <li>Now let’s load the pre-trained model using the config:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code32\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code32\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForSequenceClassification</span>\n\n<span class=\"c1\"># Load the pre-trained model for classification, passing in the `config` from\n# above.\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForSequenceClassification</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span>\n    <span class=\"n\">pretrained_model_name_or_path</span><span class=\"o\">=</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">Model type:'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code32\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code32\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForSequenceClassification</span>\n\n<span class=\"c1\"># Load the pre-trained model for classification, passing in the `config` from\n# above.\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForSequenceClassification</span><span class=\"p\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span>\n    <span class=\"n\">pretrained_model_name_or_path</span><span class=\"o\">=</span><span class=\"s\">'bert-base-uncased'</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">Model type:'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code33\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code33\">HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…\n\nWARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nWARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nModel type: &lt;class 'transformers.modeling_bert.BertForSequenceClassification'&gt;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code33\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code33\">HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…\n\nWARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nWARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nModel type: &lt;class 'transformers.modeling_bert.BertForSequenceClassification'&gt;\n</code></pre>\n<ul>\n  <li>\n    <p>Connect to the GPU and load our model onto it.</p>\n  </li>\n  <li>\n    <p>It’s worth taking note of which GPU you’re given. The Tesla P100 is much faster, for example, than the Tesla K80.</p>\n  </li>\n</ul>\n<p>Connect to the GPU and load our model onto it.</p>\n<p>It’s worth taking note of which GPU you’re given. The Tesla P100 is much faster, for example, than the Tesla K80.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code34\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code34\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">Loading model to GPU...'</span><span class=\"p\">)</span>\n\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s\">'cuda'</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  GPU:'</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">get_device_name</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n\n<span class=\"n\">desc</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'    DONE.'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code34\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code34\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">Loading model to GPU...'</span><span class=\"p\">)</span>\n\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s\">'cuda'</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  GPU:'</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">get_device_name</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n\n<span class=\"n\">desc</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'    DONE.'</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code35\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code35\">Loading model to GPU...\n  GPU: Tesla K80\n    DONE.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code35\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code35\">Loading model to GPU...\n  GPU: Tesla K80\n    DONE.\n</code></pre>",
    "contentMarkdown": "*   We’ll use BERT-base-uncased for this example.\n    \n*   `transformers` defines these `Auto` classes which will automatically select the correct class for the pre-trained model that you specified.\n    \n\nWe’ll use BERT-base-uncased for this example.\n\n`transformers` defines these `Auto` classes which will automatically select the correct class for the pre-trained model that you specified.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoConfig  # Load the Config object, with an output configured for classification. config = AutoConfig.from_pretrained(pretrained_model_name_or_path='bert-base-uncased',                                     num_labels=2)  print('Config type:', str(type(config)), '\\n')`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AutoConfig  # Load the Config object, with an output configured for classification. config = AutoConfig.from_pretrained(pretrained_model_name_or_path='bert-base-uncased',                                     num_labels=2)  print('Config type:', str(type(config)), '\\n')`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…  Config type: <class 'transformers.configuration_bert.BertConfig'>` \n\n![](https://aman.ai/images/copy.png)\n\n`HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…  Config type: <class 'transformers.configuration_bert.BertConfig'>` \n\n*   Now let’s load the pre-trained model using the config:\n\n![](https://aman.ai/images/copy.png)\n\n``from transformers import AutoModelForSequenceClassification  # Load the pre-trained model for classification, passing in the `config` from # above. model = AutoModelForSequenceClassification.from_pretrained(     pretrained_model_name_or_path='bert-base-uncased',     config=config)  print('\\nModel type:', str(type(model)))``\n\n![](https://aman.ai/images/copy.png)\n\n``from transformers import AutoModelForSequenceClassification  # Load the pre-trained model for classification, passing in the `config` from # above. model = AutoModelForSequenceClassification.from_pretrained(     pretrained_model_name_or_path='bert-base-uncased',     config=config)  print('\\nModel type:', str(type(model)))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…  WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  Model type: <class 'transformers.modeling_bert.BertForSequenceClassification'>`\n\n![](https://aman.ai/images/copy.png)\n\n`HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…  WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  Model type: <class 'transformers.modeling_bert.BertForSequenceClassification'>`\n\n*   Connect to the GPU and load our model onto it.\n    \n*   It’s worth taking note of which GPU you’re given. The Tesla P100 is much faster, for example, than the Tesla K80.\n    \n\nConnect to the GPU and load our model onto it.\n\nIt’s worth taking note of which GPU you’re given. The Tesla P100 is much faster, for example, than the Tesla K80.\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  print('\\nLoading model to GPU...')  device = torch.device('cuda')  print('  GPU:', torch.cuda.get_device_name(0))  desc = model.to(device)  print('    DONE.')`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  print('\\nLoading model to GPU...')  device = torch.device('cuda')  print('  GPU:', torch.cuda.get_device_name(0))  desc = model.to(device)  print('    DONE.')`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Loading model to GPU...   GPU: Tesla K80     DONE.`\n\n![](https://aman.ai/images/copy.png)\n\n`Loading model to GPU...   GPU: Tesla K80     DONE.`",
    "contentLength": 15044,
    "wordCount": 575,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#load-pre-trained-model"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-optimizer-learning-rate-scheduler-12",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Fine-Tune BERT",
    "title": "Optimizer & Learning Rate Scheduler",
    "order": 12,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Set up our optimizer and learning rate scheduler for training.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code36\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code36\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AdamW</span>\n\n<span class=\"c1\"># Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# The 'W' likely stands for 'Weight Decay fix\"\n</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n                  <span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">5e-5</span><span class=\"p\">,</span> <span class=\"c1\"># This is the value Michael used.\n</span>                  <span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span> <span class=\"c1\"># args.adam_epsilon  - default is 1e-8.\n</span>                <span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code36\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code36\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AdamW</span>\n\n<span class=\"c1\"># Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# The 'W' likely stands for 'Weight Decay fix\"\n</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n                  <span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">5e-5</span><span class=\"p\">,</span> <span class=\"c1\"># This is the value Michael used.\n</span>                  <span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span> <span class=\"c1\"># args.adam_epsilon  - default is 1e-8.\n</span>                <span class=\"p\">)</span>\n</code></pre>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code37\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code37\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">get_linear_schedule_with_warmup</span>\n\n<span class=\"c1\"># Number of training epochs. We choose to train for one epoch simply because the training\n# time is long. More epochs may improve the model's accuracy.\n</span><span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n<span class=\"c1\"># Total number of training steps is [number of batches] x [number of epochs]. \n# Note that it's the number of *batches*, not *samples*!\n</span><span class=\"n\">total_steps</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">epochs</span>\n\n<span class=\"c1\"># Create the learning rate scheduler.\n</span><span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">get_linear_schedule_with_warmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> \n                                            <span class=\"n\">num_warmup_steps</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"c1\"># Default value in run_glue.py\n</span>                                            <span class=\"n\">num_training_steps</span> <span class=\"o\">=</span> <span class=\"n\">total_steps</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code37\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code37\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">get_linear_schedule_with_warmup</span>\n\n<span class=\"c1\"># Number of training epochs. We choose to train for one epoch simply because the training\n# time is long. More epochs may improve the model's accuracy.\n</span><span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n<span class=\"c1\"># Total number of training steps is [number of batches] x [number of epochs]. \n# Note that it's the number of *batches*, not *samples*!\n</span><span class=\"n\">total_steps</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">epochs</span>\n\n<span class=\"c1\"># Create the learning rate scheduler.\n</span><span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">get_linear_schedule_with_warmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> \n                                            <span class=\"n\">num_warmup_steps</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"c1\"># Default value in run_glue.py\n</span>                                            <span class=\"n\">num_training_steps</span> <span class=\"o\">=</span> <span class=\"n\">total_steps</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "*   Set up our optimizer and learning rate scheduler for training.\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AdamW  # Note: AdamW is a class from the huggingface library (as opposed to pytorch)  # The 'W' likely stands for 'Weight Decay fix\" optimizer = AdamW(model.parameters(),                   lr = 5e-5, # This is the value Michael used.                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8.                 )`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import AdamW  # Note: AdamW is a class from the huggingface library (as opposed to pytorch)  # The 'W' likely stands for 'Weight Decay fix\" optimizer = AdamW(model.parameters(),                   lr = 5e-5, # This is the value Michael used.                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8.                 )`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import get_linear_schedule_with_warmup  # Number of training epochs. We choose to train for one epoch simply because the training # time is long. More epochs may improve the model's accuracy. epochs = 1  # Total number of training steps is [number of batches] x [number of epochs].  # Note that it's the number of *batches*, not *samples*! total_steps = len(py_inputs) * epochs  # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer,                                              num_warmup_steps = 0, # Default value in run_glue.py                                             num_training_steps = total_steps)`\n\n![](https://aman.ai/images/copy.png)\n\n`from transformers import get_linear_schedule_with_warmup  # Number of training epochs. We choose to train for one epoch simply because the training # time is long. More epochs may improve the model's accuracy. epochs = 1  # Total number of training steps is [number of batches] x [number of epochs].  # Note that it's the number of *batches*, not *samples*! total_steps = len(py_inputs) * epochs  # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer,                                              num_warmup_steps = 0, # Default value in run_glue.py                                             num_training_steps = total_steps)`",
    "contentLength": 6040,
    "wordCount": 281,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#optimizer-&-learning-rate-scheduler"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-training-loop-13",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Fine-Tune BERT",
    "title": "Training Loop",
    "order": 13,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>In previous examples, we’ve made use of the PyTorch <code class=\"language-plaintext highlighter-rouge\">Dataset</code> and <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> classes, but because of the smart batching we’re not using them in this Notebook. However, check out the section below on how to use the <a href=\"#collate-function\"><code class=\"language-plaintext highlighter-rouge\">collate</code> function</a> with a standard <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> pipeline. However, check out the section below on how to use the <a href=\"#collate-function\"><code class=\"language-plaintext highlighter-rouge\">collate</code> function</a> with a standard <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> pipeline.</p>\n  </li>\n  <li>\n    <p>Note: If you have modified the code here to run for more than one epoch, you’ll need the <code class=\"language-plaintext highlighter-rouge\">make_smart_batches</code> function defined in the <a href=\"#make-smart-batches\">make_smart_batches</a> section.</p>\n  </li>\n  <li>\n    <p>We’re ready to kick off the training!</p>\n  </li>\n</ul>\n<p>In previous examples, we’ve made use of the PyTorch <code class=\"language-plaintext highlighter-rouge\">Dataset</code> and <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> classes, but because of the smart batching we’re not using them in this Notebook. However, check out the section below on how to use the <a href=\"#collate-function\"><code class=\"language-plaintext highlighter-rouge\">collate</code> function</a> with a standard <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> pipeline. However, check out the section below on how to use the <a href=\"#collate-function\"><code class=\"language-plaintext highlighter-rouge\">collate</code> function</a> with a standard <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> pipeline.</p>\n<p>Note: If you have modified the code here to run for more than one epoch, you’ll need the <code class=\"language-plaintext highlighter-rouge\">make_smart_batches</code> function defined in the <a href=\"#make-smart-batches\">make_smart_batches</a> section.</p>\n<p>We’re ready to kick off the training!</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code38\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code38\"><span class=\"kn\">import</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n</span>\n<span class=\"c1\"># Set the seed value all over the place to make this reproducible.\n</span><span class=\"n\">seed_val</span> <span class=\"o\">=</span> <span class=\"mi\">321</span>\n\n<span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">manual_seed_all</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\n</span><span class=\"n\">training_stats</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Update every `update_interval` batches.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Measure the total training time for the whole run.\n</span><span class=\"n\">total_t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># For each epoch...\n</span><span class=\"k\">for</span> <span class=\"n\">epoch_i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">):</span>\n    \n    <span class=\"c1\"># ========================================\n</span>    <span class=\"c1\">#               Training\n</span>    <span class=\"c1\"># ========================================\n</span>    \n    <span class=\"c1\"># Perform one full pass over the training set.\n</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'======== Epoch {:} / {:} ========'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">epoch_i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># At the start of each epoch (except for the first) we need to re-randomize\n</span>    <span class=\"c1\"># our training data.\n</span>    <span class=\"k\">if</span> <span class=\"n\">epoch_i</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Use our `make_smart_batches` function (from 6.1.) to re-shuffle the \n</span>        <span class=\"c1\"># dataset into new batches.\n</span>        <span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">,</span> <span class=\"n\">train_labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Training on {:,} batches...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Measure how long the training epoch takes.\n</span>    <span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Reset the total loss for this epoch.\n</span>    <span class=\"n\">total_train_loss</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"c1\"># Put the model into training mode. Don't be misled--the call to \n</span>    <span class=\"c1\"># `train` just changes the *mode*, it doesn't *perform* the training.\n</span>    <span class=\"c1\"># `dropout` and `batchnorm` layers behave differently during training\n</span>    <span class=\"c1\"># vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n</span>    <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># For each batch of training data...\n</span>    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)):</span>\n\n        <span class=\"c1\"># Progress update every, e.g., 100 batches.\n</span>        <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">step</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Calculate elapsed time in minutes.\n</span>            <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n            \n            <span class=\"c1\"># Calculate the time remaining based on our progress.\n</span>            <span class=\"n\">steps_per_sec</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">step</span>\n            <span class=\"n\">remaining_sec</span> <span class=\"o\">=</span> <span class=\"n\">steps_per_sec</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">step</span><span class=\"p\">)</span>\n            <span class=\"n\">remaining</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">remaining_sec</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Report progress.\n</span>            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Batch {:&gt;7,}  of  {:&gt;7,}.    Elapsed: {:}.  Remaining: {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">elapsed</span><span class=\"p\">,</span> <span class=\"n\">remaining</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># Copy the current training batch to the GPU using the `to` method.\n</span>        <span class=\"n\">b_input_ids</span> <span class=\"o\">=</span> <span class=\"n\">py_inputs</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">b_input_mask</span> <span class=\"o\">=</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">b_labels</span> <span class=\"o\">=</span> <span class=\"n\">py_labels</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Always clear any previously calculated gradients before performing a\n</span>        <span class=\"c1\"># backward pass.\n</span>        <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>        \n\n        <span class=\"c1\"># Perform a forward pass (evaluate the model on this training batch).\n</span>        <span class=\"c1\"># The call returns the loss (because we provided labels) and the \n</span>        <span class=\"c1\"># \"logits\"--the model outputs prior to activation.\n</span>        <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">b_input_ids</span><span class=\"p\">,</span> \n                             <span class=\"n\">token_type_ids</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> \n                             <span class=\"n\">attention_mask</span><span class=\"o\">=</span><span class=\"n\">b_input_mask</span><span class=\"p\">,</span> \n                             <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">b_labels</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Accumulate the training loss over all of the batches so that we can\n</span>        <span class=\"c1\"># calculate the average loss at the end. `loss` is a Tensor containing a\n</span>        <span class=\"c1\"># single value; the `.item()` function just returns the Python value \n</span>        <span class=\"c1\"># from the tensor.\n</span>        <span class=\"n\">total_train_loss</span> <span class=\"o\">+=</span> <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Perform a backward pass to calculate the gradients.\n</span>        <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Clip the norm of the gradients to 1.0.\n</span>        <span class=\"c1\"># This is to help prevent the \"exploding gradients\" problem.\n</span>        <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"mf\">1.0</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Update parameters and take a step using the computed gradient.\n</span>        <span class=\"c1\"># The optimizer dictates the \"update rule\"--how the parameters are\n</span>        <span class=\"c1\"># modified based on their gradients, the learning rate, etc.\n</span>        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Update the learning rate.\n</span>        <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Calculate the average loss over all of the batches.\n</span>    <span class=\"n\">avg_train_loss</span> <span class=\"o\">=</span> <span class=\"n\">total_train_loss</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span>            \n    \n    <span class=\"c1\"># Measure how long this epoch took.\n</span>    <span class=\"n\">training_time</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"  Average training loss: {0:.2f}\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">avg_train_loss</span><span class=\"p\">))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"  Training epcoh took: {:}\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">training_time</span><span class=\"p\">))</span>\n        \n    <span class=\"c1\"># Record all statistics from this epoch.\n</span>    <span class=\"n\">training_stats</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n        <span class=\"p\">{</span>\n            <span class=\"s\">'epoch'</span><span class=\"p\">:</span> <span class=\"n\">epoch_i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n            <span class=\"s\">'Training Loss'</span><span class=\"p\">:</span> <span class=\"n\">avg_train_loss</span><span class=\"p\">,</span>\n            <span class=\"s\">'Training Time'</span><span class=\"p\">:</span> <span class=\"n\">training_time</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Training complete!\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Total training took {:} (h:mm:ss)\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span><span class=\"o\">-</span><span class=\"n\">total_t0</span><span class=\"p\">)))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code38\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code38\"><span class=\"kn\">import</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n</span>\n<span class=\"c1\"># Set the seed value all over the place to make this reproducible.\n</span><span class=\"n\">seed_val</span> <span class=\"o\">=</span> <span class=\"mi\">321</span>\n\n<span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">manual_seed_all</span><span class=\"p\">(</span><span class=\"n\">seed_val</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\n</span><span class=\"n\">training_stats</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Update every `update_interval` batches.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Measure the total training time for the whole run.\n</span><span class=\"n\">total_t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># For each epoch...\n</span><span class=\"k\">for</span> <span class=\"n\">epoch_i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">):</span>\n    \n    <span class=\"c1\"># ========================================\n</span>    <span class=\"c1\">#               Training\n</span>    <span class=\"c1\"># ========================================\n</span>    \n    <span class=\"c1\"># Perform one full pass over the training set.\n</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'======== Epoch {:} / {:} ========'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">epoch_i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># At the start of each epoch (except for the first) we need to re-randomize\n</span>    <span class=\"c1\"># our training data.\n</span>    <span class=\"k\">if</span> <span class=\"n\">epoch_i</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Use our `make_smart_batches` function (from 6.1.) to re-shuffle the \n</span>        <span class=\"c1\"># dataset into new batches.\n</span>        <span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">train_text</span><span class=\"p\">,</span> <span class=\"n\">train_labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Training on {:,} batches...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Measure how long the training epoch takes.\n</span>    <span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Reset the total loss for this epoch.\n</span>    <span class=\"n\">total_train_loss</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"c1\"># Put the model into training mode. Don't be misled--the call to \n</span>    <span class=\"c1\"># `train` just changes the *mode*, it doesn't *perform* the training.\n</span>    <span class=\"c1\"># `dropout` and `batchnorm` layers behave differently during training\n</span>    <span class=\"c1\"># vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n</span>    <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># For each batch of training data...\n</span>    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)):</span>\n\n        <span class=\"c1\"># Progress update every, e.g., 100 batches.\n</span>        <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">step</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Calculate elapsed time in minutes.\n</span>            <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n            \n            <span class=\"c1\"># Calculate the time remaining based on our progress.\n</span>            <span class=\"n\">steps_per_sec</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">step</span>\n            <span class=\"n\">remaining_sec</span> <span class=\"o\">=</span> <span class=\"n\">steps_per_sec</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">step</span><span class=\"p\">)</span>\n            <span class=\"n\">remaining</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">remaining_sec</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Report progress.\n</span>            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Batch {:&gt;7,}  of  {:&gt;7,}.    Elapsed: {:}.  Remaining: {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">elapsed</span><span class=\"p\">,</span> <span class=\"n\">remaining</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># Copy the current training batch to the GPU using the `to` method.\n</span>        <span class=\"n\">b_input_ids</span> <span class=\"o\">=</span> <span class=\"n\">py_inputs</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">b_input_mask</span> <span class=\"o\">=</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">b_labels</span> <span class=\"o\">=</span> <span class=\"n\">py_labels</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Always clear any previously calculated gradients before performing a\n</span>        <span class=\"c1\"># backward pass.\n</span>        <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>        \n\n        <span class=\"c1\"># Perform a forward pass (evaluate the model on this training batch).\n</span>        <span class=\"c1\"># The call returns the loss (because we provided labels) and the \n</span>        <span class=\"c1\"># \"logits\"--the model outputs prior to activation.\n</span>        <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">b_input_ids</span><span class=\"p\">,</span> \n                             <span class=\"n\">token_type_ids</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> \n                             <span class=\"n\">attention_mask</span><span class=\"o\">=</span><span class=\"n\">b_input_mask</span><span class=\"p\">,</span> \n                             <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">b_labels</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Accumulate the training loss over all of the batches so that we can\n</span>        <span class=\"c1\"># calculate the average loss at the end. `loss` is a Tensor containing a\n</span>        <span class=\"c1\"># single value; the `.item()` function just returns the Python value \n</span>        <span class=\"c1\"># from the tensor.\n</span>        <span class=\"n\">total_train_loss</span> <span class=\"o\">+=</span> <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Perform a backward pass to calculate the gradients.\n</span>        <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Clip the norm of the gradients to 1.0.\n</span>        <span class=\"c1\"># This is to help prevent the \"exploding gradients\" problem.\n</span>        <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"mf\">1.0</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Update parameters and take a step using the computed gradient.\n</span>        <span class=\"c1\"># The optimizer dictates the \"update rule\"--how the parameters are\n</span>        <span class=\"c1\"># modified based on their gradients, the learning rate, etc.\n</span>        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Update the learning rate.\n</span>        <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Calculate the average loss over all of the batches.\n</span>    <span class=\"n\">avg_train_loss</span> <span class=\"o\">=</span> <span class=\"n\">total_train_loss</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span>            \n    \n    <span class=\"c1\"># Measure how long this epoch took.\n</span>    <span class=\"n\">training_time</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"  Average training loss: {0:.2f}\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">avg_train_loss</span><span class=\"p\">))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"  Training epcoh took: {:}\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">training_time</span><span class=\"p\">))</span>\n        \n    <span class=\"c1\"># Record all statistics from this epoch.\n</span>    <span class=\"n\">training_stats</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n        <span class=\"p\">{</span>\n            <span class=\"s\">'epoch'</span><span class=\"p\">:</span> <span class=\"n\">epoch_i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n            <span class=\"s\">'Training Loss'</span><span class=\"p\">:</span> <span class=\"n\">avg_train_loss</span><span class=\"p\">,</span>\n            <span class=\"s\">'Training Time'</span><span class=\"p\">:</span> <span class=\"n\">training_time</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"\"</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Training complete!\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Total training took {:} (h:mm:ss)\"</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span><span class=\"o\">-</span><span class=\"n\">total_t0</span><span class=\"p\">)))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code39\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code39\">======== Epoch 1 / 1 ========\nTraining on 1,563 batches...\n  Batch     200  of    1,563.    Elapsed: 0:04:33.  Remaining: 0:30:58\n  Batch     400  of    1,563.    Elapsed: 0:08:52.  Remaining: 0:25:47\n  Batch     600  of    1,563.    Elapsed: 0:13:15.  Remaining: 0:21:16\n  Batch     800  of    1,563.    Elapsed: 0:17:47.  Remaining: 0:16:58\n  Batch   1,000  of    1,563.    Elapsed: 0:22:22.  Remaining: 0:12:36\n  Batch   1,200  of    1,563.    Elapsed: 0:27:04.  Remaining: 0:08:11\n  Batch   1,400  of    1,563.    Elapsed: 0:31:19.  Remaining: 0:03:39\n\n  Average training loss: 0.25\n  Training epcoh took: 0:35:06\n\nTraining complete!\nTotal training took 0:35:06 (h:mm:ss)\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code39\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code39\">======== Epoch 1 / 1 ========\nTraining on 1,563 batches...\n  Batch     200  of    1,563.    Elapsed: 0:04:33.  Remaining: 0:30:58\n  Batch     400  of    1,563.    Elapsed: 0:08:52.  Remaining: 0:25:47\n  Batch     600  of    1,563.    Elapsed: 0:13:15.  Remaining: 0:21:16\n  Batch     800  of    1,563.    Elapsed: 0:17:47.  Remaining: 0:16:58\n  Batch   1,000  of    1,563.    Elapsed: 0:22:22.  Remaining: 0:12:36\n  Batch   1,200  of    1,563.    Elapsed: 0:27:04.  Remaining: 0:08:11\n  Batch   1,400  of    1,563.    Elapsed: 0:31:19.  Remaining: 0:03:39\n\n  Average training loss: 0.25\n  Training epcoh took: 0:35:06\n\nTraining complete!\nTotal training took 0:35:06 (h:mm:ss)\n</code></pre>",
    "contentMarkdown": "*   In previous examples, we’ve made use of the PyTorch `Dataset` and `DataLoader` classes, but because of the smart batching we’re not using them in this Notebook. However, check out the section below on how to use the [`collate` function](#collate-function) with a standard `DataLoader` pipeline. However, check out the section below on how to use the [`collate` function](#collate-function) with a standard `DataLoader` pipeline.\n    \n*   Note: If you have modified the code here to run for more than one epoch, you’ll need the `make_smart_batches` function defined in the [make\\_smart\\_batches](#make-smart-batches) section.\n    \n*   We’re ready to kick off the training!\n    \n\nIn previous examples, we’ve made use of the PyTorch `Dataset` and `DataLoader` classes, but because of the smart batching we’re not using them in this Notebook. However, check out the section below on how to use the [`collate` function](#collate-function) with a standard `DataLoader` pipeline. However, check out the section below on how to use the [`collate` function](#collate-function) with a standard `DataLoader` pipeline.\n\nNote: If you have modified the code here to run for more than one epoch, you’ll need the `make_smart_batches` function defined in the [make\\_smart\\_batches](#make-smart-batches) section.\n\nWe’re ready to kick off the training!\n\n![](https://aman.ai/images/copy.png)\n\n``import random import numpy as np  # This training code is based on the `run_glue.py` script here: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128 # Set the seed value all over the place to make this reproducible. seed_val = 321  random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val)  # We'll store a number of quantities such as training and validation loss,  # validation accuracy, and timings. training_stats = []  # Update every `update_interval` batches. update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)  # Measure the total training time for the whole run. total_t0 = time.time()  # For each epoch... for epoch_i in range(0, epochs):          # ========================================     #               Training     # ========================================          # Perform one full pass over the training set.     print(\"\")     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))          # At the start of each epoch (except for the first) we need to re-randomize     # our training data.     if epoch_i > 0:         # Use our `make_smart_batches` function (from 6.1.) to re-shuffle the          # dataset into new batches.         (py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_text, train_labels, batch_size)          print('Training on {:,} batches...'.format(len(py_inputs)))      # Measure how long the training epoch takes.     t0 = time.time()      # Reset the total loss for this epoch.     total_train_loss = 0      # Put the model into training mode. Don't be misled--the call to      # `train` just changes the *mode*, it doesn't *perform* the training.     # `dropout` and `batchnorm` layers behave differently during training     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)     model.train()      # For each batch of training data...     for step in range(0, len(py_inputs)):          # Progress update every, e.g., 100 batches.         if step % update_interval == 0 and not step == 0:             # Calculate elapsed time in minutes.             elapsed = format_time(time.time() - t0)                          # Calculate the time remaining based on our progress.             steps_per_sec = (time.time() - t0) / step             remaining_sec = steps_per_sec * (len(py_inputs) - step)             remaining = format_time(remaining_sec)              # Report progress.             print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))          # Copy the current training batch to the GPU using the `to` method.         b_input_ids = py_inputs[step].to(device)         b_input_mask = py_attn_masks[step].to(device)         b_labels = py_labels[step].to(device)          # Always clear any previously calculated gradients before performing a         # backward pass.         model.zero_grad()                  # Perform a forward pass (evaluate the model on this training batch).         # The call returns the loss (because we provided labels) and the          # \"logits\"--the model outputs prior to activation.         loss, logits = model(b_input_ids,                               token_type_ids=None,                               attention_mask=b_input_mask,                               labels=b_labels)          # Accumulate the training loss over all of the batches so that we can         # calculate the average loss at the end. `loss` is a Tensor containing a         # single value; the `.item()` function just returns the Python value          # from the tensor.         total_train_loss += loss.item()          # Perform a backward pass to calculate the gradients.         loss.backward()          # Clip the norm of the gradients to 1.0.         # This is to help prevent the \"exploding gradients\" problem.         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)          # Update parameters and take a step using the computed gradient.         # The optimizer dictates the \"update rule\"--how the parameters are         # modified based on their gradients, the learning rate, etc.         optimizer.step()          # Update the learning rate.         scheduler.step()      # Calculate the average loss over all of the batches.     avg_train_loss = total_train_loss / len(py_inputs)                      # Measure how long this epoch took.     training_time = format_time(time.time() - t0)      print(\"\")     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))     print(\"  Training epcoh took: {:}\".format(training_time))              # Record all statistics from this epoch.     training_stats.append(         {             'epoch': epoch_i + 1,             'Training Loss': avg_train_loss,             'Training Time': training_time,         }     )  print(\"\") print(\"Training complete!\")  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))``\n\n![](https://aman.ai/images/copy.png)\n\n``import random import numpy as np  # This training code is based on the `run_glue.py` script here: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128 # Set the seed value all over the place to make this reproducible. seed_val = 321  random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val)  # We'll store a number of quantities such as training and validation loss,  # validation accuracy, and timings. training_stats = []  # Update every `update_interval` batches. update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)  # Measure the total training time for the whole run. total_t0 = time.time()  # For each epoch... for epoch_i in range(0, epochs):          # ========================================     #               Training     # ========================================          # Perform one full pass over the training set.     print(\"\")     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))          # At the start of each epoch (except for the first) we need to re-randomize     # our training data.     if epoch_i > 0:         # Use our `make_smart_batches` function (from 6.1.) to re-shuffle the          # dataset into new batches.         (py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_text, train_labels, batch_size)          print('Training on {:,} batches...'.format(len(py_inputs)))      # Measure how long the training epoch takes.     t0 = time.time()      # Reset the total loss for this epoch.     total_train_loss = 0      # Put the model into training mode. Don't be misled--the call to      # `train` just changes the *mode*, it doesn't *perform* the training.     # `dropout` and `batchnorm` layers behave differently during training     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)     model.train()      # For each batch of training data...     for step in range(0, len(py_inputs)):          # Progress update every, e.g., 100 batches.         if step % update_interval == 0 and not step == 0:             # Calculate elapsed time in minutes.             elapsed = format_time(time.time() - t0)                          # Calculate the time remaining based on our progress.             steps_per_sec = (time.time() - t0) / step             remaining_sec = steps_per_sec * (len(py_inputs) - step)             remaining = format_time(remaining_sec)              # Report progress.             print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))          # Copy the current training batch to the GPU using the `to` method.         b_input_ids = py_inputs[step].to(device)         b_input_mask = py_attn_masks[step].to(device)         b_labels = py_labels[step].to(device)          # Always clear any previously calculated gradients before performing a         # backward pass.         model.zero_grad()                  # Perform a forward pass (evaluate the model on this training batch).         # The call returns the loss (because we provided labels) and the          # \"logits\"--the model outputs prior to activation.         loss, logits = model(b_input_ids,                               token_type_ids=None,                               attention_mask=b_input_mask,                               labels=b_labels)          # Accumulate the training loss over all of the batches so that we can         # calculate the average loss at the end. `loss` is a Tensor containing a         # single value; the `.item()` function just returns the Python value          # from the tensor.         total_train_loss += loss.item()          # Perform a backward pass to calculate the gradients.         loss.backward()          # Clip the norm of the gradients to 1.0.         # This is to help prevent the \"exploding gradients\" problem.         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)          # Update parameters and take a step using the computed gradient.         # The optimizer dictates the \"update rule\"--how the parameters are         # modified based on their gradients, the learning rate, etc.         optimizer.step()          # Update the learning rate.         scheduler.step()      # Calculate the average loss over all of the batches.     avg_train_loss = total_train_loss / len(py_inputs)                      # Measure how long this epoch took.     training_time = format_time(time.time() - t0)      print(\"\")     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))     print(\"  Training epcoh took: {:}\".format(training_time))              # Record all statistics from this epoch.     training_stats.append(         {             'epoch': epoch_i + 1,             'Training Loss': avg_train_loss,             'Training Time': training_time,         }     )  print(\"\") print(\"Training complete!\")  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))``\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`======== Epoch 1 / 1 ======== Training on 1,563 batches...   Batch     200  of    1,563.    Elapsed: 0:04:33.  Remaining: 0:30:58   Batch     400  of    1,563.    Elapsed: 0:08:52.  Remaining: 0:25:47   Batch     600  of    1,563.    Elapsed: 0:13:15.  Remaining: 0:21:16   Batch     800  of    1,563.    Elapsed: 0:17:47.  Remaining: 0:16:58   Batch   1,000  of    1,563.    Elapsed: 0:22:22.  Remaining: 0:12:36   Batch   1,200  of    1,563.    Elapsed: 0:27:04.  Remaining: 0:08:11   Batch   1,400  of    1,563.    Elapsed: 0:31:19.  Remaining: 0:03:39    Average training loss: 0.25   Training epcoh took: 0:35:06  Training complete! Total training took 0:35:06 (h:mm:ss)`\n\n![](https://aman.ai/images/copy.png)\n\n`======== Epoch 1 / 1 ======== Training on 1,563 batches...   Batch     200  of    1,563.    Elapsed: 0:04:33.  Remaining: 0:30:58   Batch     400  of    1,563.    Elapsed: 0:08:52.  Remaining: 0:25:47   Batch     600  of    1,563.    Elapsed: 0:13:15.  Remaining: 0:21:16   Batch     800  of    1,563.    Elapsed: 0:17:47.  Remaining: 0:16:58   Batch   1,000  of    1,563.    Elapsed: 0:22:22.  Remaining: 0:12:36   Batch   1,200  of    1,563.    Elapsed: 0:27:04.  Remaining: 0:08:11   Batch   1,400  of    1,563.    Elapsed: 0:31:19.  Remaining: 0:03:39    Average training loss: 0.25   Training epcoh took: 0:35:06  Training complete! Total training took 0:35:06 (h:mm:ss)`",
    "contentLength": 34759,
    "wordCount": 1486,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#training-loop"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-makesmartbatches-14",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Evaluate on Test Set",
    "title": "make_smart_batches()",
    "order": 14,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>This function combines all of the steps from the “Smart Batching” section into a single (re-usable) function. You can use this in your own Notebook for applying smart batching to both your training and test sets.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code42\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code42\"><span class=\"k\">def</span> <span class=\"nf\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">text_samples</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    This function combines all of the required steps to prepare batches.\n    '''</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating Smart Batches from {:,} examples with batch size {:,}...</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text_samples</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#   Tokenize &amp; Truncate\n</span>    <span class=\"c1\"># =========================\n</span>\n    <span class=\"n\">full_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"c1\"># Tokenize all training examples\n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Choose an interval on which to print progress updates.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># For each training example...\n</span>    <span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">text_samples</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n        <span class=\"c1\"># Tokenize the sample.\n</span>        <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">text</span><span class=\"p\">,</span>              <span class=\"c1\"># Text to encode.\n</span>                                    <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Do add specials.\n</span>                                    <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"n\">max_len</span><span class=\"p\">,</span>      <span class=\"c1\"># Do Truncate!\n</span>                                    <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>         <span class=\"c1\"># Do Truncate!\n</span>                                    <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>           <span class=\"c1\"># DO NOT pad.\n</span>                                    \n        <span class=\"c1\"># Add the tokenized result to our list.\n</span>        <span class=\"n\">full_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n        \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'DONE.'</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#      Select Batches\n</span>    <span class=\"c1\"># =========================    \n</span>\n    <span class=\"c1\"># Sort the two lists together by the length of the input sequence.\n</span>    <span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples after sorting</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)))</span>\n\n    <span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n    <span class=\"c1\"># List of batches that we'll construct.\n</span>    <span class=\"n\">batch_ordered_sentences</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batch_ordered_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating batches of size {:}...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Choose an interval on which to print progress updates.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Loop over all of the input samples...    \n</span>    <span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> \\\n            <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Selected {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n        <span class=\"c1\"># `to_take` is our actual batch size. It will be `batch_size` until \n</span>        <span class=\"c1\"># we get to the last batch, which may be smaller. \n</span>        <span class=\"n\">to_take</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># Pick a random index in the list of remaining samples to start\n</span>        <span class=\"c1\"># our batch at.\n</span>        <span class=\"n\">select</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">to_take</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Select a contiguous batch of samples starting at `select`.\n</span>        <span class=\"c1\">#print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n</span>        <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:(</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">)]</span>\n\n        <span class=\"c1\">#print(\"Batch length:\", len(batch))\n</span>\n        <span class=\"c1\"># Each sample is a tuple--split them apart to create a separate list of \n</span>        <span class=\"c1\"># sequences and a list of labels for this batch.\n</span>        <span class=\"n\">batch_ordered_sentences</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n        <span class=\"n\">batch_ordered_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># Remove these samples from the list.\n</span>        <span class=\"k\">del</span> <span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">]</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">  DONE - Selected {:,} batches.</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#        Add Padding\n</span>    <span class=\"c1\"># =========================    \n</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Padding out sequences within each batch...'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"c1\"># For each batch...\n</span>    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_labels</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">,</span> <span class=\"n\">batch_ordered_labels</span><span class=\"p\">):</span>\n\n        <span class=\"c1\"># New version of the batch, this time with padded sequences and now with\n</span>        <span class=\"c1\"># attention masks defined.\n</span>        <span class=\"n\">batch_padded_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"n\">batch_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        \n        <span class=\"c1\"># First, find the longest sample in the batch. \n</span>        <span class=\"c1\"># Note that the sequences do currently include the special tokens!\n</span>        <span class=\"n\">max_size</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">([</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># For each input in this batch...\n</span>        <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">:</span>\n            \n            <span class=\"c1\"># How many pad tokens do we need to add?\n</span>            <span class=\"n\">num_pads</span> <span class=\"o\">=</span> <span class=\"n\">max_size</span> <span class=\"o\">-</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Add `num_pads` padding tokens to the end of the sequence.\n</span>            <span class=\"n\">padded_input</span> <span class=\"o\">=</span> <span class=\"n\">sen</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pad_token_id</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">num_pads</span>\n\n            <span class=\"c1\"># Define the attention mask--it's just a `1` for every real token\n</span>            <span class=\"c1\"># and a `0` for every padding token.\n</span>            <span class=\"n\">attn_mask</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">num_pads</span>\n\n            <span class=\"c1\"># Add the padded results to the batch.\n</span>            <span class=\"n\">batch_padded_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">padded_input</span><span class=\"p\">)</span>\n            <span class=\"n\">batch_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">attn_mask</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Our batch has been padded, so we need to save this updated batch.\n</span>        <span class=\"c1\"># We also need the inputs to be PyTorch tensors, so we'll do that here.\n</span>        <span class=\"c1\"># Todo - Michael's code specified \"dtype=torch.long\"\n</span>        <span class=\"n\">py_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_padded_inputs</span><span class=\"p\">))</span>\n        <span class=\"n\">py_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_attn_masks</span><span class=\"p\">))</span>\n        <span class=\"n\">py_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_labels</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  DONE.'</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Return the smart-batched dataset!\n</span>    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code42\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code42\"><span class=\"k\">def</span> <span class=\"nf\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">text_samples</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">):</span>\n    <span class=\"s\">'''\n    This function combines all of the required steps to prepare batches.\n    '''</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating Smart Batches from {:,} examples with batch size {:,}...</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text_samples</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#   Tokenize &amp; Truncate\n</span>    <span class=\"c1\"># =========================\n</span>\n    <span class=\"n\">full_input_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"c1\"># Tokenize all training examples\n</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Tokenizing {:,} samples...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># Choose an interval on which to print progress updates.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># For each training example...\n</span>    <span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">text_samples</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Tokenized {:,} samples.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n        <span class=\"c1\"># Tokenize the sample.\n</span>        <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">text</span><span class=\"p\">,</span>              <span class=\"c1\"># Text to encode.\n</span>                                    <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Do add specials.\n</span>                                    <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"n\">max_len</span><span class=\"p\">,</span>      <span class=\"c1\"># Do Truncate!\n</span>                                    <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>         <span class=\"c1\"># Do Truncate!\n</span>                                    <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>           <span class=\"c1\"># DO NOT pad.\n</span>                                    \n        <span class=\"c1\"># Add the tokenized result to our list.\n</span>        <span class=\"n\">full_input_ids</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n        \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'DONE.'</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#      Select Batches\n</span>    <span class=\"c1\"># =========================    \n</span>\n    <span class=\"c1\"># Sort the two lists together by the length of the input sequence.\n</span>    <span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">full_input_ids</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'{:&gt;10,} samples after sorting</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)))</span>\n\n    <span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n    <span class=\"c1\"># List of batches that we'll construct.\n</span>    <span class=\"n\">batch_ordered_sentences</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">batch_ordered_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Creating batches of size {:}...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Choose an interval on which to print progress updates.\n</span>    <span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Loop over all of the input samples...    \n</span>    <span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> \\\n            <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Selected {:,} batches.'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n        <span class=\"c1\"># `to_take` is our actual batch size. It will be `batch_size` until \n</span>        <span class=\"c1\"># we get to the last batch, which may be smaller. \n</span>        <span class=\"n\">to_take</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">))</span>\n\n        <span class=\"c1\"># Pick a random index in the list of remaining samples to start\n</span>        <span class=\"c1\"># our batch at.\n</span>        <span class=\"n\">select</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">to_take</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Select a contiguous batch of samples starting at `select`.\n</span>        <span class=\"c1\">#print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n</span>        <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:(</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">)]</span>\n\n        <span class=\"c1\">#print(\"Batch length:\", len(batch))\n</span>\n        <span class=\"c1\"># Each sample is a tuple--split them apart to create a separate list of \n</span>        <span class=\"c1\"># sequences and a list of labels for this batch.\n</span>        <span class=\"n\">batch_ordered_sentences</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n        <span class=\"n\">batch_ordered_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># Remove these samples from the list.\n</span>        <span class=\"k\">del</span> <span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">select</span><span class=\"p\">:</span><span class=\"n\">select</span> <span class=\"o\">+</span> <span class=\"n\">to_take</span><span class=\"p\">]</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">  DONE - Selected {:,} batches.</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">)))</span>\n\n    <span class=\"c1\"># =========================\n</span>    <span class=\"c1\">#        Add Padding\n</span>    <span class=\"c1\"># =========================    \n</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Padding out sequences within each batch...'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">py_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">py_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">py_labels</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"c1\"># For each batch...\n</span>    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_labels</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">batch_ordered_sentences</span><span class=\"p\">,</span> <span class=\"n\">batch_ordered_labels</span><span class=\"p\">):</span>\n\n        <span class=\"c1\"># New version of the batch, this time with padded sequences and now with\n</span>        <span class=\"c1\"># attention masks defined.\n</span>        <span class=\"n\">batch_padded_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"n\">batch_attn_masks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        \n        <span class=\"c1\"># First, find the longest sample in the batch. \n</span>        <span class=\"c1\"># Note that the sequences do currently include the special tokens!\n</span>        <span class=\"n\">max_size</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">([</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># For each input in this batch...\n</span>        <span class=\"k\">for</span> <span class=\"n\">sen</span> <span class=\"ow\">in</span> <span class=\"n\">batch_inputs</span><span class=\"p\">:</span>\n            \n            <span class=\"c1\"># How many pad tokens do we need to add?\n</span>            <span class=\"n\">num_pads</span> <span class=\"o\">=</span> <span class=\"n\">max_size</span> <span class=\"o\">-</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Add `num_pads` padding tokens to the end of the sequence.\n</span>            <span class=\"n\">padded_input</span> <span class=\"o\">=</span> <span class=\"n\">sen</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">pad_token_id</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">num_pads</span>\n\n            <span class=\"c1\"># Define the attention mask--it's just a `1` for every real token\n</span>            <span class=\"c1\"># and a `0` for every padding token.\n</span>            <span class=\"n\">attn_mask</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sen</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">num_pads</span>\n\n            <span class=\"c1\"># Add the padded results to the batch.\n</span>            <span class=\"n\">batch_padded_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">padded_input</span><span class=\"p\">)</span>\n            <span class=\"n\">batch_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">attn_mask</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Our batch has been padded, so we need to save this updated batch.\n</span>        <span class=\"c1\"># We also need the inputs to be PyTorch tensors, so we'll do that here.\n</span>        <span class=\"c1\"># Todo - Michael's code specified \"dtype=torch.long\"\n</span>        <span class=\"n\">py_inputs</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_padded_inputs</span><span class=\"p\">))</span>\n        <span class=\"n\">py_attn_masks</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_attn_masks</span><span class=\"p\">))</span>\n        <span class=\"n\">py_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">batch_labels</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  DONE.'</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Return the smart-batched dataset!\n</span>    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "*   This function combines all of the steps from the “Smart Batching” section into a single (re-usable) function. You can use this in your own Notebook for applying smart batching to both your training and test sets.\n\n![](https://aman.ai/images/copy.png)\n\n``def make_smart_batches(text_samples, labels, batch_size):     '''     This function combines all of the required steps to prepare batches.     '''      print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))      # =========================     #   Tokenize & Truncate     # =========================     full_input_ids = []      # Tokenize all training examples     print('Tokenizing {:,} samples...'.format(len(labels)))      # Choose an interval on which to print progress updates.     update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)      # For each training example...     for text in text_samples:                  # Report progress.         if ((len(full_input_ids) % update_interval) == 0):             print('  Tokenized {:,} samples.'.format(len(full_input_ids)))          # Tokenize the sample.         input_ids = tokenizer.encode(text=text,              # Text to encode.                                     add_special_tokens=True, # Do add specials.                                     max_length=max_len,      # Do Truncate!                                     truncation=True,         # Do Truncate!                                     padding=False)           # DO NOT pad.                                              # Add the tokenized result to our list.         full_input_ids.append(input_ids)              print('DONE.')     print('{:>10,} samples\\n'.format(len(full_input_ids)))      # =========================     #      Select Batches     # =========================         # Sort the two lists together by the length of the input sequence.     samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))      print('{:>10,} samples after sorting\\n'.format(len(samples)))      import random      # List of batches that we'll construct.     batch_ordered_sentences = []     batch_ordered_labels = []      print('Creating batches of size {:}...'.format(batch_size))      # Choose an interval on which to print progress updates.     update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)          # Loop over all of the input samples...         while len(samples) > 0:                  # Report progress.         if ((len(batch_ordered_sentences) % update_interval) == 0 \\             and not len(batch_ordered_sentences) == 0):             print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))          # `to_take` is our actual batch size. It will be `batch_size` until          # we get to the last batch, which may be smaller.          to_take = min(batch_size, len(samples))          # Pick a random index in the list of remaining samples to start         # our batch at.         select = random.randint(0, len(samples) - to_take)          # Select a contiguous batch of samples starting at `select`.         #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))         batch = samples[select:(select + to_take)]          #print(\"Batch length:\", len(batch))         # Each sample is a tuple--split them apart to create a separate list of          # sequences and a list of labels for this batch.         batch_ordered_sentences.append([s[0] for s in batch])         batch_ordered_labels.append([s[1] for s in batch])          # Remove these samples from the list.         del samples[select:select + to_take]      print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))      # =========================     #        Add Padding     # =========================         print('Padding out sequences within each batch...')      py_inputs = []     py_attn_masks = []     py_labels = []      # For each batch...     for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):          # New version of the batch, this time with padded sequences and now with         # attention masks defined.         batch_padded_inputs = []         batch_attn_masks = []                  # First, find the longest sample in the batch.          # Note that the sequences do currently include the special tokens!         max_size = max([len(sen) for sen in batch_inputs])          # For each input in this batch...         for sen in batch_inputs:                          # How many pad tokens do we need to add?             num_pads = max_size - len(sen)              # Add `num_pads` padding tokens to the end of the sequence.             padded_input = sen + [tokenizer.pad_token_id]*num_pads              # Define the attention mask--it's just a `1` for every real token             # and a `0` for every padding token.             attn_mask = [1] * len(sen) + [0] * num_pads              # Add the padded results to the batch.             batch_padded_inputs.append(padded_input)             batch_attn_masks.append(attn_mask)          # Our batch has been padded, so we need to save this updated batch.         # We also need the inputs to be PyTorch tensors, so we'll do that here.         # Todo - Michael's code specified \"dtype=torch.long\"         py_inputs.append(torch.tensor(batch_padded_inputs))         py_attn_masks.append(torch.tensor(batch_attn_masks))         py_labels.append(torch.tensor(batch_labels))          print('  DONE.')      # Return the smart-batched dataset!     return (py_inputs, py_attn_masks, py_labels)``\n\n![](https://aman.ai/images/copy.png)\n\n``def make_smart_batches(text_samples, labels, batch_size):     '''     This function combines all of the required steps to prepare batches.     '''      print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))      # =========================     #   Tokenize & Truncate     # =========================     full_input_ids = []      # Tokenize all training examples     print('Tokenizing {:,} samples...'.format(len(labels)))      # Choose an interval on which to print progress updates.     update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)      # For each training example...     for text in text_samples:                  # Report progress.         if ((len(full_input_ids) % update_interval) == 0):             print('  Tokenized {:,} samples.'.format(len(full_input_ids)))          # Tokenize the sample.         input_ids = tokenizer.encode(text=text,              # Text to encode.                                     add_special_tokens=True, # Do add specials.                                     max_length=max_len,      # Do Truncate!                                     truncation=True,         # Do Truncate!                                     padding=False)           # DO NOT pad.                                              # Add the tokenized result to our list.         full_input_ids.append(input_ids)              print('DONE.')     print('{:>10,} samples\\n'.format(len(full_input_ids)))      # =========================     #      Select Batches     # =========================         # Sort the two lists together by the length of the input sequence.     samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))      print('{:>10,} samples after sorting\\n'.format(len(samples)))      import random      # List of batches that we'll construct.     batch_ordered_sentences = []     batch_ordered_labels = []      print('Creating batches of size {:}...'.format(batch_size))      # Choose an interval on which to print progress updates.     update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)          # Loop over all of the input samples...         while len(samples) > 0:                  # Report progress.         if ((len(batch_ordered_sentences) % update_interval) == 0 \\             and not len(batch_ordered_sentences) == 0):             print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))          # `to_take` is our actual batch size. It will be `batch_size` until          # we get to the last batch, which may be smaller.          to_take = min(batch_size, len(samples))          # Pick a random index in the list of remaining samples to start         # our batch at.         select = random.randint(0, len(samples) - to_take)          # Select a contiguous batch of samples starting at `select`.         #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))         batch = samples[select:(select + to_take)]          #print(\"Batch length:\", len(batch))         # Each sample is a tuple--split them apart to create a separate list of          # sequences and a list of labels for this batch.         batch_ordered_sentences.append([s[0] for s in batch])         batch_ordered_labels.append([s[1] for s in batch])          # Remove these samples from the list.         del samples[select:select + to_take]      print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))      # =========================     #        Add Padding     # =========================         print('Padding out sequences within each batch...')      py_inputs = []     py_attn_masks = []     py_labels = []      # For each batch...     for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):          # New version of the batch, this time with padded sequences and now with         # attention masks defined.         batch_padded_inputs = []         batch_attn_masks = []                  # First, find the longest sample in the batch.          # Note that the sequences do currently include the special tokens!         max_size = max([len(sen) for sen in batch_inputs])          # For each input in this batch...         for sen in batch_inputs:                          # How many pad tokens do we need to add?             num_pads = max_size - len(sen)              # Add `num_pads` padding tokens to the end of the sequence.             padded_input = sen + [tokenizer.pad_token_id]*num_pads              # Define the attention mask--it's just a `1` for every real token             # and a `0` for every padding token.             attn_mask = [1] * len(sen) + [0] * num_pads              # Add the padded results to the batch.             batch_padded_inputs.append(padded_input)             batch_attn_masks.append(attn_mask)          # Our batch has been padded, so we need to save this updated batch.         # We also need the inputs to be PyTorch tensors, so we'll do that here.         # Todo - Michael's code specified \"dtype=torch.long\"         py_inputs.append(torch.tensor(batch_padded_inputs))         py_attn_masks.append(torch.tensor(batch_attn_masks))         py_labels.append(torch.tensor(batch_labels))          print('  DONE.')      # Return the smart-batched dataset!     return (py_inputs, py_attn_masks, py_labels)``",
    "contentLength": 34924,
    "wordCount": 1159,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#make_smart_batches()"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-load-test-dataset-smart-batch-15",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Evaluate on Test Set",
    "title": "Load Test Dataset & Smart Batch",
    "order": 15,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Load the test dataset. This file has more columns, and contains rows for different languages (we’ll only select the French test samples).</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code43\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code43\"><span class=\"c1\"># Use our new function to completely prepare our dataset.\n</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">test_text</span><span class=\"p\">,</span> <span class=\"n\">test_labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code43\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code43\"><span class=\"c1\"># Use our new function to completely prepare our dataset.\n</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">,</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">,</span> <span class=\"n\">py_labels</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">make_smart_batches</span><span class=\"p\">(</span><span class=\"n\">test_text</span><span class=\"p\">,</span> <span class=\"n\">test_labels</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code44\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code44\">Creating Smart Batches from 25,000 examples with batch size 16...\n\nTokenizing 25,000 samples...\n  Tokenized 0 samples.\n  Tokenized 2,000 samples.\n  Tokenized 4,000 samples.\n  Tokenized 6,000 samples.\n  Tokenized 8,000 samples.\n  Tokenized 10,000 samples.\n  Tokenized 12,000 samples.\n  Tokenized 14,000 samples.\n  Tokenized 16,000 samples.\n  Tokenized 18,000 samples.\n  Tokenized 20,000 samples.\n  Tokenized 22,000 samples.\n  Tokenized 24,000 samples.\nDONE.\n    25,000 samples\n\n    25,000 samples after sorting\n\nCreating batches of size 16...\n\n  DONE - Selected 1,563 batches.\n\nPadding out sequences within each batch...\n  DONE.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code44\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code44\">Creating Smart Batches from 25,000 examples with batch size 16...\n\nTokenizing 25,000 samples...\n  Tokenized 0 samples.\n  Tokenized 2,000 samples.\n  Tokenized 4,000 samples.\n  Tokenized 6,000 samples.\n  Tokenized 8,000 samples.\n  Tokenized 10,000 samples.\n  Tokenized 12,000 samples.\n  Tokenized 14,000 samples.\n  Tokenized 16,000 samples.\n  Tokenized 18,000 samples.\n  Tokenized 20,000 samples.\n  Tokenized 22,000 samples.\n  Tokenized 24,000 samples.\nDONE.\n    25,000 samples\n\n    25,000 samples after sorting\n\nCreating batches of size 16...\n\n  DONE - Selected 1,563 batches.\n\nPadding out sequences within each batch...\n  DONE.\n</code></pre>",
    "contentMarkdown": "*   Load the test dataset. This file has more columns, and contains rows for different languages (we’ll only select the French test samples).\n\n![](https://aman.ai/images/copy.png)\n\n`# Use our new function to completely prepare our dataset. (py_inputs, py_attn_masks, py_labels) = make_smart_batches(test_text, test_labels, batch_size)`\n\n![](https://aman.ai/images/copy.png)\n\n`# Use our new function to completely prepare our dataset. (py_inputs, py_attn_masks, py_labels) = make_smart_batches(test_text, test_labels, batch_size)`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Creating Smart Batches from 25,000 examples with batch size 16...  Tokenizing 25,000 samples...   Tokenized 0 samples.   Tokenized 2,000 samples.   Tokenized 4,000 samples.   Tokenized 6,000 samples.   Tokenized 8,000 samples.   Tokenized 10,000 samples.   Tokenized 12,000 samples.   Tokenized 14,000 samples.   Tokenized 16,000 samples.   Tokenized 18,000 samples.   Tokenized 20,000 samples.   Tokenized 22,000 samples.   Tokenized 24,000 samples. DONE.     25,000 samples      25,000 samples after sorting  Creating batches of size 16...    DONE - Selected 1,563 batches.  Padding out sequences within each batch...   DONE.`\n\n![](https://aman.ai/images/copy.png)\n\n`Creating Smart Batches from 25,000 examples with batch size 16...  Tokenizing 25,000 samples...   Tokenized 0 samples.   Tokenized 2,000 samples.   Tokenized 4,000 samples.   Tokenized 6,000 samples.   Tokenized 8,000 samples.   Tokenized 10,000 samples.   Tokenized 12,000 samples.   Tokenized 14,000 samples.   Tokenized 16,000 samples.   Tokenized 18,000 samples.   Tokenized 20,000 samples.   Tokenized 22,000 samples.   Tokenized 24,000 samples. DONE.     25,000 samples      25,000 samples after sorting  Creating batches of size 16...    DONE - Selected 1,563 batches.  Padding out sequences within each batch...   DONE.`",
    "contentLength": 3865,
    "wordCount": 216,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#load-test-dataset-&-smart-batch"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-evaluate-16",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Evaluate on Test Set",
    "title": "Evaluate",
    "order": 16,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code45\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code45\"><span class=\"c1\"># Prediction on test set\n</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Predicting labels for {:,} test sentences...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># Tracking variables \n</span><span class=\"n\">predictions</span> <span class=\"p\">,</span> <span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"p\">[],</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Choose an interval on which to print progress updates.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Measure elapsed time.\n</span><span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Put model in evaluation mode\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># For each batch of training data...\n</span><span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)):</span>\n\n    <span class=\"c1\"># Progress update every 100 batches.\n</span>    <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">step</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Calculate elapsed time in minutes.\n</span>        <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Calculate the time remaining based on our progress.\n</span>        <span class=\"n\">steps_per_sec</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">step</span>\n        <span class=\"n\">remaining_sec</span> <span class=\"o\">=</span> <span class=\"n\">steps_per_sec</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">step</span><span class=\"p\">)</span>\n        <span class=\"n\">remaining</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">remaining_sec</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Batch {:&gt;7,}  of  {:&gt;7,}.    Elapsed: {:}.  Remaining: {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">elapsed</span><span class=\"p\">,</span> <span class=\"n\">remaining</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Copy the batch to the GPU.\n</span>    <span class=\"n\">b_input_ids</span> <span class=\"o\">=</span> <span class=\"n\">py_inputs</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">b_input_mask</span> <span class=\"o\">=</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">b_labels</span> <span class=\"o\">=</span> <span class=\"n\">py_labels</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n  \n    <span class=\"c1\"># Telling the model not to compute or store gradients, saving memory and \n</span>    <span class=\"c1\"># speeding up prediction\n</span>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"c1\"># Forward pass, calculate logit predictions\n</span>        <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">b_input_ids</span><span class=\"p\">,</span> <span class=\"n\">token_type_ids</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> \n                        <span class=\"n\">attention_mask</span><span class=\"o\">=</span><span class=\"n\">b_input_mask</span><span class=\"p\">)</span>\n\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Move logits and labels to CPU\n</span>    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">logits</span><span class=\"p\">.</span><span class=\"n\">detach</span><span class=\"p\">().</span><span class=\"n\">cpu</span><span class=\"p\">().</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n    <span class=\"n\">label_ids</span> <span class=\"o\">=</span> <span class=\"n\">b_labels</span><span class=\"p\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s\">'cpu'</span><span class=\"p\">).</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n  \n    <span class=\"c1\"># Store predictions and true labels\n</span>    <span class=\"n\">predictions</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">)</span>\n    <span class=\"n\">true_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">label_ids</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'    DONE.'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code45\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code45\"><span class=\"c1\"># Prediction on test set\n</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Predicting labels for {:,} test sentences...'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># Tracking variables \n</span><span class=\"n\">predictions</span> <span class=\"p\">,</span> <span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"p\">[],</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Choose an interval on which to print progress updates.\n</span><span class=\"n\">update_interval</span> <span class=\"o\">=</span> <span class=\"n\">good_update_interval</span><span class=\"p\">(</span><span class=\"n\">total_iters</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">num_desired_updates</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Measure elapsed time.\n</span><span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Put model in evaluation mode\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># For each batch of training data...\n</span><span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)):</span>\n\n    <span class=\"c1\"># Progress update every 100 batches.\n</span>    <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"n\">update_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">step</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Calculate elapsed time in minutes.\n</span>        <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Calculate the time remaining based on our progress.\n</span>        <span class=\"n\">steps_per_sec</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">step</span>\n        <span class=\"n\">remaining_sec</span> <span class=\"o\">=</span> <span class=\"n\">steps_per_sec</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">step</span><span class=\"p\">)</span>\n        <span class=\"n\">remaining</span> <span class=\"o\">=</span> <span class=\"n\">format_time</span><span class=\"p\">(</span><span class=\"n\">remaining_sec</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Report progress.\n</span>        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'  Batch {:&gt;7,}  of  {:&gt;7,}.    Elapsed: {:}.  Remaining: {:}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">py_inputs</span><span class=\"p\">),</span> <span class=\"n\">elapsed</span><span class=\"p\">,</span> <span class=\"n\">remaining</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Copy the batch to the GPU.\n</span>    <span class=\"n\">b_input_ids</span> <span class=\"o\">=</span> <span class=\"n\">py_inputs</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">b_input_mask</span> <span class=\"o\">=</span> <span class=\"n\">py_attn_masks</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">b_labels</span> <span class=\"o\">=</span> <span class=\"n\">py_labels</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"p\">].</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n  \n    <span class=\"c1\"># Telling the model not to compute or store gradients, saving memory and \n</span>    <span class=\"c1\"># speeding up prediction\n</span>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"c1\"># Forward pass, calculate logit predictions\n</span>        <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">b_input_ids</span><span class=\"p\">,</span> <span class=\"n\">token_type_ids</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> \n                        <span class=\"n\">attention_mask</span><span class=\"o\">=</span><span class=\"n\">b_input_mask</span><span class=\"p\">)</span>\n\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Move logits and labels to CPU\n</span>    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">logits</span><span class=\"p\">.</span><span class=\"n\">detach</span><span class=\"p\">().</span><span class=\"n\">cpu</span><span class=\"p\">().</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n    <span class=\"n\">label_ids</span> <span class=\"o\">=</span> <span class=\"n\">b_labels</span><span class=\"p\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s\">'cpu'</span><span class=\"p\">).</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n  \n    <span class=\"c1\"># Store predictions and true labels\n</span>    <span class=\"n\">predictions</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">)</span>\n    <span class=\"n\">true_labels</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">label_ids</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'    DONE.'</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code46\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code46\">Predicting labels for 25,000 test sentences...\n  Batch     200  of    1,563.    Elapsed: 0:01:32.  Remaining: 0:10:27\n  Batch     400  of    1,563.    Elapsed: 0:03:03.  Remaining: 0:08:53\n  Batch     600  of    1,563.    Elapsed: 0:04:35.  Remaining: 0:07:21\n  Batch     800  of    1,563.    Elapsed: 0:06:05.  Remaining: 0:05:48\n  Batch   1,000  of    1,563.    Elapsed: 0:07:37.  Remaining: 0:04:17\n  Batch   1,200  of    1,563.    Elapsed: 0:09:08.  Remaining: 0:02:46\n  Batch   1,400  of    1,563.    Elapsed: 0:10:44.  Remaining: 0:01:15\n    DONE.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code46\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code46\">Predicting labels for 25,000 test sentences...\n  Batch     200  of    1,563.    Elapsed: 0:01:32.  Remaining: 0:10:27\n  Batch     400  of    1,563.    Elapsed: 0:03:03.  Remaining: 0:08:53\n  Batch     600  of    1,563.    Elapsed: 0:04:35.  Remaining: 0:07:21\n  Batch     800  of    1,563.    Elapsed: 0:06:05.  Remaining: 0:05:48\n  Batch   1,000  of    1,563.    Elapsed: 0:07:37.  Remaining: 0:04:17\n  Batch   1,200  of    1,563.    Elapsed: 0:09:08.  Remaining: 0:02:46\n  Batch   1,400  of    1,563.    Elapsed: 0:10:44.  Remaining: 0:01:15\n    DONE.\n</code></pre>\n<ul>\n  <li>Now we can calculate the test set accuracy!</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code47\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code47\"><span class=\"c1\"># Combine the results across the batches.\n</span><span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Choose the label with the highest score as our prediction.\n</span><span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"n\">flatten</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Calculate simple flat accuracy -- number correct over total number.\n</span><span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">preds</span> <span class=\"o\">==</span> <span class=\"n\">true_labels</span><span class=\"p\">).</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Accuracy: {:.3f}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">accuracy</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code47\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code47\"><span class=\"c1\"># Combine the results across the batches.\n</span><span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Choose the label with the highest score as our prediction.\n</span><span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"n\">flatten</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Calculate simple flat accuracy -- number correct over total number.\n</span><span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">preds</span> <span class=\"o\">==</span> <span class=\"n\">true_labels</span><span class=\"p\">).</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Accuracy: {:.3f}'</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">accuracy</span><span class=\"p\">))</span>\n</code></pre>\n<ul>\n  <li>which outputs:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code48\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code48\">Accuracy: 0.935\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code48\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code48\">Accuracy: 0.935\n</code></pre>\n<ul>\n  <li>Here are our results from running a single training epoch on a Tesla K80, comparing fixed-padding to smart batching.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/fine-tune-and-eval-BERT/res.jpg\" alt=\"\"></p>\n<ul>\n  <li>The fixed padding approach took 51.7% longer to train the model than Smart Batching!</li>\n</ul>",
    "contentMarkdown": "*   With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set.\n\n![](https://aman.ai/images/copy.png)\n\n`# Prediction on test set print('Predicting labels for {:,} test sentences...'.format(len(test_labels)))  # Tracking variables  predictions , true_labels = [], []  # Choose an interval on which to print progress updates. update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)  # Measure elapsed time. t0 = time.time()  # Put model in evaluation mode model.eval()  # For each batch of training data... for step in range(0, len(py_inputs)):      # Progress update every 100 batches.     if step % update_interval == 0 and not step == 0:         # Calculate elapsed time in minutes.         elapsed = format_time(time.time() - t0)                  # Calculate the time remaining based on our progress.         steps_per_sec = (time.time() - t0) / step         remaining_sec = steps_per_sec * (len(py_inputs) - step)         remaining = format_time(remaining_sec)          # Report progress.         print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))      # Copy the batch to the GPU.     b_input_ids = py_inputs[step].to(device)     b_input_mask = py_attn_masks[step].to(device)     b_labels = py_labels[step].to(device)        # Telling the model not to compute or store gradients, saving memory and      # speeding up prediction     with torch.no_grad():         # Forward pass, calculate logit predictions         outputs = model(b_input_ids, token_type_ids=None,                          attention_mask=b_input_mask)      logits = outputs[0]      # Move logits and labels to CPU     logits = logits.detach().cpu().numpy()     label_ids = b_labels.to('cpu').numpy()        # Store predictions and true labels     predictions.append(logits)     true_labels.append(label_ids)  print('    DONE.')`\n\n![](https://aman.ai/images/copy.png)\n\n`# Prediction on test set print('Predicting labels for {:,} test sentences...'.format(len(test_labels)))  # Tracking variables  predictions , true_labels = [], []  # Choose an interval on which to print progress updates. update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)  # Measure elapsed time. t0 = time.time()  # Put model in evaluation mode model.eval()  # For each batch of training data... for step in range(0, len(py_inputs)):      # Progress update every 100 batches.     if step % update_interval == 0 and not step == 0:         # Calculate elapsed time in minutes.         elapsed = format_time(time.time() - t0)                  # Calculate the time remaining based on our progress.         steps_per_sec = (time.time() - t0) / step         remaining_sec = steps_per_sec * (len(py_inputs) - step)         remaining = format_time(remaining_sec)          # Report progress.         print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))      # Copy the batch to the GPU.     b_input_ids = py_inputs[step].to(device)     b_input_mask = py_attn_masks[step].to(device)     b_labels = py_labels[step].to(device)        # Telling the model not to compute or store gradients, saving memory and      # speeding up prediction     with torch.no_grad():         # Forward pass, calculate logit predictions         outputs = model(b_input_ids, token_type_ids=None,                          attention_mask=b_input_mask)      logits = outputs[0]      # Move logits and labels to CPU     logits = logits.detach().cpu().numpy()     label_ids = b_labels.to('cpu').numpy()        # Store predictions and true labels     predictions.append(logits)     true_labels.append(label_ids)  print('    DONE.')`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Predicting labels for 25,000 test sentences...   Batch     200  of    1,563.    Elapsed: 0:01:32.  Remaining: 0:10:27   Batch     400  of    1,563.    Elapsed: 0:03:03.  Remaining: 0:08:53   Batch     600  of    1,563.    Elapsed: 0:04:35.  Remaining: 0:07:21   Batch     800  of    1,563.    Elapsed: 0:06:05.  Remaining: 0:05:48   Batch   1,000  of    1,563.    Elapsed: 0:07:37.  Remaining: 0:04:17   Batch   1,200  of    1,563.    Elapsed: 0:09:08.  Remaining: 0:02:46   Batch   1,400  of    1,563.    Elapsed: 0:10:44.  Remaining: 0:01:15     DONE.`\n\n![](https://aman.ai/images/copy.png)\n\n`Predicting labels for 25,000 test sentences...   Batch     200  of    1,563.    Elapsed: 0:01:32.  Remaining: 0:10:27   Batch     400  of    1,563.    Elapsed: 0:03:03.  Remaining: 0:08:53   Batch     600  of    1,563.    Elapsed: 0:04:35.  Remaining: 0:07:21   Batch     800  of    1,563.    Elapsed: 0:06:05.  Remaining: 0:05:48   Batch   1,000  of    1,563.    Elapsed: 0:07:37.  Remaining: 0:04:17   Batch   1,200  of    1,563.    Elapsed: 0:09:08.  Remaining: 0:02:46   Batch   1,400  of    1,563.    Elapsed: 0:10:44.  Remaining: 0:01:15     DONE.`\n\n*   Now we can calculate the test set accuracy!\n\n![](https://aman.ai/images/copy.png)\n\n`# Combine the results across the batches. predictions = np.concatenate(predictions, axis=0) true_labels = np.concatenate(true_labels, axis=0)  # Choose the label with the highest score as our prediction. preds = np.argmax(predictions, axis=1).flatten()  # Calculate simple flat accuracy -- number correct over total number. accuracy = (preds == true_labels).mean()  print('Accuracy: {:.3f}'.format(accuracy))`\n\n![](https://aman.ai/images/copy.png)\n\n`# Combine the results across the batches. predictions = np.concatenate(predictions, axis=0) true_labels = np.concatenate(true_labels, axis=0)  # Choose the label with the highest score as our prediction. preds = np.argmax(predictions, axis=1).flatten()  # Calculate simple flat accuracy -- number correct over total number. accuracy = (preds == true_labels).mean()  print('Accuracy: {:.3f}'.format(accuracy))`\n\n*   which outputs:\n\n![](https://aman.ai/images/copy.png)\n\n`Accuracy: 0.935`\n\n![](https://aman.ai/images/copy.png)\n\n`Accuracy: 0.935`\n\n*   Here are our results from running a single training epoch on a Tesla K80, comparing fixed-padding to smart batching.\n\n![](/primers/ai/assets/fine-tune-and-eval-BERT/res.jpg)\n\n*   The fixed padding approach took 51.7% longer to train the model than Smart Batching!",
    "contentLength": 21991,
    "wordCount": 706,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#evaluate"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-smart-batching-with-batchencodeplus-and-dataloader-17",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Appendix",
    "title": "Smart Batching with batch_encode_plus and DataLoader",
    "order": 17,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>This section discusses how “smart batching” might be implemented in a more formal way with the PyTorch <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> class and using the features currently available in huggingface <code class=\"language-plaintext highlighter-rouge\">transformers</code>.</p>\n  </li>\n  <li>\n    <p>In <code class=\"language-plaintext highlighter-rouge\">transformers</code>, the <code class=\"language-plaintext highlighter-rouge\">batch_encode_plus</code> function does have support for “Dynamic Padding”–it can automatically pad all of the samples in a batch out to match the longest sequence length in the batch.</p>\n  </li>\n  <li>\n    <p>Check out the explanation given in this Warning in the source code, in <a href=\"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L1458\">tokenization_utils_base.py</a>:</p>\n  </li>\n</ul>\n<p>This section discusses how “smart batching” might be implemented in a more formal way with the PyTorch <code class=\"language-plaintext highlighter-rouge\">DataLoader</code> class and using the features currently available in huggingface <code class=\"language-plaintext highlighter-rouge\">transformers</code>.</p>\n<p>In <code class=\"language-plaintext highlighter-rouge\">transformers</code>, the <code class=\"language-plaintext highlighter-rouge\">batch_encode_plus</code> function does have support for “Dynamic Padding”–it can automatically pad all of the samples in a batch out to match the longest sequence length in the batch.</p>\n<p>Check out the explanation given in this Warning in the source code, in <a href=\"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L1458\">tokenization_utils_base.py</a>:</p>\n<blockquote>\n  <p>“The pad_to_max_length argument is deprecated and will be removed in a future version, use padding=True or padding=’longest’ to pad to the longest sequence in the batch, or use padding=’max_length’ to pad to a max length. In this case, you can give a specific length with max_length (e.g. max_length=45) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).”</p>\n</blockquote>\n<p>“The pad_to_max_length argument is deprecated and will be removed in a future version, use padding=True or padding=’longest’ to pad to the longest sequence in the batch, or use padding=’max_length’ to pad to a max length. In this case, you can give a specific length with max_length (e.g. max_length=45) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).”</p>\n<ul>\n  <li>So the call would look like this:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code49\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code49\"><span class=\"c1\"># Encode a batch of sentences with dynamic padding.\n</span><span class=\"n\">encoded_dict</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">batch_encode_plus</span><span class=\"p\">(</span>\n                    <span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">],</span> <span class=\"c1\"># Batch of sentences to encode.\n</span>                    <span class=\"n\">add_special_tokens</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Add '[CLS]' and '[SEP]'\n</span>                    <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"s\">'longest'</span><span class=\"p\">,</span>        <span class=\"c1\"># Pad to longest in batch.\n</span>                    <span class=\"n\">truncation</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>          <span class=\"c1\"># Truncate sentences to `max_length`.\n</span>                    <span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"mi\">400</span><span class=\"p\">,</span>           \n                    <span class=\"n\">return_attention_mask</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Construct attn. masks.\n</span>                    <span class=\"n\">return_tensors</span> <span class=\"o\">=</span> <span class=\"s\">'pt'</span><span class=\"p\">,</span>        <span class=\"c1\"># Return pytorch tensors.\n</span>            <span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code49\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code49\"><span class=\"c1\"># Encode a batch of sentences with dynamic padding.\n</span><span class=\"n\">encoded_dict</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">.</span><span class=\"n\">batch_encode_plus</span><span class=\"p\">(</span>\n                    <span class=\"n\">train_text</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">],</span> <span class=\"c1\"># Batch of sentences to encode.\n</span>                    <span class=\"n\">add_special_tokens</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Add '[CLS]' and '[SEP]'\n</span>                    <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"s\">'longest'</span><span class=\"p\">,</span>        <span class=\"c1\"># Pad to longest in batch.\n</span>                    <span class=\"n\">truncation</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>          <span class=\"c1\"># Truncate sentences to `max_length`.\n</span>                    <span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"mi\">400</span><span class=\"p\">,</span>           \n                    <span class=\"n\">return_attention_mask</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"c1\"># Construct attn. masks.\n</span>                    <span class=\"n\">return_tensors</span> <span class=\"o\">=</span> <span class=\"s\">'pt'</span><span class=\"p\">,</span>        <span class=\"c1\"># Return pytorch tensors.\n</span>            <span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>\n    <p>However, if you do this, you need wait to actually encode your text until the random training samples for the batch have been selected! i.e., you can’t pre-tokenize all of your sentences the way we have been.</p>\n  </li>\n  <li>\n    <p>The “correct” approach would be to implement a custom DataLoader class which:</p>\n\n    <ol>\n      <li>Stores the original training sample strings, sorted by string length.</li>\n      <li>Selects a contiguous batch of samples starting at a random point in the list.</li>\n      <li>Calls <code class=\"language-plaintext highlighter-rouge\">batch_encode_plus</code> to encode the samples with dynamic padding, then returns the training batch.</li>\n    </ol>\n  </li>\n</ul>\n<p>However, if you do this, you need wait to actually encode your text until the random training samples for the batch have been selected! i.e., you can’t pre-tokenize all of your sentences the way we have been.</p>\n<p>The “correct” approach would be to implement a custom DataLoader class which:</p>\n<ol>\n      <li>Stores the original training sample strings, sorted by string length.</li>\n      <li>Selects a contiguous batch of samples starting at a random point in the list.</li>\n      <li>Calls <code class=\"language-plaintext highlighter-rouge\">batch_encode_plus</code> to encode the samples with dynamic padding, then returns the training batch.</li>\n    </ol>",
    "contentMarkdown": "*   This section discusses how “smart batching” might be implemented in a more formal way with the PyTorch `DataLoader` class and using the features currently available in huggingface `transformers`.\n    \n*   In `transformers`, the `batch_encode_plus` function does have support for “Dynamic Padding”–it can automatically pad all of the samples in a batch out to match the longest sequence length in the batch.\n    \n*   Check out the explanation given in this Warning in the source code, in [tokenization\\_utils\\_base.py](https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L1458):\n    \n\nThis section discusses how “smart batching” might be implemented in a more formal way with the PyTorch `DataLoader` class and using the features currently available in huggingface `transformers`.\n\nIn `transformers`, the `batch_encode_plus` function does have support for “Dynamic Padding”–it can automatically pad all of the samples in a batch out to match the longest sequence length in the batch.\n\nCheck out the explanation given in this Warning in the source code, in [tokenization\\_utils\\_base.py](https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L1458):\n\n> “The pad\\_to\\_max\\_length argument is deprecated and will be removed in a future version, use padding=True or padding=’longest’ to pad to the longest sequence in the batch, or use padding=’max\\_length’ to pad to a max length. In this case, you can give a specific length with max\\_length (e.g. max\\_length=45) or leave max\\_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).”\n\n“The pad\\_to\\_max\\_length argument is deprecated and will be removed in a future version, use padding=True or padding=’longest’ to pad to the longest sequence in the batch, or use padding=’max\\_length’ to pad to a max length. In this case, you can give a specific length with max\\_length (e.g. max\\_length=45) or leave max\\_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).”\n\n*   So the call would look like this:\n\n![](https://aman.ai/images/copy.png)\n\n``# Encode a batch of sentences with dynamic padding. encoded_dict = tokenizer.batch_encode_plus(                     train_text[i:i+batch_size], # Batch of sentences to encode.                     add_special_tokens = True,  # Add '[CLS]' and '[SEP]'                     padding = 'longest',        # Pad to longest in batch.                     truncation = True,          # Truncate sentences to `max_length`.                     max_length = 400,                                return_attention_mask = True, # Construct attn. masks.                     return_tensors = 'pt',        # Return pytorch tensors.             )``\n\n![](https://aman.ai/images/copy.png)\n\n``# Encode a batch of sentences with dynamic padding. encoded_dict = tokenizer.batch_encode_plus(                     train_text[i:i+batch_size], # Batch of sentences to encode.                     add_special_tokens = True,  # Add '[CLS]' and '[SEP]'                     padding = 'longest',        # Pad to longest in batch.                     truncation = True,          # Truncate sentences to `max_length`.                     max_length = 400,                                return_attention_mask = True, # Construct attn. masks.                     return_tensors = 'pt',        # Return pytorch tensors.             )``\n\n*   However, if you do this, you need wait to actually encode your text until the random training samples for the batch have been selected! i.e., you can’t pre-tokenize all of your sentences the way we have been.\n    \n*   The “correct” approach would be to implement a custom DataLoader class which:\n    \n    1.  Stores the original training sample strings, sorted by string length.\n    2.  Selects a contiguous batch of samples starting at a random point in the list.\n    3.  Calls `batch_encode_plus` to encode the samples with dynamic padding, then returns the training batch.\n\nHowever, if you do this, you need wait to actually encode your text until the random training samples for the batch have been selected! i.e., you can’t pre-tokenize all of your sentences the way we have been.\n\nThe “correct” approach would be to implement a custom DataLoader class which:\n\n1.  Stores the original training sample strings, sorted by string length.\n2.  Selects a contiguous batch of samples starting at a random point in the list.\n3.  Calls `batch_encode_plus` to encode the samples with dynamic padding, then returns the training batch.",
    "contentLength": 8128,
    "wordCount": 600,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#smart-batching-with-batch_encode_plus-and-dataloader"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-impact-of-pad-tokens-on-accuracy-18",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Appendix",
    "title": "Impact of [PAD] Tokens on Accuracy",
    "order": 18,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>The difference in accuracy (0.93 for fixed-padding and 0.935 for smart batching) is interesting. It makes me curious to look at the attention mask implementation more–perhaps the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens are still having some small influence on the results.</p>\n  </li>\n  <li>\n    <p>Maybe the Attention scores are calculated for all tokens, including the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens, and then the scores are multiplied against the mask, to zero out the scores for the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens? Because the SoftMax makes all of the attention scores sum to 1.0, that would mean the presence of <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens would cause the scores for the real words to be lower…</p>\n  </li>\n</ul>\n<p>The difference in accuracy (0.93 for fixed-padding and 0.935 for smart batching) is interesting. It makes me curious to look at the attention mask implementation more–perhaps the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens are still having some small influence on the results.</p>\n<p>Maybe the Attention scores are calculated for all tokens, including the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens, and then the scores are multiplied against the mask, to zero out the scores for the <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens? Because the SoftMax makes all of the attention scores sum to 1.0, that would mean the presence of <code class=\"language-plaintext highlighter-rouge\">[PAD]</code> tokens would cause the scores for the real words to be lower…</p>",
    "contentMarkdown": "*   The difference in accuracy (0.93 for fixed-padding and 0.935 for smart batching) is interesting. It makes me curious to look at the attention mask implementation more–perhaps the `[PAD]` tokens are still having some small influence on the results.\n    \n*   Maybe the Attention scores are calculated for all tokens, including the `[PAD]` tokens, and then the scores are multiplied against the mask, to zero out the scores for the `[PAD]` tokens? Because the SoftMax makes all of the attention scores sum to 1.0, that would mean the presence of `[PAD]` tokens would cause the scores for the real words to be lower…\n    \n\nThe difference in accuracy (0.93 for fixed-padding and 0.935 for smart batching) is interesting. It makes me curious to look at the attention mask implementation more–perhaps the `[PAD]` tokens are still having some small influence on the results.\n\nMaybe the Attention scores are calculated for all tokens, including the `[PAD]` tokens, and then the scores are multiplied against the mask, to zero out the scores for the `[PAD]` tokens? Because the SoftMax makes all of the attention scores sum to 1.0, that would mean the presence of `[PAD]` tokens would cause the scores for the real words to be lower…",
    "contentLength": 1732,
    "wordCount": 202,
    "hasCode": true,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#impact-of-[pad]-tokens-on-accuracy"
  },
  {
    "id": "ai-fine-tune-and-eval-BERT-out-of-memory-oom-errors-19",
    "articleSlug": "fine-tune-and-eval-BERT",
    "articleTitle": "Fine-Tuning and Evaluating BERT",
    "category": "Data/Training",
    "chapter": "Appendix",
    "title": "Out of Memory (OOM) Errors",
    "order": 19,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>OOM errors can happen due to bugs in your code or using the wrong version of PyTorch/CUDA. Some OOMs come from sub-optimal code in do_sample, such as storing extra tensors (e.g., storing the entire set of logits rather than just the sampled token index) or failing to wrap your sampling code with <code class=\"language-plaintext highlighter-rouge\">torch.inference_mode()</code>.</li>\n  <li>\n    <p>Another way to save memory in <code class=\"language-plaintext highlighter-rouge\">do_sample()</code> is to avoid saving the full outputs object from the model. That is, instead of:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code50\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code50\">  <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"p\">.</span><span class=\"n\">logits</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:]</span> \n</code></pre></div>    </div>\n\n    <ul>\n      <li>maybe just do</li>\n    </ul>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code51\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code51\">  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">).</span><span class=\"n\">logits</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li>so you aren’t saving the full model outputs variable (including the full logits output tensor, which can add up to a few hundred MB of memory).</li>\n    </ul>\n  </li>\n</ul>\n<p>Another way to save memory in <code class=\"language-plaintext highlighter-rouge\">do_sample()</code> is to avoid saving the full outputs object from the model. That is, instead of:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code50\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code50\">  <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)</span>\n  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"p\">.</span><span class=\"n\">logits</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:]</span> \n</code></pre>\n<ul>\n      <li>maybe just do</li>\n    </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code51\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code51\">  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">).</span><span class=\"n\">logits</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n</code></pre>\n<ul>\n      <li>so you aren’t saving the full model outputs variable (including the full logits output tensor, which can add up to a few hundred MB of memory).</li>\n    </ul>",
    "contentMarkdown": "*   OOM errors can happen due to bugs in your code or using the wrong version of PyTorch/CUDA. Some OOMs come from sub-optimal code in do\\_sample, such as storing extra tensors (e.g., storing the entire set of logits rather than just the sampled token index) or failing to wrap your sampling code with `torch.inference_mode()`.\n*   Another way to save memory in `do_sample()` is to avoid saving the full outputs object from the model. That is, instead of:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `output = model(input_ids)   logits = output.logits[:, -1, :]` \n    \n    *   maybe just do\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `logits = model(input_ids).logits[:, -1, :]`\n    \n    *   so you aren’t saving the full model outputs variable (including the full logits output tensor, which can add up to a few hundred MB of memory).\n\nAnother way to save memory in `do_sample()` is to avoid saving the full outputs object from the model. That is, instead of:\n\n![](https://aman.ai/images/copy.png)\n\n  `output = model(input_ids)   logits = output.logits[:, -1, :]` \n\n*   maybe just do\n\n![](https://aman.ai/images/copy.png)\n\n  `logits = model(input_ids).logits[:, -1, :]`\n\n*   so you aren’t saving the full model outputs variable (including the full logits output tensor, which can add up to a few hundred MB of memory).",
    "contentLength": 4087,
    "wordCount": 191,
    "hasCode": true,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/fine-tune-and-eval-BERT/#out-of-memory-(oom)-errors"
  }
]