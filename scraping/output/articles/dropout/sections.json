[
  {
    "id": "ai-dropout-use-with-all-network-types-1",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Use with All Network Types",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Dropout regularization is a generic approach.</p>\n  </li>\n  <li>\n    <p>It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks.</p>\n  </li>\n  <li>\n    <p>In the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent connections.</p>\n  </li>\n</ul>\n<p>Dropout regularization is a generic approach.</p>\n<p>It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks.</p>\n<p>In the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent connections.</p>",
    "contentMarkdown": "*   Dropout regularization is a generic approach.\n    \n*   It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks.\n    \n*   In the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent connections.\n    \n\nDropout regularization is a generic approach.\n\nIt can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks.\n\nIn the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent connections.",
    "contentLength": 875,
    "wordCount": 121,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#use-with-all-network-types"
  },
  {
    "id": "ai-dropout-dropout-rate-2",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Dropout Rate",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer.</p>\n  </li>\n  <li>\n    <p>A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.</p>\n  </li>\n</ul>\n<p>The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer.</p>\n<p>A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.</p>",
    "contentMarkdown": "*   The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer.\n    \n*   A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.\n    \n\nThe default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer.\n\nA good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.",
    "contentLength": 686,
    "wordCount": 114,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#dropout-rate"
  },
  {
    "id": "ai-dropout-use-a-larger-network-3",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Use a Larger Network",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>It is common for larger networks (more layers or more nodes) to more easily overfit the training data.</p>\n  </li>\n  <li>\n    <p>When using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network.</p>\n  </li>\n  <li>\n    <p>A good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.</p>\n  </li>\n</ul>\n<p>It is common for larger networks (more layers or more nodes) to more easily overfit the training data.</p>\n<p>When using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network.</p>\n<p>A good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.</p>\n<blockquote>\n  <p>If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 0.604em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n</script> is the number of hidden units in any layer and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 0.604em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.502em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">p</script> is the probability of retaining a unit […] a good dropout net should have at least <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 1.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.299em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1001.25em, 2.502em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"texatom\" id=\"MathJax-Span-40\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mo\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.169em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">n/p</script> units</p>\n</blockquote>\n<p>If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 0.604em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n</script> is the number of hidden units in any layer and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 0.604em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.502em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">p</script> is the probability of retaining a unit […] a good dropout net should have at least <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>p</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 1.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.299em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1001.25em, 2.502em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"texatom\" id=\"MathJax-Span-40\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mo\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic;\">p</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.169em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>p</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">n/p</script> units</p>\n<p>— <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014.</p>",
    "contentMarkdown": "*   It is common for larger networks (more layers or more nodes) to more easily overfit the training data.\n    \n*   When using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network.\n    \n*   A good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n    \n\nIt is common for larger networks (more layers or more nodes) to more easily overfit the training data.\n\nWhen using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network.\n\nA good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n\n> If nnn is the number of hidden units in any layer and ppp is the probability of retaining a unit \\[…\\] a good dropout net should have at least n/pn/pn/p units\n\nIf nnn is the number of hidden units in any layer and ppp is the probability of retaining a unit \\[…\\] a good dropout net should have at least n/pn/pn/p units\n\n— [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.",
    "contentLength": 9732,
    "wordCount": 312,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#use-a-larger-network"
  },
  {
    "id": "ai-dropout-grid-search-dropout-rate-4",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Grid Search Dropout Rate",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>Rather than guess at a suitable dropout rate for your network, test different rates systematically.</p>\n  </li>\n  <li>\n    <p>For example, test values between 1.0 and 0.1 in increments of 0.1.</p>\n  </li>\n  <li>\n    <p>This will both help you discover what works best for your specific model and dataset, as well as how sensitive the model is to the dropout rate. A more sensitive model may be unstable and could benefit from an increase in size.</p>\n  </li>\n</ul>\n<p>Rather than guess at a suitable dropout rate for your network, test different rates systematically.</p>\n<p>For example, test values between 1.0 and 0.1 in increments of 0.1.</p>\n<p>This will both help you discover what works best for your specific model and dataset, as well as how sensitive the model is to the dropout rate. A more sensitive model may be unstable and could benefit from an increase in size.</p>",
    "contentMarkdown": "*   Rather than guess at a suitable dropout rate for your network, test different rates systematically.\n    \n*   For example, test values between 1.0 and 0.1 in increments of 0.1.\n    \n*   This will both help you discover what works best for your specific model and dataset, as well as how sensitive the model is to the dropout rate. A more sensitive model may be unstable and could benefit from an increase in size.\n    \n\nRather than guess at a suitable dropout rate for your network, test different rates systematically.\n\nFor example, test values between 1.0 and 0.1 in increments of 0.1.\n\nThis will both help you discover what works best for your specific model and dataset, as well as how sensitive the model is to the dropout rate. A more sensitive model may be unstable and could benefit from an increase in size.",
    "contentLength": 899,
    "wordCount": 141,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#grid-search-dropout-rate"
  },
  {
    "id": "ai-dropout-use-a-weight-constraint-5",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Use a Weight Constraint",
    "order": 5,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Network weights will increase in size in response to the probabilistic removal of layer activations.</p>\n  </li>\n  <li>\n    <p>Large weight size can be a sign of an unstable network.</p>\n  </li>\n  <li>\n    <p>To counter this effect a weight constraint can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value. For example, the maximum norm constraint is recommended with a value between 3-4.</p>\n  </li>\n</ul>\n<p>Network weights will increase in size in response to the probabilistic removal of layer activations.</p>\n<p>Large weight size can be a sign of an unstable network.</p>\n<p>To counter this effect a weight constraint can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value. For example, the maximum norm constraint is recommended with a value between 3-4.</p>\n<blockquote>\n  <p>[…] we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-44\" style=\"width: 0.558em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-45\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">c</script>. Typical values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-47\" style=\"width: 0.558em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-48\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">c</script> range from 3 to 4. — <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014.</p>\n</blockquote>\n<p>[…] we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-44\" style=\"width: 0.558em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-45\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">c</script>. Typical values of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-47\" style=\"width: 0.558em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.576em, 1000.47em, 2.317em, -999.998em); top: -2.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-48\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.178em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">c</script> range from 3 to 4. — <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014.</p>\n<p>This does introduce an additional hyperparameter that may require tuning for the model.</p>",
    "contentMarkdown": "*   Network weights will increase in size in response to the probabilistic removal of layer activations.\n    \n*   Large weight size can be a sign of an unstable network.\n    \n*   To counter this effect a weight constraint can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value. For example, the maximum norm constraint is recommended with a value between 3-4.\n    \n\nNetwork weights will increase in size in response to the probabilistic removal of layer activations.\n\nLarge weight size can be a sign of an unstable network.\n\nTo counter this effect a weight constraint can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value. For example, the maximum norm constraint is recommended with a value between 3-4.\n\n> \\[…\\] we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant ccc. Typical values of ccc range from 3 to 4. — [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.\n\n\\[…\\] we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant ccc. Typical values of ccc range from 3 to 4. — [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.\n\nThis does introduce an additional hyperparameter that may require tuning for the model.",
    "contentLength": 6355,
    "wordCount": 245,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#use-a-weight-constraint"
  },
  {
    "id": "ai-dropout-use-with-smaller-datasets-6",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Tips for Using Dropout Regularization",
    "title": "Use with Smaller Datasets",
    "order": 6,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>Like other regularization methods, dropout is more effective on those problems where there is a limited amount of training data and the model is likely to overfit the training data.</p>\n  </li>\n  <li>\n    <p>Problems where there is a large amount of training data may see less benefit from using dropout.</p>\n  </li>\n</ul>\n<p>Like other regularization methods, dropout is more effective on those problems where there is a limited amount of training data and the model is likely to overfit the training data.</p>\n<p>Problems where there is a large amount of training data may see less benefit from using dropout.</p>\n<blockquote>\n  <p>For very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization. — Page 265, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016.</p>\n</blockquote>\n<p>For very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization. — Page 265, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016.</p>",
    "contentMarkdown": "*   Like other regularization methods, dropout is more effective on those problems where there is a limited amount of training data and the model is likely to overfit the training data.\n    \n*   Problems where there is a large amount of training data may see less benefit from using dropout.\n    \n\nLike other regularization methods, dropout is more effective on those problems where there is a limited amount of training data and the model is likely to overfit the training data.\n\nProblems where there is a large amount of training data may see less benefit from using dropout.\n\n> For very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization. — Page 265, [Deep Learning](https://amzn.to/2NJW3gE), 2016.\n\nFor very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization. — Page 265, [Deep Learning](https://amzn.to/2NJW3gE), 2016.",
    "contentLength": 1235,
    "wordCount": 167,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#use-with-smaller-datasets"
  },
  {
    "id": "ai-dropout-books-7",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Books",
    "order": 7,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Section 7.12 Dropout, <a href=\"https://amzn.to/2NJW3gE\">Deep Learning</a>, 2016</li>\n  <li>Section 4.4.3 Adding dropout, <a href=\"https://amzn.to/2wVqZDq\">Deep Learning With Python</a>, 2017</li>\n</ul>",
    "contentMarkdown": "*   Section 7.12 Dropout, [Deep Learning](https://amzn.to/2NJW3gE), 2016\n*   Section 4.4.3 Adding dropout, [Deep Learning With Python](https://amzn.to/2wVqZDq), 2017",
    "contentLength": 212,
    "wordCount": 17,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#books"
  },
  {
    "id": "ai-dropout-papers-8",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Papers",
    "order": 8,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/1207.0580\">Improving neural networks by preventing co-adaptation of feature detectors</a>, 2012</li>\n  <li><a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014</li>\n  <li><a href=\"https://ieeexplore.ieee.org/document/6639346/\">Improving deep neural networks for LVCSR using rectified linear units and dropout</a>, 2013</li>\n  <li><a href=\"https://arxiv.org/abs/1307.1493\">Dropout Training as Adaptive Regularization</a>, 2013</li>\n</ul>",
    "contentMarkdown": "*   [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580), 2012\n*   [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014\n*   [Improving deep neural networks for LVCSR using rectified linear units and dropout](https://ieeexplore.ieee.org/document/6639346/), 2013\n*   [Dropout Training as Adaptive Regularization](https://arxiv.org/abs/1307.1493), 2013",
    "contentLength": 557,
    "wordCount": 44,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#papers"
  },
  {
    "id": "ai-dropout-articles-9",
    "articleSlug": "dropout",
    "articleTitle": "Dropout",
    "category": "Data/Training",
    "chapter": "Further Reading",
    "title": "Articles",
    "order": 9,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><a href=\"https://en.wikipedia.org/wiki/Dropout_(neural_networks)\">Dropout (neural networks), Wikipedia</a></li>\n  <li><a href=\"https://cs231n.github.io/neural-networks-2/#reg\">Regularization, CS231n Convolutional Neural Networks for Visual Recognition</a></li>\n  <li><a href=\"https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/d64yyas\">How was ‘Dropout’ conceived? Was there an ‘aha’ moment?</a></li>\n</ul>",
    "contentMarkdown": "*   [Dropout (neural networks), Wikipedia](https://en.wikipedia.org/wiki/Dropout_\\(neural_networks\\))\n*   [Regularization, CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-2/#reg)\n*   [How was ‘Dropout’ conceived? Was there an ‘aha’ moment?](https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/d64yyas)",
    "contentLength": 468,
    "wordCount": 24,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/dropout/#articles"
  }
]