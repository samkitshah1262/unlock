[
  {
    "id": "ai-double-descent-model-wise-double-descent-1",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Double Descent",
    "articleSlug": "double-descent",
    "chapter": "(Deep) Double Descent",
    "title": "Model-wise Double Descent",
    "subtitle": "(Deep) Double Descent",
    "contentHtml": "<ul>\n  <li>There is a regime where bigger models are worse, as shown in the graph below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/10.jpg\" alt=\"\"></p>\n<ul>\n  <li>The model-wise double descent phenomenon can lead to a regime where training on more data hurts. In the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set.</li>\n  <li>In all cases we’ve observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, the number of train samples, or the amount of label noise) also affect the location of the test error peak correspondingly. The double descent phenomena is most prominent in settings with added label noise; without it, the peak is smaller and easy to miss. Adding label noise amplifies this general behavior and allows us to easily investigate.</li>\n</ul>",
    "contentMarkdown": "*   There is a regime where bigger models are worse, as shown in the graph below:\n\n![](/primers/ai/assets/learning-theory/10.jpg)\n\n*   The model-wise double descent phenomenon can lead to a regime where training on more data hurts. In the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set.\n*   In all cases we’ve observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, the number of train samples, or the amount of label noise) also affect the location of the test error peak correspondingly. The double descent phenomena is most prominent in settings with added label noise; without it, the peak is smaller and easy to miss. Adding label noise amplifies this general behavior and allows us to easily investigate.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 136,
      "contentLength": 919
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/double-descent/#model-wise-double-descent",
    "scrapedAt": "2025-12-28T11:49:32.078Z"
  },
  {
    "id": "ai-double-descent-sample-wise-non-monotonicity-2",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Double Descent",
    "articleSlug": "double-descent",
    "chapter": "(Deep) Double Descent",
    "title": "Sample-wise Non-monotonicity",
    "subtitle": "(Deep) Double Descent",
    "contentHtml": "<ul>\n  <li>There is a regime where more samples hurts, as shown in the graph below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/11.jpg\" alt=\"\"></p>\n<ul>\n  <li>The above chart shows transformers trained on a language-translation task with no added label noise. As expected, increasing the number of samples shifts the curve downwards towards lower test error. However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right.</li>\n  <li>For intermediate model sizes (red arrows), these two effects combine, and we see that training on 4.5x more samples actually hurts test performance.</li>\n</ul>",
    "contentMarkdown": "*   There is a regime where more samples hurts, as shown in the graph below:\n\n![](/primers/ai/assets/learning-theory/11.jpg)\n\n*   The above chart shows transformers trained on a language-translation task with no added label noise. As expected, increasing the number of samples shifts the curve downwards towards lower test error. However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right.\n*   For intermediate model sizes (red arrows), these two effects combine, and we see that training on 4.5x more samples actually hurts test performance.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "datatraining",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 98,
      "contentLength": 714
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/double-descent/#sample-wise-non-monotonicity",
    "scrapedAt": "2025-12-28T11:49:32.078Z"
  },
  {
    "id": "ai-double-descent-epoch-wise-double-descent-3",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Double Descent",
    "articleSlug": "double-descent",
    "chapter": "(Deep) Double Descent",
    "title": "Epoch-wise Double Descent",
    "subtitle": "(Deep) Double Descent",
    "contentHtml": "<ul>\n  <li>There is a regime where training longer reverses overfitting, as shown in the graphs below:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/12.jpg\" alt=\"\"></p>\n<p><img src=\"/primers/ai/assets/learning-theory/13.jpg\" alt=\"\"></p>\n<ul>\n  <li>The charts above show test and train error as a function of both model size and number of optimization steps. For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size (fixed x-coordinate), as training proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent.</li>\n  <li>In general, the peak of test error appears systematically when models are just barely able to fit the train set.</li>\n  <li>Our intuition is that, for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly noisy or misspecified labels will destroy its global structure. That is, there are no “good models” which both interpolate the train set and perform well on the test set. However, in the over-parameterized regime, there are many models that fit the train set and there exist such good models. Moreover, the implicit bias of stochastic gradient descent (SGD) leads it to such good models, for reasons we don’t yet understand.</li>\n  <li>Nakkiran et al. (2019) leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question.</li>\n</ul>",
    "contentMarkdown": "*   There is a regime where training longer reverses overfitting, as shown in the graphs below:\n\n![](/primers/ai/assets/learning-theory/12.jpg)\n\n![](/primers/ai/assets/learning-theory/13.jpg)\n\n*   The charts above show test and train error as a function of both model size and number of optimization steps. For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size (fixed x-coordinate), as training proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent.\n*   In general, the peak of test error appears systematically when models are just barely able to fit the train set.\n*   Our intuition is that, for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly noisy or misspecified labels will destroy its global structure. That is, there are no “good models” which both interpolate the train set and perform well on the test set. However, in the over-parameterized regime, there are many models that fit the train set and there exist such good models. Moreover, the implicit bias of stochastic gradient descent (SGD) leads it to such good models, for reasons we don’t yet understand.\n*   Nakkiran et al. (2019) leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "datatraining",
      "neural network",
      "optimization",
      "gradient descent"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 224,
      "contentLength": 1558
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/double-descent/#epoch-wise-double-descent",
    "scrapedAt": "2025-12-28T11:49:32.078Z"
  },
  {
    "id": "ai-double-descent-example-natural-cubic-splines-4",
    "domain": "ai_primers",
    "category": "Data/Training",
    "article": "Double Descent",
    "articleSlug": "double-descent",
    "chapter": "(Deep) Double Descent",
    "title": "Example: Natural Cubic Splines",
    "subtitle": "(Deep) Double Descent",
    "contentHtml": "<ul>\n  <li>To understand double descent, let’s check out a simple example that has nothing to do with deep learning: natural cubic splines. What’s a spline? Basically, it’s a way to fit the model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mi>&amp;#x03F5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 6.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.42em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>ϵ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">Y=f(X)+\\epsilon</script>, with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-55\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1001.57em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">f(\\cdot)</script> being non-parametric, using very smooth piecewise polynomials.</li>\n  <li>To fit a spline, we construct some basis functions and then fit the response Y to the basis functions via least squares.</li>\n  <li>The number of basis functions we use is the number of <strong>degrees of freedom</strong> of the spline.</li>\n  <li>A typical basis function is given below:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03C8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><msubsup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>+</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msubsup><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03C8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></msub><msubsup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>+</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 12.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1010.47em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-66\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.68em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Italic;\">ψ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.107em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-68\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mn\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-71\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-73\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"mn\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.221em, -999.997em); top: -3.799em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-76\"><span class=\"mrow\" id=\"MathJax-Span-77\"><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">(</span><span class=\"mi\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-85\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.68em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Italic;\">ψ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.107em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-87\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-90\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-92\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mn\" id=\"MathJax-Span-94\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.58em, 4.221em, -999.997em); top: -3.799em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">(</mo><mi>X</mi><mo>−</mo><msub><mi>ψ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><msubsup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>+</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msubsup><mo>,</mo><mo>…</mo><mo>,</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo>−</mo><msub><mi>ψ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></msub><msubsup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>+</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msubsup></math></span></span></div>\n<ul>\n  <li>Suppose we have <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><mn>20</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-98\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.71em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-99\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">20</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><mn>20</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n = 20</script> (X,Y) pairs, and we want to estimate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi><mi>n</mi><mi>Y</mi><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mi>&amp;#x03F5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-103\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1008.13em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-104\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mi>i</mi><mi>n</mi><mi>Y</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>ϵ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">f(X) in Y=f(X)+\\epsilon</script> (here <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>sin</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-119\" style=\"width: 6.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.68em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-120\"><span class=\"mi\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">sin</span><span class=\"mo\" id=\"MathJax-Span-127\"></span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">f(X)=\\sin(X)</script>) using a spline.</li>\n  <li>First, we fit a spline with 4 DF. The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><mn>20</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.71em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">20</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><mn>20</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">n = 20</script> observations are in gray, true function <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-136\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.62em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-137\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">f(x)</script> is in black, and the fitted function is in light blue. Not bad!</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/3.jpg\" alt=\"\"></p>\n<ul>\n  <li>Now let’s try again, this time with 6 degrees of freedom. This looks awesome, as shown below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/4.jpg\" alt=\"\"></p>\n<ul>\n  <li>Now what if we use 20 degrees of freedom? Intuitively, it seems like a bad idea, because we have n=20 observations and to fit a spline with 20 DF, we need to run least squares with 20 features. We’ll get zero training error (i.e. interpolate the training set) and bad test error!</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/5.jpg\" alt=\"\"></p>\n<ul>\n  <li>The interpolation threshold is roughly where parameters == data points which can be seen from the results in the figure, just as the bias-variance trade-off predicts. All’s well in the world.</li>\n  <li>Next, we’re trying to fit a spline using least squares with n=20 and 36 DF (i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>=</mo><mn>36</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">36</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>=</mo><mn>36</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">p=36</script>). With <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-147\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-148\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">p>n</script> the LS solution isn’t even unique!</li>\n  <li>To select among the infinite number of solutions, let’s choose the “minimum” norm fit: the one with the smallest sum of squared coefficients. [Easy to compute using everybody’s favorite matrix decomp, the SVD.] The result is expected to be horrible, because <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-152\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">p > n</script>. Here’s what we get:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/6.jpg\" alt=\"\"></p>\n<ul>\n  <li>Now, let’s compare the results with 20 DF to 36 DF. We expected the fit with 36 DF to look worse than the one with 20 DF, however, surprisingly it looks a little better!</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/7.jpg\" alt=\"\"></p>\n<ul>\n  <li>Upon taking a peek at the training and test error, we see that the test error (briefly) decreases when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">p > n</script>. However, that sounds counter intuitive since it is literally the opposite of what the bias-variance trade-off says should happen!</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning-theory/8.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>The key point is with 20 DF, n=p, and there’s <strong>exactly one</strong> least squares fit that has zero training error. And that fit happens to have oodles of wiggles, but as we increase the DF so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-162\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-163\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p>n</script>, there are tons of interpolating least squares fits.</p>\n  </li>\n  <li>\n    <p>The <strong>minimum norm least squares</strong> fit is the “least wiggly” of those zillions of fits. Also, note that the “least wiggly” among them is even less wiggly than the fit when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>=</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-167\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>=</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">p=n</script>.</p>\n  </li>\n  <li>\n    <p>So, “double descent” is happening because DF isn’t really the right quantity for the the X-axis: like, the fact that we are choosing the minimum norm least squares fit actually means that the spline with 36 DF is <strong>less</strong> flexible than the spline with 20 DF.</p>\n  </li>\n  <li>\n    <p>Now, what if had used a ridge penalty when fitting the spline (instead of least squares)? In that case, we wouldn’t have interpolated training set, we wouldn’t have seen double descent, and we would have gotten better test error (for the right value of the tuning parameter!).</p>\n  </li>\n  <li>\n    <p>How does this relate to deep learning? When we use (stochastic) gradient descent to fit a neural net, we are actually picking out the minimum norm solution!! So the spline example is a pretty good analogy for what is happening when we see double descent for neural nets.</p>\n  </li>\n  <li>\n    <p><strong>Key takeaways</strong></p>\n    <ul>\n      <li>Double descent is observed and is understandable through the lens of stat ML and the bias/variance trade-off.</li>\n      <li>Actually, the bias/variance trade-off helps us understand <strong>why</strong> DD is happening!</li>\n    </ul>\n  </li>\n</ul>\n<p>The key point is with 20 DF, n=p, and there’s <strong>exactly one</strong> least squares fit that has zero training error. And that fit happens to have oodles of wiggles, but as we increase the DF so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>&amp;gt;</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-162\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-163\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>&gt;</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p>n</script>, there are tons of interpolating least squares fits.</p>\n<p>The <strong>minimum norm least squares</strong> fit is the “least wiggly” of those zillions of fits. Also, note that the “least wiggly” among them is even less wiggly than the fit when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo>=</mo><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-167\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.19em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo>=</mo><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">p=n</script>.</p>\n<p>So, “double descent” is happening because DF isn’t really the right quantity for the the X-axis: like, the fact that we are choosing the minimum norm least squares fit actually means that the spline with 36 DF is <strong>less</strong> flexible than the spline with 20 DF.</p>\n<p>Now, what if had used a ridge penalty when fitting the spline (instead of least squares)? In that case, we wouldn’t have interpolated training set, we wouldn’t have seen double descent, and we would have gotten better test error (for the right value of the tuning parameter!).</p>\n<p>How does this relate to deep learning? When we use (stochastic) gradient descent to fit a neural net, we are actually picking out the minimum norm solution!! So the spline example is a pretty good analogy for what is happening when we see double descent for neural nets.</p>\n<p><strong>Key takeaways</strong></p>\n<ul>\n      <li>Double descent is observed and is understandable through the lens of stat ML and the bias/variance trade-off.</li>\n      <li>Actually, the bias/variance trade-off helps us understand <strong>why</strong> DD is happening!</li>\n    </ul>",
    "contentMarkdown": "*   To understand double descent, let’s check out a simple example that has nothing to do with deep learning: natural cubic splines. What’s a spline? Basically, it’s a way to fit the model Y\\=f(X)+ϵY\\=f(X)+ϵY=f(X)+\\\\epsilon, with f(⋅)f(⋅)f(\\\\cdot) being non-parametric, using very smooth piecewise polynomials.\n*   To fit a spline, we construct some basis functions and then fit the response Y to the basis functions via least squares.\n*   The number of basis functions we use is the number of **degrees of freedom** of the spline.\n*   A typical basis function is given below:\n\n(X−ψ1)3+,…,(X−ψK)3+(X−ψ1)+3,…,(X−ψK)+3\n\n*   Suppose we have n\\=20n\\=20n = 20 (X,Y) pairs, and we want to estimate f(X)inY\\=f(X)+ϵf(X)inY\\=f(X)+ϵf(X) in Y=f(X)+\\\\epsilon (here f(X)\\=sin(X)f(X)\\=sin⁡(X)f(X)=\\\\sin(X)) using a spline.\n*   First, we fit a spline with 4 DF. The n\\=20n\\=20n = 20 observations are in gray, true function f(x)f(x)f(x) is in black, and the fitted function is in light blue. Not bad!\n\n![](/primers/ai/assets/learning-theory/3.jpg)\n\n*   Now let’s try again, this time with 6 degrees of freedom. This looks awesome, as shown below.\n\n![](/primers/ai/assets/learning-theory/4.jpg)\n\n*   Now what if we use 20 degrees of freedom? Intuitively, it seems like a bad idea, because we have n=20 observations and to fit a spline with 20 DF, we need to run least squares with 20 features. We’ll get zero training error (i.e. interpolate the training set) and bad test error!\n\n![](/primers/ai/assets/learning-theory/5.jpg)\n\n*   The interpolation threshold is roughly where parameters == data points which can be seen from the results in the figure, just as the bias-variance trade-off predicts. All’s well in the world.\n*   Next, we’re trying to fit a spline using least squares with n=20 and 36 DF (i.e., p\\=36p\\=36p=36). With p\\>np\\>np>n the LS solution isn’t even unique!\n*   To select among the infinite number of solutions, let’s choose the “minimum” norm fit: the one with the smallest sum of squared coefficients. \\[Easy to compute using everybody’s favorite matrix decomp, the SVD.\\] The result is expected to be horrible, because p\\>np\\>np > n. Here’s what we get:\n\n![](/primers/ai/assets/learning-theory/6.jpg)\n\n*   Now, let’s compare the results with 20 DF to 36 DF. We expected the fit with 36 DF to look worse than the one with 20 DF, however, surprisingly it looks a little better!\n\n![](/primers/ai/assets/learning-theory/7.jpg)\n\n*   Upon taking a peek at the training and test error, we see that the test error (briefly) decreases when p\\>np\\>np > n. However, that sounds counter intuitive since it is literally the opposite of what the bias-variance trade-off says should happen!\n\n![](/primers/ai/assets/learning-theory/8.jpg)\n\n*   The key point is with 20 DF, n=p, and there’s **exactly one** least squares fit that has zero training error. And that fit happens to have oodles of wiggles, but as we increase the DF so that p\\>np\\>np>n, there are tons of interpolating least squares fits.\n    \n*   The **minimum norm least squares** fit is the “least wiggly” of those zillions of fits. Also, note that the “least wiggly” among them is even less wiggly than the fit when p\\=np\\=np=n.\n    \n*   So, “double descent” is happening because DF isn’t really the right quantity for the the X-axis: like, the fact that we are choosing the minimum norm least squares fit actually means that the spline with 36 DF is **less** flexible than the spline with 20 DF.\n    \n*   Now, what if had used a ridge penalty when fitting the spline (instead of least squares)? In that case, we wouldn’t have interpolated training set, we wouldn’t have seen double descent, and we would have gotten better test error (for the right value of the tuning parameter!).\n    \n*   How does this relate to deep learning? When we use (stochastic) gradient descent to fit a neural net, we are actually picking out the minimum norm solution!! So the spline example is a pretty good analogy for what is happening when we see double descent for neural nets.\n    \n*   **Key takeaways**\n    \n    *   Double descent is observed and is understandable through the lens of stat ML and the bias/variance trade-off.\n    *   Actually, the bias/variance trade-off helps us understand **why** DD is happening!\n\nThe key point is with 20 DF, n=p, and there’s **exactly one** least squares fit that has zero training error. And that fit happens to have oodles of wiggles, but as we increase the DF so that p\\>np\\>np>n, there are tons of interpolating least squares fits.\n\nThe **minimum norm least squares** fit is the “least wiggly” of those zillions of fits. Also, note that the “least wiggly” among them is even less wiggly than the fit when p\\=np\\=np=n.\n\nSo, “double descent” is happening because DF isn’t really the right quantity for the the X-axis: like, the fact that we are choosing the minimum norm least squares fit actually means that the spline with 36 DF is **less** flexible than the spline with 20 DF.\n\nNow, what if had used a ridge penalty when fitting the spline (instead of least squares)? In that case, we wouldn’t have interpolated training set, we wouldn’t have seen double descent, and we would have gotten better test error (for the right value of the tuning parameter!).\n\nHow does this relate to deep learning? When we use (stochastic) gradient descent to fit a neural net, we are actually picking out the minimum norm solution!! So the spline example is a pretty good analogy for what is happening when we see double descent for neural nets.\n\n**Key takeaways**\n\n*   Double descent is observed and is understandable through the lens of stat ML and the bias/variance trade-off.\n*   Actually, the bias/variance trade-off helps us understand **why** DD is happening!",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 5,
    "tags": [
      "datatraining",
      "deep learning",
      "gradient descent"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 917,
      "contentLength": 40161
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/double-descent/#example:-natural-cubic-splines",
    "scrapedAt": "2025-12-28T11:49:32.078Z"
  }
]