[
  {
    "id": "ai-hallucination-retrieval-augmented-generation-rag-1",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Prompting-Based Techniques",
    "title": "Retrieval-Augmented Generation (RAG)",
    "subtitle": "Prompting-Based Techniques",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Lewis et al. in <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> and Shuster et al. in <a href=\"https://arxiv.org/abs/2104.07567\">Retrieval Augmentation Reduces Hallucination in Conversation</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Mitigation</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>Retrieval-Augmented Generation (RAG) is a technique designed to enhance the factual accuracy of Large Language Models (LLMs) by integrating external knowledge sources into the generation process. Traditional LLMs generate responses based solely on their pre-trained knowledge, which can lead to hallucinations—instances where the model produces plausible but incorrect or fabricated information. RAG addresses this limitation by retrieving relevant documents from an external corpus and conditioning the generation process on this retrieved information.</li>\n      <li>\n        <p>Retrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.</p>\n      </li>\n      <li>The RAG architecture comprises two main components:\n        <ol>\n          <li><strong>Retriever</strong>: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.</li>\n          <li><strong>Generator</strong>: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n            <ul>\n              <li>By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.</li>\n            </ul>\n          </li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation Details</strong>:</p>\n\n    <ul>\n      <li>\n        <p><strong>Indexing</strong>: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.</p>\n      </li>\n      <li>\n        <p><strong>Retrieval</strong>: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.</p>\n      </li>\n      <li>\n        <p><strong>Generation</strong>: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>The integration of RAG has demonstrated significant improvements in reducing hallucinations across various applications:\n        <ul>\n          <li><strong>Enhanced Accuracy</strong>: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.</li>\n          <li><strong>Domain Adaptability</strong>: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.</li>\n          <li><strong>Transparency</strong>: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Challenges and Considerations</strong>:</p>\n\n    <ul>\n      <li>\n        <p>While RAG offers substantial benefits, it also introduces certain challenges:</p>\n\n        <ul>\n          <li>\n            <p><strong>Retrieval Quality</strong>: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.</p>\n          </li>\n          <li>\n            <p><strong>Computational Overhead</strong>: The retrieval process adds computational complexity and latency to the response generation pipeline.</p>\n          </li>\n          <li>\n            <p><strong>Integration Complexity</strong>: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Lewis et al. in <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> and Shuster et al. in <a href=\"https://arxiv.org/abs/2104.07567\">Retrieval Augmentation Reduces Hallucination in Conversation</a></p>\n<p><strong>Classification</strong>: Mitigation</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>Retrieval-Augmented Generation (RAG) is a technique designed to enhance the factual accuracy of Large Language Models (LLMs) by integrating external knowledge sources into the generation process. Traditional LLMs generate responses based solely on their pre-trained knowledge, which can lead to hallucinations—instances where the model produces plausible but incorrect or fabricated information. RAG addresses this limitation by retrieving relevant documents from an external corpus and conditioning the generation process on this retrieved information.</li>\n      <li>\n        <p>Retrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.</p>\n      </li>\n      <li>The RAG architecture comprises two main components:\n        <ol>\n          <li><strong>Retriever</strong>: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.</li>\n          <li><strong>Generator</strong>: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n            <ul>\n              <li>By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.</li>\n            </ul>\n          </li>\n        </ol>\n      </li>\n    </ul>\n<p>Retrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.</p>\n<ol>\n          <li><strong>Retriever</strong>: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.</li>\n          <li><strong>Generator</strong>: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n            <ul>\n              <li>By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.</li>\n            </ul>\n          </li>\n        </ol>\n<ul>\n              <li>By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.</li>\n            </ul>\n<p><strong>Implementation Details</strong>:</p>\n<ul>\n      <li>\n        <p><strong>Indexing</strong>: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.</p>\n      </li>\n      <li>\n        <p><strong>Retrieval</strong>: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.</p>\n      </li>\n      <li>\n        <p><strong>Generation</strong>: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.</p>\n      </li>\n    </ul>\n<p><strong>Indexing</strong>: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.</p>\n<p><strong>Retrieval</strong>: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.</p>\n<p><strong>Generation</strong>: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>The integration of RAG has demonstrated significant improvements in reducing hallucinations across various applications:\n        <ul>\n          <li><strong>Enhanced Accuracy</strong>: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.</li>\n          <li><strong>Domain Adaptability</strong>: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.</li>\n          <li><strong>Transparency</strong>: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Enhanced Accuracy</strong>: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.</li>\n          <li><strong>Domain Adaptability</strong>: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.</li>\n          <li><strong>Transparency</strong>: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.</li>\n        </ul>\n<p><strong>Challenges and Considerations</strong>:</p>\n<ul>\n      <li>\n        <p>While RAG offers substantial benefits, it also introduces certain challenges:</p>\n\n        <ul>\n          <li>\n            <p><strong>Retrieval Quality</strong>: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.</p>\n          </li>\n          <li>\n            <p><strong>Computational Overhead</strong>: The retrieval process adds computational complexity and latency to the response generation pipeline.</p>\n          </li>\n          <li>\n            <p><strong>Integration Complexity</strong>: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>While RAG offers substantial benefits, it also introduces certain challenges:</p>\n<ul>\n          <li>\n            <p><strong>Retrieval Quality</strong>: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.</p>\n          </li>\n          <li>\n            <p><strong>Computational Overhead</strong>: The retrieval process adds computational complexity and latency to the response generation pipeline.</p>\n          </li>\n          <li>\n            <p><strong>Integration Complexity</strong>: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.</p>\n          </li>\n        </ul>\n<p><strong>Retrieval Quality</strong>: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.</p>\n<p><strong>Computational Overhead</strong>: The retrieval process adds computational complexity and latency to the response generation pipeline.</p>\n<p><strong>Integration Complexity</strong>: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.</p>",
    "contentMarkdown": "*   **Proposed by**: Lewis et al. in [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) and Shuster et al. in [Retrieval Augmentation Reduces Hallucination in Conversation](https://arxiv.org/abs/2104.07567)\n    \n*   **Classification**: Mitigation\n    \n*   **Description**:\n    \n    *   Retrieval-Augmented Generation (RAG) is a technique designed to enhance the factual accuracy of Large Language Models (LLMs) by integrating external knowledge sources into the generation process. Traditional LLMs generate responses based solely on their pre-trained knowledge, which can lead to hallucinations—instances where the model produces plausible but incorrect or fabricated information. RAG addresses this limitation by retrieving relevant documents from an external corpus and conditioning the generation process on this retrieved information.\n    *   Retrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.\n        \n    *   The RAG architecture comprises two main components:\n        1.  **Retriever**: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.\n        2.  **Generator**: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n            *   By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.\n*   **Implementation Details**:\n    \n    *   **Indexing**: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.\n        \n    *   **Retrieval**: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.\n        \n    *   **Generation**: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.\n        \n*   **Impact**:\n    \n    *   The integration of RAG has demonstrated significant improvements in reducing hallucinations across various applications:\n        *   **Enhanced Accuracy**: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.\n        *   **Domain Adaptability**: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.\n        *   **Transparency**: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.\n*   **Challenges and Considerations**:\n    \n    *   While RAG offers substantial benefits, it also introduces certain challenges:\n        \n        *   **Retrieval Quality**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.\n            \n        *   **Computational Overhead**: The retrieval process adds computational complexity and latency to the response generation pipeline.\n            \n        *   **Integration Complexity**: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.\n            \n\n**Proposed by**: Lewis et al. in [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) and Shuster et al. in [Retrieval Augmentation Reduces Hallucination in Conversation](https://arxiv.org/abs/2104.07567)\n\n**Classification**: Mitigation\n\n**Description**:\n\n*   Retrieval-Augmented Generation (RAG) is a technique designed to enhance the factual accuracy of Large Language Models (LLMs) by integrating external knowledge sources into the generation process. Traditional LLMs generate responses based solely on their pre-trained knowledge, which can lead to hallucinations—instances where the model produces plausible but incorrect or fabricated information. RAG addresses this limitation by retrieving relevant documents from an external corpus and conditioning the generation process on this retrieved information.\n*   Retrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.\n    \n*   The RAG architecture comprises two main components:\n    1.  **Retriever**: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.\n    2.  **Generator**: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n        *   By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.\n\nRetrieval-Augmented Generation represents a significant advancement in mitigating hallucinations in LLMs by grounding their outputs in external, verifiable information. By combining retrieval mechanisms with generative models, RAG enhances the factual accuracy, adaptability, and transparency of AI systems, making them more reliable for real-world applications.\n\n1.  **Retriever**: Given a user query, the retriever searches an external knowledge base to find documents relevant to the query. This is typically achieved using dense vector representations and similarity search techniques.\n2.  **Generator**: The generator, often a sequence-to-sequence model like BART, takes the retrieved documents and the original query as input to produce a response that is grounded in the retrieved information.\n    *   By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.\n\n*   By incorporating up-to-date and context-specific information at inference time, RAG enables LLMs to generate more accurate and relevant responses without the need for retraining on new data.\n\n**Implementation Details**:\n\n*   **Indexing**: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.\n    \n*   **Retrieval**: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.\n    \n*   **Generation**: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.\n    \n\n**Indexing**: The external knowledge base is preprocessed, and documents are converted into vector embeddings using models like DPR (Dense Passage Retrieval). These embeddings are stored in a vector database for efficient retrieval.\n\n**Retrieval**: At inference time, the retriever computes the similarity between the query and the document embeddings to fetch the top-k relevant documents.\n\n**Generation**: The generator receives the concatenated retrieved documents and the original query to produce a response. This process ensures that the generated output is informed by the most relevant external information.\n\n**Impact**:\n\n*   The integration of RAG has demonstrated significant improvements in reducing hallucinations across various applications:\n    *   **Enhanced Accuracy**: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.\n    *   **Domain Adaptability**: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.\n    *   **Transparency**: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.\n\n*   **Enhanced Accuracy**: By grounding responses in retrieved documents, RAG reduces the likelihood of generating incorrect information.\n*   **Domain Adaptability**: RAG allows LLMs to access domain-specific knowledge bases, making them more adaptable to specialized fields without extensive retraining.\n*   **Transparency**: Since responses are based on retrieved documents, it’s possible to trace back the information to its source, enhancing the transparency and trustworthiness of the model’s outputs.\n\n**Challenges and Considerations**:\n\n*   While RAG offers substantial benefits, it also introduces certain challenges:\n    \n    *   **Retrieval Quality**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.\n        \n    *   **Computational Overhead**: The retrieval process adds computational complexity and latency to the response generation pipeline.\n        \n    *   **Integration Complexity**: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.\n        \n\nWhile RAG offers substantial benefits, it also introduces certain challenges:\n\n*   **Retrieval Quality**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.\n    \n*   **Computational Overhead**: The retrieval process adds computational complexity and latency to the response generation pipeline.\n    \n*   **Integration Complexity**: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.\n    \n\n**Retrieval Quality**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inadequate or misleading responses.\n\n**Computational Overhead**: The retrieval process adds computational complexity and latency to the response generation pipeline.\n\n**Integration Complexity**: Implementing RAG requires maintaining an up-to-date and comprehensive external knowledge base, which can be resource-intensive.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 8,
    "tags": [
      "nlpllms",
      "embedding",
      "llm",
      "nlp"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1429,
      "contentLength": 13562
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#retrieval-augmented-generation-(rag)",
    "scrapedAt": "2025-12-28T11:54:05.683Z"
  },
  {
    "id": "ai-hallucination-chain-of-verification-cove-2",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Prompting-Based Techniques",
    "title": "Chain-of-Verification (CoVe)",
    "subtitle": "Prompting-Based Techniques",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Dhuliawala et al. <a href=\"https://arxiv.org/abs/2309.11495\">Chain-of-Verification Reduces Hallucination in Large Language Models</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>The Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.</p>\n      </li>\n      <li>\n        <p><strong>CoVe Process:</strong></p>\n\n        <ol>\n          <li>\n            <p><strong>Baseline Response Generation</strong>: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Verification Planning</strong>: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.</p>\n          </li>\n          <li>\n            <p><strong>Independent Verification Execution</strong>: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.</p>\n          </li>\n          <li>\n            <p><strong>Final Response Generation</strong>: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This methodology is illustrated in the following diagram from the original paper:</p>\n\n        <p><img src=\"../../../images/papers/CoVe.jpg\" alt=\"\"></p>\n      </li>\n      <li>\n        <p><strong>CoVe Variants:</strong></p>\n      </li>\n      <li>\n        <p>The authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:</p>\n\n        <ul>\n          <li>\n            <p><strong>Joint CoVe</strong>: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Two-Step CoVe</strong>: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.</p>\n          </li>\n          <li>\n            <p><strong>Factored CoVe</strong>: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Factor+Revise</strong>: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Empirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.</p>\n      </li>\n      <li>\n        <p>Furthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.</p>\n      </li>\n      <li>\n        <p>In summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Dhuliawala et al. <a href=\"https://arxiv.org/abs/2309.11495\">Chain-of-Verification Reduces Hallucination in Large Language Models</a></p>\n<p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>The Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.</p>\n      </li>\n      <li>\n        <p><strong>CoVe Process:</strong></p>\n\n        <ol>\n          <li>\n            <p><strong>Baseline Response Generation</strong>: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Verification Planning</strong>: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.</p>\n          </li>\n          <li>\n            <p><strong>Independent Verification Execution</strong>: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.</p>\n          </li>\n          <li>\n            <p><strong>Final Response Generation</strong>: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This methodology is illustrated in the following diagram from the original paper:</p>\n\n        <p><img src=\"../../../images/papers/CoVe.jpg\" alt=\"\"></p>\n      </li>\n      <li>\n        <p><strong>CoVe Variants:</strong></p>\n      </li>\n      <li>\n        <p>The authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:</p>\n\n        <ul>\n          <li>\n            <p><strong>Joint CoVe</strong>: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Two-Step CoVe</strong>: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.</p>\n          </li>\n          <li>\n            <p><strong>Factored CoVe</strong>: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Factor+Revise</strong>: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>The Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.</p>\n<p><strong>CoVe Process:</strong></p>\n<ol>\n          <li>\n            <p><strong>Baseline Response Generation</strong>: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Verification Planning</strong>: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.</p>\n          </li>\n          <li>\n            <p><strong>Independent Verification Execution</strong>: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.</p>\n          </li>\n          <li>\n            <p><strong>Final Response Generation</strong>: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.</p>\n          </li>\n        </ol>\n<p><strong>Baseline Response Generation</strong>: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.</p>\n<p><strong>Verification Planning</strong>: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.</p>\n<p><strong>Independent Verification Execution</strong>: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.</p>\n<p><strong>Final Response Generation</strong>: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.</p>\n<p>This methodology is illustrated in the following diagram from the original paper:</p>\n<p><img src=\"../../../images/papers/CoVe.jpg\" alt=\"\"></p>\n<p><strong>CoVe Variants:</strong></p>\n<p>The authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:</p>\n<ul>\n          <li>\n            <p><strong>Joint CoVe</strong>: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Two-Step CoVe</strong>: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.</p>\n          </li>\n          <li>\n            <p><strong>Factored CoVe</strong>: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.</p>\n          </li>\n          <li>\n            <p><strong>Factor+Revise</strong>: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.</p>\n          </li>\n        </ul>\n<p><strong>Joint CoVe</strong>: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.</p>\n<p><strong>Two-Step CoVe</strong>: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.</p>\n<p><strong>Factored CoVe</strong>: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.</p>\n<p><strong>Factor+Revise</strong>: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>\n        <p>Empirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.</p>\n      </li>\n      <li>\n        <p>Furthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.</p>\n      </li>\n      <li>\n        <p>In summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.</p>\n      </li>\n    </ul>\n<p>Empirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.</p>\n<p>Furthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.</p>\n<p>In summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.</p>",
    "contentMarkdown": "*   **Proposed by**: Dhuliawala et al. [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495)\n    \n*   **Classification**: Detection & Mitigation\n    \n*   **Description**:\n    \n    *   The Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.\n        \n    *   **CoVe Process:**\n        \n        1.  **Baseline Response Generation**: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.\n            \n        2.  **Verification Planning**: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.\n            \n        3.  **Independent Verification Execution**: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.\n            \n        4.  **Final Response Generation**: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.\n            \n    *   This methodology is illustrated in the following diagram from the original paper:\n        \n        ![](../../../images/papers/CoVe.jpg)\n        \n    *   **CoVe Variants:**\n        \n    *   The authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:\n        \n        *   **Joint CoVe**: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.\n            \n        *   **Two-Step CoVe**: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.\n            \n        *   **Factored CoVe**: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.\n            \n        *   **Factor+Revise**: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.\n            \n*   **Impact**:\n    \n    *   Empirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.\n        \n    *   Furthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.\n        \n    *   In summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.\n        \n\n**Proposed by**: Dhuliawala et al. [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495)\n\n**Classification**: Detection & Mitigation\n\n**Description**:\n\n*   The Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.\n    \n*   **CoVe Process:**\n    \n    1.  **Baseline Response Generation**: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.\n        \n    2.  **Verification Planning**: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.\n        \n    3.  **Independent Verification Execution**: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.\n        \n    4.  **Final Response Generation**: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.\n        \n*   This methodology is illustrated in the following diagram from the original paper:\n    \n    ![](../../../images/papers/CoVe.jpg)\n    \n*   **CoVe Variants:**\n    \n*   The authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:\n    \n    *   **Joint CoVe**: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.\n        \n    *   **Two-Step CoVe**: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.\n        \n    *   **Factored CoVe**: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.\n        \n    *   **Factor+Revise**: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.\n        \n\nThe Chain-of-Verification (CoVe) method addresses the issue of hallucinations—plausible yet incorrect information—in LLM outputs. CoVe introduces a structured, multi-step process that encourages the model to self-verify its responses, thereby enhancing factual accuracy.\n\n**CoVe Process:**\n\n1.  **Baseline Response Generation**: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.\n    \n2.  **Verification Planning**: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.\n    \n3.  **Independent Verification Execution**: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.\n    \n4.  **Final Response Generation**: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.\n    \n\n**Baseline Response Generation**: The LLM generates an initial response to a given query. This response may contain inaccuracies or hallucinations.\n\n**Verification Planning**: Based on the initial response, the LLM formulates a set of verification questions aimed at fact-checking specific claims made in the baseline response.\n\n**Independent Verification Execution**: Each verification question is answered independently by the LLM, without access to the initial response, to prevent bias and ensure objectivity.\n\n**Final Response Generation**: The LLM synthesizes the answers to the verification questions to produce a revised, fact-checked final response.\n\nThis methodology is illustrated in the following diagram from the original paper:\n\n![](../../../images/papers/CoVe.jpg)\n\n**CoVe Variants:**\n\nThe authors explore several variants of the CoVe method to assess the impact of different attention mechanisms and prompt structures:\n\n*   **Joint CoVe**: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.\n    \n*   **Two-Step CoVe**: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.\n    \n*   **Factored CoVe**: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.\n    \n*   **Factor+Revise**: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.\n    \n\n**Joint CoVe**: Combines the planning and answering of verification questions in a single prompt. While efficient, this approach allows the model to attend to the initial (possibly incorrect) response, increasing the risk of repeating hallucinations.\n\n**Two-Step CoVe**: Separates the planning and answering stages into distinct prompts. This reduces the model’s exposure to the initial response during verification but still allows attention across all question-answer pairs, which may lead to some cross-contamination.\n\n**Factored CoVe**: Each verification question is answered in complete isolation, with no access to the initial response or other verification answers. This strict separation minimizes the risk of propagating errors and has been shown to be the most effective variant in reducing hallucinations.\n\n**Factor+Revise**: Builds upon Factored CoVe by introducing an additional revision step. The model cross-checks the original response against the independently obtained verification answers to identify and correct inconsistencies.\n\n**Impact**:\n\n*   Empirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.\n    \n*   Furthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.\n    \n*   In summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.\n    \n\nEmpirical evaluations demonstrate that CoVe significantly reduces hallucination rates across various tasks, including list-based questions from Wikidata, closed-book MultiSpanQA, and long-form text generation. Notably, the Factored CoVe variant achieves the highest performance gains, highlighting the importance of independent verification in mitigating false information.\n\nFurthermore, the study finds that verification questions are often answered more accurately than the initial claims in the baseline response, underscoring the effectiveness of prompting LLMs to critically assess their outputs.\n\nIn summary, CoVe enhances the factual reliability of LLM-generated content by incorporating a self-verification mechanism that systematically identifies and corrects potential inaccuracies.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 8,
    "tags": [
      "nlpllms",
      "attention",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1495,
      "contentLength": 14279
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#chain-of-verification-(cove)",
    "scrapedAt": "2025-12-28T11:54:05.683Z"
  },
  {
    "id": "ai-hallucination-active-detection-and-mitigation-via-low-confidence-3",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Model",
    "title": "Active Detection and Mitigation Via Low-Confidence Validation",
    "subtitle": "Model",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Varshney et al. in <a href=\"https://arxiv.org/abs/2307.03987\">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Actively Validating Low-Confidence Generation</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>: This approach actively detects and mitigates hallucinations during the generation process by:</p>\n\n    <ol>\n      <li>\n        <p><strong>Detection</strong>: Identifying low-confidence elements in the generated text based on model uncertainty metrics.</p>\n      </li>\n      <li>\n        <p><strong>Validation</strong>: Generating specific validation questions targeting these low-confidence elements.</p>\n      </li>\n      <li>\n        <p><strong>Mitigation</strong>: Using the answers to the validation questions to correct any detected hallucinations in the text.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Impact</strong>: The method achieves a detection recall of approximately 88% and successfully mitigates 57.6% of the correctly detected hallucinations. Notably, it does not introduce new hallucinations even in cases of false positives.</p>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Varshney et al. in <a href=\"https://arxiv.org/abs/2307.03987\">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Actively Validating Low-Confidence Generation</a></p>\n<p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n<p><strong>Description</strong>: This approach actively detects and mitigates hallucinations during the generation process by:</p>\n<ol>\n      <li>\n        <p><strong>Detection</strong>: Identifying low-confidence elements in the generated text based on model uncertainty metrics.</p>\n      </li>\n      <li>\n        <p><strong>Validation</strong>: Generating specific validation questions targeting these low-confidence elements.</p>\n      </li>\n      <li>\n        <p><strong>Mitigation</strong>: Using the answers to the validation questions to correct any detected hallucinations in the text.</p>\n      </li>\n    </ol>\n<p><strong>Detection</strong>: Identifying low-confidence elements in the generated text based on model uncertainty metrics.</p>\n<p><strong>Validation</strong>: Generating specific validation questions targeting these low-confidence elements.</p>\n<p><strong>Mitigation</strong>: Using the answers to the validation questions to correct any detected hallucinations in the text.</p>\n<p><strong>Impact</strong>: The method achieves a detection recall of approximately 88% and successfully mitigates 57.6% of the correctly detected hallucinations. Notably, it does not introduce new hallucinations even in cases of false positives.</p>",
    "contentMarkdown": "*   **Proposed by**: Varshney et al. in [A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Actively Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)\n    \n*   **Classification**: Detection & Mitigation\n    \n*   **Description**: This approach actively detects and mitigates hallucinations during the generation process by:\n    \n    1.  **Detection**: Identifying low-confidence elements in the generated text based on model uncertainty metrics.\n        \n    2.  **Validation**: Generating specific validation questions targeting these low-confidence elements.\n        \n    3.  **Mitigation**: Using the answers to the validation questions to correct any detected hallucinations in the text.\n        \n*   **Impact**: The method achieves a detection recall of approximately 88% and successfully mitigates 57.6% of the correctly detected hallucinations. Notably, it does not introduce new hallucinations even in cases of false positives.\n    \n\n**Proposed by**: Varshney et al. in [A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Actively Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)\n\n**Classification**: Detection & Mitigation\n\n**Description**: This approach actively detects and mitigates hallucinations during the generation process by:\n\n1.  **Detection**: Identifying low-confidence elements in the generated text based on model uncertainty metrics.\n    \n2.  **Validation**: Generating specific validation questions targeting these low-confidence elements.\n    \n3.  **Mitigation**: Using the answers to the validation questions to correct any detected hallucinations in the text.\n    \n\n**Detection**: Identifying low-confidence elements in the generated text based on model uncertainty metrics.\n\n**Validation**: Generating specific validation questions targeting these low-confidence elements.\n\n**Mitigation**: Using the answers to the validation questions to correct any detected hallucinations in the text.\n\n**Impact**: The method achieves a detection recall of approximately 88% and successfully mitigates 57.6% of the correctly detected hallucinations. Notably, it does not introduce new hallucinations even in cases of false positives.",
    "order": 3,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 268,
      "contentLength": 2806
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#active-detection-and-mitigation-via-low-confidence-validation",
    "scrapedAt": "2025-12-28T11:54:05.683Z"
  },
  {
    "id": "ai-hallucination-decoding-by-contrasting-layers-dola-4",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation Techniques",
    "title": "Decoding by Contrasting Layers (DoLa)",
    "subtitle": "Post-Response Generation Techniques",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Chuang et al. in <a href=\"https://arxiv.org/abs/2309.03883\">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Mitigation</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>DoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.</p>\n      </li>\n      <li>\n        <p><strong>Key Components:</strong></p>\n\n        <ol>\n          <li>\n            <p><strong>Dynamic Premature Layer Selection</strong>: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.</p>\n          </li>\n          <li>\n            <p><strong>Contrastive Decoding</strong>: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.</p>\n          </li>\n          <li>\n            <p><strong>Adaptive Mechanism</strong>: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p><strong>Illustrative Example:</strong></p>\n\n        <ul>\n          <li>In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Empirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:</p>\n\n        <ul>\n          <li>\n            <p><strong>TruthfulQA</strong>: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.</p>\n          </li>\n          <li>\n            <p><strong>Open-Ended Generation</strong>: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.</p>\n          </li>\n          <li>\n            <p><strong>Efficiency</strong>: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>DoLa’s innovative approach of dynamically contrasting transformer layers during decoding offers a lightweight yet effective mechanism to mitigate hallucinations in LLMs, enhancing their reliability in generating factual content.</p>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Chuang et al. in <a href=\"https://arxiv.org/abs/2309.03883\">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a></p>\n<p><strong>Classification</strong>: Mitigation</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>DoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.</p>\n      </li>\n      <li>\n        <p><strong>Key Components:</strong></p>\n\n        <ol>\n          <li>\n            <p><strong>Dynamic Premature Layer Selection</strong>: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.</p>\n          </li>\n          <li>\n            <p><strong>Contrastive Decoding</strong>: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.</p>\n          </li>\n          <li>\n            <p><strong>Adaptive Mechanism</strong>: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p><strong>Illustrative Example:</strong></p>\n\n        <ul>\n          <li>In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.</li>\n        </ul>\n      </li>\n    </ul>\n<p>DoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.</p>\n<p><strong>Key Components:</strong></p>\n<ol>\n          <li>\n            <p><strong>Dynamic Premature Layer Selection</strong>: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.</p>\n          </li>\n          <li>\n            <p><strong>Contrastive Decoding</strong>: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.</p>\n          </li>\n          <li>\n            <p><strong>Adaptive Mechanism</strong>: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.</p>\n          </li>\n        </ol>\n<p><strong>Dynamic Premature Layer Selection</strong>: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.</p>\n<p><strong>Contrastive Decoding</strong>: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.</p>\n<p><strong>Adaptive Mechanism</strong>: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.</p>\n<p><strong>Illustrative Example:</strong></p>\n<ul>\n          <li>In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.</li>\n        </ul>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>\n        <p>Empirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:</p>\n\n        <ul>\n          <li>\n            <p><strong>TruthfulQA</strong>: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.</p>\n          </li>\n          <li>\n            <p><strong>Open-Ended Generation</strong>: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.</p>\n          </li>\n          <li>\n            <p><strong>Efficiency</strong>: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Empirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:</p>\n<ul>\n          <li>\n            <p><strong>TruthfulQA</strong>: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.</p>\n          </li>\n          <li>\n            <p><strong>Open-Ended Generation</strong>: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.</p>\n          </li>\n          <li>\n            <p><strong>Efficiency</strong>: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.</p>\n          </li>\n        </ul>\n<p><strong>TruthfulQA</strong>: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.</p>\n<p><strong>Open-Ended Generation</strong>: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.</p>\n<p><strong>Efficiency</strong>: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.</p>\n<p>DoLa’s innovative approach of dynamically contrasting transformer layers during decoding offers a lightweight yet effective mechanism to mitigate hallucinations in LLMs, enhancing their reliability in generating factual content.</p>",
    "contentMarkdown": "*   **Proposed by**: Chuang et al. in [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883)\n    \n*   **Classification**: Mitigation\n    \n*   **Description**:\n    \n    *   DoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.\n        \n    *   **Key Components:**\n        \n        1.  **Dynamic Premature Layer Selection**: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.\n            \n        2.  **Contrastive Decoding**: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.\n            \n        3.  **Adaptive Mechanism**: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.\n            \n    *   **Illustrative Example:**\n        \n        *   In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.\n*   **Impact**:\n    \n    *   Empirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:\n        \n        *   **TruthfulQA**: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.\n            \n        *   **Open-Ended Generation**: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.\n            \n        *   **Efficiency**: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.\n            \n*   DoLa’s innovative approach of dynamically contrasting transformer layers during decoding offers a lightweight yet effective mechanism to mitigate hallucinations in LLMs, enhancing their reliability in generating factual content.\n    \n\n**Proposed by**: Chuang et al. in [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883)\n\n**Classification**: Mitigation\n\n**Description**:\n\n*   DoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.\n    \n*   **Key Components:**\n    \n    1.  **Dynamic Premature Layer Selection**: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.\n        \n    2.  **Contrastive Decoding**: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.\n        \n    3.  **Adaptive Mechanism**: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.\n        \n*   **Illustrative Example:**\n    \n    *   In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.\n\nDoLa introduces a novel decoding strategy aimed at reducing hallucinations in large language models (LLMs) without requiring additional training or external retrieval mechanisms. The core idea is to leverage the observation that factual knowledge in LLMs is predominantly encoded in the later (mature) layers of the transformer architecture, while earlier (premature) layers capture more general linguistic patterns.\n\n**Key Components:**\n\n1.  **Dynamic Premature Layer Selection**: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.\n    \n2.  **Contrastive Decoding**: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.\n    \n3.  **Adaptive Mechanism**: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.\n    \n\n**Dynamic Premature Layer Selection**: For each token during generation, DoLa dynamically selects a premature layer by identifying which earlier layer’s output distribution is most divergent from the mature layer’s distribution. This divergence is quantified using the Jensen-Shannon Divergence (JSD), a symmetric measure of dissimilarity between two probability distributions.\n\n**Contrastive Decoding**: Once the appropriate premature layer is selected, DoLa computes the next token’s probability distribution by contrasting the logit outputs of the mature layer with those of the chosen premature layer. Specifically, it subtracts the log probabilities of the premature layer from those of the mature layer, effectively amplifying the factual content while suppressing generic linguistic patterns.\n\n**Adaptive Mechanism**: The dynamic selection allows DoLa to adapt to the complexity of the token being generated. For straightforward tokens with similar distributions across layers (low JSD), earlier layers might be selected. Conversely, for tokens requiring intricate real-world knowledge, layers closer to the mature layer are chosen to enhance factual accuracy.\n\n**Illustrative Example:**\n\n*   In a scenario where the model is prompted with “What is the capital of Washington State?”, earlier layers might assign higher probabilities to “Seattle” due to its prominence, while the mature layer correctly identifies “Olympia” as the capital. DoLa’s contrastive approach ensures that the factual answer “Olympia” is favored in the final output.\n\n**Impact**:\n\n*   Empirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:\n    \n    *   **TruthfulQA**: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.\n        \n    *   **Open-Ended Generation**: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.\n        \n    *   **Efficiency**: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.\n        \n\nEmpirical evaluations demonstrate that DoLa significantly enhances the factuality of LLM outputs across various tasks:\n\n*   **TruthfulQA**: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.\n    \n*   **Open-Ended Generation**: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.\n    \n*   **Efficiency**: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.\n    \n\n**TruthfulQA**: DoLa improved the performance of LLaMA family models by 12–17% absolute points, indicating a substantial reduction in hallucinated content.\n\n**Open-Ended Generation**: In tasks requiring open-ended responses, DoLa generated more informative and factually accurate outputs, receiving better ratings from evaluators, including GPT-4.\n\n**Efficiency**: Despite its benefits, DoLa introduces only a minimal computational overhead during inference, making it a practical solution for enhancing LLM reliability without significant performance trade-offs.\n\nDoLa’s innovative approach of dynamically contrasting transformer layers during decoding offers a lightweight yet effective mechanism to mitigate hallucinations in LLMs, enhancing their reliability in generating factual content.",
    "order": 4,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 8,
    "tags": [
      "nlpllms",
      "transformer",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1419,
      "contentLength": 12901
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#decoding-by-contrasting-layers-(dola)",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-selfcheckgpt-5",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation",
    "title": "SelfCheckGPT",
    "subtitle": "Post-Response Generation",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Manakul et al. in <a href=\"https://arxiv.org/abs/2303.08896\">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>SelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:</p>\n\n        <ol>\n          <li>\n            <p><strong>Sampling</strong>: Generating multiple stochastic responses for a given prompt using the same LLM.</p>\n          </li>\n          <li>\n            <p><strong>Consistency Checking</strong>: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>SelfCheckGPT effectively detects hallucinations at both sentence and passage levels. It outperforms several baselines in sentence-level hallucination detection and passage-level factuality assessment, providing a valuable tool for evaluating the reliability of LLM outputs.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Manakul et al. in <a href=\"https://arxiv.org/abs/2303.08896\">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a></p>\n<p><strong>Classification</strong>: Detection</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>SelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:</p>\n\n        <ol>\n          <li>\n            <p><strong>Sampling</strong>: Generating multiple stochastic responses for a given prompt using the same LLM.</p>\n          </li>\n          <li>\n            <p><strong>Consistency Checking</strong>: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.</p>\n      </li>\n    </ul>\n<p>SelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:</p>\n<ol>\n          <li>\n            <p><strong>Sampling</strong>: Generating multiple stochastic responses for a given prompt using the same LLM.</p>\n          </li>\n          <li>\n            <p><strong>Consistency Checking</strong>: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.</p>\n          </li>\n        </ol>\n<p><strong>Sampling</strong>: Generating multiple stochastic responses for a given prompt using the same LLM.</p>\n<p><strong>Consistency Checking</strong>: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.</p>\n<p>This approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>SelfCheckGPT effectively detects hallucinations at both sentence and passage levels. It outperforms several baselines in sentence-level hallucination detection and passage-level factuality assessment, providing a valuable tool for evaluating the reliability of LLM outputs.</li>\n    </ul>",
    "contentMarkdown": "*   **Proposed by**: Manakul et al. in [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)\n    \n*   **Classification**: Detection\n    \n*   **Description**:\n    \n    *   SelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:\n        \n        1.  **Sampling**: Generating multiple stochastic responses for a given prompt using the same LLM.\n            \n        2.  **Consistency Checking**: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.\n            \n    *   This approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.\n        \n*   **Impact**:\n    \n    *   SelfCheckGPT effectively detects hallucinations at both sentence and passage levels. It outperforms several baselines in sentence-level hallucination detection and passage-level factuality assessment, providing a valuable tool for evaluating the reliability of LLM outputs.\n\n**Proposed by**: Manakul et al. in [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)\n\n**Classification**: Detection\n\n**Description**:\n\n*   SelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:\n    \n    1.  **Sampling**: Generating multiple stochastic responses for a given prompt using the same LLM.\n        \n    2.  **Consistency Checking**: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.\n        \n*   This approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.\n    \n\nSelfCheckGPT is a sampling-based approach designed to detect hallucinations without requiring access to external databases or model internals. The methodology involves:\n\n1.  **Sampling**: Generating multiple stochastic responses for a given prompt using the same LLM.\n    \n2.  **Consistency Checking**: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.\n    \n\n**Sampling**: Generating multiple stochastic responses for a given prompt using the same LLM.\n\n**Consistency Checking**: Assessing the consistency across these samples. Consistent outputs suggest factual accuracy, while divergence indicates potential hallucinations.\n\nThis approach leverages the premise that if an LLM has accurate knowledge about a topic, its responses will be consistent across multiple generations. In contrast, hallucinated content will result in varied and contradictory outputs.\n\n**Impact**:\n\n*   SelfCheckGPT effectively detects hallucinations at both sentence and passage levels. It outperforms several baselines in sentence-level hallucination detection and passage-level factuality assessment, providing a valuable tool for evaluating the reliability of LLM outputs.",
    "order": 5,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 411,
      "contentLength": 4306
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#selfcheckgpt",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-factscore-6",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation",
    "title": "FActScore",
    "subtitle": "Post-Response Generation",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Min et al. in <a href=\"https://arxiv.org/abs/2305.14251\">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>FActScore offers a fine-grained evaluation of factual precision by:</p>\n\n        <ol>\n          <li>\n            <p><strong>Decomposition</strong>: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.</p>\n          </li>\n          <li>\n            <p><strong>Verification</strong>: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.</p>\n          </li>\n          <li>\n            <p><strong>Scoring</strong>: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>FActScore reveals that even advanced models like ChatGPT achieve only 58% factual precision, highlighting the prevalence of hallucinations in long-form text generation. The introduction of an automated model that estimates FActScore with less than a 2% error rate facilitates scalable evaluations, making it a practical tool for benchmarking LLMs.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Min et al. in <a href=\"https://arxiv.org/abs/2305.14251\">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></p>\n<p><strong>Classification</strong>: Detection</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>FActScore offers a fine-grained evaluation of factual precision by:</p>\n\n        <ol>\n          <li>\n            <p><strong>Decomposition</strong>: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.</p>\n          </li>\n          <li>\n            <p><strong>Verification</strong>: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.</p>\n          </li>\n          <li>\n            <p><strong>Scoring</strong>: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.</p>\n      </li>\n    </ul>\n<p>FActScore offers a fine-grained evaluation of factual precision by:</p>\n<ol>\n          <li>\n            <p><strong>Decomposition</strong>: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.</p>\n          </li>\n          <li>\n            <p><strong>Verification</strong>: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.</p>\n          </li>\n          <li>\n            <p><strong>Scoring</strong>: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.</p>\n          </li>\n        </ol>\n<p><strong>Decomposition</strong>: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.</p>\n<p><strong>Verification</strong>: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.</p>\n<p><strong>Scoring</strong>: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.</p>\n<p>This method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>FActScore reveals that even advanced models like ChatGPT achieve only 58% factual precision, highlighting the prevalence of hallucinations in long-form text generation. The introduction of an automated model that estimates FActScore with less than a 2% error rate facilitates scalable evaluations, making it a practical tool for benchmarking LLMs.</li>\n    </ul>",
    "contentMarkdown": "*   **Proposed by**: Min et al. in [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)\n    \n*   **Classification**: Detection\n    \n*   **Description**:\n    \n    *   FActScore offers a fine-grained evaluation of factual precision by:\n        \n        1.  **Decomposition**: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.\n            \n        2.  **Verification**: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.\n            \n        3.  **Scoring**: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.\n            \n    *   This method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.\n        \n*   **Impact**:\n    \n    *   FActScore reveals that even advanced models like ChatGPT achieve only 58% factual precision, highlighting the prevalence of hallucinations in long-form text generation. The introduction of an automated model that estimates FActScore with less than a 2% error rate facilitates scalable evaluations, making it a practical tool for benchmarking LLMs.\n\n**Proposed by**: Min et al. in [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)\n\n**Classification**: Detection\n\n**Description**:\n\n*   FActScore offers a fine-grained evaluation of factual precision by:\n    \n    1.  **Decomposition**: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.\n        \n    2.  **Verification**: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.\n        \n    3.  **Scoring**: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.\n        \n*   This method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.\n    \n\nFActScore offers a fine-grained evaluation of factual precision by:\n\n1.  **Decomposition**: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.\n    \n2.  **Verification**: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.\n    \n3.  **Scoring**: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.\n    \n\n**Decomposition**: Breaking down long-form text into a series of atomic facts, each representing a singular, verifiable statement.\n\n**Verification**: Assessing each atomic fact against a reliable knowledge source to determine its factual accuracy.\n\n**Scoring**: Computing the percentage of atomic facts supported by the knowledge source to derive the FActScore.\n\nThis method allows for a nuanced assessment of an LLM’s output, identifying specific areas where factual inaccuracies occur.\n\n**Impact**:\n\n*   FActScore reveals that even advanced models like ChatGPT achieve only 58% factual precision, highlighting the prevalence of hallucinations in long-form text generation. The introduction of an automated model that estimates FActScore with less than a 2% error rate facilitates scalable evaluations, making it a practical tool for benchmarking LLMs.",
    "order": 6,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 434,
      "contentLength": 4387
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#factscore",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-g-eval-7",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation",
    "title": "G-Eval",
    "subtitle": "Post-Response Generation",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Wang et al. in <a href=\"https://arxiv.org/abs/2303.16634\">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>G-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:</p>\n\n        <ol>\n          <li>\n            <p><strong>Task Introduction and Evaluation Criteria</strong>: Providing the LLM with the context of the task and the specific criteria for evaluation.</p>\n          </li>\n          <li>\n            <p><strong>Chain-of-Thought Generation</strong>: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.</p>\n          </li>\n          <li>\n            <p><strong>Form-Filling</strong>: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>G-Eval demonstrates a strong correlation with human judgments, outperforming traditional automatic metrics like BLEU and ROUGE. By providing a more human-aligned evaluation of NLG outputs, G-Eval serves as a valuable tool for assessing and improving the quality of LLM-generated text.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Wang et al. in <a href=\"https://arxiv.org/abs/2303.16634\">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</a></p>\n<p><strong>Classification</strong>: Detection</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>G-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:</p>\n\n        <ol>\n          <li>\n            <p><strong>Task Introduction and Evaluation Criteria</strong>: Providing the LLM with the context of the task and the specific criteria for evaluation.</p>\n          </li>\n          <li>\n            <p><strong>Chain-of-Thought Generation</strong>: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.</p>\n          </li>\n          <li>\n            <p><strong>Form-Filling</strong>: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.</p>\n      </li>\n    </ul>\n<p>G-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:</p>\n<ol>\n          <li>\n            <p><strong>Task Introduction and Evaluation Criteria</strong>: Providing the LLM with the context of the task and the specific criteria for evaluation.</p>\n          </li>\n          <li>\n            <p><strong>Chain-of-Thought Generation</strong>: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.</p>\n          </li>\n          <li>\n            <p><strong>Form-Filling</strong>: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.</p>\n          </li>\n        </ol>\n<p><strong>Task Introduction and Evaluation Criteria</strong>: Providing the LLM with the context of the task and the specific criteria for evaluation.</p>\n<p><strong>Chain-of-Thought Generation</strong>: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.</p>\n<p><strong>Form-Filling</strong>: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.</p>\n<p>This approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>G-Eval demonstrates a strong correlation with human judgments, outperforming traditional automatic metrics like BLEU and ROUGE. By providing a more human-aligned evaluation of NLG outputs, G-Eval serves as a valuable tool for assessing and improving the quality of LLM-generated text.</li>\n    </ul>",
    "contentMarkdown": "*   **Proposed by**: Wang et al. in [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)\n    \n*   **Classification**: Detection\n    \n*   **Description**:\n    \n    *   G-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:\n        \n        1.  **Task Introduction and Evaluation Criteria**: Providing the LLM with the context of the task and the specific criteria for evaluation.\n            \n        2.  **Chain-of-Thought Generation**: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.\n            \n        3.  **Form-Filling**: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.\n            \n    *   This approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.\n        \n*   **Impact**:\n    \n    *   G-Eval demonstrates a strong correlation with human judgments, outperforming traditional automatic metrics like BLEU and ROUGE. By providing a more human-aligned evaluation of NLG outputs, G-Eval serves as a valuable tool for assessing and improving the quality of LLM-generated text.\n\n**Proposed by**: Wang et al. in [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)\n\n**Classification**: Detection\n\n**Description**:\n\n*   G-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:\n    \n    1.  **Task Introduction and Evaluation Criteria**: Providing the LLM with the context of the task and the specific criteria for evaluation.\n        \n    2.  **Chain-of-Thought Generation**: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.\n        \n    3.  **Form-Filling**: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.\n        \n*   This approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.\n    \n\nG-Eval is a framework that employs large language models with chain-of-thought (CoT) prompting and a form-filling paradigm to assess the quality of NLG outputs. The evaluation process includes:\n\n1.  **Task Introduction and Evaluation Criteria**: Providing the LLM with the context of the task and the specific criteria for evaluation.\n    \n2.  **Chain-of-Thought Generation**: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.\n    \n3.  **Form-Filling**: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.\n    \n\n**Task Introduction and Evaluation Criteria**: Providing the LLM with the context of the task and the specific criteria for evaluation.\n\n**Chain-of-Thought Generation**: Prompting the LLM to generate a detailed reasoning process (CoT) that leads to the evaluation decision.\n\n**Form-Filling**: Using the CoT to fill out an evaluation form that scores various aspects such as coherence, factual consistency, relevance, and grammaticality.\n\nThis approach aligns the evaluation process more closely with human judgment by making the reasoning behind scores explicit.\n\n**Impact**:\n\n*   G-Eval demonstrates a strong correlation with human judgments, outperforming traditional automatic metrics like BLEU and ROUGE. By providing a more human-aligned evaluation of NLG outputs, G-Eval serves as a valuable tool for assessing and improving the quality of LLM-generated text.",
    "order": 7,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 515,
      "contentLength": 4883
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#g-eval",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-mixalign-interactive-question-knowledge-alignment-8",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation",
    "title": "MixAlign: Interactive Question-Knowledge Alignment",
    "subtitle": "Post-Response Generation",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Proposed by</strong>: Zhang et al. in <a href=\"https://arxiv.org/abs/2305.13669\">Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment</a></p>\n  </li>\n  <li>\n    <p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n  </li>\n  <li>\n    <p><strong>Description</strong>:</p>\n\n    <ul>\n      <li>\n        <p>MixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:</p>\n\n        <ol>\n          <li>\n            <p><strong>Model-Based Question-Knowledge Alignment</strong>: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.</p>\n          </li>\n          <li>\n            <p><strong>Human-Assisted Question-Knowledge Alignment</strong>: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.</p>\n      </li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/hallucination/mixalign.png\" alt=\"MixAlign Framework Diagram\"></p>\n  </li>\n  <li>\n    <p><strong>Impact</strong>:</p>\n\n    <ul>\n      <li>Empirical evaluations demonstrate that MixAlign significantly enhances the alignment between user queries and knowledge bases, leading to improved accuracy in LLM responses. Specifically, the framework achieved up to a 22.2% improvement in model performance and a 27.1% reduction in hallucinations compared to baseline methods. These results underscore the efficacy of incorporating human-in-the-loop strategies to address the limitations of automated systems in handling ambiguous or misaligned queries.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Proposed by</strong>: Zhang et al. in <a href=\"https://arxiv.org/abs/2305.13669\">Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment</a></p>\n<p><strong>Classification</strong>: Detection &amp; Mitigation</p>\n<p><strong>Description</strong>:</p>\n<ul>\n      <li>\n        <p>MixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:</p>\n\n        <ol>\n          <li>\n            <p><strong>Model-Based Question-Knowledge Alignment</strong>: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.</p>\n          </li>\n          <li>\n            <p><strong>Human-Assisted Question-Knowledge Alignment</strong>: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.</p>\n          </li>\n        </ol>\n      </li>\n      <li>\n        <p>This interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.</p>\n      </li>\n    </ul>\n<p>MixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:</p>\n<ol>\n          <li>\n            <p><strong>Model-Based Question-Knowledge Alignment</strong>: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.</p>\n          </li>\n          <li>\n            <p><strong>Human-Assisted Question-Knowledge Alignment</strong>: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.</p>\n          </li>\n        </ol>\n<p><strong>Model-Based Question-Knowledge Alignment</strong>: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.</p>\n<p><strong>Human-Assisted Question-Knowledge Alignment</strong>: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.</p>\n<p>This interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.</p>\n<p><img src=\"/primers/ai/assets/hallucination/mixalign.png\" alt=\"MixAlign Framework Diagram\"></p>\n<p><strong>Impact</strong>:</p>\n<ul>\n      <li>Empirical evaluations demonstrate that MixAlign significantly enhances the alignment between user queries and knowledge bases, leading to improved accuracy in LLM responses. Specifically, the framework achieved up to a 22.2% improvement in model performance and a 27.1% reduction in hallucinations compared to baseline methods. These results underscore the efficacy of incorporating human-in-the-loop strategies to address the limitations of automated systems in handling ambiguous or misaligned queries.</li>\n    </ul>",
    "contentMarkdown": "*   **Proposed by**: Zhang et al. in [Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)\n    \n*   **Classification**: Detection & Mitigation\n    \n*   **Description**:\n    \n    *   MixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:\n        \n        1.  **Model-Based Question-Knowledge Alignment**: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.\n            \n        2.  **Human-Assisted Question-Knowledge Alignment**: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.\n            \n    *   This interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.\n        \n    \n    ![MixAlign Framework Diagram](/primers/ai/assets/hallucination/mixalign.png)\n    \n*   **Impact**:\n    \n    *   Empirical evaluations demonstrate that MixAlign significantly enhances the alignment between user queries and knowledge bases, leading to improved accuracy in LLM responses. Specifically, the framework achieved up to a 22.2% improvement in model performance and a 27.1% reduction in hallucinations compared to baseline methods. These results underscore the efficacy of incorporating human-in-the-loop strategies to address the limitations of automated systems in handling ambiguous or misaligned queries.\n\n**Proposed by**: Zhang et al. in [Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)\n\n**Classification**: Detection & Mitigation\n\n**Description**:\n\n*   MixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:\n    \n    1.  **Model-Based Question-Knowledge Alignment**: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.\n        \n    2.  **Human-Assisted Question-Knowledge Alignment**: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.\n        \n*   This interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.\n    \n\nMixAlign addresses the challenge of hallucinations in large language models (LLMs) by focusing on the misalignment between user queries and the structure of external knowledge bases. Recognizing that users may pose questions that don’t directly correspond to the stored information, MixAlign introduces a two-pronged approach:\n\n1.  **Model-Based Question-Knowledge Alignment**: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.\n    \n2.  **Human-Assisted Question-Knowledge Alignment**: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.\n    \n\n**Model-Based Question-Knowledge Alignment**: The system first employs an LLM to automatically refine the user’s question, aligning it with the schema of the knowledge base. This involves identifying and substituting attribute values to match the knowledge base’s structure, facilitating accurate retrieval of relevant information.\n\n**Human-Assisted Question-Knowledge Alignment**: If ambiguities persist after the initial refinement—such as multiple candidate answers differing in specific attributes—the system engages the user through clarifying questions. For instance, if the ambiguity revolves around the “season” attribute, the system might ask, “Which season are you referring to?” The user’s response then guides the LLM to generate a more precise and contextually appropriate answer.\n\nThis interactive framework ensures that the LLM’s outputs are grounded in accurate and relevant information, effectively mitigating the risk of hallucinations.\n\n![MixAlign Framework Diagram](/primers/ai/assets/hallucination/mixalign.png)\n\n**Impact**:\n\n*   Empirical evaluations demonstrate that MixAlign significantly enhances the alignment between user queries and knowledge bases, leading to improved accuracy in LLM responses. Specifically, the framework achieved up to a 22.2% improvement in model performance and a 27.1% reduction in hallucinations compared to baseline methods. These results underscore the efficacy of incorporating human-in-the-loop strategies to address the limitations of automated systems in handling ambiguous or misaligned queries.",
    "order": 8,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 5,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 808,
      "contentLength": 7333
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#mixalign:-interactive-question-knowledge-alignment",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-human-in-the-loop-9",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Post-Response Generation",
    "title": "Human-in-the-loop",
    "subtitle": "Post-Response Generation",
    "contentHtml": "<ul>\n  <li>In general, employing human reviewers to validate the LLMs output can help mitigate the impact of hallucination and improve the overall quality and reliability of the generated text.</li>\n</ul>",
    "contentMarkdown": "*   In general, employing human reviewers to validate the LLMs output can help mitigate the impact of hallucination and improve the overall quality and reliability of the generated text.",
    "order": 9,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 29,
      "contentLength": 204
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#human-in-the-loop",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-factscore-fine-grained-atomic-evaluation-of-factua-10",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Related Papers",
    "title": "FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    "subtitle": "Related Papers",
    "contentHtml": "<ul>\n  <li>This paper by Min et al. from UW, University of Massachusetts Amherst, Allen Institute for AI, and Meta AI focuses on evaluating the factual accuracy of long-form text generated by large language models (LMs).</li>\n  <li>The paper introduces FACTSCORE, a novel evaluation method that measures the factual precision of text generated by LMs. It breaks down a generation into atomic facts and calculates the percentage of these facts supported by a reliable knowledge source. This method is particularly necessary since text generations often contain a mix of supported and unsupported information, making binary judgments of quality inadequate.</li>\n  <li>FACTSCORE addresses two key ideas: using atomic facts as units for evaluation and assessing factual precision based on a specific knowledge source. It defines an atomic fact as a short sentence containing a single piece of information. This approach allows for a more fine-grained evaluation of factual precision than previous methods. The paper uses people biographies as a basis for evaluation due to their objective nature and covers diverse nationalities, professions, and rarity levels.</li>\n  <li>The following image from the paper shows an overview of FACTSCORE, a fraction of atomic facts (pieces of information) supported by a given knowledge source. FACTSCORE allows a more fine-grained evaluation of factual precision, e.g., in the figure, the top model gets a score of 66.7% and the bottom model gets 10.0%, whereas prior work would assign 0.0 to both. FACTSCORE can either be based on human evaluation, or be automated, which allows evaluation of a large set of LMs with no human efforts.</li>\n</ul>\n<p><img src=\"../../../images/papers/FACTSCORE.jpg\" alt=\"\"></p>\n<ul>\n  <li>Their automated estimator of FACTSCORE first breaks a generation into a series of atomic facts and then validates each against the given knowledge source. They use LLAMA 7B trained on Super Natural Instructions  and ChatGPT as an LMEVAL, and Generalizable T5-based Retrievers for passage retrieval.</li>\n  <li>The paper evaluates three state-of-the-art commercial LMs: InstructGPT, ChatGPT, and PerplexityAI. These models struggle with factual precision errors, with FACTSCOREs of 42%, 58%, and 71%, respectively. The study highlights that the factual precision of these LMs significantly drops with the rarity of the entities discussed in the text.</li>\n  <li>To address the time-consuming and costly nature of human evaluation, the authors propose an automated model to estimate FACTSCORE. This model decomposes text into atomic facts and validates each against a knowledge source, closely approximating FACTSCORE with less than a 2% error rate. It allows the evaluation of a large set of new LMs without manual human effort.</li>\n  <li>The paper also showcases the application of this automated estimator by evaluating 12 recently released LMs, offering insights into their factual accuracy. This approach could have cost $65K if evaluated by humans, highlighting the cost-effectiveness of the automated method.</li>\n  <li>Finally, the paper suggests future work to enhance FACTSCORE, including considering other aspects of factuality such as recall (coverage of factual information), improving the estimator for better approximation of factual precision, and leveraging FACTSCORE to correct model generations.</li>\n  <li>Overall, FACTSCORE represents a significant advancement in evaluating the factual precision of text generated by LMs, providing a detailed and cost-effective method for assessing the accuracy of long-form text.</li>\n</ul>",
    "contentMarkdown": "*   This paper by Min et al. from UW, University of Massachusetts Amherst, Allen Institute for AI, and Meta AI focuses on evaluating the factual accuracy of long-form text generated by large language models (LMs).\n*   The paper introduces FACTSCORE, a novel evaluation method that measures the factual precision of text generated by LMs. It breaks down a generation into atomic facts and calculates the percentage of these facts supported by a reliable knowledge source. This method is particularly necessary since text generations often contain a mix of supported and unsupported information, making binary judgments of quality inadequate.\n*   FACTSCORE addresses two key ideas: using atomic facts as units for evaluation and assessing factual precision based on a specific knowledge source. It defines an atomic fact as a short sentence containing a single piece of information. This approach allows for a more fine-grained evaluation of factual precision than previous methods. The paper uses people biographies as a basis for evaluation due to their objective nature and covers diverse nationalities, professions, and rarity levels.\n*   The following image from the paper shows an overview of FACTSCORE, a fraction of atomic facts (pieces of information) supported by a given knowledge source. FACTSCORE allows a more fine-grained evaluation of factual precision, e.g., in the figure, the top model gets a score of 66.7% and the bottom model gets 10.0%, whereas prior work would assign 0.0 to both. FACTSCORE can either be based on human evaluation, or be automated, which allows evaluation of a large set of LMs with no human efforts.\n\n![](../../../images/papers/FACTSCORE.jpg)\n\n*   Their automated estimator of FACTSCORE first breaks a generation into a series of atomic facts and then validates each against the given knowledge source. They use LLAMA 7B trained on Super Natural Instructions and ChatGPT as an LMEVAL, and Generalizable T5-based Retrievers for passage retrieval.\n*   The paper evaluates three state-of-the-art commercial LMs: InstructGPT, ChatGPT, and PerplexityAI. These models struggle with factual precision errors, with FACTSCOREs of 42%, 58%, and 71%, respectively. The study highlights that the factual precision of these LMs significantly drops with the rarity of the entities discussed in the text.\n*   To address the time-consuming and costly nature of human evaluation, the authors propose an automated model to estimate FACTSCORE. This model decomposes text into atomic facts and validates each against a knowledge source, closely approximating FACTSCORE with less than a 2% error rate. It allows the evaluation of a large set of new LMs without manual human effort.\n*   The paper also showcases the application of this automated estimator by evaluating 12 recently released LMs, offering insights into their factual accuracy. This approach could have cost $65K if evaluated by humans, highlighting the cost-effectiveness of the automated method.\n*   Finally, the paper suggests future work to enhance FACTSCORE, including considering other aspects of factuality such as recall (coverage of factual information), improving the estimator for better approximation of factual precision, and leveraging FACTSCORE to correct model generations.\n*   Overall, FACTSCORE represents a significant advancement in evaluating the factual precision of text generated by LMs, providing a detailed and cost-effective method for assessing the accuracy of long-form text.",
    "order": 10,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 522,
      "contentLength": 3599
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#factscore:-fine-grained-atomic-evaluation-of-factual-precision-in-long-form-text-generation",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-g-eval-nlg-evaluation-using-gpt-4-with-better-huma-11",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Related Papers",
    "title": "G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment",
    "subtitle": "Related Papers",
    "contentHtml": "<ul>\n  <li>The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators.</li>\n  <li>This paper by Liu et al. from presents G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs.</li>\n  <li>The following table from the paper illustrates the overall framework of G-Eval. We first input Task Introduction and Evaluation Criteria to the LLM, and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the output scores as the final score.</li>\n</ul>\n<p><img src=\"../../../images/papers/G-Eval.jpg\" alt=\"\"></p>\n<ul>\n  <li>They experiment with two generation tasks, text summarization and dialogue generation. They show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin.</li>\n  <li>They also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts.</li>\n  <li><a href=\"https://github.com/nlpyang/geval\">Code</a></li>\n</ul>",
    "contentMarkdown": "*   The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators.\n*   This paper by Liu et al. from presents G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs.\n*   The following table from the paper illustrates the overall framework of G-Eval. We first input Task Introduction and Evaluation Criteria to the LLM, and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the output scores as the final score.\n\n![](../../../images/papers/G-Eval.jpg)\n\n*   They experiment with two generation tasks, text summarization and dialogue generation. They show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin.\n*   They also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts.\n*   [Code](https://github.com/nlpyang/geval)",
    "order": 11,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "nlp"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 255,
      "contentLength": 1852
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#g-eval:-nlg-evaluation-using-gpt-4-with-better-human-alignment",
    "scrapedAt": "2025-12-28T11:54:05.684Z"
  },
  {
    "id": "ai-hallucination-aligning-large-multimodal-models-with-factually-au-12",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Related Papers",
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "subtitle": "Related Papers",
    "contentHtml": "<ul>\n  <li>This paper by Sun et al. from UC Berkeley, CMU, UIUC, UW–Madison, UMass Amherst, MSR, MIT-IBM Watson AI Lab addresses the issue of multimodal misalignment in large multimodal models (LMMs), which can lead to hallucinations—generating textual outputs not grounded in multimodal context. To mitigate this, the authors propose adapting Reinforcement Learning from Human Feedback (RLHF) to vision-language alignment and introducing Factually Augmented RLHF (Fact-RLHF).</li>\n  <li>The proposed method involves several key steps:\n    <ol>\n      <li><strong>Multimodal Supervised Fine-Tuning (SFT)</strong>: The initial stage involves fine-tuning a vision encoder and a pre-trained large language model (LLM) on an instruction-following demonstration dataset to create a supervised fine-tuned model (πSFT).</li>\n      <li><strong>Multimodal Preference Modeling</strong>: This stage trains a reward model to score responses based on human annotations. The reward model uses pairwise comparison data to learn to prefer less hallucinated responses. The training employs a cross-entropy loss function to adjust the model’s preferences.</li>\n      <li><strong>Reinforcement Learning</strong>: The policy model is fine-tuned using Proximal Policy Optimization (PPO) to maximize the reward signal from the preference model. A KL penalty is applied to prevent over-optimization and reward hacking.</li>\n      <li><strong>Factually Augmented RLHF (Fact-RLHF)</strong>: To enhance the reward model, it is augmented with factual information such as image captions and ground-truth multi-choice options. This addition helps the reward model avoid being misled by hallucinations that are not grounded in the actual image content.</li>\n      <li><strong>Enhancing Training Data</strong>: The authors improve the training data by augmenting GPT-4-generated vision instruction data with existing high-quality human-annotated image-text pairs. This includes data from VQA-v2, A-OKVQA, and Flickr30k, converted into suitable formats for vision-language tasks.</li>\n      <li><strong>MMHAL-BENCH</strong>: To evaluate the proposed approach, the authors develop a new benchmark, MMHAL-BENCH, focusing on penalizing hallucinations. This benchmark covers various types of questions that often lead to hallucinations in LMMs, such as object attributes, adversarial objects, comparisons, counting, spatial relations, and environment descriptions.</li>\n    </ol>\n  </li>\n  <li>The figure below from the paper illustrates that hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model.</li>\n</ul>\n<ol>\n      <li><strong>Multimodal Supervised Fine-Tuning (SFT)</strong>: The initial stage involves fine-tuning a vision encoder and a pre-trained large language model (LLM) on an instruction-following demonstration dataset to create a supervised fine-tuned model (πSFT).</li>\n      <li><strong>Multimodal Preference Modeling</strong>: This stage trains a reward model to score responses based on human annotations. The reward model uses pairwise comparison data to learn to prefer less hallucinated responses. The training employs a cross-entropy loss function to adjust the model’s preferences.</li>\n      <li><strong>Reinforcement Learning</strong>: The policy model is fine-tuned using Proximal Policy Optimization (PPO) to maximize the reward signal from the preference model. A KL penalty is applied to prevent over-optimization and reward hacking.</li>\n      <li><strong>Factually Augmented RLHF (Fact-RLHF)</strong>: To enhance the reward model, it is augmented with factual information such as image captions and ground-truth multi-choice options. This addition helps the reward model avoid being misled by hallucinations that are not grounded in the actual image content.</li>\n      <li><strong>Enhancing Training Data</strong>: The authors improve the training data by augmenting GPT-4-generated vision instruction data with existing high-quality human-annotated image-text pairs. This includes data from VQA-v2, A-OKVQA, and Flickr30k, converted into suitable formats for vision-language tasks.</li>\n      <li><strong>MMHAL-BENCH</strong>: To evaluate the proposed approach, the authors develop a new benchmark, MMHAL-BENCH, focusing on penalizing hallucinations. This benchmark covers various types of questions that often lead to hallucinations in LMMs, such as object attributes, adversarial objects, comparisons, counting, spatial relations, and environment descriptions.</li>\n    </ol>\n<p><img src=\"../../../images/papers/Fact-RLHF.jpg\" alt=\"\"></p>\n<ul>\n  <li>The implementation of Fact-RLHF shows significant improvements:\n    <ul>\n      <li><strong>Improved Alignment</strong>: LLaVA-RLHF, the model trained with Fact-RLHF, achieves 94% of the performance level of text-only GPT-4 on the LLaVA-Bench dataset, compared to 87% by previous best methods.</li>\n      <li><strong>Reduced Hallucinations</strong>: On MMHAL-BENCH, LLaVA-RLHF outperforms other baselines by 60%, showing a substantial reduction in hallucinated responses.</li>\n      <li><strong>Enhanced Performance</strong>: The model also sets new performance benchmarks on MMBench and POPE datasets, demonstrating improved general capabilities and alignment with human preferences.</li>\n    </ul>\n  </li>\n  <li>Overall, the paper highlights the effectiveness of integrating factual augmentation in RLHF to address multimodal misalignment, thereby reducing hallucinations and enhancing the reliability of large multimodal models. The authors have open-sourced their code, model, and data for further research and development in this area.</li>\n  <li><a href=\"https://llava-rlhf.github.io/\">Code</a></li>\n</ul>\n<ul>\n      <li><strong>Improved Alignment</strong>: LLaVA-RLHF, the model trained with Fact-RLHF, achieves 94% of the performance level of text-only GPT-4 on the LLaVA-Bench dataset, compared to 87% by previous best methods.</li>\n      <li><strong>Reduced Hallucinations</strong>: On MMHAL-BENCH, LLaVA-RLHF outperforms other baselines by 60%, showing a substantial reduction in hallucinated responses.</li>\n      <li><strong>Enhanced Performance</strong>: The model also sets new performance benchmarks on MMBench and POPE datasets, demonstrating improved general capabilities and alignment with human preferences.</li>\n    </ul>",
    "contentMarkdown": "*   This paper by Sun et al. from UC Berkeley, CMU, UIUC, UW–Madison, UMass Amherst, MSR, MIT-IBM Watson AI Lab addresses the issue of multimodal misalignment in large multimodal models (LMMs), which can lead to hallucinations—generating textual outputs not grounded in multimodal context. To mitigate this, the authors propose adapting Reinforcement Learning from Human Feedback (RLHF) to vision-language alignment and introducing Factually Augmented RLHF (Fact-RLHF).\n*   The proposed method involves several key steps:\n    1.  **Multimodal Supervised Fine-Tuning (SFT)**: The initial stage involves fine-tuning a vision encoder and a pre-trained large language model (LLM) on an instruction-following demonstration dataset to create a supervised fine-tuned model (πSFT).\n    2.  **Multimodal Preference Modeling**: This stage trains a reward model to score responses based on human annotations. The reward model uses pairwise comparison data to learn to prefer less hallucinated responses. The training employs a cross-entropy loss function to adjust the model’s preferences.\n    3.  **Reinforcement Learning**: The policy model is fine-tuned using Proximal Policy Optimization (PPO) to maximize the reward signal from the preference model. A KL penalty is applied to prevent over-optimization and reward hacking.\n    4.  **Factually Augmented RLHF (Fact-RLHF)**: To enhance the reward model, it is augmented with factual information such as image captions and ground-truth multi-choice options. This addition helps the reward model avoid being misled by hallucinations that are not grounded in the actual image content.\n    5.  **Enhancing Training Data**: The authors improve the training data by augmenting GPT-4-generated vision instruction data with existing high-quality human-annotated image-text pairs. This includes data from VQA-v2, A-OKVQA, and Flickr30k, converted into suitable formats for vision-language tasks.\n    6.  **MMHAL-BENCH**: To evaluate the proposed approach, the authors develop a new benchmark, MMHAL-BENCH, focusing on penalizing hallucinations. This benchmark covers various types of questions that often lead to hallucinations in LMMs, such as object attributes, adversarial objects, comparisons, counting, spatial relations, and environment descriptions.\n*   The figure below from the paper illustrates that hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model.\n\n1.  **Multimodal Supervised Fine-Tuning (SFT)**: The initial stage involves fine-tuning a vision encoder and a pre-trained large language model (LLM) on an instruction-following demonstration dataset to create a supervised fine-tuned model (πSFT).\n2.  **Multimodal Preference Modeling**: This stage trains a reward model to score responses based on human annotations. The reward model uses pairwise comparison data to learn to prefer less hallucinated responses. The training employs a cross-entropy loss function to adjust the model’s preferences.\n3.  **Reinforcement Learning**: The policy model is fine-tuned using Proximal Policy Optimization (PPO) to maximize the reward signal from the preference model. A KL penalty is applied to prevent over-optimization and reward hacking.\n4.  **Factually Augmented RLHF (Fact-RLHF)**: To enhance the reward model, it is augmented with factual information such as image captions and ground-truth multi-choice options. This addition helps the reward model avoid being misled by hallucinations that are not grounded in the actual image content.\n5.  **Enhancing Training Data**: The authors improve the training data by augmenting GPT-4-generated vision instruction data with existing high-quality human-annotated image-text pairs. This includes data from VQA-v2, A-OKVQA, and Flickr30k, converted into suitable formats for vision-language tasks.\n6.  **MMHAL-BENCH**: To evaluate the proposed approach, the authors develop a new benchmark, MMHAL-BENCH, focusing on penalizing hallucinations. This benchmark covers various types of questions that often lead to hallucinations in LMMs, such as object attributes, adversarial objects, comparisons, counting, spatial relations, and environment descriptions.\n\n![](../../../images/papers/Fact-RLHF.jpg)\n\n*   The implementation of Fact-RLHF shows significant improvements:\n    *   **Improved Alignment**: LLaVA-RLHF, the model trained with Fact-RLHF, achieves 94% of the performance level of text-only GPT-4 on the LLaVA-Bench dataset, compared to 87% by previous best methods.\n    *   **Reduced Hallucinations**: On MMHAL-BENCH, LLaVA-RLHF outperforms other baselines by 60%, showing a substantial reduction in hallucinated responses.\n    *   **Enhanced Performance**: The model also sets new performance benchmarks on MMBench and POPE datasets, demonstrating improved general capabilities and alignment with human preferences.\n*   Overall, the paper highlights the effectiveness of integrating factual augmentation in RLHF to address multimodal misalignment, thereby reducing hallucinations and enhancing the reliability of large multimodal models. The authors have open-sourced their code, model, and data for further research and development in this area.\n*   [Code](https://llava-rlhf.github.io/)\n\n*   **Improved Alignment**: LLaVA-RLHF, the model trained with Fact-RLHF, achieves 94% of the performance level of text-only GPT-4 on the LLaVA-Bench dataset, compared to 87% by previous best methods.\n*   **Reduced Hallucinations**: On MMHAL-BENCH, LLaVA-RLHF outperforms other baselines by 60%, showing a substantial reduction in hallucinated responses.\n*   **Enhanced Performance**: The model also sets new performance benchmarks on MMBench and POPE datasets, demonstrating improved general capabilities and alignment with human preferences.",
    "order": 12,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "gpt",
      "llm",
      "reinforcement learning",
      "optimization",
      "loss function",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 782,
      "contentLength": 6456
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#aligning-large-multimodal-models-with-factually-augmented-rlhf",
    "scrapedAt": "2025-12-28T11:54:05.685Z"
  },
  {
    "id": "ai-hallucination-dola-decoding-by-contrasting-layers-improves-factu-13",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Hallucination Detection and Mitigation",
    "articleSlug": "hallucination",
    "chapter": "Related Papers",
    "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
    "subtitle": "Related Papers",
    "contentHtml": "<ul>\n  <li>Microsoft’s research introduces a novel method called Decoding by Contrasting Layers (DoLa), aiming to mitigate hallucinations in large language models (LLMs) without necessitating additional training or retrieval. This technique rests upon the understanding that factual knowledge in LLMs is predominantly encoded within the latter or more mature layers of a transformer architecture.</li>\n  <li>During the process of text generation, DoLa does not rely on a static selection of a premature layer (an earlier layer in the transformer architecture) for contrasting. Instead, it actively selects the premature layer suited for each token’s decoding. The way DoLa determines this layer is by calculating the Jensen-Shannon divergence (JSD) between the output distribution of the premature layer and that of the mature layer, which is the final layer in the architecture. JSD serves as a measure of dissimilarity between two probability distributions. The underlying logic is to select a premature layer that exhibits the maximum JSD when juxtaposed with the mature layer, effectively maximizing the contrast between the factual knowledge and linguistic tendencies encapsulated in these layers.</li>\n  <li>\n    <p>Here’s a closer examination of its functionality:</p>\n\n    <ol>\n      <li>For every token being decoded, DoLa dynamically picks a premature layer by identifying which layer’s output distribution is the most distinct (in terms of JSD) from the mature layer’s distribution.</li>\n      <li>A higher JSD indicates more pronounced differences between the factual and linguistic content encoded in the two layers.</li>\n      <li>The premature layer embodies more fundamental linguistic patterns, while the mature layer is more representative of factual knowledge.</li>\n      <li>DoLa then calculates the next token’s probability distribution by contrasting the logit outputs of the mature layer and the chosen premature layer. Specifically, this involves subtracting the log probabilities of the premature layer from those of the mature layer.</li>\n      <li>As a result, the generated probability distribution accentuates factual information while diminishing mere linguistic patterns.</li>\n    </ol>\n  </li>\n  <li>This method is versatile. For tokens that are relatively simple and where the distributions between layers are alike (manifested by a lower JSD), early layers might be selected as the premature layer. Conversely, for tokens necessitating intricate real-world knowledge, a higher premature layer might be selected to enhance the contrast with the mature layer.</li>\n  <li>In empirical terms, when DoLa was tested across various tasks like multiple choice QA, open-ended QA, and text generation, the method showcased noticeable improvements in factuality and truthfulness, surpassing traditional decoding and other contrastive decoding techniques. Additionally, DoLa introduces only a minimal computational overhead during the inference phase, making it a lightweight yet effective approach.</li>\n  <li>\n    <p>In essence, DoLa offers a dynamic method of contrasting knowledge encoded in transformer layers to minimize hallucinations, and its ability to adaptively choose the appropriate premature layer for each token is central to its efficacy.</p>\n  </li>\n  <li>The method employed by DoLa, as described, involves contrasting the outputs between a “premature” layer and a “mature” layer. The mature layer, typically the final layer in the model, is believed to encode more of the factual knowledge, while the premature layers, being earlier in the network, contain more basic linguistic patterns.</li>\n  <li>\n    <p>The reason for dynamically picking a premature layer (as opposed to the mature layer) lies in the very objective of the method:</p>\n\n    <ol>\n      <li><strong>Contrast Mechanism</strong>: By contrasting the outputs of two layers (premature and mature), DoLa aims to amplify the factual information encoded in the mature layer while de-emphasizing the basic linguistic patterns in the premature layer.</li>\n      <li><strong>Dynamic Adaptability</strong>: While the mature layer remains consistent (as it’s always the final layer), choosing a premature layer dynamically provides adaptability. For different tokens or contexts, the distinction between the mature layer and a particular premature layer might be more pronounced, leading to a higher Jensen-Shannon divergence. By selecting different premature layers for different tokens, DoLa can better maximize this distinction.</li>\n      <li><strong>Highlighting Factual Information</strong>: The mature layer’s outputs are already expected to be more factual. The value in choosing the premature layer is in contrasting it with the mature layer. This emphasizes the factual content in the mature layer’s outputs even further.</li>\n      <li><strong>Flexibility</strong>: The range of possible outputs from premature layers provides a spectrum of linguistic patterns. By having the flexibility to select from this spectrum, DoLa can adaptively contrast different types of linguistic patterns against the factual knowledge in the mature layer, depending on the context.</li>\n    </ol>\n  </li>\n  <li>In essence, the mature layer acts as a consistent reference point, while the choice of premature layer allows the model to adaptively emphasize factual content over linguistic patterns, thereby aiming to reduce hallucinations and improve the factual accuracy of the generated content.</li>\n</ul>\n<p>Here’s a closer examination of its functionality:</p>\n<ol>\n      <li>For every token being decoded, DoLa dynamically picks a premature layer by identifying which layer’s output distribution is the most distinct (in terms of JSD) from the mature layer’s distribution.</li>\n      <li>A higher JSD indicates more pronounced differences between the factual and linguistic content encoded in the two layers.</li>\n      <li>The premature layer embodies more fundamental linguistic patterns, while the mature layer is more representative of factual knowledge.</li>\n      <li>DoLa then calculates the next token’s probability distribution by contrasting the logit outputs of the mature layer and the chosen premature layer. Specifically, this involves subtracting the log probabilities of the premature layer from those of the mature layer.</li>\n      <li>As a result, the generated probability distribution accentuates factual information while diminishing mere linguistic patterns.</li>\n    </ol>\n<p>In essence, DoLa offers a dynamic method of contrasting knowledge encoded in transformer layers to minimize hallucinations, and its ability to adaptively choose the appropriate premature layer for each token is central to its efficacy.</p>\n<p>The reason for dynamically picking a premature layer (as opposed to the mature layer) lies in the very objective of the method:</p>\n<ol>\n      <li><strong>Contrast Mechanism</strong>: By contrasting the outputs of two layers (premature and mature), DoLa aims to amplify the factual information encoded in the mature layer while de-emphasizing the basic linguistic patterns in the premature layer.</li>\n      <li><strong>Dynamic Adaptability</strong>: While the mature layer remains consistent (as it’s always the final layer), choosing a premature layer dynamically provides adaptability. For different tokens or contexts, the distinction between the mature layer and a particular premature layer might be more pronounced, leading to a higher Jensen-Shannon divergence. By selecting different premature layers for different tokens, DoLa can better maximize this distinction.</li>\n      <li><strong>Highlighting Factual Information</strong>: The mature layer’s outputs are already expected to be more factual. The value in choosing the premature layer is in contrasting it with the mature layer. This emphasizes the factual content in the mature layer’s outputs even further.</li>\n      <li><strong>Flexibility</strong>: The range of possible outputs from premature layers provides a spectrum of linguistic patterns. By having the flexibility to select from this spectrum, DoLa can adaptively contrast different types of linguistic patterns against the factual knowledge in the mature layer, depending on the context.</li>\n    </ol>",
    "contentMarkdown": "*   Microsoft’s research introduces a novel method called Decoding by Contrasting Layers (DoLa), aiming to mitigate hallucinations in large language models (LLMs) without necessitating additional training or retrieval. This technique rests upon the understanding that factual knowledge in LLMs is predominantly encoded within the latter or more mature layers of a transformer architecture.\n*   During the process of text generation, DoLa does not rely on a static selection of a premature layer (an earlier layer in the transformer architecture) for contrasting. Instead, it actively selects the premature layer suited for each token’s decoding. The way DoLa determines this layer is by calculating the Jensen-Shannon divergence (JSD) between the output distribution of the premature layer and that of the mature layer, which is the final layer in the architecture. JSD serves as a measure of dissimilarity between two probability distributions. The underlying logic is to select a premature layer that exhibits the maximum JSD when juxtaposed with the mature layer, effectively maximizing the contrast between the factual knowledge and linguistic tendencies encapsulated in these layers.\n*   Here’s a closer examination of its functionality:\n    \n    1.  For every token being decoded, DoLa dynamically picks a premature layer by identifying which layer’s output distribution is the most distinct (in terms of JSD) from the mature layer’s distribution.\n    2.  A higher JSD indicates more pronounced differences between the factual and linguistic content encoded in the two layers.\n    3.  The premature layer embodies more fundamental linguistic patterns, while the mature layer is more representative of factual knowledge.\n    4.  DoLa then calculates the next token’s probability distribution by contrasting the logit outputs of the mature layer and the chosen premature layer. Specifically, this involves subtracting the log probabilities of the premature layer from those of the mature layer.\n    5.  As a result, the generated probability distribution accentuates factual information while diminishing mere linguistic patterns.\n*   This method is versatile. For tokens that are relatively simple and where the distributions between layers are alike (manifested by a lower JSD), early layers might be selected as the premature layer. Conversely, for tokens necessitating intricate real-world knowledge, a higher premature layer might be selected to enhance the contrast with the mature layer.\n*   In empirical terms, when DoLa was tested across various tasks like multiple choice QA, open-ended QA, and text generation, the method showcased noticeable improvements in factuality and truthfulness, surpassing traditional decoding and other contrastive decoding techniques. Additionally, DoLa introduces only a minimal computational overhead during the inference phase, making it a lightweight yet effective approach.\n*   In essence, DoLa offers a dynamic method of contrasting knowledge encoded in transformer layers to minimize hallucinations, and its ability to adaptively choose the appropriate premature layer for each token is central to its efficacy.\n    \n*   The method employed by DoLa, as described, involves contrasting the outputs between a “premature” layer and a “mature” layer. The mature layer, typically the final layer in the model, is believed to encode more of the factual knowledge, while the premature layers, being earlier in the network, contain more basic linguistic patterns.\n*   The reason for dynamically picking a premature layer (as opposed to the mature layer) lies in the very objective of the method:\n    \n    1.  **Contrast Mechanism**: By contrasting the outputs of two layers (premature and mature), DoLa aims to amplify the factual information encoded in the mature layer while de-emphasizing the basic linguistic patterns in the premature layer.\n    2.  **Dynamic Adaptability**: While the mature layer remains consistent (as it’s always the final layer), choosing a premature layer dynamically provides adaptability. For different tokens or contexts, the distinction between the mature layer and a particular premature layer might be more pronounced, leading to a higher Jensen-Shannon divergence. By selecting different premature layers for different tokens, DoLa can better maximize this distinction.\n    3.  **Highlighting Factual Information**: The mature layer’s outputs are already expected to be more factual. The value in choosing the premature layer is in contrasting it with the mature layer. This emphasizes the factual content in the mature layer’s outputs even further.\n    4.  **Flexibility**: The range of possible outputs from premature layers provides a spectrum of linguistic patterns. By having the flexibility to select from this spectrum, DoLa can adaptively contrast different types of linguistic patterns against the factual knowledge in the mature layer, depending on the context.\n*   In essence, the mature layer acts as a consistent reference point, while the choice of premature layer allows the model to adaptively emphasize factual content over linguistic patterns, thereby aiming to reduce hallucinations and improve the factual accuracy of the generated content.\n\nHere’s a closer examination of its functionality:\n\n1.  For every token being decoded, DoLa dynamically picks a premature layer by identifying which layer’s output distribution is the most distinct (in terms of JSD) from the mature layer’s distribution.\n2.  A higher JSD indicates more pronounced differences between the factual and linguistic content encoded in the two layers.\n3.  The premature layer embodies more fundamental linguistic patterns, while the mature layer is more representative of factual knowledge.\n4.  DoLa then calculates the next token’s probability distribution by contrasting the logit outputs of the mature layer and the chosen premature layer. Specifically, this involves subtracting the log probabilities of the premature layer from those of the mature layer.\n5.  As a result, the generated probability distribution accentuates factual information while diminishing mere linguistic patterns.\n\nIn essence, DoLa offers a dynamic method of contrasting knowledge encoded in transformer layers to minimize hallucinations, and its ability to adaptively choose the appropriate premature layer for each token is central to its efficacy.\n\nThe reason for dynamically picking a premature layer (as opposed to the mature layer) lies in the very objective of the method:\n\n1.  **Contrast Mechanism**: By contrasting the outputs of two layers (premature and mature), DoLa aims to amplify the factual information encoded in the mature layer while de-emphasizing the basic linguistic patterns in the premature layer.\n2.  **Dynamic Adaptability**: While the mature layer remains consistent (as it’s always the final layer), choosing a premature layer dynamically provides adaptability. For different tokens or contexts, the distinction between the mature layer and a particular premature layer might be more pronounced, leading to a higher Jensen-Shannon divergence. By selecting different premature layers for different tokens, DoLa can better maximize this distinction.\n3.  **Highlighting Factual Information**: The mature layer’s outputs are already expected to be more factual. The value in choosing the premature layer is in contrasting it with the mature layer. This emphasizes the factual content in the mature layer’s outputs even further.\n4.  **Flexibility**: The range of possible outputs from premature layers provides a spectrum of linguistic patterns. By having the flexibility to select from this spectrum, DoLa can adaptively contrast different types of linguistic patterns against the factual knowledge in the mature layer, depending on the context.",
    "order": 13,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 6,
    "tags": [
      "nlpllms",
      "transformer",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1132,
      "contentLength": 8267
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/hallucination/#dola:-decoding-by-contrasting-layers-improves-factuality-in-large-language-models",
    "scrapedAt": "2025-12-28T11:54:05.685Z"
  }
]