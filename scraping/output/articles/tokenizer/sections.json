[
  {
    "id": "ai-tokenizer-example-1",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "WordPiece",
    "title": "Example",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>To illustrate the process of WordPiece tokenization, consider a simple sentence “she walked. he is a dog walker. i walk” and see how the algorithm constructs the vocabulary and tokenizes the text step-by-step.</li>\n</ul>\n<ol>\n  <li><strong>Initial Inventory:</strong>\n    <ul>\n      <li>WordPiece starts with individual characters as the initial inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n    </ul>\n  </li>\n  <li><strong>Building the Vocabulary:</strong>\n    <ol>\n      <li><strong>First Iteration</strong>:\n        <ul>\n          <li>Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Second Iteration</strong>:\n        <ul>\n          <li>Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Third Iteration</strong>:\n        <ul>\n          <li>Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Further Iterations</strong>:\n        <ul>\n          <li>The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Tokenizing the Text:</strong>\n    <ul>\n      <li>“she walked. he is a dog walker. i walk”\n        <ul>\n          <li>With the current state of the vocabulary, the algorithm would tokenize the text as follows:</li>\n          <li>‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’</li>\n          <li>Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.</li>\n          <li>‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>WordPiece starts with individual characters as the initial inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n    </ul>\n<ol>\n      <li><strong>First Iteration</strong>:\n        <ul>\n          <li>Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Second Iteration</strong>:\n        <ul>\n          <li>Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Third Iteration</strong>:\n        <ul>\n          <li>Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n      </li>\n      <li><strong>Further Iterations</strong>:\n        <ul>\n          <li>The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n<ul>\n          <li>Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n<ul>\n          <li>Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.</li>\n          <li>New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n        </ul>\n<ul>\n          <li>The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.</li>\n        </ul>\n<ul>\n      <li>“she walked. he is a dog walker. i walk”\n        <ul>\n          <li>With the current state of the vocabulary, the algorithm would tokenize the text as follows:</li>\n          <li>‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’</li>\n          <li>Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.</li>\n          <li>‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>With the current state of the vocabulary, the algorithm would tokenize the text as follows:</li>\n          <li>‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’</li>\n          <li>Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.</li>\n          <li>‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.</li>\n        </ul>\n<ul>\n  <li><strong>Note:</strong>\n    <ul>\n      <li>In a real-world scenario, the WordPiece algorithm would perform many more iterations, and the decision to combine tokens depends on complex statistical properties of the training data.</li>\n      <li>The examples given are simplified and serve to illustrate the process. The actual tokens generated would depend on the specific training corpus and the parameters set for the vocabulary size and likelihood thresholds.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>In a real-world scenario, the WordPiece algorithm would perform many more iterations, and the decision to combine tokens depends on complex statistical properties of the training data.</li>\n      <li>The examples given are simplified and serve to illustrate the process. The actual tokens generated would depend on the specific training corpus and the parameters set for the vocabulary size and likelihood thresholds.</li>\n    </ul>",
    "contentMarkdown": "*   To illustrate the process of WordPiece tokenization, consider a simple sentence “she walked. he is a dog walker. i walk” and see how the algorithm constructs the vocabulary and tokenizes the text step-by-step.\n\n1.  **Initial Inventory:**\n    *   WordPiece starts with individual characters as the initial inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n2.  **Building the Vocabulary:**\n    1.  **First Iteration**:\n        *   Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.\n        *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n    2.  **Second Iteration**:\n        *   Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.\n        *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n    3.  **Third Iteration**:\n        *   Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.\n        *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n    4.  **Further Iterations**:\n        *   The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.\n3.  **Tokenizing the Text:**\n    *   “she walked. he is a dog walker. i walk”\n        *   With the current state of the vocabulary, the algorithm would tokenize the text as follows:\n        *   ‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’\n        *   Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.\n        *   ‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.\n\n*   WordPiece starts with individual characters as the initial inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n\n1.  **First Iteration**:\n    *   Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.\n    *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n2.  **Second Iteration**:\n    *   Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.\n    *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n3.  **Third Iteration**:\n    *   Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.\n    *   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n4.  **Further Iterations**:\n    *   The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.\n\n*   Suppose the most significant likelihood increase comes from combining ‘w’ and ‘a’ to form ‘wa’.\n*   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n\n*   Next, the combination ‘l’ and ‘k’ gives a good likelihood boost, forming ‘lk’.\n*   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘wa’, ‘lk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n\n*   Let’s say combining ‘wa’ and ‘lk’ into ‘walk’ is the next most beneficial addition.\n*   New Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n\n*   The process continues, adding more combinations like ‘er’, ‘ed’, etc., based on what increases the likelihood the most.\n\n*   “she walked. he is a dog walker. i walk”\n    *   With the current state of the vocabulary, the algorithm would tokenize the text as follows:\n    *   ‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’\n    *   Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.\n    *   ‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.\n\n*   With the current state of the vocabulary, the algorithm would tokenize the text as follows:\n*   ‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘walk’, ‘er’, ‘.’, ‘i’, ‘walk’\n*   Here, words like ‘she’, ‘he’, ‘is’, ‘a’, ‘dog’, ‘i’ remain as they are since they’re either already in the initial inventory or don’t present a combination that increases likelihood significantly.\n*   ‘walked’ is broken down into ‘walk’ and ‘ed’, ‘walker’ into ‘walk’ and ‘er’, as these subwords (‘walk’, ‘ed’, ‘er’) are present in the expanded inventory.\n\n*   **Note:**\n    *   In a real-world scenario, the WordPiece algorithm would perform many more iterations, and the decision to combine tokens depends on complex statistical properties of the training data.\n    *   The examples given are simplified and serve to illustrate the process. The actual tokens generated would depend on the specific training corpus and the parameters set for the vocabulary size and likelihood thresholds.\n\n*   In a real-world scenario, the WordPiece algorithm would perform many more iterations, and the decision to combine tokens depends on complex statistical properties of the training data.\n*   The examples given are simplified and serve to illustrate the process. The actual tokens generated would depend on the specific training corpus and the parameters set for the vocabulary size and likelihood thresholds.",
    "contentLength": 6728,
    "wordCount": 862,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#example"
  },
  {
    "id": "ai-tokenizer-example-2",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Byte Pair Encoding (BPE)",
    "title": "Example",
    "order": 2,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Consider the input text: “she walked. he is a dog walker. i walk”. The steps that BPE follows for tokenization are as below:</li>\n</ul>\n<ol>\n  <li>BPE Merges:\n    <ul>\n      <li><strong>First Merge</strong>: If ‘w’ and ‘a’ are the most frequent pair, they merge to form ‘wa’.</li>\n      <li><strong>Second Merge</strong>: Then, if ‘l’ and ‘k’ frequently occur together, they merge to form ‘lk’.</li>\n      <li><strong>Third Merge</strong>: Next, ‘wa’ and ‘lk’ might merge to form ‘walk’.</li>\n      <li><strong>Vocabulary</strong>: At this stage, the vocabulary includes all individual characters, plus ‘wa’, ‘lk’, and ‘walk’.</li>\n    </ul>\n  </li>\n  <li>Handling Rare/Out-of-Vocabulary (OOV) Words\n    <ul>\n      <li><strong>Subword Segmentation</strong>: Any word not in the vocabulary is broken down into subword units based on the vocabulary.</li>\n      <li><strong>Example</strong>: For a rare word like ‘walking’, if ‘walking’ is not in the vocabulary but ‘walk’ and ‘ing’ are, it gets segmented into ‘walk’ and ‘ing’.</li>\n    </ul>\n  </li>\n  <li>Benefits\n    <ul>\n      <li><strong>Common Subwords</strong>: This method ensures that words with common roots or morphemes, like ‘walked’, ‘walker’, ‘walks’, are represented using common subwords (e.g., ‘walk@@’). This helps the model to better understand and process these variations, as ‘walk@@’ would appear more frequently in the training data.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li><strong>First Merge</strong>: If ‘w’ and ‘a’ are the most frequent pair, they merge to form ‘wa’.</li>\n      <li><strong>Second Merge</strong>: Then, if ‘l’ and ‘k’ frequently occur together, they merge to form ‘lk’.</li>\n      <li><strong>Third Merge</strong>: Next, ‘wa’ and ‘lk’ might merge to form ‘walk’.</li>\n      <li><strong>Vocabulary</strong>: At this stage, the vocabulary includes all individual characters, plus ‘wa’, ‘lk’, and ‘walk’.</li>\n    </ul>\n<ul>\n      <li><strong>Subword Segmentation</strong>: Any word not in the vocabulary is broken down into subword units based on the vocabulary.</li>\n      <li><strong>Example</strong>: For a rare word like ‘walking’, if ‘walking’ is not in the vocabulary but ‘walk’ and ‘ing’ are, it gets segmented into ‘walk’ and ‘ing’.</li>\n    </ul>\n<ul>\n      <li><strong>Common Subwords</strong>: This method ensures that words with common roots or morphemes, like ‘walked’, ‘walker’, ‘walks’, are represented using common subwords (e.g., ‘walk@@’). This helps the model to better understand and process these variations, as ‘walk@@’ would appear more frequently in the training data.</li>\n    </ul>",
    "contentMarkdown": "*   Consider the input text: “she walked. he is a dog walker. i walk”. The steps that BPE follows for tokenization are as below:\n\n1.  BPE Merges:\n    *   **First Merge**: If ‘w’ and ‘a’ are the most frequent pair, they merge to form ‘wa’.\n    *   **Second Merge**: Then, if ‘l’ and ‘k’ frequently occur together, they merge to form ‘lk’.\n    *   **Third Merge**: Next, ‘wa’ and ‘lk’ might merge to form ‘walk’.\n    *   **Vocabulary**: At this stage, the vocabulary includes all individual characters, plus ‘wa’, ‘lk’, and ‘walk’.\n2.  Handling Rare/Out-of-Vocabulary (OOV) Words\n    *   **Subword Segmentation**: Any word not in the vocabulary is broken down into subword units based on the vocabulary.\n    *   **Example**: For a rare word like ‘walking’, if ‘walking’ is not in the vocabulary but ‘walk’ and ‘ing’ are, it gets segmented into ‘walk’ and ‘ing’.\n3.  Benefits\n    *   **Common Subwords**: This method ensures that words with common roots or morphemes, like ‘walked’, ‘walker’, ‘walks’, are represented using common subwords (e.g., ‘walk@@’). This helps the model to better understand and process these variations, as ‘walk@@’ would appear more frequently in the training data.\n\n*   **First Merge**: If ‘w’ and ‘a’ are the most frequent pair, they merge to form ‘wa’.\n*   **Second Merge**: Then, if ‘l’ and ‘k’ frequently occur together, they merge to form ‘lk’.\n*   **Third Merge**: Next, ‘wa’ and ‘lk’ might merge to form ‘walk’.\n*   **Vocabulary**: At this stage, the vocabulary includes all individual characters, plus ‘wa’, ‘lk’, and ‘walk’.\n\n*   **Subword Segmentation**: Any word not in the vocabulary is broken down into subword units based on the vocabulary.\n*   **Example**: For a rare word like ‘walking’, if ‘walking’ is not in the vocabulary but ‘walk’ and ‘ing’ are, it gets segmented into ‘walk’ and ‘ing’.\n\n*   **Common Subwords**: This method ensures that words with common roots or morphemes, like ‘walked’, ‘walker’, ‘walks’, are represented using common subwords (e.g., ‘walk@@’). This helps the model to better understand and process these variations, as ‘walk@@’ would appear more frequently in the training data.",
    "contentLength": 2612,
    "wordCount": 338,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#example"
  },
  {
    "id": "ai-tokenizer-example-3",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "SentencePiece",
    "title": "Example",
    "order": 3,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Applying SentencePiece to the input text “she walked. he is a dog walker. i walk” would involve the following steps:</li>\n</ul>\n<h4 id=\"vocabulary-building\">Vocabulary Building</h4>\n<ol>\n  <li><strong>Unsegmented Text Input:</strong>\n    <ul>\n      <li>SentencePiece would take the raw, unsegmented text as input, treating each character (including spaces and punctuation) as a distinct symbol.</li>\n      <li>Initial Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n    </ul>\n  </li>\n  <li><strong>First Iteration</strong>:\n    <ul>\n      <li>The algorithm might begin by combining frequent pairs of characters, including spaces, based on statistical frequency.</li>\n      <li>New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘e ‘, ‘ w’, ‘wa’</li>\n    </ul>\n  </li>\n  <li><strong>Subsequent Iterations</strong>:\n    <ul>\n      <li>The process continues, combining characters and character sequences into longer subword units based on frequency and context (which will in turn, maximize the likelihood of the text data), without any pre-existing notions of word boundaries. The goal is to find the optimal way to split the text into subword units that best represent the training data. For instance, ‘w’, ‘a’, ‘l’, ‘k’ might combine to form ‘walk’, ‘he’ might become a unit, and so on.</li>\n      <li>New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘she’, ‘ed’, ‘he’, ‘ is’</li>\n    </ul>\n  </li>\n  <li><strong>Until Vocabulary Size is Reached</strong>:\n    <ul>\n      <li>This iterative process of combining continues until a predefined vocabulary size is reached. The size of the vocabulary is a hyperparameter that can be adjusted depending on the desired granularity and model capacity.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>SentencePiece would take the raw, unsegmented text as input, treating each character (including spaces and punctuation) as a distinct symbol.</li>\n      <li>Initial Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’</li>\n    </ul>\n<ul>\n      <li>The algorithm might begin by combining frequent pairs of characters, including spaces, based on statistical frequency.</li>\n      <li>New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘e ‘, ‘ w’, ‘wa’</li>\n    </ul>\n<ul>\n      <li>The process continues, combining characters and character sequences into longer subword units based on frequency and context (which will in turn, maximize the likelihood of the text data), without any pre-existing notions of word boundaries. The goal is to find the optimal way to split the text into subword units that best represent the training data. For instance, ‘w’, ‘a’, ‘l’, ‘k’ might combine to form ‘walk’, ‘he’ might become a unit, and so on.</li>\n      <li>New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘she’, ‘ed’, ‘he’, ‘ is’</li>\n    </ul>\n<ul>\n      <li>This iterative process of combining continues until a predefined vocabulary size is reached. The size of the vocabulary is a hyperparameter that can be adjusted depending on the desired granularity and model capacity.</li>\n    </ul>\n<h4 id=\"tokenizing-the-text\">Tokenizing the Text</h4>\n<ul>\n  <li><strong>Final Tokenization</strong>: Once the vocabulary is established, SentencePiece tokenizes the text based on the learned subword units. For the example sentence, the tokenization might look like this:\n    <ul>\n      <li>Original: “she walked. he is a dog walker. i walk”</li>\n      <li>Tokenized: [‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘ is’, ‘a’, ‘ dog’, ‘ walk’, ‘er’, ‘.’, ‘i’, ‘ walk’]</li>\n      <li>Notice how punctuation and spaces are treated as tokens (e.g., ‘ed’, ‘is’, ‘walk’).</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Original: “she walked. he is a dog walker. i walk”</li>\n      <li>Tokenized: [‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘ is’, ‘a’, ‘ dog’, ‘ walk’, ‘er’, ‘.’, ‘i’, ‘ walk’]</li>\n      <li>Notice how punctuation and spaces are treated as tokens (e.g., ‘ed’, ‘is’, ‘walk’).</li>\n    </ul>",
    "contentMarkdown": "*   Applying SentencePiece to the input text “she walked. he is a dog walker. i walk” would involve the following steps:\n\n#### Vocabulary Building\n\n1.  **Unsegmented Text Input:**\n    *   SentencePiece would take the raw, unsegmented text as input, treating each character (including spaces and punctuation) as a distinct symbol.\n    *   Initial Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n2.  **First Iteration**:\n    *   The algorithm might begin by combining frequent pairs of characters, including spaces, based on statistical frequency.\n    *   New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘e ‘, ‘ w’, ‘wa’\n3.  **Subsequent Iterations**:\n    *   The process continues, combining characters and character sequences into longer subword units based on frequency and context (which will in turn, maximize the likelihood of the text data), without any pre-existing notions of word boundaries. The goal is to find the optimal way to split the text into subword units that best represent the training data. For instance, ‘w’, ‘a’, ‘l’, ‘k’ might combine to form ‘walk’, ‘he’ might become a unit, and so on.\n    *   New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘she’, ‘ed’, ‘he’, ‘ is’\n4.  **Until Vocabulary Size is Reached**:\n    *   This iterative process of combining continues until a predefined vocabulary size is reached. The size of the vocabulary is a hyperparameter that can be adjusted depending on the desired granularity and model capacity.\n\n*   SentencePiece would take the raw, unsegmented text as input, treating each character (including spaces and punctuation) as a distinct symbol.\n*   Initial Inventory: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’\n\n*   The algorithm might begin by combining frequent pairs of characters, including spaces, based on statistical frequency.\n*   New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘w’, ‘a’, ‘l’, ‘k’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘e ‘, ‘ w’, ‘wa’\n\n*   The process continues, combining characters and character sequences into longer subword units based on frequency and context (which will in turn, maximize the likelihood of the text data), without any pre-existing notions of word boundaries. The goal is to find the optimal way to split the text into subword units that best represent the training data. For instance, ‘w’, ‘a’, ‘l’, ‘k’ might combine to form ‘walk’, ‘he’ might become a unit, and so on.\n*   New Inventory Example: ‘s’, ‘h’, ‘e’, ‘ ‘, ‘walk’, ‘d’, ‘.’, ‘i’, ‘o’, ‘g’, ‘r’, ‘she’, ‘ed’, ‘he’, ‘ is’\n\n*   This iterative process of combining continues until a predefined vocabulary size is reached. The size of the vocabulary is a hyperparameter that can be adjusted depending on the desired granularity and model capacity.\n\n#### Tokenizing the Text\n\n*   **Final Tokenization**: Once the vocabulary is established, SentencePiece tokenizes the text based on the learned subword units. For the example sentence, the tokenization might look like this:\n    *   Original: “she walked. he is a dog walker. i walk”\n    *   Tokenized: \\[‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘ is’, ‘a’, ‘ dog’, ‘ walk’, ‘er’, ‘.’, ‘i’, ‘ walk’\\]\n    *   Notice how punctuation and spaces are treated as tokens (e.g., ‘ed’, ‘is’, ‘walk’).\n\n*   Original: “she walked. he is a dog walker. i walk”\n*   Tokenized: \\[‘she’, ‘walk’, ‘ed’, ‘.’, ‘he’, ‘ is’, ‘a’, ‘ dog’, ‘ walk’, ‘er’, ‘.’, ‘i’, ‘ walk’\\]\n*   Notice how punctuation and spaces are treated as tokens (e.g., ‘ed’, ‘is’, ‘walk’).",
    "contentLength": 4121,
    "wordCount": 582,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#example"
  },
  {
    "id": "ai-tokenizer-handling-rareout-of-vocabulary-oov-words-4",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "SentencePiece",
    "title": "Handling Rare/Out-of-Vocabulary (OOV) Words",
    "order": 4,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><strong>Robust to Rare/Out-of-Vocabulary (OOV) Words</strong>: SentencePiece, like WordPiece and BPE, is designed to handle OOV words by breaking them down into subword units that are in the vocabulary. This flexibility allows it to effectively manage rare words, ensuring that even if a whole word is not in the vocabulary, its components will be. SentencePiece’s method of working with raw text makes it particularly adept at capturing nuanced subword patterns, including handling variations in spacing and punctuation. This capability enhances its robustness in multilingual contexts and languages with complex orthography.</li>\n</ul>",
    "contentMarkdown": "*   **Robust to Rare/Out-of-Vocabulary (OOV) Words**: SentencePiece, like WordPiece and BPE, is designed to handle OOV words by breaking them down into subword units that are in the vocabulary. This flexibility allows it to effectively manage rare words, ensuring that even if a whole word is not in the vocabulary, its components will be. SentencePiece’s method of working with raw text makes it particularly adept at capturing nuanced subword patterns, including handling variations in spacing and punctuation. This capability enhances its robustness in multilingual contexts and languages with complex orthography.",
    "contentLength": 648,
    "wordCount": 90,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#handling-rare/out-of-vocabulary-(oov)-words"
  },
  {
    "id": "ai-tokenizer-benefits-5",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "SentencePiece",
    "title": "Benefits",
    "order": 5,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><strong>No Need for Pre-tokenization</strong>: SentencePiece eliminates the need for language-specific pre-tokenization. This makes it particularly useful for languages (such as Japanese, Chinese, Thai, and Vietnamese) where word boundaries are not clear or in multilingual contexts.</li>\n  <li><strong>Uniform Treatment of Characters</strong>: By treating spaces and other characters equally, SentencePiece can capture a broader range of linguistic patterns, which is beneficial for modeling diverse languages and text types.</li>\n  <li>In summary, SentencePiece offers a unique approach to tokenization by working directly on raw text and treating all characters, including spaces, on an equal footing. This method is particularly beneficial for languages with complex word structures and for multilingual models.</li>\n</ul>",
    "contentMarkdown": "*   **No Need for Pre-tokenization**: SentencePiece eliminates the need for language-specific pre-tokenization. This makes it particularly useful for languages (such as Japanese, Chinese, Thai, and Vietnamese) where word boundaries are not clear or in multilingual contexts.\n*   **Uniform Treatment of Characters**: By treating spaces and other characters equally, SentencePiece can capture a broader range of linguistic patterns, which is beneficial for modeling diverse languages and text types.\n*   In summary, SentencePiece offers a unique approach to tokenization by working directly on raw text and treating all characters, including spaces, on an equal footing. This method is particularly beneficial for languages with complex word structures and for multilingual models.",
    "contentLength": 837,
    "wordCount": 108,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#benefits"
  },
  {
    "id": "ai-tokenizer-unigram-subword-tokenization-6",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Summary",
    "title": "Unigram Subword Tokenization",
    "order": 6,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>It starts with a set of words, and then iteratively splits the most probable word into smaller parts.</li>\n  <li>It assigns a probability to the newly created subwords based on their frequency in the text.</li>\n  <li>It is less popular compare to other subword tokenization methods like BPE or SentencePiece.</li>\n  <li>It has been reported to have good performance in some NLP tasks such as language modeling and text-to-speech.</li>\n</ul>",
    "contentMarkdown": "*   It starts with a set of words, and then iteratively splits the most probable word into smaller parts.\n*   It assigns a probability to the newly created subwords based on their frequency in the text.\n*   It is less popular compare to other subword tokenization methods like BPE or SentencePiece.\n*   It has been reported to have good performance in some NLP tasks such as language modeling and text-to-speech.",
    "contentLength": 451,
    "wordCount": 70,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#unigram-subword-tokenization"
  },
  {
    "id": "ai-tokenizer-bpe-7",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Summary",
    "title": "BPE",
    "order": 7,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><strong>How It Works</strong>: BPE is a data compression technique that has been adapted for use in NLP. It starts with a base vocabulary of individual characters and iteratively merges the most frequent pair of tokens to form new, longer tokens. This process continues for a specified number of merge operations.</li>\n  <li><strong>Advantages</strong>: BPE is effective in handling rare or unknown words, as it can decompose them into subword units that it has seen during training. It also strikes a balance between the number of tokens (length of the input sequence) and the size of the vocabulary.</li>\n  <li><strong>Use in NLP</strong>: BPE is widely used in models like GPT-2 and GPT-3.</li>\n</ul>",
    "contentMarkdown": "*   **How It Works**: BPE is a data compression technique that has been adapted for use in NLP. It starts with a base vocabulary of individual characters and iteratively merges the most frequent pair of tokens to form new, longer tokens. This process continues for a specified number of merge operations.\n*   **Advantages**: BPE is effective in handling rare or unknown words, as it can decompose them into subword units that it has seen during training. It also strikes a balance between the number of tokens (length of the input sequence) and the size of the vocabulary.\n*   **Use in NLP**: BPE is widely used in models like GPT-2 and GPT-3.",
    "contentLength": 714,
    "wordCount": 111,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#bpe"
  },
  {
    "id": "ai-tokenizer-wordpiece-8",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Summary",
    "title": "WordPiece",
    "order": 8,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><strong>How It Works</strong>: WordPiece is similar to BPE but differs slightly in its approach to creating new tokens. Instead of just merging the most frequent pairs, WordPiece looks at the likelihood of the entire vocabulary and adds the token that increases the likelihood of the data the most.</li>\n  <li><strong>Advantages</strong>: This method often leads to a more efficient segmentation of words into subwords compared to BPE. It’s particularly good at handling words not seen during training.</li>\n  <li><strong>Use in NLP</strong>: WordPiece is used in models like BERT and Google’s neural machine translation system.</li>\n</ul>",
    "contentMarkdown": "*   **How It Works**: WordPiece is similar to BPE but differs slightly in its approach to creating new tokens. Instead of just merging the most frequent pairs, WordPiece looks at the likelihood of the entire vocabulary and adds the token that increases the likelihood of the data the most.\n*   **Advantages**: This method often leads to a more efficient segmentation of words into subwords compared to BPE. It’s particularly good at handling words not seen during training.\n*   **Use in NLP**: WordPiece is used in models like BERT and Google’s neural machine translation system.",
    "contentLength": 650,
    "wordCount": 94,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#wordpiece"
  },
  {
    "id": "ai-tokenizer-sentencepiece-9",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Summary",
    "title": "SentencePiece",
    "order": 9,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li><strong>How It Works</strong>: SentencePiece is a tokenization method that does not rely on pre-tokenized text. It can directly process raw text (including spaces and special characters) into tokens. SentencePiece can be trained to use either the BPE or unigram language model methodologies.</li>\n  <li><strong>Advantages</strong>: One of the key benefits of SentencePiece is its ability to handle multiple languages and scripts without needing pre-tokenization or language-specific logic. This makes it particularly useful for multilingual models.</li>\n  <li><strong>Use in NLP</strong>: SentencePiece is used in models like ALBERT and T5.</li>\n</ul>",
    "contentMarkdown": "*   **How It Works**: SentencePiece is a tokenization method that does not rely on pre-tokenized text. It can directly process raw text (including spaces and special characters) into tokens. SentencePiece can be trained to use either the BPE or unigram language model methodologies.\n*   **Advantages**: One of the key benefits of SentencePiece is its ability to handle multiple languages and scripts without needing pre-tokenization or language-specific logic. This makes it particularly useful for multilingual models.\n*   **Use in NLP**: SentencePiece is used in models like ALBERT and T5.",
    "contentLength": 662,
    "wordCount": 88,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#sentencepiece"
  },
  {
    "id": "ai-tokenizer-wordpiece-10",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Use-cases",
    "title": "WordPiece",
    "order": 10,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Some models that utilize WordPiece include:</p>\n\n    <ol>\n      <li>\n        <p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.</p>\n      </li>\n      <li>\n        <p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong>: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.</p>\n      </li>\n      <li>\n        <p><strong>DistilBERT</strong>: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.</p>\n      </li>\n      <li>\n        <p><strong>ERNIE (Enhanced Representation through kNowledge Integration)</strong>: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>WordPiece is chosen in these models due to its efficiency in handling a large vocabulary by breaking down words into smaller, manageable sub-words or tokens. This method allows for better handling of rare words and morphological variations in different languages.</p>\n  </li>\n</ul>\n<p>Some models that utilize WordPiece include:</p>\n<ol>\n      <li>\n        <p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.</p>\n      </li>\n      <li>\n        <p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong>: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.</p>\n      </li>\n      <li>\n        <p><strong>DistilBERT</strong>: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.</p>\n      </li>\n      <li>\n        <p><strong>ERNIE (Enhanced Representation through kNowledge Integration)</strong>: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.</p>\n      </li>\n    </ol>\n<p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.</p>\n<p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong>: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.</p>\n<p><strong>DistilBERT</strong>: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.</p>\n<p><strong>ERNIE (Enhanced Representation through kNowledge Integration)</strong>: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.</p>\n<p>WordPiece is chosen in these models due to its efficiency in handling a large vocabulary by breaking down words into smaller, manageable sub-words or tokens. This method allows for better handling of rare words and morphological variations in different languages.</p>",
    "contentMarkdown": "*   Some models that utilize WordPiece include:\n    \n    1.  **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.\n        \n    2.  **RoBERTa (Robustly Optimized BERT Approach)**: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.\n        \n    3.  **DistilBERT**: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.\n        \n    4.  **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.\n        \n*   WordPiece is chosen in these models due to its efficiency in handling a large vocabulary by breaking down words into smaller, manageable sub-words or tokens. This method allows for better handling of rare words and morphological variations in different languages.\n    \n\nSome models that utilize WordPiece include:\n\n1.  **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.\n    \n2.  **RoBERTa (Robustly Optimized BERT Approach)**: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.\n    \n3.  **DistilBERT**: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.\n    \n4.  **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.\n    \n\n**BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT uses WordPiece for tokenization, enabling it to effectively handle a wide range of linguistic structures and vocabularies.\n\n**RoBERTa (Robustly Optimized BERT Approach)**: As an optimized version of BERT, RoBERTa also employs WordPiece encoding. It’s designed for more robust performance on natural language processing tasks.\n\n**DistilBERT**: This is a smaller, faster, and lighter version of BERT that retains most of its performance. DistilBERT uses WordPiece for its tokenization.\n\n**ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu, ERNIE is a language model that builds on the BERT architecture and uses WordPiece encoding.\n\nWordPiece is chosen in these models due to its efficiency in handling a large vocabulary by breaking down words into smaller, manageable sub-words or tokens. This method allows for better handling of rare words and morphological variations in different languages.",
    "contentLength": 3480,
    "wordCount": 405,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#wordpiece"
  },
  {
    "id": "ai-tokenizer-bpe-11",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Use-cases",
    "title": "BPE",
    "order": 11,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Some models that employ BPE include:</p>\n\n    <ol>\n      <li>\n        <p><strong>Llama</strong>: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) <a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">(source)</a>.</p>\n      </li>\n      <li>\n        <p><strong>GPT Series (up until GPT-2)</strong>: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found <a href=\"https://platform.openai.com/tokenizer\">here</a>. Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use <a href=\"#tiktoken\">tiktoken</a>.</p>\n      </li>\n      <li>\n        <p><strong>XLNet</strong>: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.</p>\n      </li>\n      <li>\n        <p><strong>Fairseq’s RoBERTa</strong>: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.</p>\n      </li>\n      <li>\n        <p><strong>BART (Bidirectional and Auto-Regressive Transformers)</strong>: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>BPE is chosen in these models due to its ability to effectively handle a diverse set of languages and scripts, and its efficiency in managing a large and complex vocabulary, which is particularly useful in generative tasks.</p>\n  </li>\n</ul>\n<p>Some models that employ BPE include:</p>\n<ol>\n      <li>\n        <p><strong>Llama</strong>: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) <a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">(source)</a>.</p>\n      </li>\n      <li>\n        <p><strong>GPT Series (up until GPT-2)</strong>: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found <a href=\"https://platform.openai.com/tokenizer\">here</a>. Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use <a href=\"#tiktoken\">tiktoken</a>.</p>\n      </li>\n      <li>\n        <p><strong>XLNet</strong>: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.</p>\n      </li>\n      <li>\n        <p><strong>Fairseq’s RoBERTa</strong>: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.</p>\n      </li>\n      <li>\n        <p><strong>BART (Bidirectional and Auto-Regressive Transformers)</strong>: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.</p>\n      </li>\n    </ol>\n<p><strong>Llama</strong>: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) <a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">(source)</a>.</p>\n<p><strong>GPT Series (up until GPT-2)</strong>: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found <a href=\"https://platform.openai.com/tokenizer\">here</a>. Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use <a href=\"#tiktoken\">tiktoken</a>.</p>\n<p><strong>XLNet</strong>: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.</p>\n<p><strong>Fairseq’s RoBERTa</strong>: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.</p>\n<p><strong>BART (Bidirectional and Auto-Regressive Transformers)</strong>: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.</p>\n<p>BPE is chosen in these models due to its ability to effectively handle a diverse set of languages and scripts, and its efficiency in managing a large and complex vocabulary, which is particularly useful in generative tasks.</p>",
    "contentMarkdown": "*   Some models that employ BPE include:\n    \n    1.  **Llama**: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) [(source)](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n        \n    2.  **GPT Series (up until GPT-2)**: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found [here](https://platform.openai.com/tokenizer). Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use [tiktoken](#tiktoken).\n        \n    3.  **XLNet**: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.\n        \n    4.  **Fairseq’s RoBERTa**: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.\n        \n    5.  **BART (Bidirectional and Auto-Regressive Transformers)**: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.\n        \n*   BPE is chosen in these models due to its ability to effectively handle a diverse set of languages and scripts, and its efficiency in managing a large and complex vocabulary, which is particularly useful in generative tasks.\n    \n\nSome models that employ BPE include:\n\n1.  **Llama**: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) [(source)](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n    \n2.  **GPT Series (up until GPT-2)**: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found [here](https://platform.openai.com/tokenizer). Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use [tiktoken](#tiktoken).\n    \n3.  **XLNet**: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.\n    \n4.  **Fairseq’s RoBERTa**: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.\n    \n5.  **BART (Bidirectional and Auto-Regressive Transformers)**: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.\n    \n\n**Llama**: Llama, developed by Meta AI, use BPE for their tokenization, allowing it to handle diverse linguistic structures effectively. Recent Llama models (3 and 3.1) [(source)](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**GPT Series (up until GPT-2)**: Developed by OpenAI, the GPT models also use BPE, enabling efficient management of a vast range of vocabulary and linguistic structures. OpenAI’s tokenizer for GPT-3.5 and GPT-4 can be found [here](https://platform.openai.com/tokenizer). Recent GPT models (GPT-3 and GPT-4 series models, including GPT-4-turbo and GPT-4o) use [tiktoken](#tiktoken).\n\n**XLNet**: A transformer-based model that surpasses BERT in some benchmarks, XLNet utilizes BPE for its tokenization process.\n\n**Fairseq’s RoBERTa**: While the original RoBERTa model from Facebook AI uses BPE, variations in implementations across different platforms might use different tokenization methods.\n\n**BART (Bidirectional and Auto-Regressive Transformers)**: Created by Facebook AI, BART, which combines features of auto-regressive models like GPT and auto-encoding models like BERT, uses BPE for tokenization.\n\nBPE is chosen in these models due to its ability to effectively handle a diverse set of languages and scripts, and its efficiency in managing a large and complex vocabulary, which is particularly useful in generative tasks.",
    "contentLength": 4801,
    "wordCount": 527,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#bpe"
  },
  {
    "id": "ai-tokenizer-sentencepiece-12",
    "articleSlug": "tokenizer",
    "articleTitle": "Tokenization",
    "category": "NLP/LLMs",
    "chapter": "Use-cases",
    "title": "SentencePiece",
    "order": 12,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Notable models utilizing SentencePiece encoding include:</p>\n\n    <ol>\n      <li><strong>Llama 2</strong>: Llama 2 uses SentencePiece for tokenization <a href=\"https://github.com/meta-llama/llama/blob/main/llama/tokenizer.py\">(source)</a>.</li>\n      <li><strong>T5 (Text-to-Text Transfer Transformer)</strong>: Developed by Google, T5 uses SentencePiece for its text processing, enabling it to handle a wide range of languages and tasks effectively.</li>\n      <li><strong>mT5</strong>: As an extension of T5, mT5 (multilingual T5) is specifically designed for multilingual capabilities, employing SentencePiece to process text across multiple languages.</li>\n      <li><strong>ALBERT (A Lite BERT)</strong>: This model, a lighter version of BERT (Bidirectional Encoder Representations from Transformers), uses SentencePiece. It’s optimized for efficiency and lower memory consumption.</li>\n      <li><strong>XLNet</strong>: An advanced transformer model that surpasses BERT in some respects, XLNet also uses SentencePiece for tokenization.</li>\n    </ol>\n  </li>\n  <li>\n    <p>SentencePiece is favored in these models for its versatility and efficiency in dealing with various languages, including those with non-Latin scripts, and for its ability to tokenize raw text directly without the need for pre-tokenization.</p>\n  </li>\n</ul>\n<p>Notable models utilizing SentencePiece encoding include:</p>\n<ol>\n      <li><strong>Llama 2</strong>: Llama 2 uses SentencePiece for tokenization <a href=\"https://github.com/meta-llama/llama/blob/main/llama/tokenizer.py\">(source)</a>.</li>\n      <li><strong>T5 (Text-to-Text Transfer Transformer)</strong>: Developed by Google, T5 uses SentencePiece for its text processing, enabling it to handle a wide range of languages and tasks effectively.</li>\n      <li><strong>mT5</strong>: As an extension of T5, mT5 (multilingual T5) is specifically designed for multilingual capabilities, employing SentencePiece to process text across multiple languages.</li>\n      <li><strong>ALBERT (A Lite BERT)</strong>: This model, a lighter version of BERT (Bidirectional Encoder Representations from Transformers), uses SentencePiece. It’s optimized for efficiency and lower memory consumption.</li>\n      <li><strong>XLNet</strong>: An advanced transformer model that surpasses BERT in some respects, XLNet also uses SentencePiece for tokenization.</li>\n    </ol>\n<p>SentencePiece is favored in these models for its versatility and efficiency in dealing with various languages, including those with non-Latin scripts, and for its ability to tokenize raw text directly without the need for pre-tokenization.</p>",
    "contentMarkdown": "*   Notable models utilizing SentencePiece encoding include:\n    \n    1.  **Llama 2**: Llama 2 uses SentencePiece for tokenization [(source)](https://github.com/meta-llama/llama/blob/main/llama/tokenizer.py).\n    2.  **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 uses SentencePiece for its text processing, enabling it to handle a wide range of languages and tasks effectively.\n    3.  **mT5**: As an extension of T5, mT5 (multilingual T5) is specifically designed for multilingual capabilities, employing SentencePiece to process text across multiple languages.\n    4.  **ALBERT (A Lite BERT)**: This model, a lighter version of BERT (Bidirectional Encoder Representations from Transformers), uses SentencePiece. It’s optimized for efficiency and lower memory consumption.\n    5.  **XLNet**: An advanced transformer model that surpasses BERT in some respects, XLNet also uses SentencePiece for tokenization.\n*   SentencePiece is favored in these models for its versatility and efficiency in dealing with various languages, including those with non-Latin scripts, and for its ability to tokenize raw text directly without the need for pre-tokenization.\n    \n\nNotable models utilizing SentencePiece encoding include:\n\n1.  **Llama 2**: Llama 2 uses SentencePiece for tokenization [(source)](https://github.com/meta-llama/llama/blob/main/llama/tokenizer.py).\n2.  **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 uses SentencePiece for its text processing, enabling it to handle a wide range of languages and tasks effectively.\n3.  **mT5**: As an extension of T5, mT5 (multilingual T5) is specifically designed for multilingual capabilities, employing SentencePiece to process text across multiple languages.\n4.  **ALBERT (A Lite BERT)**: This model, a lighter version of BERT (Bidirectional Encoder Representations from Transformers), uses SentencePiece. It’s optimized for efficiency and lower memory consumption.\n5.  **XLNet**: An advanced transformer model that surpasses BERT in some respects, XLNet also uses SentencePiece for tokenization.\n\nSentencePiece is favored in these models for its versatility and efficiency in dealing with various languages, including those with non-Latin scripts, and for its ability to tokenize raw text directly without the need for pre-tokenization.",
    "contentLength": 2657,
    "wordCount": 296,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/tokenizer/#sentencepiece"
  }
]