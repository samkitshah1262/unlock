[
  {
    "id": "ai-top-30-papers-the-first-law-of-complexodynamics-1",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "The First Law of Complexodynamics",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Author: Scott Aaronson</li>\n  <li>The article “The First Law of Complexodynamics” discusses an intriguing question posed by Sean Carroll at the FQXi’s Setting Time Aright conference, which brought together experts from various fields to discuss the nature of time. Carroll’s question revolves around why the complexity of physical systems seems to increase, hit a maximum, and then decrease over time, unlike entropy, which consistently increases.</li>\n  <li>The article explains that entropy measures how disordered a system is and increases monotonically. However, complexity behaves differently, peaking at intermediate times before decreasing. To delve into this phenomenon, the author introduces concepts from Kolmogorov complexity. Kolmogorov complexity is defined as the length of the shortest computer program that can produce a given string. A related concept, sophistication, measures the complexity of a string as the shortest program describing a set of which the string is a typical member.</li>\n  <li>To address Carroll’s question, the author proposes the concept of “complextropy” as a measure of complexity that considers computational resource bounds. Complextropy should reflect the number of bits in the shortest efficient program that outputs a sample from a set such that the target string appears random with respect to that set. The conjecture is that complextropy will be small at the beginning and end of a system’s evolution but large at intermediate times, mirroring the observed pattern in complexity.</li>\n  <li>Proving this conjecture, either theoretically or empirically, presents challenges, particularly due to the difficulty of computing complextropy. One practical approach suggested is using the size of a gzip compressed file as an approximation for Kolmogorov complexity. The author mentions an ongoing research project aimed at empirically verifying the conjecture using this method.</li>\n  <li>The article also the idea that complexity, or complextropy, changes over time, peaking at intermediate stages. The author suggests using computational resource bounds to define this measure and discusses both theoretical and empirical approaches to validating the conjecture that complexity behaves in this manner. This exploration provides valuable insights into understanding the dynamic nature of complexity in physical systems.</li>\n</ul>",
    "contentMarkdown": "*   Author: Scott Aaronson\n*   The article “The First Law of Complexodynamics” discusses an intriguing question posed by Sean Carroll at the FQXi’s Setting Time Aright conference, which brought together experts from various fields to discuss the nature of time. Carroll’s question revolves around why the complexity of physical systems seems to increase, hit a maximum, and then decrease over time, unlike entropy, which consistently increases.\n*   The article explains that entropy measures how disordered a system is and increases monotonically. However, complexity behaves differently, peaking at intermediate times before decreasing. To delve into this phenomenon, the author introduces concepts from Kolmogorov complexity. Kolmogorov complexity is defined as the length of the shortest computer program that can produce a given string. A related concept, sophistication, measures the complexity of a string as the shortest program describing a set of which the string is a typical member.\n*   To address Carroll’s question, the author proposes the concept of “complextropy” as a measure of complexity that considers computational resource bounds. Complextropy should reflect the number of bits in the shortest efficient program that outputs a sample from a set such that the target string appears random with respect to that set. The conjecture is that complextropy will be small at the beginning and end of a system’s evolution but large at intermediate times, mirroring the observed pattern in complexity.\n*   Proving this conjecture, either theoretically or empirically, presents challenges, particularly due to the difficulty of computing complextropy. One practical approach suggested is using the size of a gzip compressed file as an approximation for Kolmogorov complexity. The author mentions an ongoing research project aimed at empirically verifying the conjecture using this method.\n*   The article also the idea that complexity, or complextropy, changes over time, peaking at intermediate stages. The author suggests using computational resource bounds to define this measure and discusses both theoretical and empirical approaches to validating the conjecture that complexity behaves in this manner. This exploration provides valuable insights into understanding the dynamic nature of complexity in physical systems.",
    "contentLength": 2387,
    "wordCount": 340,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#the-first-law-of-complexodynamics"
  },
  {
    "id": "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "The Unreasonable Effectiveness of Recurrent Neural Networks",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Author: Andrej Karpathy</li>\n  <li>The article “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy dives into the amazing abilities of Recurrent Neural Networks (RNNs). Karpathy talks about his first experience with training RNNs for image captioning, where even with random settings, the RNN started making believable image descriptions. This success was surprising because many people thought RNNs were hard to train, showing just how simple and powerful they can be.</li>\n  <li>RNNs are special because they can handle sequences of vectors, making them perfect for tasks that involve sequences as input and output. Unlike regular neural networks that deal with fixed-size inputs and outputs, RNNs can work with sequences of any length, making them very useful in many areas. Karpathy explains that RNNs work by keeping a hidden state that stores information from previous inputs, allowing them to “remember” past data.</li>\n  <li>Karpathy goes into detail about how RNNs work, including a simple interface where an input vector affects the output vector and considers all previous inputs. He shows how RNNs update their hidden state using matrix multiplications and non-linear functions. He also mentions Long Short-Term Memory (LSTM) networks, which are a more advanced type of RNN that solve some practical issues and are widely used.</li>\n  <li>To show how powerful RNNs can be, Karpathy describes training character-level language models. By feeding a large amount of text into an RNN, it learns to predict the next character in a sequence, allowing it to create text one character at a time. He gives examples of RNN-generated text from different sources, like Paul Graham’s essays, Shakespeare’s works, Wikipedia articles, algebraic geometry in LaTeX, Linux source code, and baby names. These examples show how RNNs can learn complex structures, grammar, and context from raw text.</li>\n  <li>Karpathy also talks about the training process and how the text generated by the RNN improves over time, showing how the model gradually gets better at understanding language. He visualizes the inner workings of the RNN, showing how different neurons react to specific patterns, like URLs or markdown syntax, which helps explain how the model learns.</li>\n  <li>Finally, Karpathy encourages readers to try out RNNs using the code he shared on GitHub, highlighting the fun and educational aspects of training character-level language models. He briefly touches on the bigger picture of RNN research and their growing importance in fields like natural language processing, computer vision, and machine learning. The article wraps up with a fun note, showing an RNN-generated sample from the article itself, proving how effective and versatile RNNs are.</li>\n</ul>",
    "contentMarkdown": "*   Author: Andrej Karpathy\n*   The article “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy dives into the amazing abilities of Recurrent Neural Networks (RNNs). Karpathy talks about his first experience with training RNNs for image captioning, where even with random settings, the RNN started making believable image descriptions. This success was surprising because many people thought RNNs were hard to train, showing just how simple and powerful they can be.\n*   RNNs are special because they can handle sequences of vectors, making them perfect for tasks that involve sequences as input and output. Unlike regular neural networks that deal with fixed-size inputs and outputs, RNNs can work with sequences of any length, making them very useful in many areas. Karpathy explains that RNNs work by keeping a hidden state that stores information from previous inputs, allowing them to “remember” past data.\n*   Karpathy goes into detail about how RNNs work, including a simple interface where an input vector affects the output vector and considers all previous inputs. He shows how RNNs update their hidden state using matrix multiplications and non-linear functions. He also mentions Long Short-Term Memory (LSTM) networks, which are a more advanced type of RNN that solve some practical issues and are widely used.\n*   To show how powerful RNNs can be, Karpathy describes training character-level language models. By feeding a large amount of text into an RNN, it learns to predict the next character in a sequence, allowing it to create text one character at a time. He gives examples of RNN-generated text from different sources, like Paul Graham’s essays, Shakespeare’s works, Wikipedia articles, algebraic geometry in LaTeX, Linux source code, and baby names. These examples show how RNNs can learn complex structures, grammar, and context from raw text.\n*   Karpathy also talks about the training process and how the text generated by the RNN improves over time, showing how the model gradually gets better at understanding language. He visualizes the inner workings of the RNN, showing how different neurons react to specific patterns, like URLs or markdown syntax, which helps explain how the model learns.\n*   Finally, Karpathy encourages readers to try out RNNs using the code he shared on GitHub, highlighting the fun and educational aspects of training character-level language models. He briefly touches on the bigger picture of RNN research and their growing importance in fields like natural language processing, computer vision, and machine learning. The article wraps up with a fun note, showing an RNN-generated sample from the article itself, proving how effective and versatile RNNs are.",
    "contentLength": 2803,
    "wordCount": 427,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#the-unreasonable-effectiveness-of-recurrent-neural-networks"
  },
  {
    "id": "ai-top-30-papers-understanding-lstm-networks-3",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Understanding LSTM Networks",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Author: Christopher Olah</li>\n  <li>The article “Understanding LSTM Networks” by Christopher Olah explains the structure and functioning of Long Short-Term Memory (LSTM) networks, a special kind of Recurrent Neural Network (RNN) that addresses the limitations of traditional RNNs in handling long-term dependencies.</li>\n  <li>Olah begins by highlighting the limitations of traditional neural networks and RNNs in maintaining persistent information, which is crucial for tasks involving sequences and lists, such as language modeling, translation, and speech recognition.</li>\n  <li>RNNs have loops that allow information to persist, making them suitable for sequential data. However, they struggle with long-term dependencies, where relevant information from earlier inputs is needed much later in the sequence.</li>\n  <li>The article introduces LSTMs, designed to overcome this limitation. LSTMs have a unique architecture that includes a cell state and three gates (input, forget, and output) that regulate the flow of information. These gates allow LSTMs to remember and forget information selectively, making them effective in learning long-term dependencies.</li>\n  <li>The forget gate decides what information to discard from the cell state, the input gate determines which new information to add, and the output gate controls what information is passed to the next step.</li>\n  <li>Olah explains the step-by-step functioning of LSTMs using diagrams and notations, making it easier to understand the complex interactions within the network. He also discusses variations of LSTMs, such as peephole connections and Gated Recurrent Units (GRUs), which offer different ways to handle long-term dependencies.</li>\n  <li>The article concludes by emphasizing the significance of LSTMs in achieving remarkable results in various applications and hints at future advancements in RNN research, such as attention mechanisms and Grid LSTMs, which further enhance the capabilities of neural networks.</li>\n</ul>",
    "contentMarkdown": "*   Author: Christopher Olah\n*   The article “Understanding LSTM Networks” by Christopher Olah explains the structure and functioning of Long Short-Term Memory (LSTM) networks, a special kind of Recurrent Neural Network (RNN) that addresses the limitations of traditional RNNs in handling long-term dependencies.\n*   Olah begins by highlighting the limitations of traditional neural networks and RNNs in maintaining persistent information, which is crucial for tasks involving sequences and lists, such as language modeling, translation, and speech recognition.\n*   RNNs have loops that allow information to persist, making them suitable for sequential data. However, they struggle with long-term dependencies, where relevant information from earlier inputs is needed much later in the sequence.\n*   The article introduces LSTMs, designed to overcome this limitation. LSTMs have a unique architecture that includes a cell state and three gates (input, forget, and output) that regulate the flow of information. These gates allow LSTMs to remember and forget information selectively, making them effective in learning long-term dependencies.\n*   The forget gate decides what information to discard from the cell state, the input gate determines which new information to add, and the output gate controls what information is passed to the next step.\n*   Olah explains the step-by-step functioning of LSTMs using diagrams and notations, making it easier to understand the complex interactions within the network. He also discusses variations of LSTMs, such as peephole connections and Gated Recurrent Units (GRUs), which offer different ways to handle long-term dependencies.\n*   The article concludes by emphasizing the significance of LSTMs in achieving remarkable results in various applications and hints at future advancements in RNN research, such as attention mechanisms and Grid LSTMs, which further enhance the capabilities of neural networks.",
    "contentLength": 2016,
    "wordCount": 282,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#understanding-lstm-networks"
  },
  {
    "id": "ai-top-30-papers-recurrent-neural-network-regularization-4",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Recurrent Neural Network Regularization",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals</li>\n  <li>The paper “Recurrent Neural Network Regularization” presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections, which hampers learning. The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the network’s ability to retain information over long sequences while reducing overfitting.</li>\n  <li>The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved better word-level perplexity on the Penn Tree Bank dataset compared to non-regularized models. The medium and large regularized LSTMs showed substantial reductions in perplexity, highlighting the efficacy of the proposed method.</li>\n  <li>For speech recognition, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves frame accuracy, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved better generalization, indicating the potential of the proposed regularization technique for improving acoustic modeling.</li>\n  <li>In machine translation, the method was evaluated on the WMT’14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating higher BLEU scores, which measure translation quality. Although the regularized LSTM did not surpass the phrase-based LIUM SMT system, the results affirmed that dropout enhances translation performance.</li>\n  <li>The image caption generation task involved testing the dropout variant on an LSTM model that converts image vectors into captions. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps improve caption quality, with regularized models performing comparably to model ensembles.</li>\n  <li>Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.</li>\n</ul>",
    "contentMarkdown": "*   Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals\n*   The paper “Recurrent Neural Network Regularization” presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections, which hampers learning. The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the network’s ability to retain information over long sequences while reducing overfitting.\n*   The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved better word-level perplexity on the Penn Tree Bank dataset compared to non-regularized models. The medium and large regularized LSTMs showed substantial reductions in perplexity, highlighting the efficacy of the proposed method.\n*   For speech recognition, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves frame accuracy, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved better generalization, indicating the potential of the proposed regularization technique for improving acoustic modeling.\n*   In machine translation, the method was evaluated on the WMT’14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating higher BLEU scores, which measure translation quality. Although the regularized LSTM did not surpass the phrase-based LIUM SMT system, the results affirmed that dropout enhances translation performance.\n*   The image caption generation task involved testing the dropout variant on an LSTM model that converts image vectors into captions. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps improve caption quality, with regularized models performing comparably to model ensembles.\n*   Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.",
    "contentLength": 2500,
    "wordCount": 323,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#recurrent-neural-network-regularization"
  },
  {
    "id": "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Keeping Neural Networks Simple by Minimizing the Description Length of the Weights",
    "order": 5,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>Authors: Geoffrey E. Hinton and Drew van Camp</li>\n  <li>The paper “Keeping Neural Networks Simple by Minimizing the Description Length of the Weights” by Hinton and van Camp introduces a method to regularize neural networks by penalizing the information content in the weights. The key idea is to add Gaussian noise to the weights and adapt the noise level during training to balance the trade-off between the network’s error and the complexity of the weights.</li>\n  <li>The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.</li>\n  <li>By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the network’s performance while keeping the weights as simple as possible.</li>\n  <li>The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.</li>\n  <li>The authors introduce the concept of “noisy weights” where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the network’s error.</li>\n  <li>The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.</li>\n  <li>Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the network’s ability to generalize from limited data.</li>\n  <li>Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.</li>\n  <li>The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.</li>\n</ul>",
    "contentMarkdown": "*   Authors: Geoffrey E. Hinton and Drew van Camp\n*   The paper “Keeping Neural Networks Simple by Minimizing the Description Length of the Weights” by Hinton and van Camp introduces a method to regularize neural networks by penalizing the information content in the weights. The key idea is to add Gaussian noise to the weights and adapt the noise level during training to balance the trade-off between the network’s error and the complexity of the weights.\n*   The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.\n*   By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the network’s performance while keeping the weights as simple as possible.\n*   The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.\n*   The authors introduce the concept of “noisy weights” where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the network’s error.\n*   The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.\n*   Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the network’s ability to generalize from limited data.\n*   Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.\n*   The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.",
    "contentLength": 2985,
    "wordCount": 424,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights"
  },
  {
    "id": "ai-top-30-papers-pointer-networks-6",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Pointer Networks",
    "order": 6,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly</li>\n  <li>\n    <p>The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.</li>\n      <li>Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.</li>\n      <li>The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.</li>\n    </ul>\n  </li>\n  <li><strong>Models:</strong>\n    <ul>\n      <li><strong>Sequence-to-Sequence Model:</strong> This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.</li>\n      <li><strong>Content Based Input Attention:</strong> An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.</li>\n      <li><strong>Pointer Networks (Ptr-Net):</strong> Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.</li>\n    </ul>\n  </li>\n  <li><strong>Empirical Results:</strong>\n    <ul>\n      <li><strong>Convex Hull:</strong> Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.</li>\n      <li><strong>Delaunay Triangulation:</strong> Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.</li>\n      <li><strong>Travelling Salesman Problem (TSP):</strong> Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.</p>\n<ul>\n      <li>The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.</li>\n      <li>Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.</li>\n      <li>The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.</li>\n    </ul>\n<ul>\n      <li><strong>Sequence-to-Sequence Model:</strong> This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.</li>\n      <li><strong>Content Based Input Attention:</strong> An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.</li>\n      <li><strong>Pointer Networks (Ptr-Net):</strong> Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.</li>\n    </ul>\n<ul>\n      <li><strong>Convex Hull:</strong> Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.</li>\n      <li><strong>Delaunay Triangulation:</strong> Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.</li>\n      <li><strong>Travelling Salesman Problem (TSP):</strong> Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.</li>\n    </ul>\n<ul>\n      <li>The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly\n*   The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.\n    \n*   **Key Contributions:**\n    *   The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.\n    *   Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.\n    *   The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.\n*   **Models:**\n    *   **Sequence-to-Sequence Model:** This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.\n    *   **Content Based Input Attention:** An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.\n    *   **Pointer Networks (Ptr-Net):** Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.\n*   **Empirical Results:**\n    *   **Convex Hull:** Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.\n    *   **Delaunay Triangulation:** Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.\n    *   **Travelling Salesman Problem (TSP):** Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.\n*   **Conclusion:**\n    *   The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.\n\nThe paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.\n\n*   The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.\n*   Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.\n*   The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.\n\n*   **Sequence-to-Sequence Model:** This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.\n*   **Content Based Input Attention:** An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.\n*   **Pointer Networks (Ptr-Net):** Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.\n\n*   **Convex Hull:** Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.\n*   **Delaunay Triangulation:** Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.\n*   **Travelling Salesman Problem (TSP):** Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.\n\n*   The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.",
    "contentLength": 8218,
    "wordCount": 1013,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#pointer-networks"
  },
  {
    "id": "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "ImageNet Classification with Deep Convolutional Neural Networks",
    "order": 7,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</li>\n  <li>\n    <p>The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.</li>\n      <li>To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.</li>\n      <li>The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.</li>\n      <li>Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.</li>\n      <li>Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.</li>\n      <li>To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.</li>\n      <li>Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.</li>\n    </ul>\n  </li>\n  <li><strong>Empirical Results:</strong>\n    <ul>\n      <li>On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.</li>\n      <li>On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.</li>\n      <li>Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.</li>\n      <li>The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.</p>\n<ul>\n      <li>The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.</li>\n      <li>To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.</li>\n      <li>The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.</li>\n      <li>Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.</li>\n      <li>Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.</li>\n      <li>To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.</li>\n      <li>Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.</li>\n    </ul>\n<ul>\n      <li>On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.</li>\n      <li>On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.</li>\n      <li>Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.</li>\n    </ul>\n<ul>\n      <li>The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.</li>\n      <li>The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n*   The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.\n    \n*   **Key Contributions:**\n    *   The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.\n    *   To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.\n    *   The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.\n    *   Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.\n    *   Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.\n    *   To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.\n    *   Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.\n*   **Empirical Results:**\n    *   On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.\n    *   On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.\n    *   Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.\n*   **Conclusion:**\n    *   The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.\n    *   The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.\n\nThe paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.\n\n*   The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.\n*   To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.\n*   The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.\n*   Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.\n*   Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.\n*   To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.\n*   Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.\n\n*   On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.\n*   On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.\n*   Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.\n\n*   The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.\n*   The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.",
    "contentLength": 7624,
    "wordCount": 1020,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#imagenet-classification-with-deep-convolutional-neural-networks"
  },
  {
    "id": "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Order Matters: Sequence to Sequence for Sets",
    "order": 8,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur</li>\n  <li>\n    <p>The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.</li>\n      <li>They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.</li>\n      <li>For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.</li>\n    </ul>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ul>\n      <li><strong>Language Modeling:</strong> The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.</li>\n      <li><strong>Combinatorial Problems:</strong> The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.</li>\n      <li><strong>Graphical Models:</strong> The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.</li>\n    </ul>\n  </li>\n  <li><strong>Model Architecture:</strong>\n    <ul>\n      <li><strong>Read, Process, Write Model:</strong> The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.</li>\n      <li><strong>Attention Mechanisms:</strong> The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.</li>\n      <li><strong>Finding Optimal Orderings:</strong> To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.</p>\n<ul>\n      <li>The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.</li>\n      <li>They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.</li>\n      <li>For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.</li>\n    </ul>\n<ul>\n      <li><strong>Language Modeling:</strong> The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.</li>\n      <li><strong>Combinatorial Problems:</strong> The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.</li>\n      <li><strong>Graphical Models:</strong> The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.</li>\n    </ul>\n<ul>\n      <li><strong>Read, Process, Write Model:</strong> The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.</li>\n      <li><strong>Attention Mechanisms:</strong> The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.</li>\n      <li><strong>Finding Optimal Orderings:</strong> To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.</li>\n    </ul>\n<ul>\n      <li>The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur\n*   The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.\n    \n*   **Key Contributions:**\n    *   The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.\n    *   They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.\n    *   For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.\n*   **Experiments and Results:**\n    *   **Language Modeling:** The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.\n    *   **Combinatorial Problems:** The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.\n    *   **Graphical Models:** The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.\n*   **Model Architecture:**\n    *   **Read, Process, Write Model:** The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.\n    *   **Attention Mechanisms:** The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.\n    *   **Finding Optimal Orderings:** To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.\n*   **Conclusion:**\n    *   The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.\n\nThe paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.\n\n*   The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.\n*   They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.\n*   For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.\n\n*   **Language Modeling:** The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.\n*   **Combinatorial Problems:** The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.\n*   **Graphical Models:** The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.\n\n*   **Read, Process, Write Model:** The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.\n*   **Attention Mechanisms:** The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.\n*   **Finding Optimal Orderings:** To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.\n\n*   The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.",
    "contentLength": 7914,
    "wordCount": 1047,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#order-matters:-sequence-to-sequence-for-sets"
  },
  {
    "id": "ai-top-30-papers-gpipe-easy-scaling-with-micro-batch-pipeline-paral-9",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism",
    "order": 9,
    "orderInChapter": 9,
    "contentHtml": "<ul>\n  <li>Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen</li>\n  <li>\n    <p>The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li><strong>GPipe Architecture:</strong> The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.</li>\n      <li><strong>Batch-Splitting Pipeline Parallelism:</strong> GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.</li>\n      <li><strong>Synchronous Gradient Descent:</strong> The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.</li>\n    </ul>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ul>\n      <li><strong>Image Classification:</strong> GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.</li>\n      <li><strong>Multilingual Neural Machine Translation:</strong> GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.</li>\n    </ul>\n  </li>\n  <li><strong>Performance Optimization:</strong>\n    <ul>\n      <li><strong>Re-materialization:</strong> To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.</li>\n      <li><strong>Load Balancing:</strong> The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.</li>\n    </ul>\n  </li>\n  <li><strong>Design Features and Trade-Offs:</strong>\n    <ul>\n      <li><strong>Flexibility:</strong> GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.</li>\n      <li><strong>Efficiency:</strong> By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.</li>\n      <li><strong>Training Stability:</strong> The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.</p>\n<ul>\n      <li><strong>GPipe Architecture:</strong> The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.</li>\n      <li><strong>Batch-Splitting Pipeline Parallelism:</strong> GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.</li>\n      <li><strong>Synchronous Gradient Descent:</strong> The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.</li>\n    </ul>\n<ul>\n      <li><strong>Image Classification:</strong> GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.</li>\n      <li><strong>Multilingual Neural Machine Translation:</strong> GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.</li>\n    </ul>\n<ul>\n      <li><strong>Re-materialization:</strong> To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.</li>\n      <li><strong>Load Balancing:</strong> The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.</li>\n    </ul>\n<ul>\n      <li><strong>Flexibility:</strong> GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.</li>\n      <li><strong>Efficiency:</strong> By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.</li>\n      <li><strong>Training Stability:</strong> The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.</li>\n    </ul>\n<ul>\n      <li>The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen\n*   The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.\n    \n*   **Key Contributions:**\n    *   **GPipe Architecture:** The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.\n    *   **Batch-Splitting Pipeline Parallelism:** GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.\n    *   **Synchronous Gradient Descent:** The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.\n*   **Experiments and Results:**\n    *   **Image Classification:** GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.\n    *   **Multilingual Neural Machine Translation:** GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.\n*   **Performance Optimization:**\n    *   **Re-materialization:** To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.\n    *   **Load Balancing:** The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.\n*   **Design Features and Trade-Offs:**\n    *   **Flexibility:** GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.\n    *   **Efficiency:** By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.\n    *   **Training Stability:** The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.\n*   **Conclusion:**\n    *   The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.\n\nThe paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.\n\n*   **GPipe Architecture:** The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.\n*   **Batch-Splitting Pipeline Parallelism:** GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.\n*   **Synchronous Gradient Descent:** The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.\n\n*   **Image Classification:** GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.\n*   **Multilingual Neural Machine Translation:** GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.\n\n*   **Re-materialization:** To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.\n*   **Load Balancing:** The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.\n\n*   **Flexibility:** GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.\n*   **Efficiency:** By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.\n*   **Training Stability:** The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.\n\n*   The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.",
    "contentLength": 8232,
    "wordCount": 968,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#gpipe:-easy-scaling-with-micro-batch-pipeline-parallelism"
  },
  {
    "id": "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Deep Residual Learning for Image Recognition",
    "order": 10,
    "orderInChapter": 10,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>\n  <li>\n    <p><strong>Affiliation</strong>: Microsoft Research</p>\n  </li>\n  <li>This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.</li>\n</ul>\n<p><strong>Affiliation</strong>: Microsoft Research</p>\n<p>Key innovations and findings from the paper include:</p>\n<ol>\n  <li><strong>Residual Learning Framework</strong>: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.</li>\n  <li><strong>Ease of Optimization</strong>: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.</li>\n  <li><strong>Superior Performance on Deep Networks</strong>: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.</li>\n  <li><strong>Broad Applicability</strong>: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.</li>\n</ol>\n<ul>\n  <li>These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.</li>\n</ul>",
    "contentMarkdown": "*   **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n*   **Affiliation**: Microsoft Research\n    \n*   This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.\n\n**Affiliation**: Microsoft Research\n\nKey innovations and findings from the paper include:\n\n1.  **Residual Learning Framework**: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.\n2.  **Ease of Optimization**: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.\n3.  **Superior Performance on Deep Networks**: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.\n4.  **Broad Applicability**: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.\n\n*   These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.",
    "contentLength": 2060,
    "wordCount": 260,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#deep-residual-learning-for-image-recognition"
  },
  {
    "id": "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
    "order": 11,
    "orderInChapter": 11,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Fisher Yu, Vladlen Koltun</li>\n  <li>\n    <p><strong>Affiliations</strong>: Princeton University, Intel Labs</p>\n  </li>\n  <li>\n    <p>The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Dilated Convolutions</strong>:</li>\n    </ol>\n    <ul>\n      <li>Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.</li>\n      <li>Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Princeton University, Intel Labs</p>\n<p>The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.</p>\n<ol>\n      <li><strong>Dilated Convolutions</strong>:</li>\n    </ol>\n<ul>\n      <li>Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.</li>\n      <li>Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.</li>\n    </ul>\n<ol>\n  <li><strong>Multi-Scale Context Aggregation</strong>:\n    <ul>\n      <li>Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.</li>\n      <li>The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.</li>\n    </ul>\n  </li>\n  <li><strong>Simplified Network Design</strong>:\n    <ul>\n      <li>Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.</li>\n      <li>Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.</li>\n    </ul>\n  </li>\n  <li><strong>Controlled Experiments</strong>:\n    <ul>\n      <li>Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.</li>\n      <li>Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.</li>\n    </ul>\n  </li>\n  <li><strong>Performance Improvement</strong>:\n    <ul>\n      <li>The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.</li>\n      <li>The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.</li>\n      <li>The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.</li>\n    </ul>\n<ul>\n      <li>Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.</li>\n      <li>Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.</li>\n    </ul>\n<ul>\n      <li>Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.</li>\n      <li>Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.</li>\n    </ul>\n<ul>\n      <li>The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.</li>\n      <li>The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.</li>\n    </ul>\n<ul>\n  <li>Experiments:\n    <ul>\n      <li><strong>Dataset</strong>: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.</li>\n      <li><strong>Training Procedure</strong>: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.</li>\n      <li><strong>Evaluation</strong>: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.</li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.</li>\n      <li>The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.</li>\n      <li>The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Dataset</strong>: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.</li>\n      <li><strong>Training Procedure</strong>: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.</li>\n      <li><strong>Evaluation</strong>: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.</li>\n    </ul>\n<ul>\n      <li>The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.</li>\n      <li>The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.</li>\n      <li>The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.</li>\n    </ul>",
    "contentMarkdown": "*   **Authors**: Fisher Yu, Vladlen Koltun\n*   **Affiliations**: Princeton University, Intel Labs\n    \n*   The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.\n    \n*   Key Contributions:\n    \n    1.  **Dilated Convolutions**:\n    \n    *   Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.\n    *   Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.\n\n**Affiliations**: Princeton University, Intel Labs\n\nThe paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.\n\n1.  **Dilated Convolutions**:\n\n*   Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.\n*   Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.\n\n1.  **Multi-Scale Context Aggregation**:\n    *   Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.\n    *   The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.\n2.  **Simplified Network Design**:\n    *   Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.\n    *   Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.\n3.  **Controlled Experiments**:\n    *   Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.\n    *   Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.\n4.  **Performance Improvement**:\n    *   The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.\n    *   The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.\n\n*   Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.\n*   The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.\n\n*   Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.\n*   Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.\n\n*   Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.\n*   Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.\n\n*   The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.\n*   The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.\n\n*   Experiments:\n    *   **Dataset**: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.\n    *   **Training Procedure**: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.\n    *   **Evaluation**: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.\n*   Conclusion:\n    *   The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.\n    *   The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.\n    *   The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.\n\n*   **Dataset**: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.\n*   **Training Procedure**: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.\n*   **Evaluation**: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.\n\n*   The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.\n*   The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.\n*   The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.",
    "contentLength": 7420,
    "wordCount": 835,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#multi-scale-context-aggregation-by-dilated-convolutions"
  },
  {
    "id": "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Neural Message Passing for Quantum Chemistry",
    "order": 12,
    "orderInChapter": 12,
    "contentHtml": "<ul>\n  <li>Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl</li>\n  <li>The paper “Neural Message Passing for Quantum Chemistry” introduces Message Passing Neural Networks (MPNNs), a framework for supervised learning on molecular graphs that is invariant to molecular symmetries. The goal is to predict quantum mechanical properties of molecules, which is crucial in fields such as drug discovery and materials science.</li>\n  <li><strong>Introduction</strong>:\n    <ul>\n      <li>The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.</li>\n      <li>MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.</li>\n    </ul>\n  </li>\n  <li><strong>Methodology</strong>:\n    <ul>\n      <li><strong>Message Passing Phase</strong>: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        <ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n      </li>\n      <li><strong>Readout Phase</strong>: After the message passing phase, a readout function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">R</script> aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.</li>\n    </ul>\n  </li>\n  <li><strong>Key Contributions</strong>:\n    <ul>\n      <li><strong>State of the Art Results</strong>: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.</li>\n      <li><strong>Chemical Accuracy</strong>: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.</li>\n      <li><strong>Scalability</strong>: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.</li>\n    </ul>\n  </li>\n  <li><strong>Results</strong>:\n    <ul>\n      <li>The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.</li>\n      <li>They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion</strong>:\n    <ul>\n      <li>The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.</li>\n      <li>Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.</li>\n      <li>MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.</li>\n    </ul>\n<ul>\n      <li><strong>Message Passing Phase</strong>: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        <ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n      </li>\n      <li><strong>Readout Phase</strong>: After the message passing phase, a readout function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">R</script> aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.</li>\n    </ul>\n<ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n<ul>\n      <li><strong>State of the Art Results</strong>: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.</li>\n      <li><strong>Chemical Accuracy</strong>: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.</li>\n      <li><strong>Scalability</strong>: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.</li>\n    </ul>\n<ul>\n      <li>The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.</li>\n      <li>They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.</li>\n    </ul>\n<ul>\n      <li>The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.</li>\n      <li>Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl\n*   The paper “Neural Message Passing for Quantum Chemistry” introduces Message Passing Neural Networks (MPNNs), a framework for supervised learning on molecular graphs that is invariant to molecular symmetries. The goal is to predict quantum mechanical properties of molecules, which is crucial in fields such as drug discovery and materials science.\n*   **Introduction**:\n    *   The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.\n    *   MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.\n*   **Methodology**:\n    *   **Message Passing Phase**: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        *   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n        *   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n    *   **Readout Phase**: After the message passing phase, a readout functionRRR aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.\n*   **Key Contributions**:\n    *   **State of the Art Results**: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.\n    *   **Chemical Accuracy**: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.\n    *   **Scalability**: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.\n*   **Results**:\n    *   The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.\n    *   They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.\n*   **Conclusion**:\n    *   The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.\n    *   Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.\n\n*   The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.\n*   MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.\n\n*   **Message Passing Phase**: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n    *   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n    *   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n*   **Readout Phase**: After the message passing phase, a readout functionRRR aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.\n\n*   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n*   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n\n*   **State of the Art Results**: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.\n*   **Chemical Accuracy**: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.\n*   **Scalability**: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.\n\n*   The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.\n*   They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.\n\n*   The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.\n*   Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.",
    "contentLength": 103662,
    "wordCount": 885,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#neural-message-passing-for-quantum-chemistry"
  },
  {
    "id": "ai-top-30-papers-attention-is-all-you-need-13",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Attention is All You Need",
    "order": 13,
    "orderInChapter": 13,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</li>\n  <li>\n    <p><strong>Affiliations</strong>: Google Brain, Google Research, University of Toronto</p>\n  </li>\n  <li>\n    <p>The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Transformer Architecture</strong>:\n        <ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n      </li>\n      <li><strong>Self-Attention Mechanism</strong>:\n        <ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n      </li>\n      <li><strong>Positional Encoding</strong>:\n        <ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n      </li>\n      <li><strong>Training Efficiency and Performance</strong>:\n        <ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n      </li>\n      <li><strong>Generalization to Other Tasks</strong>:\n        <ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n      </li>\n      <li><strong>Advantages Over Previous Models</strong>:\n        <ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Experimental Results:\n    <ul>\n      <li><strong>Machine Translation</strong>: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.</li>\n      <li><strong>Model Variations</strong>: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.</li>\n      <li><strong>English Constituency Parsing</strong>: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.</li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.</li>\n      <li>Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Google Brain, Google Research, University of Toronto</p>\n<p>The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.</p>\n<ol>\n      <li><strong>Transformer Architecture</strong>:\n        <ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n      </li>\n      <li><strong>Self-Attention Mechanism</strong>:\n        <ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n      </li>\n      <li><strong>Positional Encoding</strong>:\n        <ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n      </li>\n      <li><strong>Training Efficiency and Performance</strong>:\n        <ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n      </li>\n      <li><strong>Generalization to Other Tasks</strong>:\n        <ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n      </li>\n      <li><strong>Advantages Over Previous Models</strong>:\n        <ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n<ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n<ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n<ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n<ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n<ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n<ul>\n      <li><strong>Machine Translation</strong>: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.</li>\n      <li><strong>Model Variations</strong>: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.</li>\n      <li><strong>English Constituency Parsing</strong>: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.</li>\n    </ul>\n<ul>\n      <li>The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.</li>\n      <li>Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.</li>\n    </ul>",
    "contentMarkdown": "*   **Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n*   **Affiliations**: Google Brain, Google Research, University of Toronto\n    \n*   The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.\n    \n*   Key Contributions:\n    1.  **Transformer Architecture**:\n        *   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n        *   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n    2.  **Self-Attention Mechanism**:\n        *   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n        *   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n    3.  **Positional Encoding**:\n        *   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n    4.  **Training Efficiency and Performance**:\n        *   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n        *   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n    5.  **Generalization to Other Tasks**:\n        *   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n    6.  **Advantages Over Previous Models**:\n        *   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n        *   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n*   Experimental Results:\n    *   **Machine Translation**: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.\n    *   **Model Variations**: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.\n    *   **English Constituency Parsing**: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.\n*   Conclusion:\n    *   The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.\n    *   Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.\n\n**Affiliations**: Google Brain, Google Research, University of Toronto\n\nThe paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.\n\n1.  **Transformer Architecture**:\n    *   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n    *   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n2.  **Self-Attention Mechanism**:\n    *   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n    *   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n3.  **Positional Encoding**:\n    *   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n4.  **Training Efficiency and Performance**:\n    *   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n    *   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n5.  **Generalization to Other Tasks**:\n    *   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n6.  **Advantages Over Previous Models**:\n    *   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n    *   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n\n*   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n*   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n\n*   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n*   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n\n*   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n\n*   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n*   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n\n*   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n\n*   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n*   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n\n*   **Machine Translation**: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.\n*   **Model Variations**: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.\n*   **English Constituency Parsing**: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.\n\n*   The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.\n*   Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.",
    "contentLength": 12619,
    "wordCount": 1467,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#attention-is-all-you-need"
  },
  {
    "id": "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "order": 14,
    "orderInChapter": 14,
    "contentHtml": "<ul>\n  <li><strong>Author</strong>: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio</li>\n  <li><strong>Abstract</strong>: Neural machine translation (NMT) is an emerging approach that builds a single neural network to maximize translation performance. Unlike traditional methods, NMT uses encoder-decoder architectures to translate sentences. This paper introduces a method allowing the model to search for relevant parts of a source sentence during translation, enhancing performance.</li>\n  <li><strong>Key Concepts</strong>:\n    <ul>\n      <li><strong>Encoder-Decoder Model</strong>: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.</li>\n      <li><strong>Fixed-Length Vector Bottleneck</strong>: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.</li>\n      <li><strong>Attention Mechanism</strong>: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.</li>\n    </ul>\n  </li>\n  <li><strong>Proposed Model</strong>:\n    <ul>\n      <li><strong>Bidirectional RNN Encoder</strong>: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.</li>\n      <li><strong>Attention-Based Decoder</strong>: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.</li>\n    </ul>\n  </li>\n  <li><strong>Performance</strong>:\n    <ul>\n      <li>The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.</li>\n      <li>Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.</li>\n      <li>Qualitative analysis shows that the alignments produced by the model are linguistically plausible.</li>\n    </ul>\n  </li>\n  <li><strong>Experiment</strong>:\n    <ul>\n      <li>The models were tested on the WMT ’14 English-to-French translation task.</li>\n      <li>The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion</strong>:\n    <ul>\n      <li>The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.</li>\n      <li>Future work should address handling unknown or rare words to further improve translation performance.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Encoder-Decoder Model</strong>: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.</li>\n      <li><strong>Fixed-Length Vector Bottleneck</strong>: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.</li>\n      <li><strong>Attention Mechanism</strong>: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.</li>\n    </ul>\n<ul>\n      <li><strong>Bidirectional RNN Encoder</strong>: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.</li>\n      <li><strong>Attention-Based Decoder</strong>: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.</li>\n    </ul>\n<ul>\n      <li>The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.</li>\n      <li>Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.</li>\n      <li>Qualitative analysis shows that the alignments produced by the model are linguistically plausible.</li>\n    </ul>\n<ul>\n      <li>The models were tested on the WMT ’14 English-to-French translation task.</li>\n      <li>The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.</li>\n    </ul>\n<ul>\n      <li>The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.</li>\n      <li>Future work should address handling unknown or rare words to further improve translation performance.</li>\n    </ul>",
    "contentMarkdown": "*   **Author**: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio\n*   **Abstract**: Neural machine translation (NMT) is an emerging approach that builds a single neural network to maximize translation performance. Unlike traditional methods, NMT uses encoder-decoder architectures to translate sentences. This paper introduces a method allowing the model to search for relevant parts of a source sentence during translation, enhancing performance.\n*   **Key Concepts**:\n    *   **Encoder-Decoder Model**: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.\n    *   **Fixed-Length Vector Bottleneck**: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.\n    *   **Attention Mechanism**: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.\n*   **Proposed Model**:\n    *   **Bidirectional RNN Encoder**: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.\n    *   **Attention-Based Decoder**: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.\n*   **Performance**:\n    *   The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.\n    *   Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.\n    *   Qualitative analysis shows that the alignments produced by the model are linguistically plausible.\n*   **Experiment**:\n    *   The models were tested on the WMT ’14 English-to-French translation task.\n    *   The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.\n*   **Conclusion**:\n    *   The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.\n    *   Future work should address handling unknown or rare words to further improve translation performance.\n\n*   **Encoder-Decoder Model**: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.\n*   **Fixed-Length Vector Bottleneck**: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.\n*   **Attention Mechanism**: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.\n\n*   **Bidirectional RNN Encoder**: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.\n*   **Attention-Based Decoder**: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.\n\n*   The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.\n*   Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.\n*   Qualitative analysis shows that the alignments produced by the model are linguistically plausible.\n\n*   The models were tested on the WMT ’14 English-to-French translation task.\n*   The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.\n\n*   The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.\n*   Future work should address handling unknown or rare words to further improve translation performance.",
    "contentLength": 4685,
    "wordCount": 531,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#neural-machine-translation-by-jointly-learning-to-align-and-translate"
  },
  {
    "id": "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Identity Mappings in Deep Residual Networks",
    "order": 15,
    "orderInChapter": 15,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>\n  <li>\n    <p><strong>Affiliations</strong>: Microsoft Research</p>\n  </li>\n  <li>\n    <p>The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Analysis of Identity Mappings</strong>:\n        <ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n      </li>\n      <li><strong>Proposed Residual Unit</strong>:\n        <ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n      </li>\n      <li><strong>Empirical Validation</strong>:\n        <ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n      </li>\n      <li><strong>Deep Residual Networks</strong>:\n        <ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Experimental Results:\n    <ul>\n      <li><strong>CIFAR-10 and CIFAR-100</strong>:\n        <ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n      </li>\n      <li><strong>ImageNet</strong>:\n        <ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.</li>\n      <li>By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.</li>\n      <li>The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Microsoft Research</p>\n<p>The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.</p>\n<ol>\n      <li><strong>Analysis of Identity Mappings</strong>:\n        <ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n      </li>\n      <li><strong>Proposed Residual Unit</strong>:\n        <ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n      </li>\n      <li><strong>Empirical Validation</strong>:\n        <ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n      </li>\n      <li><strong>Deep Residual Networks</strong>:\n        <ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n<ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n<ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n<ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n<ul>\n      <li><strong>CIFAR-10 and CIFAR-100</strong>:\n        <ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n      </li>\n      <li><strong>ImageNet</strong>:\n        <ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n<ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n<ul>\n      <li>The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.</li>\n      <li>By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.</li>\n      <li>The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.</li>\n    </ul>",
    "contentMarkdown": "*   **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n*   **Affiliations**: Microsoft Research\n    \n*   The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.\n    \n*   Key Contributions:\n    1.  **Analysis of Identity Mappings**:\n        *   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n        *   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n    2.  **Proposed Residual Unit**:\n        *   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n        *   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n    3.  **Empirical Validation**:\n        *   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n        *   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n    4.  **Deep Residual Networks**:\n        *   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n        *   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n*   Experimental Results:\n    *   **CIFAR-10 and CIFAR-100**:\n        *   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n        *   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n    *   **ImageNet**:\n        *   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n*   Conclusion:\n    *   The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.\n    *   By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.\n    *   The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.\n\n**Affiliations**: Microsoft Research\n\nThe paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.\n\n1.  **Analysis of Identity Mappings**:\n    *   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n    *   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n2.  **Proposed Residual Unit**:\n    *   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n    *   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n3.  **Empirical Validation**:\n    *   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n    *   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n4.  **Deep Residual Networks**:\n    *   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n    *   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n\n*   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n*   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n\n*   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n*   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n\n*   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n*   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n\n*   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n*   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n\n*   **CIFAR-10 and CIFAR-100**:\n    *   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n    *   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n*   **ImageNet**:\n    *   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n\n*   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n*   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n\n*   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n\n*   The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.\n*   By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.\n*   The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.",
    "contentLength": 8569,
    "wordCount": 965,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#identity-mappings-in-deep-residual-networks"
  },
  {
    "id": "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "A Simple Neural Network Module for Relational Reasoning",
    "order": 16,
    "orderInChapter": 16,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap</li>\n  <li>\n    <p><strong>Affiliations</strong>: DeepMind, London, United Kingdom</p>\n  </li>\n  <li>\n    <p>The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Introduction of Relation Networks (RNs)</strong>:\n        <ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n      </li>\n      <li><strong>Application to Visual Question Answering (CLEVR)</strong>:\n        <ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n      </li>\n      <li><strong>Sort-of-CLEVR Dataset</strong>:\n        <ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Text-Based Question Answering (bAbI)</strong>:\n        <ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Model Details:</li>\n  <li><strong>Architecture</strong>:\n    <ul>\n      <li>RNs operate on sets of objects, where each object is represented by a feature vector.</li>\n      <li>\n        <p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n      </li>\n      <li><strong>Training</strong>:\n        <ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Results:</li>\n  <li><strong>CLEVR</strong>:\n    <ul>\n      <li>\n        <p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n      </li>\n      <li><strong>Sort-of-CLEVR</strong>:\n        <ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n      </li>\n      <li><strong>bAbI</strong>:\n        <ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.</li>\n      <li>RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.</li>\n      <li>The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: DeepMind, London, United Kingdom</p>\n<p>The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.</p>\n<ol>\n      <li><strong>Introduction of Relation Networks (RNs)</strong>:\n        <ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n      </li>\n      <li><strong>Application to Visual Question Answering (CLEVR)</strong>:\n        <ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n      </li>\n      <li><strong>Sort-of-CLEVR Dataset</strong>:\n        <ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Text-Based Question Answering (bAbI)</strong>:\n        <ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n<ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n<ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n<ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n<ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n<ul>\n      <li>RNs operate on sets of objects, where each object is represented by a feature vector.</li>\n      <li>\n        <p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n      </li>\n      <li><strong>Training</strong>:\n        <ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n<ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n<ul>\n      <li>\n        <p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n      </li>\n      <li><strong>Sort-of-CLEVR</strong>:\n        <ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n      </li>\n      <li><strong>bAbI</strong>:\n        <ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n<ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n<ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n<ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n<ul>\n      <li>The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.</li>\n      <li>RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.</li>\n      <li>The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.</li>\n    </ul>",
    "contentMarkdown": "*   **Authors**: Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap\n*   **Affiliations**: DeepMind, London, United Kingdom\n    \n*   The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.\n    \n*   Key Contributions:\n    1.  **Introduction of Relation Networks (RNs)**:\n        *   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n        *   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n    2.  **Application to Visual Question Answering (CLEVR)**:\n        *   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n        *   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n    3.  **Sort-of-CLEVR Dataset**:\n        *   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n        *   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n    4.  **Text-Based Question Answering (bAbI)**:\n        *   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n        *   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n    5.  **Dynamic Physical Systems**:\n        *   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n        *   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n*   Model Details:\n*   **Architecture**:\n    *   RNs operate on sets of objects, where each object is represented by a feature vector.\n    *   The RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n        \n    *   **Training**:\n        *   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n*   Results:\n*   **CLEVR**:\n    *   The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n        \n    *   **Sort-of-CLEVR**:\n        *   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n    *   **bAbI**:\n        *   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n    *   **Dynamic Physical Systems**:\n        *   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n*   Conclusion:\n    *   The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.\n    *   RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.\n    *   The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.\n\n**Affiliations**: DeepMind, London, United Kingdom\n\nThe paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.\n\n1.  **Introduction of Relation Networks (RNs)**:\n    *   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n    *   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n2.  **Application to Visual Question Answering (CLEVR)**:\n    *   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n    *   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n3.  **Sort-of-CLEVR Dataset**:\n    *   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n    *   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n4.  **Text-Based Question Answering (bAbI)**:\n    *   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n    *   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n5.  **Dynamic Physical Systems**:\n    *   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n    *   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n\n*   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n*   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n\n*   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n*   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n\n*   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n*   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n\n*   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n*   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n\n*   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n*   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n\n*   RNs operate on sets of objects, where each object is represented by a feature vector.\n*   The RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n    \n*   **Training**:\n    *   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n\nThe RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n\n*   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n\n*   The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n    \n*   **Sort-of-CLEVR**:\n    *   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n*   **bAbI**:\n    *   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n*   **Dynamic Physical Systems**:\n    *   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n\nThe RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n\n*   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n\n*   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n\n*   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n\n*   The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.\n*   RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.\n*   The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.",
    "contentLength": 25164,
    "wordCount": 1401,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#a-simple-neural-network-module-for-relational-reasoning"
  },
  {
    "id": "ai-top-30-papers-variational-lossy-autoencoder-17",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Variational Lossy Autoencoder",
    "order": 17,
    "orderInChapter": 17,
    "contentHtml": "<ul>\n  <li>Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel</li>\n  <li>Published: ICLR 2017</li>\n  <li>\n    <p>Institutions: UC Berkeley, OpenAI</p>\n  </li>\n  <li>\n    <table>\n      <tbody>\n        <tr>\n          <td>The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.57em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">p(z)</script> and the decoding distribution$$ p(x</td>\n          <td>z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.</td>\n        </tr>\n      </tbody>\n    </table>\n  </li>\n  <li><strong>Key Concepts:</strong>\n    <ol>\n      <li><strong>Representation Learning:</strong> Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.</li>\n      <li><strong>Variational Autoencoder (VAE):</strong> VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.</li>\n      <li><strong>Autoregressive Models:</strong> These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.</li>\n    </ol>\n  </li>\n  <li><strong>Technical Highlights:</strong>\n    <ol>\n      <li><strong>Combination of VAE and Autoregressive Models:</strong>\n        <ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n      </li>\n      <li><strong>Bits-Back Coding and Information Preference:</strong>\n        <ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n      </li>\n      <li><strong>Lossy Code via Explicit Information Placement:</strong>\n        <ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n      </li>\n      <li><strong>Learned Prior with Autoregressive Flow:</strong>\n        <ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ol>\n      <li><strong>Datasets:</strong>\n        <ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n      </li>\n      <li><strong>Performance:</strong>\n        <ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n      </li>\n      <li><strong>Visualization:</strong>\n        <ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Conclusion:</strong></li>\n  <li>The Variational Lossy Autoencoder (VLAE) effectively combines the strengths of VAEs and autoregressive models, enabling controllable representation learning and improved density estimation. The model’s design ensures that the latent code captures essential global information, making it suitable for various generative tasks. Future work includes extending VLAE to other data types, such as audio and video, and designing task-specific representations to enhance semi-supervised learning.</li>\n</ul>\n<p>Institutions: UC Berkeley, OpenAI</p>\n<table>\n      <tbody>\n        <tr>\n          <td>The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.57em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">p(z)</script> and the decoding distribution$$ p(x</td>\n          <td>z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.</td>\n        </tr>\n      </tbody>\n    </table>\n<ol>\n      <li><strong>Representation Learning:</strong> Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.</li>\n      <li><strong>Variational Autoencoder (VAE):</strong> VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.</li>\n      <li><strong>Autoregressive Models:</strong> These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.</li>\n    </ol>\n<ol>\n      <li><strong>Combination of VAE and Autoregressive Models:</strong>\n        <ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n      </li>\n      <li><strong>Bits-Back Coding and Information Preference:</strong>\n        <ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n      </li>\n      <li><strong>Lossy Code via Explicit Information Placement:</strong>\n        <ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n      </li>\n      <li><strong>Learned Prior with Autoregressive Flow:</strong>\n        <ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n<ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n<ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n<ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n<ol>\n      <li><strong>Datasets:</strong>\n        <ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n      </li>\n      <li><strong>Performance:</strong>\n        <ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n      </li>\n      <li><strong>Visualization:</strong>\n        <ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n<ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n<ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>",
    "contentMarkdown": "*   Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel\n*   Published: ICLR 2017\n*   Institutions: UC Berkeley, OpenAI\n    \n*   The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distributionp(z)p(z)p(z) and the decoding distribution$$ p(x\n    \n    z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.\n    \n*   **Key Concepts:**\n    1.  **Representation Learning:** Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.\n    2.  **Variational Autoencoder (VAE):** VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.\n    3.  **Autoregressive Models:** These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.\n*   **Technical Highlights:**\n    1.  **Combination of VAE and Autoregressive Models:**\n        *   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n        *   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n    2.  **Bits-Back Coding and Information Preference:**\n        *   Bits-Back Coding is an information-theoretic view of Variational Inference.\n        *   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n    3.  **Lossy Code via Explicit Information Placement:**\n        *   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n        *   This results in a lossy compression that retains essential global structures while discarding local details.\n    4.  **Learned Prior with Autoregressive Flow:**\n        *   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n        *   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n*   **Experiments and Results:**\n    1.  **Datasets:**\n        *   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n    2.  **Performance:**\n        *   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n        *   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n        *   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n    3.  **Visualization:**\n        *   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.\n*   **Conclusion:**\n*   The Variational Lossy Autoencoder (VLAE) effectively combines the strengths of VAEs and autoregressive models, enabling controllable representation learning and improved density estimation. The model’s design ensures that the latent code captures essential global information, making it suitable for various generative tasks. Future work includes extending VLAE to other data types, such as audio and video, and designing task-specific representations to enhance semi-supervised learning.\n\nInstitutions: UC Berkeley, OpenAI\n\nThe paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distributionp(z)p(z)p(z) and the decoding distribution$$ p(x\n\nz)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.\n\n1.  **Representation Learning:** Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.\n2.  **Variational Autoencoder (VAE):** VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.\n3.  **Autoregressive Models:** These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.\n\n1.  **Combination of VAE and Autoregressive Models:**\n    *   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n    *   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n2.  **Bits-Back Coding and Information Preference:**\n    *   Bits-Back Coding is an information-theoretic view of Variational Inference.\n    *   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n3.  **Lossy Code via Explicit Information Placement:**\n    *   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n    *   This results in a lossy compression that retains essential global structures while discarding local details.\n4.  **Learned Prior with Autoregressive Flow:**\n    *   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n    *   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n\n*   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n*   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n\n*   Bits-Back Coding is an information-theoretic view of Variational Inference.\n*   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n\n*   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n*   This results in a lossy compression that retains essential global structures while discarding local details.\n\n*   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n*   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n\n1.  **Datasets:**\n    *   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n2.  **Performance:**\n    *   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n    *   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n    *   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n3.  **Visualization:**\n    *   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.\n\n*   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n\n*   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n*   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n*   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n\n*   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.",
    "contentLength": 19673,
    "wordCount": 1123,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#variational-lossy-autoencoder"
  },
  {
    "id": "ai-top-30-papers-relational-recurrent-neural-networks-18",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Relational Recurrent Neural Networks",
    "order": 18,
    "orderInChapter": 18,
    "contentHtml": "<ul>\n  <li><strong>Authors:</strong> Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap</li>\n  <li><strong>Institution:</strong> DeepMind, University College London</li>\n  <li>\n    <p><strong>Abstract:</strong> The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.</p>\n  </li>\n  <li><strong>Key Points:</strong>\n    <ul>\n      <li><strong>Relational Reasoning Deficits in Standard Architectures:</strong> Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.</li>\n      <li><strong>Introduction of Relational Memory Core (RMC):</strong> The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.</li>\n      <li><strong>Application and Results:</strong>\n        <ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n      </li>\n      <li><strong>Model Design and Functionality:</strong>\n        <ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong> The introduction of the RMC shows that explicit modeling of memory interactions can enhance the performance of neural networks on tasks that require complex relational reasoning across sequential information. The study emphasizes the importance of enabling interactions between memory vectors to improve relational reasoning capabilities in recurrent neural networks.</li>\n</ul>\n<p><strong>Abstract:</strong> The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.</p>\n<ul>\n      <li><strong>Relational Reasoning Deficits in Standard Architectures:</strong> Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.</li>\n      <li><strong>Introduction of Relational Memory Core (RMC):</strong> The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.</li>\n      <li><strong>Application and Results:</strong>\n        <ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n      </li>\n      <li><strong>Model Design and Functionality:</strong>\n        <ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n<ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>",
    "contentMarkdown": "*   **Authors:** Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap\n*   **Institution:** DeepMind, University College London\n*   **Abstract:** The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.\n    \n*   **Key Points:**\n    *   **Relational Reasoning Deficits in Standard Architectures:** Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.\n    *   **Introduction of Relational Memory Core (RMC):** The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.\n    *   **Application and Results:**\n        *   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n        *   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n        *   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n    *   **Model Design and Functionality:**\n        *   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n        *   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.\n*   **Conclusion:** The introduction of the RMC shows that explicit modeling of memory interactions can enhance the performance of neural networks on tasks that require complex relational reasoning across sequential information. The study emphasizes the importance of enabling interactions between memory vectors to improve relational reasoning capabilities in recurrent neural networks.\n\n**Abstract:** The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.\n\n*   **Relational Reasoning Deficits in Standard Architectures:** Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.\n*   **Introduction of Relational Memory Core (RMC):** The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.\n*   **Application and Results:**\n    *   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n    *   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n    *   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n*   **Model Design and Functionality:**\n    *   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n    *   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.\n\n*   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n*   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n*   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n\n*   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n*   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.",
    "contentLength": 6511,
    "wordCount": 710,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#relational-recurrent-neural-networks"
  },
  {
    "id": "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton",
    "order": 19,
    "orderInChapter": 19,
    "contentHtml": "<ul>\n  <li>Authors: Scott Aaronson, Sean M. Carroll, Lauren Ouellette</li>\n  <li>\n    <p>The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.</p>\n  </li>\n  <li>\n    <p><strong>Introduction</strong>: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.</p>\n  </li>\n  <li><strong>Background</strong>: Several concepts of entropy and complexity are discussed:\n    <ul>\n      <li><strong>Entropy</strong>: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.</li>\n      <li><strong>Complexity</strong>: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Apparent Complexity</strong>: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.</p>\n  </li>\n  <li>\n    <p><strong>Sophistication</strong>: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.</p>\n  </li>\n  <li>\n    <p><strong>Logical Depth</strong>: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.</p>\n  </li>\n  <li>\n    <p><strong>Light-Cone Complexity</strong>: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.</p>\n  </li>\n  <li><strong>Coffee Automaton Models</strong>:\n    <ul>\n      <li><strong>Interacting Model</strong>: Particles interact, swapping positions if they are adjacent and different.</li>\n      <li><strong>Non-Interacting Model</strong>: Particles move independently in random walks.</li>\n    </ul>\n  </li>\n  <li><strong>Experiment and Results</strong>:\n    <ul>\n      <li>The automaton begins with separated coffee and cream, mixing over time.</li>\n      <li><strong>Coarse-Graining</strong>: The state is averaged over local regions to produce a coarse-grained version.</li>\n      <li><strong>Measurements</strong>: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.</li>\n      <li>Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.</li>\n    </ul>\n  </li>\n  <li><strong>Adjusted Coarse-Graining</strong>:\n    <ul>\n      <li>To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusions and Further Work</strong>:\n    <ul>\n      <li>The coarse-graining approach effectively mirrors human intuition of complexity.</li>\n      <li>Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.</p>\n<p><strong>Introduction</strong>: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.</p>\n<ul>\n      <li><strong>Entropy</strong>: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.</li>\n      <li><strong>Complexity</strong>: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.</li>\n    </ul>\n<p><strong>Apparent Complexity</strong>: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.</p>\n<p><strong>Sophistication</strong>: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.</p>\n<p><strong>Logical Depth</strong>: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.</p>\n<p><strong>Light-Cone Complexity</strong>: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.</p>\n<ul>\n      <li><strong>Interacting Model</strong>: Particles interact, swapping positions if they are adjacent and different.</li>\n      <li><strong>Non-Interacting Model</strong>: Particles move independently in random walks.</li>\n    </ul>\n<ul>\n      <li>The automaton begins with separated coffee and cream, mixing over time.</li>\n      <li><strong>Coarse-Graining</strong>: The state is averaged over local regions to produce a coarse-grained version.</li>\n      <li><strong>Measurements</strong>: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.</li>\n      <li>Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.</li>\n    </ul>\n<ul>\n      <li>To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.</li>\n    </ul>\n<ul>\n      <li>The coarse-graining approach effectively mirrors human intuition of complexity.</li>\n      <li>Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Scott Aaronson, Sean M. Carroll, Lauren Ouellette\n*   The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.\n    \n*   **Introduction**: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.\n    \n*   **Background**: Several concepts of entropy and complexity are discussed:\n    *   **Entropy**: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.\n    *   **Complexity**: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.\n*   **Apparent Complexity**: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.\n    \n*   **Sophistication**: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.\n    \n*   **Logical Depth**: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.\n    \n*   **Light-Cone Complexity**: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.\n    \n*   **Coffee Automaton Models**:\n    *   **Interacting Model**: Particles interact, swapping positions if they are adjacent and different.\n    *   **Non-Interacting Model**: Particles move independently in random walks.\n*   **Experiment and Results**:\n    *   The automaton begins with separated coffee and cream, mixing over time.\n    *   **Coarse-Graining**: The state is averaged over local regions to produce a coarse-grained version.\n    *   **Measurements**: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.\n    *   Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.\n*   **Adjusted Coarse-Graining**:\n    *   To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.\n*   **Conclusions and Further Work**:\n    *   The coarse-graining approach effectively mirrors human intuition of complexity.\n    *   Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.\n\nThe paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.\n\n**Introduction**: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.\n\n*   **Entropy**: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.\n*   **Complexity**: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.\n\n**Apparent Complexity**: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.\n\n**Sophistication**: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.\n\n**Logical Depth**: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.\n\n**Light-Cone Complexity**: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.\n\n*   **Interacting Model**: Particles interact, swapping positions if they are adjacent and different.\n*   **Non-Interacting Model**: Particles move independently in random walks.\n\n*   The automaton begins with separated coffee and cream, mixing over time.\n*   **Coarse-Graining**: The state is averaged over local regions to produce a coarse-grained version.\n*   **Measurements**: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.\n*   Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.\n\n*   To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.\n\n*   The coarse-graining approach effectively mirrors human intuition of complexity.\n*   Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.",
    "contentLength": 6472,
    "wordCount": 753,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#quantifying-the-rise-and-fall-of-complexity-in-closed-systems:-the-coffee-automaton"
  },
  {
    "id": "ai-top-30-papers-neural-turing-machines-20",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Neural Turing Machines",
    "order": 20,
    "orderInChapter": 20,
    "contentHtml": "<ul>\n  <li><strong>Author</strong>: Alex Graves, Greg Wayne, Ivo Danihelka</li>\n  <li><strong>Summary</strong>:\n    <ul>\n      <li><strong>Introduction</strong>:\n        <ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n      </li>\n      <li><strong>Foundational Research</strong>:\n        <ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n      </li>\n      <li><strong>Neural Turing Machines</strong>:\n        <ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n      </li>\n      <li><strong>Experiments</strong>:\n        <ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n      </li>\n      <li><strong>Conclusion</strong>:\n        <ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>This paper introduces the Neural Turing Machine architecture, highlighting its foundation, structure, and performance in various algorithmic tasks, demonstrating its potential to revolutionize neural network capabilities by integrating external memory and addressing mechanisms.</li>\n</ul>\n<ul>\n      <li><strong>Introduction</strong>:\n        <ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n      </li>\n      <li><strong>Foundational Research</strong>:\n        <ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n      </li>\n      <li><strong>Neural Turing Machines</strong>:\n        <ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n      </li>\n      <li><strong>Experiments</strong>:\n        <ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n      </li>\n      <li><strong>Conclusion</strong>:\n        <ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n<ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n<ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n<ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n<ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>",
    "contentMarkdown": "*   **Author**: Alex Graves, Greg Wayne, Ivo Danihelka\n*   **Summary**:\n    *   **Introduction**:\n        *   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n    *   **Foundational Research**:\n        *   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n        *   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n        *   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n    *   **Neural Turing Machines**:\n        *   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n        *   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n        *   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n        *   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n    *   **Experiments**:\n        *   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n        *   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n        *   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n        *   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n        *   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n        *   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n    *   **Conclusion**:\n        *   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.\n*   This paper introduces the Neural Turing Machine architecture, highlighting its foundation, structure, and performance in various algorithmic tasks, demonstrating its potential to revolutionize neural network capabilities by integrating external memory and addressing mechanisms.\n\n*   **Introduction**:\n    *   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n*   **Foundational Research**:\n    *   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n    *   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n    *   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n*   **Neural Turing Machines**:\n    *   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n    *   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n    *   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n    *   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n*   **Experiments**:\n    *   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n    *   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n    *   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n    *   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n    *   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n    *   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n*   **Conclusion**:\n    *   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.\n\n*   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n\n*   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n*   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n*   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n\n*   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n*   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n*   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n*   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n\n*   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n*   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n*   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n*   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n*   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n*   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n\n*   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.",
    "contentLength": 11408,
    "wordCount": 1264,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#neural-turing-machines"
  },
  {
    "id": "ai-top-30-papers-deep-speech-2-end-to-end-speech-recognition-in-eng-21",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
    "order": 21,
    "orderInChapter": 21,
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Authors:</strong> Baidu Research – Silicon Valley AI Lab</p>\n  </li>\n  <li>\n    <p><strong>Abstract:</strong>\nThe paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.</p>\n  </li>\n  <li>\n    <p><strong>Introduction:</strong>\nTraditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.</p>\n  </li>\n  <li>\n    <p><strong>Model Architecture:</strong>\nThe model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.</p>\n  </li>\n  <li>\n    <p><strong>Training Data:</strong>\nTraining leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.</p>\n  </li>\n  <li><strong>Results:</strong>\n    <ul>\n      <li><strong>English:</strong> Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.</li>\n      <li><strong>Mandarin:</strong> The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment:</strong>\nThe system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.</p>\n  </li>\n  <li>\n    <p><strong>Conclusion:</strong>\nDeep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.</p>\n  </li>\n  <li>This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.</li>\n</ul>\n<p><strong>Authors:</strong> Baidu Research – Silicon Valley AI Lab</p>\n<p><strong>Abstract:</strong>\nThe paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.</p>\n<p><strong>Introduction:</strong>\nTraditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.</p>\n<p><strong>Model Architecture:</strong>\nThe model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.</p>\n<p><strong>Training Data:</strong>\nTraining leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.</p>\n<ul>\n      <li><strong>English:</strong> Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.</li>\n      <li><strong>Mandarin:</strong> The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.</li>\n    </ul>\n<p><strong>Deployment:</strong>\nThe system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.</p>\n<p><strong>Conclusion:</strong>\nDeep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.</p>",
    "contentMarkdown": "*   **Authors:** Baidu Research – Silicon Valley AI Lab\n    \n*   **Abstract:** The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.\n    \n*   **Introduction:** Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.\n    \n*   **Model Architecture:** The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.\n    \n*   **Training Data:** Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.\n    \n*   **Results:**\n    *   **English:** Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.\n    *   **Mandarin:** The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.\n*   **Deployment:** The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.\n    \n*   **Conclusion:** Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.\n    \n*   This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.\n\n**Authors:** Baidu Research – Silicon Valley AI Lab\n\n**Abstract:** The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.\n\n**Introduction:** Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.\n\n**Model Architecture:** The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.\n\n**Training Data:** Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.\n\n*   **English:** Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.\n*   **Mandarin:** The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.\n\n**Deployment:** The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.\n\n**Conclusion:** Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.",
    "contentLength": 6827,
    "wordCount": 833,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#deep-speech-2:-end-to-end-speech-recognition-in-english-and-mandarin"
  },
  {
    "id": "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Scaling Laws for Neural Language Models",
    "order": 22,
    "orderInChapter": 22,
    "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</li>\n  <li>\n    <p><strong>Institution</strong>: OpenAI, Johns Hopkins University</p>\n  </li>\n  <li>The paper “Scaling Laws for Neural Language Models” explores empirical scaling laws that describe the relationship between language model performance and factors such as model size, dataset size, and computational resources used for training. The study finds that performance scales predictably according to power laws over several orders of magnitude. Key findings include:</li>\n</ul>\n<p><strong>Institution</strong>: OpenAI, Johns Hopkins University</p>\n<ol>\n  <li><strong>Power-law relationships</strong>: Language model performance improves predictably with increases in model size (number of parameters), dataset size (number of tokens), and compute (floating point operations). These improvements follow simple power-law relationships.</li>\n  <li><strong>Model size and data efficiency</strong>: Larger models are significantly more sample-efficient, meaning they require fewer data points to achieve the same level of performance compared to smaller models.</li>\n  <li><strong>Optimal compute allocation</strong>: For a fixed compute budget, it is most efficient to train very large models on a relatively modest amount of data and to stop training before full convergence.</li>\n  <li><strong>Minimal architectural effects</strong>: Performance depends strongly on scale (size, data, compute) and weakly on specific architectural hyperparameters such as network width or depth.</li>\n</ol>\n<ul>\n  <li>Key Equations</li>\n  <li><strong>Model performance as a function of parameters</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-156\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.98em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"mo\" id=\"MathJax-Span-158\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-159\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.73em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.362em;\"><span class=\"msubsup\" id=\"MathJax-Span-160\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.737em; left: 2.138em;\"><span class=\"texatom\" id=\"MathJax-Span-165\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"msubsup\" id=\"MathJax-Span-167\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mi>N</mi></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">L(N) = \\left( \\frac{N_c}{N} \\right)^{\\alpha_N}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-170\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">L</script> is the loss,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">N</script> is the number of non-embedding parameters,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>N</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"msubsup\" id=\"MathJax-Span-178\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>N</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">N_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"msubsup\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>N</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\alpha_N</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Dataset size relationship</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-186\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-187\"><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-193\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1002.03em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mo\" id=\"MathJax-Span-195\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-196\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.414em;\"><span class=\"msubsup\" id=\"MathJax-Span-197\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-199\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-200\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-201\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.737em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"msubsup\" id=\"MathJax-Span-204\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-206\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mi>D</mi></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">L(D) = \\left( \\frac{D_c}{D} \\right)^{\\alpha_D}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D</script> is the dataset size in tokens,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-210\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-211\"><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-215\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"msubsup\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>D</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">\\alpha_D</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Compute efficiency</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-220\" style=\"width: 11.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.836em, 1009.27em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-221\"><span class=\"mi\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-224\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-226\"><span class=\"mrow\" id=\"MathJax-Span-227\"><span class=\"mtext\" id=\"MathJax-Span-228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-231\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.659em, 1003.08em, 4.846em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mo\" id=\"MathJax-Span-233\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-234\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.326em, -999.997em); top: -4.581em; left: 50%; margin-left: -0.831em;\"><span class=\"msubsup\" id=\"MathJax-Span-235\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-236\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-237\"><span class=\"mrow\" id=\"MathJax-Span-238\"><span class=\"mtext\" id=\"MathJax-Span-239\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.3em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.674em;\"><span class=\"msubsup\" id=\"MathJax-Span-242\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-244\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mtext\" id=\"MathJax-Span-246\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.77em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.773em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-247\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.893em; left: 3.232em;\"><span class=\"texatom\" id=\"MathJax-Span-248\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"msubsup\" id=\"MathJax-Span-250\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mtext\" id=\"MathJax-Span-254\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">L(C_{\\text{min}}) = \\left( \\frac{C_{\\text{min}, c}}{C_{\\text{min}}} \\right)^{\\alpha_{\\text{min}, C}}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"msubsup\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-261\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mtext\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">C_{\\text{min}}</script> is the minimum compute required,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"msubsup\" id=\"MathJax-Span-266\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-268\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mtext\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">C_{\\text{min}, c}</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.4em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mtext\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\alpha_{\\text{min}, C}</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Sample efficiency</strong>: Larger models trained with the same amount of data achieve better performance due to their improved ability to utilize the data.</li>\n  <li><strong>Training dynamics</strong>: Training curves follow predictable power-laws, allowing early extrapolation to predict the final performance of the model.</li>\n  <li><strong>Generalization</strong>: Performance on different datasets improves consistently with the performance on the training dataset, suggesting that better in-distribution performance translates to better out-of-distribution performance.</li>\n  <li><strong>Model size vs. dataset size</strong>: As model size increases, the dataset size should be scaled sublinearly to avoid overfitting, implying that moderately increasing data is sufficient for much larger models.</li>\n  <li>\n    <p><strong>Compute-efficient training</strong>: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.</p>\n  </li>\n  <li>These findings provide a framework for understanding and predicting the performance of large-scale neural language models, guiding future research and practical applications in optimizing model training and deployment.</li>\n</ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-170\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">L</script> is the loss,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">N</script> is the number of non-embedding parameters,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>N</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"msubsup\" id=\"MathJax-Span-178\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>N</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">N_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"msubsup\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>N</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\alpha_N</script> is the scaling exponent.</li>\n    </ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D</script> is the dataset size in tokens,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-210\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-211\"><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-215\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"msubsup\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>D</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">\\alpha_D</script> is the scaling exponent.</li>\n    </ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"msubsup\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-261\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mtext\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">C_{\\text{min}}</script> is the minimum compute required,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"msubsup\" id=\"MathJax-Span-266\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-268\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mtext\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">C_{\\text{min}, c}</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.4em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mtext\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\alpha_{\\text{min}, C}</script> is the scaling exponent.</li>\n    </ul>\n<p><strong>Compute-efficient training</strong>: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.</p>",
    "contentMarkdown": "*   **Authors**: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei\n*   **Institution**: OpenAI, Johns Hopkins University\n    \n*   The paper “Scaling Laws for Neural Language Models” explores empirical scaling laws that describe the relationship between language model performance and factors such as model size, dataset size, and computational resources used for training. The study finds that performance scales predictably according to power laws over several orders of magnitude. Key findings include:\n\n**Institution**: OpenAI, Johns Hopkins University\n\n1.  **Power-law relationships**: Language model performance improves predictably with increases in model size (number of parameters), dataset size (number of tokens), and compute (floating point operations). These improvements follow simple power-law relationships.\n2.  **Model size and data efficiency**: Larger models are significantly more sample-efficient, meaning they require fewer data points to achieve the same level of performance compared to smaller models.\n3.  **Optimal compute allocation**: For a fixed compute budget, it is most efficient to train very large models on a relatively modest amount of data and to stop training before full convergence.\n4.  **Minimal architectural effects**: Performance depends strongly on scale (size, data, compute) and weakly on specific architectural hyperparameters such as network width or depth.\n\n*   Key Equations\n*   **Model performance as a function of parameters**: -L(N)\\=(NcN)αNL(N)\\=(NcN)αNL(N) = \\\\left( \\\\frac{N\\_c}{N} \\\\right)^{\\\\alpha\\_N}\n    *   WhereLLL is the loss,NNN is the number of non-embedding parameters,NcNcN\\_c is a constant, andαNαN\\\\alpha\\_N is the scaling exponent.\n*   **Dataset size relationship**: -L(D)\\=(DcD)αDL(D)\\=(DcD)αDL(D) = \\\\left( \\\\frac{D\\_c}{D} \\\\right)^{\\\\alpha\\_D}\n    *   WhereDDD is the dataset size in tokens,DcDcD\\_c is a constant, andαDαD\\\\alpha\\_D is the scaling exponent.\n*   **Compute efficiency**: -L(Cmin)\\=(Cmin,cCmin)αmin,CL(Cmin)\\=(Cmin,cCmin)αmin,CL(C\\_{\\\\text{min}}) = \\\\left( \\\\frac{C\\_{\\\\text{min}, c}}{C\\_{\\\\text{min}}} \\\\right)^{\\\\alpha\\_{\\\\text{min}, C}}\n    *   WhereCminCminC\\_{\\\\text{min}} is the minimum compute required,Cmin,cCmin,cC\\_{\\\\text{min}, c} is a constant, andαmin,Cαmin,C\\\\alpha\\_{\\\\text{min}, C} is the scaling exponent.\n*   **Sample efficiency**: Larger models trained with the same amount of data achieve better performance due to their improved ability to utilize the data.\n*   **Training dynamics**: Training curves follow predictable power-laws, allowing early extrapolation to predict the final performance of the model.\n*   **Generalization**: Performance on different datasets improves consistently with the performance on the training dataset, suggesting that better in-distribution performance translates to better out-of-distribution performance.\n*   **Model size vs. dataset size**: As model size increases, the dataset size should be scaled sublinearly to avoid overfitting, implying that moderately increasing data is sufficient for much larger models.\n*   **Compute-efficient training**: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.\n    \n*   These findings provide a framework for understanding and predicting the performance of large-scale neural language models, guiding future research and practical applications in optimizing model training and deployment.\n\n*   WhereLLL is the loss,NNN is the number of non-embedding parameters,NcNcN\\_c is a constant, andαNαN\\\\alpha\\_N is the scaling exponent.\n\n*   WhereDDD is the dataset size in tokens,DcDcD\\_c is a constant, andαDαD\\\\alpha\\_D is the scaling exponent.\n\n*   WhereCminCminC\\_{\\\\text{min}} is the minimum compute required,Cmin,cCmin,cC\\_{\\\\text{min}, c} is a constant, andαmin,Cαmin,C\\\\alpha\\_{\\\\text{min}, C} is the scaling exponent.\n\n**Compute-efficient training**: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.",
    "contentLength": 60479,
    "wordCount": 519,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#scaling-laws-for-neural-language-models"
  },
  {
    "id": "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "A Tutorial Introduction to the Minimum Description Length Principle",
    "order": 23,
    "orderInChapter": 23,
    "contentHtml": "<ul>\n  <li>Authors: Peter Grünwald</li>\n  <li>\n    <p>This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.</p>\n  </li>\n  <li>Key Technical Details:\n    <ol>\n      <li>\n        <p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n      </li>\n      <li>\n        <p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n      </li>\n      <li>\n        <p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n      </li>\n      <li>\n        <p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n      </li>\n      <li>\n        <p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n      </li>\n      <li>\n        <p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n      </li>\n      <li>\n        <p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>\n      </li>\n    </ol>\n  </li>\n  <li>The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .</li>\n</ul>\n<p>This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.</p>\n<ol>\n      <li>\n        <p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n      </li>\n      <li>\n        <p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n      </li>\n      <li>\n        <p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n      </li>\n      <li>\n        <p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n      </li>\n      <li>\n        <p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n      </li>\n      <li>\n        <p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n      </li>\n      <li>\n        <p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>\n      </li>\n    </ol>\n<p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n<p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n<p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n<p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n<p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n<p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n<p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>",
    "contentMarkdown": "*   Authors: Peter Grünwald\n*   This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.\n    \n*   Key Technical Details:\n    1.  **MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n        \n    2.  **Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n        \n    3.  **Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n        \n    4.  **Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n        \n    5.  **MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n        \n    6.  **Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n        \n    7.  **Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.\n        \n*   The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .\n\nThis paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.\n\n1.  **MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n    \n2.  **Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n    \n3.  **Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n    \n4.  **Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n    \n5.  **MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n    \n6.  **Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n    \n7.  **Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.\n    \n\n**MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n\n**Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n\n**Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n\n**Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n\n**MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n\n**Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n\n**Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.",
    "contentLength": 7841,
    "wordCount": 1021,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#a-tutorial-introduction-to-the-minimum-description-length-principle"
  },
  {
    "id": "ai-top-30-papers-machine-super-intelligence-24",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Machine Super Intelligence",
    "order": 24,
    "orderInChapter": 24,
    "contentHtml": "<ul>\n  <li>Shane Legg’s dissertation, “Machine Super Intelligence,” presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Framework for Intelligence Measures:</strong> Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.</p>\n  </li>\n  <li>\n    <p><strong>Superintelligence Pathways:</strong> The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.</p>\n  </li>\n  <li>\n    <p><strong>Algorithmic Insights into Intelligence:</strong> Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.</p>\n  </li>\n  <li>\n    <p><strong>Theoretical Models of Machine Learning:</strong> Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.</p>\n  </li>\n  <li>\n    <p><strong>Safety and Control:</strong> A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.</p>\n  </li>\n</ol>\n<p><strong>Framework for Intelligence Measures:</strong> Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.</p>\n<p><strong>Superintelligence Pathways:</strong> The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.</p>\n<p><strong>Algorithmic Insights into Intelligence:</strong> Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.</p>\n<p><strong>Theoretical Models of Machine Learning:</strong> Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.</p>\n<p><strong>Safety and Control:</strong> A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.</p>\n<ul>\n  <li>These components of Legg’s dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.</li>\n</ul>",
    "contentMarkdown": "*   Shane Legg’s dissertation, “Machine Super Intelligence,” presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:\n\n1.  **Framework for Intelligence Measures:** Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.\n    \n2.  **Superintelligence Pathways:** The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.\n    \n3.  **Algorithmic Insights into Intelligence:** Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.\n    \n4.  **Theoretical Models of Machine Learning:** Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.\n    \n5.  **Safety and Control:** A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.\n    \n\n**Framework for Intelligence Measures:** Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.\n\n**Superintelligence Pathways:** The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.\n\n**Algorithmic Insights into Intelligence:** Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.\n\n**Theoretical Models of Machine Learning:** Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.\n\n**Safety and Control:** A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.\n\n*   These components of Legg’s dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.",
    "contentLength": 4428,
    "wordCount": 536,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#machine-super-intelligence"
  },
  {
    "id": "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Kolmogorov Complexity and Algorithmic Randomness",
    "order": 25,
    "orderInChapter": 25,
    "contentHtml": "<ul>\n  <li>\n    <p>The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:</p>\n  </li>\n  <li><strong>Definition and Significance</strong>: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.</li>\n  <li>\n    <p><strong>Unpredictability and Random Sequences</strong>: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.</p>\n  </li>\n  <li>Theoretical Foundations\n    <ul>\n      <li><strong>Formalisms and Proofs</strong>: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.</li>\n      <li><strong>Incompressibility Method</strong>: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.</li>\n    </ul>\n  </li>\n  <li>Practical Applications\n    <ul>\n      <li><strong>Data Compression</strong>: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.</li>\n      <li><strong>Psychological Models</strong>: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.</li>\n    </ul>\n  </li>\n  <li>Advanced Topics\n    <ul>\n      <li><strong>Mutual Information</strong>: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.</li>\n      <li><strong>Conditional Complexity</strong>: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.</li>\n    </ul>\n  </li>\n  <li>Mathematical Rigor\n    <ul>\n      <li><strong>Deep Mathematical Analysis</strong>: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Future Directions</strong>: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.</p>\n  </li>\n  <li>This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.</li>\n</ul>\n<p>The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:</p>\n<p><strong>Unpredictability and Random Sequences</strong>: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.</p>\n<ul>\n      <li><strong>Formalisms and Proofs</strong>: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.</li>\n      <li><strong>Incompressibility Method</strong>: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.</li>\n    </ul>\n<ul>\n      <li><strong>Data Compression</strong>: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.</li>\n      <li><strong>Psychological Models</strong>: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.</li>\n    </ul>\n<ul>\n      <li><strong>Mutual Information</strong>: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.</li>\n      <li><strong>Conditional Complexity</strong>: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.</li>\n    </ul>\n<ul>\n      <li><strong>Deep Mathematical Analysis</strong>: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.</li>\n    </ul>\n<p><strong>Future Directions</strong>: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.</p>",
    "contentMarkdown": "*   The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:\n    \n*   **Definition and Significance**: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.\n*   **Unpredictability and Random Sequences**: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.\n    \n*   Theoretical Foundations\n    *   **Formalisms and Proofs**: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.\n    *   **Incompressibility Method**: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.\n*   Practical Applications\n    *   **Data Compression**: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.\n    *   **Psychological Models**: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.\n*   Advanced Topics\n    *   **Mutual Information**: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.\n    *   **Conditional Complexity**: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.\n*   Mathematical Rigor\n    *   **Deep Mathematical Analysis**: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.\n*   **Future Directions**: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.\n    \n*   This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.\n\nThe book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:\n\n**Unpredictability and Random Sequences**: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.\n\n*   **Formalisms and Proofs**: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.\n*   **Incompressibility Method**: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.\n\n*   **Data Compression**: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.\n*   **Psychological Models**: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.\n\n*   **Mutual Information**: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.\n*   **Conditional Complexity**: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.\n\n*   **Deep Mathematical Analysis**: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.\n\n**Future Directions**: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.",
    "contentLength": 6011,
    "wordCount": 719,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#kolmogorov-complexity-and-algorithmic-randomness"
  },
  {
    "id": "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Ilya Sutskever’s Top 30 Reading List",
    "title": "Stanford’s CS231n Convolutional Neural Networks for Visual Recognition",
    "order": 26,
    "orderInChapter": 26,
    "contentHtml": "<ul>\n  <li><strong>Purpose</strong>: The course introduces students to the fundamental concepts in convolutional neural networks (ConvNets) and their application in image recognition and processing tasks. ConvNets are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.</li>\n  <li>\n    <p><strong>Architectural Advantage</strong>: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.</p>\n  </li>\n  <li>Core Components of ConvNets\n    <ul>\n      <li><strong>Layers</strong>: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        <ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Training ConvNets\n    <ul>\n      <li><strong>Loss Functions</strong>: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.</li>\n      <li><strong>Backpropagation</strong>: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.</li>\n    </ul>\n  </li>\n  <li>Practical Challenges\n    <ul>\n      <li><strong>Overfitting</strong>: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.</li>\n      <li><strong>Hyperparameter Tuning</strong>: Includes selecting learning rates, learning rate decay, regularization constants, and more.</li>\n    </ul>\n  </li>\n  <li>Advanced Topics\n    <ul>\n      <li><strong>Batch Normalization</strong>: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.</li>\n      <li><strong>Transfer Learning and Fine-tuning</strong>: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Architectural Advantage</strong>: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.</p>\n<ul>\n      <li><strong>Layers</strong>: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        <ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n<ul>\n      <li><strong>Loss Functions</strong>: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.</li>\n      <li><strong>Backpropagation</strong>: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.</li>\n    </ul>\n<ul>\n      <li><strong>Overfitting</strong>: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.</li>\n      <li><strong>Hyperparameter Tuning</strong>: Includes selecting learning rates, learning rate decay, regularization constants, and more.</li>\n    </ul>\n<ul>\n      <li><strong>Batch Normalization</strong>: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.</li>\n      <li><strong>Transfer Learning and Fine-tuning</strong>: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.</li>\n    </ul>",
    "contentMarkdown": "*   **Purpose**: The course introduces students to the fundamental concepts in convolutional neural networks (ConvNets) and their application in image recognition and processing tasks. ConvNets are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.\n*   **Architectural Advantage**: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.\n    \n*   Core Components of ConvNets\n    *   **Layers**: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        *   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n        *   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n        *   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n*   Training ConvNets\n    *   **Loss Functions**: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.\n    *   **Backpropagation**: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.\n*   Practical Challenges\n    *   **Overfitting**: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.\n    *   **Hyperparameter Tuning**: Includes selecting learning rates, learning rate decay, regularization constants, and more.\n*   Advanced Topics\n    *   **Batch Normalization**: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.\n    *   **Transfer Learning and Fine-tuning**: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.\n\n**Architectural Advantage**: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.\n\n*   **Layers**: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n    *   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n    *   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n    *   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n\n*   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n*   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n*   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n\n*   **Loss Functions**: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.\n*   **Backpropagation**: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.\n\n*   **Overfitting**: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.\n*   **Hyperparameter Tuning**: Includes selecting learning rates, learning rate decay, regularization constants, and more.\n\n*   **Batch Normalization**: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.\n*   **Transfer Learning and Fine-tuning**: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.",
    "contentLength": 7184,
    "wordCount": 923,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#stanford’s-cs231n-convolutional-neural-networks-for-visual-recognition"
  },
  {
    "id": "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Meta",
    "title": "Better & Faster Large Language Models Via Multi-token Prediction",
    "order": 27,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve</p>\n  </li>\n  <li>The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled “Better &amp; Faster Large Language Models via Multi-token Prediction” suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.</li>\n  <li>The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.</li>\n  <li>At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.</li>\n  <li>The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.</li>\n  <li>Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.</li>\n  <li>Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.</li>\n  <li>Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.</li>\n</ul>\n<p>Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve</p>\n<h4 id=\"key-takeaways\">Key Takeaways:</h4>\n<ul>\n  <li>🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.</li>\n  <li>🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.</li>\n  <li>🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.</li>\n  <li>🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.</li>\n  <li>🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.</li>\n  <li>🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.</li>\n</ul>",
    "contentMarkdown": "*   Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve\n    \n*   The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled “Better & Faster Large Language Models via Multi-token Prediction” suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.\n*   The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.\n*   At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.\n*   The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.\n*   Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.\n*   Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.\n*   Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.\n\nAuthors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve\n\n#### Key Takeaways:\n\n*   🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.\n*   🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.\n*   🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.\n*   🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.\n*   🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.\n*   🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.",
    "contentLength": 4681,
    "wordCount": 627,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#better-&-faster-large-language-models-via-multi-token-prediction"
  },
  {
    "id": "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Meta",
    "title": "Dense Passage Retrieval for Open-Domain Question Answering",
    "order": 28,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih</li>\n  <li>In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.</li>\n  <li>Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.</li>\n  <li>Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.</li>\n  <li>Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.</li>\n</ul>\n<h4 id=\"dense-passage-retriever-dpr\">Dense Passage Retriever (DPR):</h4>\n<ol>\n  <li><strong>Purpose</strong>: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.</li>\n  <li><strong>Key Task</strong>: Given a large number <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">M</script> of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-285\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-286\"><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">k</script> most relevant passages for a given input question. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">M</script> can be very large, like 21 million passages, but <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-291\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">k</script> (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.</li>\n  <li><strong>DPR’s Mechanism</strong>:\n    <ul>\n      <li><strong>Dense Encoder for Passages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-294\" style=\"width: 2.792em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.275em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.22em, 2.534em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">EP(\\cdot)</script></strong>: It converts any text passage to a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-301\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">d</script>-dimensional real-valued vector. This encoder processes and indexes all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">M</script> passages for retrieval.</li>\n      <li><strong>Encoder for Questions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 2.947em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.43em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.38em, 2.585em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">EQ(\\cdot)</script></strong>: At runtime, when a question is posed, this encoder turns the question into a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">d</script>-dimensional vector.</li>\n      <li><strong>Similarity Measurement</strong>: The similarity between a question and a passage is calculated using the dot product of their respective vectors: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-317\" style=\"width: 12.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1010.52em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">m</span><span class=\"mo\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">sim(q, p) = EQ(q) \\cdot EP(p)</script>.</li>\n    </ul>\n  </li>\n  <li><strong>Passage Size and Boundaries</strong>: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.</li>\n  <li><strong>Encoders Implementation</strong>: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the [CLS] token as the output, meaning the output vector has 768 dimensions.</li>\n  <li><strong>Inference</strong>: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">k</script> passages with the closest embeddings are retrieved.</li>\n  <li><strong>Training</strong>:\n    <ul>\n      <li>The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.</li>\n      <li>The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.</li>\n      <li>For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.</li>\n    </ul>\n  </li>\n  <li><strong>In-batch Negatives</strong>: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.</li>\n</ol>\n<ul>\n      <li><strong>Dense Encoder for Passages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-294\" style=\"width: 2.792em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.275em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.22em, 2.534em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">EP(\\cdot)</script></strong>: It converts any text passage to a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-301\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">d</script>-dimensional real-valued vector. This encoder processes and indexes all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">M</script> passages for retrieval.</li>\n      <li><strong>Encoder for Questions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 2.947em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.43em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.38em, 2.585em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">EQ(\\cdot)</script></strong>: At runtime, when a question is posed, this encoder turns the question into a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">d</script>-dimensional vector.</li>\n      <li><strong>Similarity Measurement</strong>: The similarity between a question and a passage is calculated using the dot product of their respective vectors: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-317\" style=\"width: 12.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1010.52em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">m</span><span class=\"mo\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">sim(q, p) = EQ(q) \\cdot EP(p)</script>.</li>\n    </ul>\n<ul>\n      <li>The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.</li>\n      <li>The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.</li>\n      <li>For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\n*   In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.\n*   Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.\n*   Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.\n*   Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.\n\n#### Dense Passage Retriever (DPR):\n\n1.  **Purpose**: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.\n2.  **Key Task**: Given a large number MMM of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top kkk most relevant passages for a given input question. MMM can be very large, like 21 million passages, but kkk (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.\n3.  **DPR’s Mechanism**:\n    *   **Dense Encoder for Passages EP(⋅)EP(⋅)EP(\\\\cdot)**: It converts any text passage to a ddd\\-dimensional real-valued vector. This encoder processes and indexes all MMM passages for retrieval.\n    *   **Encoder for Questions EQ(⋅)EQ(⋅)EQ(\\\\cdot)**: At runtime, when a question is posed, this encoder turns the question into a ddd\\-dimensional vector.\n    *   **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: sim(q,p)\\=EQ(q)⋅EP(p)sim(q,p)\\=EQ(q)⋅EP(p)sim(q, p) = EQ(q) \\\\cdot EP(p).\n4.  **Passage Size and Boundaries**: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.\n5.  **Encoders Implementation**: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the \\[CLS\\] token as the output, meaning the output vector has 768 dimensions.\n6.  **Inference**: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top kkk passages with the closest embeddings are retrieved.\n7.  **Training**:\n    *   The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.\n    *   The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.\n    *   For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.\n8.  **In-batch Negatives**: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.\n\n*   **Dense Encoder for Passages EP(⋅)EP(⋅)EP(\\\\cdot)**: It converts any text passage to a ddd\\-dimensional real-valued vector. This encoder processes and indexes all MMM passages for retrieval.\n*   **Encoder for Questions EQ(⋅)EQ(⋅)EQ(\\\\cdot)**: At runtime, when a question is posed, this encoder turns the question into a ddd\\-dimensional vector.\n*   **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: sim(q,p)\\=EQ(q)⋅EP(p)sim(q,p)\\=EQ(q)⋅EP(p)sim(q, p) = EQ(q) \\\\cdot EP(p).\n\n*   The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.\n*   The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.\n*   For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.",
    "contentLength": 35251,
    "wordCount": 893,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#dense-passage-retrieval-for-open-domain-question-answering"
  },
  {
    "id": "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Meta",
    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "order": 29,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.</li>\n  <li>Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.</li>\n  <li>The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.</li>\n  <li>The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.</li>\n  <li>RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.</li>\n  <li>In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.</li>\n  <li>RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.</li>\n  <li>On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.</li>\n  <li>This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.</li>\n</ul>",
    "contentMarkdown": "*   The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.\n*   Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.\n*   The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.\n*   The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.\n*   RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.\n*   In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.\n*   RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.\n*   On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.\n*   This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.",
    "contentLength": 2229,
    "wordCount": 286,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks"
  },
  {
    "id": "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "HuggingFace",
    "title": "Zephyr: Direct Distillation of LM Alignment",
    "order": 30,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf</li>\n  <li>The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    <ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n  </li>\n  <li>They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.</li>\n  <li>Results:\n    <ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n  </li>\n  <li>The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.</li>\n  <li>The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.</li>\n  <li>Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.</li>\n  <li>The image below <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\">(source)</a> gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.</li>\n</ul>\n<ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n<ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n<p><img src=\"assets/paper/1.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf\n*   The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n    2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n    3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n*   They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.\n*   Results:\n    *   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n    *   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n    *   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n*   The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.\n*   The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.\n*   Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.\n*   The image below [(source)](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.\n\n1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n\n*   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n*   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n*   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n\n![](assets/paper/1.png)",
    "contentLength": 4920,
    "wordCount": 691,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/top-30-papers/#zephyr:-direct-distillation-of-lm-alignment"
  },
  {
    "id": "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Stanford",
    "title": "Lost in the Middle: How Language Models Use Long Contexts",
    "order": 31,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.</li>\n  <li>The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.</li>\n  <li>Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).</li>\n  <li>The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.</li>\n  <li>A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.</li>\n  <li>The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.</li>\n  <li>The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.</li>\n  <li>Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.</li>\n  <li>In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.</li>\n</ul>",
    "contentMarkdown": "*   This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.\n*   The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.\n*   Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).\n*   The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.\n*   A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.\n*   The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.\n*   The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.\n*   Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.\n*   In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.",
    "contentLength": 3111,
    "wordCount": 424,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#lost-in-the-middle:-how-language-models-use-long-contexts"
  },
  {
    "id": "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Misc",
    "title": "Precise Zero-Shot Dense Retrieval Without Relevance Labels",
    "order": 32,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.</li>\n  <li>Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.</li>\n  <li>HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.</li>\n  <li>Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.</li>\n  <li>The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.</li>\n</ul>",
    "contentMarkdown": "*   The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.\n*   Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.\n*   HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.\n*   Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.\n*   The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.",
    "contentLength": 2298,
    "wordCount": 305,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#precise-zero-shot-dense-retrieval-without-relevance-labels"
  },
  {
    "id": "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Misc",
    "title": "ALCUNA: Large Language Models Meet New Knowledge",
    "order": 33,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan</li>\n  <li>The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.</li>\n  <li>KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.</li>\n  <li>ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.</li>\n  <li>Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.</li>\n  <li>Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.</li>\n  <li>The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.</li>\n  <li>The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.</li>\n  <li>This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.</li>\n</ul>",
    "contentMarkdown": "*   Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan\n*   The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.\n*   KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.\n*   ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.\n*   Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.\n*   Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.\n*   The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.\n*   The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.\n*   This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.",
    "contentLength": 1917,
    "wordCount": 265,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#alcuna:-large-language-models-meet-new-knowledge"
  },
  {
    "id": "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
    "articleSlug": "top-30-papers",
    "articleTitle": "Ilya Sutskever’s Top 30",
    "category": "Miscellaneous",
    "chapter": "Misc",
    "title": "The Perils & Promises of Fact-checking with Large Language Models",
    "order": 34,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Authors: Dorian Quelle &amp; Alexandre Bovet</li>\n  <li>The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.</li>\n  <li>They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.</li>\n  <li>\n    <p>Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.</p>\n  </li>\n  <li>\n    <p>Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.</p>\n  </li>\n  <li>Key Results:\n    <ul>\n      <li>GPT-4 outperformed GPT-3.5 overall.</li>\n      <li>Providing context significantly improved accuracy, highlighting the importance of evidence gathering.</li>\n      <li>Models struggled with ambiguous “half-true” type verdicts.</li>\n      <li>Performance varied across languages - non-English claims saw a boost when translated to English first.</li>\n      <li>No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.</li>\n    </ul>\n  </li>\n  <li>Limitations:\n    <ul>\n      <li>Biased evaluation due to use of GPT-4 as a scorer.</li>\n      <li>Did not explore model scaling or curating better training data.</li>\n      <li>Safety/ethics of potential misinformation not addressed.</li>\n    </ul>\n  </li>\n  <li>Implications:\n    <ul>\n      <li>LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.</li>\n      <li>Critical examination of LLM reasoning is important before deployment.</li>\n      <li>Understanding model limitations and language-specific differences is key.</li>\n      <li>Continued learning after initial training needs more investigation.</li>\n    </ul>\n  </li>\n  <li>The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.</li>\n</ul>\n<p>Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.</p>\n<p>Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.</p>\n<ul>\n      <li>GPT-4 outperformed GPT-3.5 overall.</li>\n      <li>Providing context significantly improved accuracy, highlighting the importance of evidence gathering.</li>\n      <li>Models struggled with ambiguous “half-true” type verdicts.</li>\n      <li>Performance varied across languages - non-English claims saw a boost when translated to English first.</li>\n      <li>No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.</li>\n    </ul>\n<ul>\n      <li>Biased evaluation due to use of GPT-4 as a scorer.</li>\n      <li>Did not explore model scaling or curating better training data.</li>\n      <li>Safety/ethics of potential misinformation not addressed.</li>\n    </ul>\n<ul>\n      <li>LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.</li>\n      <li>Critical examination of LLM reasoning is important before deployment.</li>\n      <li>Understanding model limitations and language-specific differences is key.</li>\n      <li>Continued learning after initial training needs more investigation.</li>\n    </ul>",
    "contentMarkdown": "*   Authors: Dorian Quelle & Alexandre Bovet\n*   The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.\n*   They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.\n*   Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.\n    \n*   Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.\n    \n*   Key Results:\n    *   GPT-4 outperformed GPT-3.5 overall.\n    *   Providing context significantly improved accuracy, highlighting the importance of evidence gathering.\n    *   Models struggled with ambiguous “half-true” type verdicts.\n    *   Performance varied across languages - non-English claims saw a boost when translated to English first.\n    *   No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.\n*   Limitations:\n    *   Biased evaluation due to use of GPT-4 as a scorer.\n    *   Did not explore model scaling or curating better training data.\n    *   Safety/ethics of potential misinformation not addressed.\n*   Implications:\n    *   LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.\n    *   Critical examination of LLM reasoning is important before deployment.\n    *   Understanding model limitations and language-specific differences is key.\n    *   Continued learning after initial training needs more investigation.\n*   The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.\n\nMotivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.\n\nMethods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.\n\n*   GPT-4 outperformed GPT-3.5 overall.\n*   Providing context significantly improved accuracy, highlighting the importance of evidence gathering.\n*   Models struggled with ambiguous “half-true” type verdicts.\n*   Performance varied across languages - non-English claims saw a boost when translated to English first.\n*   No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.\n\n*   Biased evaluation due to use of GPT-4 as a scorer.\n*   Did not explore model scaling or curating better training data.\n*   Safety/ethics of potential misinformation not addressed.\n\n*   LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.\n*   Critical examination of LLM reasoning is important before deployment.\n*   Understanding model limitations and language-specific differences is key.\n*   Continued learning after initial training needs more investigation.",
    "contentLength": 4176,
    "wordCount": 505,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/top-30-papers/#the-perils-&-promises-of-fact-checking-with-large-language-models"
  }
]