<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • Ilya Sutskever's Top 30</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/top-30-papers/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Ftop-30-papers&amp;ers=2"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWUaz0Z2Jm8QjFwQPTtwwjWxfEzX1FYTkTs7yRc08cgaO43DaB1fRaRGPmuDWPQOSarJWODvFFmhZmS2LVGKsajJcoC4cEi0w52Zq5QOZiJM2Dk9Gk67JkZWTkrTsUDsilYpRgr_A==?fccs=W1siQUtzUm9sOEpsWEF4WFRQYVlFNGlQaHhPSEg2MDhVMmVXYk1XSnBMemg4X1BSTjMyUC1oQlBUV3Yyb1ozcFUzNXRkeEZRU3YwS0lOM01zc0ZIdWNpeUVfcmozbEpnV1NOaHE5Z2tNcjdyVENwM25vNnhkSkd3WmlTNXp6UUVTbFhqb2F5Rm5ZZGswM0ExYXdZVFFHVEJBWXFwQ0lFRkU2NWd3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5OTQsMzE0MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS90b3AtMzAtcGFwZXJzLyIsbnVsbCxbWzgsInNDaE5INU9zYWswIl0sWzksImVuLVVTIl0sWzI2LCIxMCJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbMzEwODIyNTRdXSJdLFsyOSwiZmFsc2UiXV1d"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxWVKIF0wpaU-43pwvhRsqe9X6UvuxOgBvphsYNqOJkvMKHpDT-f6ZGbgGfiYEOQoaU07VQ3QoF96Ht4jb_Lx7iZV5LqRVAZqx2KTc_qeME61-450bQQcTS3AIY0I0Sltu8TOMx8NQ==?fccs=W1siQUtzUm9sOEpsWEF4WFRQYVlFNGlQaHhPSEg2MDhVMmVXYk1XSnBMemg4X1BSTjMyUC1oQlBUV3Yyb1ozcFUzNXRkeEZRU3YwS0lOM01zc0ZIdWNpeUVfcmozbEpnV1NOaHE5Z2tNcjdyVENwM25vNnhkSkd3WmlTNXp6UUVTbFhqb2F5Rm5ZZGswM0ExYXdZVFFHVEJBWXFwQ0lFRkU2NWd3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5OTQsNTI1MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5XSxudWxsLDIsbnVsbCwiZW4iXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvdG9wLTMwLXBhcGVycy8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsyNiwiMTAiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI1LCJbWzMxMDgyMjU0XV0iXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxX355pPJfMl7mURweVVO667WvXwbOjhwKjI07RUHSSVxhIqWIfwk116UFBcmDUVnrh0o1G5nx9DijvdSeeOdVmFNsu4V985mEm3tnnMNofOXkLSYnQeC0WABTK0T_FakaWZU7fj_Q==?fccs=W1siQUtzUm9sOEpsWEF4WFRQYVlFNGlQaHhPSEg2MDhVMmVXYk1XSnBMemg4X1BSTjMyUC1oQlBUV3Yyb1ozcFUzNXRkeEZRU3YwS0lOM01zc0ZIdWNpeUVfcmozbEpnV1NOaHE5Z2tNcjdyVENwM25vNnhkSkd3WmlTNXp6UUVTbFhqb2F5Rm5ZZGswM0ExYXdZVFFHVEJBWXFwQ0lFRkU2NWd3PT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI5OTUsNDU4MDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbNyw5LDZdLG51bGwsMixudWxsLCJlbiIsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLDFdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS90b3AtMzAtcGFwZXJzLyIsbnVsbCxbWzgsInNDaE5INU9zYWswIl0sWzksImVuLVVTIl0sWzI2LCIxMCJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjUsIltbMzEwODIyNTRdXSJdLFsyOSwiZmFsc2UiXV1d"></script></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • Ilya Sutskever's Top 30</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#ilya-sutskevers-top-30-reading-list" id="markdown-toc-ilya-sutskevers-top-30-reading-list">Ilya Sutskever’s Top 30 Reading List</a>    <ul>
      <li><a href="#the-first-law-of-complexodynamics" id="markdown-toc-the-first-law-of-complexodynamics">The First Law of Complexodynamics</a></li>
      <li><a href="#the-unreasonable-effectiveness-of-recurrent-neural-networks" id="markdown-toc-the-unreasonable-effectiveness-of-recurrent-neural-networks">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
      <li><a href="#understanding-lstm-networks" id="markdown-toc-understanding-lstm-networks">Understanding LSTM Networks</a></li>
      <li><a href="#recurrent-neural-network-regularization" id="markdown-toc-recurrent-neural-network-regularization">Recurrent Neural Network Regularization</a></li>
      <li><a href="#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights" id="markdown-toc-keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights">Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</a></li>
      <li><a href="#pointer-networks" id="markdown-toc-pointer-networks">Pointer Networks</a></li>
      <li><a href="#imagenet-classification-with-deep-convolutional-neural-networks" id="markdown-toc-imagenet-classification-with-deep-convolutional-neural-networks">ImageNet Classification with Deep Convolutional Neural Networks</a></li>
      <li><a href="#order-matters-sequence-to-sequence-for-sets" id="markdown-toc-order-matters-sequence-to-sequence-for-sets">Order Matters: Sequence to Sequence for Sets</a></li>
      <li><a href="#gpipe-easy-scaling-with-micro-batch-pipeline-parallelism" id="markdown-toc-gpipe-easy-scaling-with-micro-batch-pipeline-parallelism">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></li>
      <li><a href="#deep-residual-learning-for-image-recognition" id="markdown-toc-deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></li>
      <li><a href="#multi-scale-context-aggregation-by-dilated-convolutions" id="markdown-toc-multi-scale-context-aggregation-by-dilated-convolutions">Multi-Scale Context Aggregation by Dilated Convolutions</a></li>
      <li><a href="#neural-message-passing-for-quantum-chemistry" id="markdown-toc-neural-message-passing-for-quantum-chemistry">Neural Message Passing for Quantum Chemistry</a></li>
      <li><a href="#attention-is-all-you-need" id="markdown-toc-attention-is-all-you-need">Attention is All You Need</a></li>
      <li><a href="#neural-machine-translation-by-jointly-learning-to-align-and-translate" id="markdown-toc-neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
      <li><a href="#identity-mappings-in-deep-residual-networks" id="markdown-toc-identity-mappings-in-deep-residual-networks">Identity Mappings in Deep Residual Networks</a></li>
      <li><a href="#a-simple-neural-network-module-for-relational-reasoning" id="markdown-toc-a-simple-neural-network-module-for-relational-reasoning">A Simple Neural Network Module for Relational Reasoning</a></li>
      <li><a href="#variational-lossy-autoencoder" id="markdown-toc-variational-lossy-autoencoder">Variational Lossy Autoencoder</a></li>
      <li><a href="#relational-recurrent-neural-networks" id="markdown-toc-relational-recurrent-neural-networks">Relational Recurrent Neural Networks</a></li>
      <li><a href="#quantifying-the-rise-and-fall-of-complexity-in-closed-systems-the-coffee-automaton" id="markdown-toc-quantifying-the-rise-and-fall-of-complexity-in-closed-systems-the-coffee-automaton">Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton</a></li>
      <li><a href="#neural-turing-machines" id="markdown-toc-neural-turing-machines">Neural Turing Machines</a></li>
      <li><a href="#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin" id="markdown-toc-deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></li>
      <li><a href="#scaling-laws-for-neural-language-models" id="markdown-toc-scaling-laws-for-neural-language-models">Scaling Laws for Neural Language Models</a></li>
      <li><a href="#a-tutorial-introduction-to-the-minimum-description-length-principle" id="markdown-toc-a-tutorial-introduction-to-the-minimum-description-length-principle">A Tutorial Introduction to the Minimum Description Length Principle</a></li>
      <li><a href="#machine-super-intelligence" id="markdown-toc-machine-super-intelligence">Machine Super Intelligence</a></li>
      <li><a href="#kolmogorov-complexity-and-algorithmic-randomness" id="markdown-toc-kolmogorov-complexity-and-algorithmic-randomness">Kolmogorov Complexity and Algorithmic Randomness</a></li>
      <li><a href="#stanfords-cs231n-convolutional-neural-networks-for-visual-recognition" id="markdown-toc-stanfords-cs231n-convolutional-neural-networks-for-visual-recognition">Stanford’s CS231n Convolutional Neural Networks for Visual Recognition</a></li>
    </ul>
  </li>
  <li><a href="#meta" id="markdown-toc-meta">Meta</a>    <ul>
      <li><a href="#better--faster-large-language-models-via-multi-token-prediction" id="markdown-toc-better--faster-large-language-models-via-multi-token-prediction">Better &amp; Faster Large Language Models Via Multi-token Prediction</a>        <ul>
          <li><a href="#key-takeaways" id="markdown-toc-key-takeaways">Key Takeaways:</a></li>
        </ul>
      </li>
      <li><a href="#dense-passage-retrieval-for-open-domain-question-answering" id="markdown-toc-dense-passage-retrieval-for-open-domain-question-answering">Dense Passage Retrieval for Open-Domain Question Answering</a>        <ul>
          <li><a href="#dense-passage-retriever-dpr" id="markdown-toc-dense-passage-retriever-dpr">Dense Passage Retriever (DPR):</a></li>
        </ul>
      </li>
      <li><a href="#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks" id="markdown-toc-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
    </ul>
  </li>
  <li><a href="#huggingface" id="markdown-toc-huggingface">HuggingFace</a>    <ul>
      <li><a href="#zephyr-direct-distillation-of-lm-alignment" id="markdown-toc-zephyr-direct-distillation-of-lm-alignment">Zephyr: Direct Distillation of LM Alignment</a></li>
    </ul>
  </li>
  <li><a href="#stanford" id="markdown-toc-stanford">Stanford</a>    <ul>
      <li><a href="#lost-in-the-middle-how-language-models-use-long-contexts" id="markdown-toc-lost-in-the-middle-how-language-models-use-long-contexts">Lost in the Middle: How Language Models Use Long Contexts</a></li>
    </ul>
  </li>
  <li><a href="#misc" id="markdown-toc-misc">Misc</a>    <ul>
      <li><a href="#precise-zero-shot-dense-retrieval-without-relevance-labels" id="markdown-toc-precise-zero-shot-dense-retrieval-without-relevance-labels">Precise Zero-Shot Dense Retrieval Without Relevance Labels</a></li>
      <li><a href="#alcuna-large-language-models-meet-new-knowledge" id="markdown-toc-alcuna-large-language-models-meet-new-knowledge">ALCUNA: Large Language Models Meet New Knowledge</a></li>
      <li><a href="#the-perils--promises-of-fact-checking-with-large-language-models" id="markdown-toc-the-perils--promises-of-fact-checking-with-large-language-models">The Perils &amp; Promises of Fact-checking with Large Language Models</a></li>
    </ul>
  </li>
</ul>

<h2 id="ilya-sutskevers-top-30-reading-list">Ilya Sutskever’s Top 30 Reading List</h2>

<ul>
  <li>Ilya Sutskever shared a list of 30 papers with John Carmack and said, “If you really learn all of these, you’ll know 90% of what matters today”. Below we will review these <a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">papers/resources</a>.</li>
</ul>

<h3 id="the-first-law-of-complexodynamics"><a href="https://scottaaronson.blog/?p=762">The First Law of Complexodynamics</a></h3>
<ul>
  <li>Author: Scott Aaronson</li>
  <li>The article “The First Law of Complexodynamics” discusses an intriguing question posed by Sean Carroll at the FQXi’s Setting Time Aright conference, which brought together experts from various fields to discuss the nature of time. Carroll’s question revolves around why the complexity of physical systems seems to increase, hit a maximum, and then decrease over time, unlike entropy, which consistently increases.</li>
  <li>The article explains that entropy measures how disordered a system is and increases monotonically. However, complexity behaves differently, peaking at intermediate times before decreasing. To delve into this phenomenon, the author introduces concepts from Kolmogorov complexity. Kolmogorov complexity is defined as the length of the shortest computer program that can produce a given string. A related concept, sophistication, measures the complexity of a string as the shortest program describing a set of which the string is a typical member.</li>
  <li>To address Carroll’s question, the author proposes the concept of “complextropy” as a measure of complexity that considers computational resource bounds. Complextropy should reflect the number of bits in the shortest efficient program that outputs a sample from a set such that the target string appears random with respect to that set. The conjecture is that complextropy will be small at the beginning and end of a system’s evolution but large at intermediate times, mirroring the observed pattern in complexity.</li>
  <li>Proving this conjecture, either theoretically or empirically, presents challenges, particularly due to the difficulty of computing complextropy. One practical approach suggested is using the size of a gzip compressed file as an approximation for Kolmogorov complexity. The author mentions an ongoing research project aimed at empirically verifying the conjecture using this method.</li>
  <li>The article also the idea that complexity, or complextropy, changes over time, peaking at intermediate stages. The author suggests using computational resource bounds to define this measure and discusses both theoretical and empirical approaches to validating the conjecture that complexity behaves in this manner. This exploration provides valuable insights into understanding the dynamic nature of complexity in physical systems.</li>
</ul>

<h3 id="the-unreasonable-effectiveness-of-recurrent-neural-networks"><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></h3>
<ul>
  <li>Author: Andrej Karpathy</li>
  <li>The article “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy dives into the amazing abilities of Recurrent Neural Networks (RNNs). Karpathy talks about his first experience with training RNNs for image captioning, where even with random settings, the RNN started making believable image descriptions. This success was surprising because many people thought RNNs were hard to train, showing just how simple and powerful they can be.</li>
  <li>RNNs are special because they can handle sequences of vectors, making them perfect for tasks that involve sequences as input and output. Unlike regular neural networks that deal with fixed-size inputs and outputs, RNNs can work with sequences of any length, making them very useful in many areas. Karpathy explains that RNNs work by keeping a hidden state that stores information from previous inputs, allowing them to “remember” past data.</li>
  <li>Karpathy goes into detail about how RNNs work, including a simple interface where an input vector affects the output vector and considers all previous inputs. He shows how RNNs update their hidden state using matrix multiplications and non-linear functions. He also mentions Long Short-Term Memory (LSTM) networks, which are a more advanced type of RNN that solve some practical issues and are widely used.</li>
  <li>To show how powerful RNNs can be, Karpathy describes training character-level language models. By feeding a large amount of text into an RNN, it learns to predict the next character in a sequence, allowing it to create text one character at a time. He gives examples of RNN-generated text from different sources, like Paul Graham’s essays, Shakespeare’s works, Wikipedia articles, algebraic geometry in LaTeX, Linux source code, and baby names. These examples show how RNNs can learn complex structures, grammar, and context from raw text.</li>
  <li>Karpathy also talks about the training process and how the text generated by the RNN improves over time, showing how the model gradually gets better at understanding language. He visualizes the inner workings of the RNN, showing how different neurons react to specific patterns, like URLs or markdown syntax, which helps explain how the model learns.</li>
  <li>Finally, Karpathy encourages readers to try out RNNs using the code he shared on GitHub, highlighting the fun and educational aspects of training character-level language models. He briefly touches on the bigger picture of RNN research and their growing importance in fields like natural language processing, computer vision, and machine learning. The article wraps up with a fun note, showing an RNN-generated sample from the article itself, proving how effective and versatile RNNs are.</li>
</ul>

<h3 id="understanding-lstm-networks"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></h3>
<ul>
  <li>Author: Christopher Olah</li>
  <li>The article “Understanding LSTM Networks” by Christopher Olah explains the structure and functioning of Long Short-Term Memory (LSTM) networks, a special kind of Recurrent Neural Network (RNN) that addresses the limitations of traditional RNNs in handling long-term dependencies.</li>
  <li>Olah begins by highlighting the limitations of traditional neural networks and RNNs in maintaining persistent information, which is crucial for tasks involving sequences and lists, such as language modeling, translation, and speech recognition.</li>
  <li>RNNs have loops that allow information to persist, making them suitable for sequential data. However, they struggle with long-term dependencies, where relevant information from earlier inputs is needed much later in the sequence.</li>
  <li>The article introduces LSTMs, designed to overcome this limitation. LSTMs have a unique architecture that includes a cell state and three gates (input, forget, and output) that regulate the flow of information. These gates allow LSTMs to remember and forget information selectively, making them effective in learning long-term dependencies.</li>
  <li>The forget gate decides what information to discard from the cell state, the input gate determines which new information to add, and the output gate controls what information is passed to the next step.</li>
  <li>Olah explains the step-by-step functioning of LSTMs using diagrams and notations, making it easier to understand the complex interactions within the network. He also discusses variations of LSTMs, such as peephole connections and Gated Recurrent Units (GRUs), which offer different ways to handle long-term dependencies.</li>
  <li>The article concludes by emphasizing the significance of LSTMs in achieving remarkable results in various applications and hints at future advancements in RNN research, such as attention mechanisms and Grid LSTMs, which further enhance the capabilities of neural networks.</li>
</ul>

<h3 id="recurrent-neural-network-regularization"><a href="https://arxiv.org/abs/1409.2329v5">Recurrent Neural Network Regularization</a></h3>

<ul>
  <li>Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals</li>
  <li>The paper “Recurrent Neural Network Regularization” presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections, which hampers learning. The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the network’s ability to retain information over long sequences while reducing overfitting.</li>
  <li>The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved better word-level perplexity on the Penn Tree Bank dataset compared to non-regularized models. The medium and large regularized LSTMs showed substantial reductions in perplexity, highlighting the efficacy of the proposed method.</li>
  <li>For speech recognition, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves frame accuracy, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved better generalization, indicating the potential of the proposed regularization technique for improving acoustic modeling.</li>
  <li>In machine translation, the method was evaluated on the WMT’14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating higher BLEU scores, which measure translation quality. Although the regularized LSTM did not surpass the phrase-based LIUM SMT system, the results affirmed that dropout enhances translation performance.</li>
  <li>The image caption generation task involved testing the dropout variant on an LSTM model that converts image vectors into captions. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps improve caption quality, with regularized models performing comparably to model ensembles.</li>
  <li>Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.</li>
</ul>

<h3 id="keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights"><a href="https://www.cs.toronto.edu/~hinton/absps/colt93.pdf">Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</a></h3>
<ul>
  <li>Authors: Geoffrey E. Hinton and Drew van Camp</li>
  <li>The paper “Keeping Neural Networks Simple by Minimizing the Description Length of the Weights” by Hinton and van Camp introduces a method to regularize neural networks by penalizing the information content in the weights. The key idea is to add Gaussian noise to the weights and adapt the noise level during training to balance the trade-off between the network’s error and the complexity of the weights.</li>
  <li>The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.</li>
  <li>By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the network’s performance while keeping the weights as simple as possible.</li>
  <li>The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.</li>
  <li>The authors introduce the concept of “noisy weights” where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the network’s error.</li>
  <li>The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.</li>
  <li>Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the network’s ability to generalize from limited data.</li>
  <li>Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.</li>
  <li>The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.</li>
</ul>

<h3 id="pointer-networks"><a href="https://arxiv.org/abs/1506.03134v2">Pointer Networks</a></h3>
<ul>
  <li>Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly</li>
  <li>
    <p>The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.</p>
  </li>
  <li><strong>Key Contributions:</strong>
    <ul>
      <li>The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.</li>
      <li>Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.</li>
      <li>The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.</li>
    </ul>
  </li>
  <li><strong>Models:</strong>
    <ul>
      <li><strong>Sequence-to-Sequence Model:</strong> This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.</li>
      <li><strong>Content Based Input Attention:</strong> An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.</li>
      <li><strong>Pointer Networks (Ptr-Net):</strong> Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.</li>
    </ul>
  </li>
  <li><strong>Empirical Results:</strong>
    <ul>
      <li><strong>Convex Hull:</strong> Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.</li>
      <li><strong>Delaunay Triangulation:</strong> Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.</li>
      <li><strong>Travelling Salesman Problem (TSP):</strong> Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.</li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong>
    <ul>
      <li>The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.</li>
    </ul>
  </li>
</ul>

<h3 id="imagenet-classification-with-deep-convolutional-neural-networks"><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></h3>
<ul>
  <li>Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</li>
  <li>
    <p>The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.</p>
  </li>
  <li><strong>Key Contributions:</strong>
    <ul>
      <li>The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.</li>
      <li>To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.</li>
      <li>The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.</li>
      <li>Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.</li>
      <li>Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.</li>
      <li>To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.</li>
      <li>Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.</li>
    </ul>
  </li>
  <li><strong>Empirical Results:</strong>
    <ul>
      <li>On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.</li>
      <li>On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.</li>
      <li>Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.</li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong>
    <ul>
      <li>The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.</li>
      <li>The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.</li>
    </ul>
  </li>
</ul>

<h3 id="order-matters-sequence-to-sequence-for-sets"><a href="https://arxiv.org/pdf/1511.06391">Order Matters: Sequence to Sequence for Sets</a></h3>
<ul>
  <li>Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur</li>
  <li>
    <p>The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.</p>
  </li>
  <li><strong>Key Contributions:</strong>
    <ul>
      <li>The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.</li>
      <li>They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.</li>
      <li>For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.</li>
    </ul>
  </li>
  <li><strong>Experiments and Results:</strong>
    <ul>
      <li><strong>Language Modeling:</strong> The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.</li>
      <li><strong>Combinatorial Problems:</strong> The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.</li>
      <li><strong>Graphical Models:</strong> The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.</li>
    </ul>
  </li>
  <li><strong>Model Architecture:</strong>
    <ul>
      <li><strong>Read, Process, Write Model:</strong> The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.</li>
      <li><strong>Attention Mechanisms:</strong> The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.</li>
      <li><strong>Finding Optimal Orderings:</strong> To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.</li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong>
    <ul>
      <li>The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.</li>
    </ul>
  </li>
</ul>

<h3 id="gpipe-easy-scaling-with-micro-batch-pipeline-parallelism"><a href="https://arxiv.org/abs/1811.06965">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></h3>
<ul>
  <li>Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen</li>
  <li>
    <p>The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.</p>
  </li>
  <li><strong>Key Contributions:</strong>
    <ul>
      <li><strong>GPipe Architecture:</strong> The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.</li>
      <li><strong>Batch-Splitting Pipeline Parallelism:</strong> GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.</li>
      <li><strong>Synchronous Gradient Descent:</strong> The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.</li>
    </ul>
  </li>
  <li><strong>Experiments and Results:</strong>
    <ul>
      <li><strong>Image Classification:</strong> GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.</li>
      <li><strong>Multilingual Neural Machine Translation:</strong> GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.</li>
    </ul>
  </li>
  <li><strong>Performance Optimization:</strong>
    <ul>
      <li><strong>Re-materialization:</strong> To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.</li>
      <li><strong>Load Balancing:</strong> The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.</li>
    </ul>
  </li>
  <li><strong>Design Features and Trade-Offs:</strong>
    <ul>
      <li><strong>Flexibility:</strong> GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.</li>
      <li><strong>Efficiency:</strong> By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.</li>
      <li><strong>Training Stability:</strong> The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.</li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong>
    <ul>
      <li>The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.</li>
    </ul>
  </li>
</ul>

<h3 id="deep-residual-learning-for-image-recognition"><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></h3>
<ul>
  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>
  <li>
    <p><strong>Affiliation</strong>: Microsoft Research</p>
  </li>
  <li>This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.</li>
</ul>

<p>Key innovations and findings from the paper include:</p>
<ol>
  <li><strong>Residual Learning Framework</strong>: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.</li>
  <li><strong>Ease of Optimization</strong>: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.</li>
  <li><strong>Superior Performance on Deep Networks</strong>: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.</li>
  <li><strong>Broad Applicability</strong>: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.</li>
</ol>

<ul>
  <li>These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.</li>
</ul>

<h3 id="multi-scale-context-aggregation-by-dilated-convolutions"><a href="https://arxiv.org/abs/1511.07122">Multi-Scale Context Aggregation by Dilated Convolutions</a></h3>
<ul>
  <li><strong>Authors</strong>: Fisher Yu, Vladlen Koltun</li>
  <li>
    <p><strong>Affiliations</strong>: Princeton University, Intel Labs</p>
  </li>
  <li>
    <p>The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.</p>
  </li>
  <li>Key Contributions:
    <ol>
      <li><strong>Dilated Convolutions</strong>:</li>
    </ol>
    <ul>
      <li>Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.</li>
      <li>Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.</li>
    </ul>
  </li>
</ul>

<ol>
  <li><strong>Multi-Scale Context Aggregation</strong>:
    <ul>
      <li>Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.</li>
      <li>The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.</li>
    </ul>
  </li>
  <li><strong>Simplified Network Design</strong>:
    <ul>
      <li>Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.</li>
      <li>Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.</li>
    </ul>
  </li>
  <li><strong>Controlled Experiments</strong>:
    <ul>
      <li>Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.</li>
      <li>Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.</li>
    </ul>
  </li>
  <li><strong>Performance Improvement</strong>:
    <ul>
      <li>The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.</li>
      <li>The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Experiments:
    <ul>
      <li><strong>Dataset</strong>: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.</li>
      <li><strong>Training Procedure</strong>: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.</li>
      <li><strong>Evaluation</strong>: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.</li>
    </ul>
  </li>
  <li>Conclusion:
    <ul>
      <li>The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.</li>
      <li>The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.</li>
      <li>The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.</li>
    </ul>
  </li>
</ul>

<h3 id="neural-message-passing-for-quantum-chemistry"><a href="https://arxiv.org/pdf/1704.01212">Neural Message Passing for Quantum Chemistry</a></h3>
<ul>
  <li>Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl</li>
  <li>The paper “Neural Message Passing for Quantum Chemistry” introduces Message Passing Neural Networks (MPNNs), a framework for supervised learning on molecular graphs that is invariant to molecular symmetries. The goal is to predict quantum mechanical properties of molecules, which is crucial in fields such as drug discovery and materials science.</li>
  <li><strong>Introduction</strong>:
    <ul>
      <li>The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.</li>
      <li>MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.</li>
    </ul>
  </li>
  <li><strong>Methodology</strong>:
    <ul>
      <li><strong>Message Passing Phase</strong>: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.
        <ul>
          <li>Formally, for a graph<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral-Italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">G</script> with node features<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="msubsup" id="MathJax-Span-6"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="mi" id="MathJax-Span-8" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-2">x_v</script> and edge features<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-9" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-10"><span class="msubsup" id="MathJax-Span-11"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-12" style="font-family: STIXGeneral-Italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="texatom" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span class="mi" id="MathJax-Span-16" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">w</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-3">e_{vw}</script>, the messages<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-17" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-18"><span class="msubsup" id="MathJax-Span-19"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-20" style="font-family: STIXGeneral-Italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-24" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-25" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-26" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>m</mi><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-4">m^{t+1}_v</script> and node updates<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-27" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-28"><span class="msubsup" id="MathJax-Span-29"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-30" style="font-family: STIXGeneral-Italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;"><span class="texatom" id="MathJax-Span-31"><span class="mrow" id="MathJax-Span-32"><span class="mi" id="MathJax-Span-33" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-34" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-35" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-36" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>h</mi><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-5">h^{t+1}_v</script> are given by:
 <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-37" style="width: 14.898em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-38"><span class="msubsup" id="MathJax-Span-39"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-40" style="font-family: STIXGeneral-Italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-41"><span class="mrow" id="MathJax-Span-42"><span class="mi" id="MathJax-Span-43" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-44" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-45" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-46" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-47" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="munderover" id="MathJax-Span-48" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 3.232em; height: 0px;"><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-49" style="font-family: STIXGeneral-Regular; vertical-align: 0.003em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.695em; left: 0.94em;"><span class="texatom" id="MathJax-Span-50"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">w</span><span class="mo" id="MathJax-Span-53" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">∈</span><span class="mi" id="MathJax-Span-54" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-55" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-56" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span class="mo" id="MathJax-Span-57" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-58" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-59" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="mi" id="MathJax-Span-60" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-61" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-62"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-63" style="font-family: STIXGeneral-Italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;"><span class="mi" id="MathJax-Span-64" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-65" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-66" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-67" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-68" style="font-family: STIXGeneral-Italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;"><span class="mi" id="MathJax-Span-69" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-70" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-71" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-72" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-73" style="font-family: STIXGeneral-Italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="texatom" id="MathJax-Span-74"><span class="mrow" id="MathJax-Span-75"><span class="mi" id="MathJax-Span-76" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span class="mi" id="MathJax-Span-77" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">w</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-78" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>m</mi><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">m^{t+1}_v = \sum_{w \in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>
 <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-79" style="width: 9.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-80"><span class="msubsup" id="MathJax-Span-81"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-82" style="font-family: STIXGeneral-Italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;"><span class="texatom" id="MathJax-Span-83"><span class="mrow" id="MathJax-Span-84"><span class="mi" id="MathJax-Span-85" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-86" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-87" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-88" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-89" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-90" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-91" style="font-family: STIXGeneral-Italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-92" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-93" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-94"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-95" style="font-family: STIXGeneral-Italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;"><span class="mi" id="MathJax-Span-96" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;"><span class="mi" id="MathJax-Span-97" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-98" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-99" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-100" style="font-family: STIXGeneral-Italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;"><span class="texatom" id="MathJax-Span-101"><span class="mrow" id="MathJax-Span-102"><span class="mi" id="MathJax-Span-103" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-104" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-105" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-106" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-107" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>h</mi><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>
          <li>The message function<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-108" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-109"><span class="msubsup" id="MathJax-Span-110"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-111" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.836em;"><span class="mi" id="MathJax-Span-112" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-8">M_t</script> and update function<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-113" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-114"><span class="msubsup" id="MathJax-Span-115"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-116" style="font-family: STIXGeneral-Italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-117" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-9">U_t</script> are learned during training.</li>
        </ul>
      </li>
      <li><strong>Readout Phase</strong>: After the message passing phase, a readout function<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-118" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-119"><span class="mi" id="MathJax-Span-120" style="font-family: STIXGeneral-Italic;">R</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math></span></span><script type="math/tex" id="MathJax-Element-10">R</script> aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.</li>
    </ul>
  </li>
  <li><strong>Key Contributions</strong>:
    <ul>
      <li><strong>State of the Art Results</strong>: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.</li>
      <li><strong>Chemical Accuracy</strong>: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.</li>
      <li><strong>Scalability</strong>: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.</li>
    </ul>
  </li>
  <li><strong>Results</strong>:
    <ul>
      <li>The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.</li>
      <li>They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.</li>
    </ul>
  </li>
  <li><strong>Conclusion</strong>:
    <ul>
      <li>The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.</li>
      <li>Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.</li>
    </ul>
  </li>
</ul>

<h3 id="attention-is-all-you-need"><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></h3>
<ul>
  <li><strong>Authors</strong>: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</li>
  <li>
    <p><strong>Affiliations</strong>: Google Brain, Google Research, University of Toronto</p>
  </li>
  <li>
    <p>The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.</p>
  </li>
  <li>Key Contributions:
    <ol>
      <li><strong>Transformer Architecture</strong>:
        <ul>
          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>
          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>
        </ul>
      </li>
      <li><strong>Self-Attention Mechanism</strong>:
        <ul>
          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>
          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>
        </ul>
      </li>
      <li><strong>Positional Encoding</strong>:
        <ul>
          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>
        </ul>
      </li>
      <li><strong>Training Efficiency and Performance</strong>:
        <ul>
          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>
          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>
        </ul>
      </li>
      <li><strong>Generalization to Other Tasks</strong>:
        <ul>
          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>
        </ul>
      </li>
      <li><strong>Advantages Over Previous Models</strong>:
        <ul>
          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>
          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Experimental Results:
    <ul>
      <li><strong>Machine Translation</strong>: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.</li>
      <li><strong>Model Variations</strong>: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.</li>
      <li><strong>English Constituency Parsing</strong>: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.</li>
    </ul>
  </li>
  <li>Conclusion:
    <ul>
      <li>The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.</li>
      <li>Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.</li>
    </ul>
  </li>
</ul>

<h3 id="neural-machine-translation-by-jointly-learning-to-align-and-translate"><a href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></h3>
<ul>
  <li><strong>Author</strong>: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio</li>
  <li><strong>Abstract</strong>: Neural machine translation (NMT) is an emerging approach that builds a single neural network to maximize translation performance. Unlike traditional methods, NMT uses encoder-decoder architectures to translate sentences. This paper introduces a method allowing the model to search for relevant parts of a source sentence during translation, enhancing performance.</li>
  <li><strong>Key Concepts</strong>:
    <ul>
      <li><strong>Encoder-Decoder Model</strong>: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.</li>
      <li><strong>Fixed-Length Vector Bottleneck</strong>: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.</li>
      <li><strong>Attention Mechanism</strong>: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.</li>
    </ul>
  </li>
  <li><strong>Proposed Model</strong>:
    <ul>
      <li><strong>Bidirectional RNN Encoder</strong>: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.</li>
      <li><strong>Attention-Based Decoder</strong>: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.</li>
    </ul>
  </li>
  <li><strong>Performance</strong>:
    <ul>
      <li>The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.</li>
      <li>Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.</li>
      <li>Qualitative analysis shows that the alignments produced by the model are linguistically plausible.</li>
    </ul>
  </li>
  <li><strong>Experiment</strong>:
    <ul>
      <li>The models were tested on the WMT ’14 English-to-French translation task.</li>
      <li>The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.</li>
    </ul>
  </li>
  <li><strong>Conclusion</strong>:
    <ul>
      <li>The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.</li>
      <li>Future work should address handling unknown or rare words to further improve translation performance.</li>
    </ul>
  </li>
</ul>

<h3 id="identity-mappings-in-deep-residual-networks"><a href="https://arxiv.org/abs/1603.05027">Identity Mappings in Deep Residual Networks</a></h3>
<ul>
  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>
  <li>
    <p><strong>Affiliations</strong>: Microsoft Research</p>
  </li>
  <li>
    <p>The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.</p>
  </li>
  <li>Key Contributions:
    <ol>
      <li><strong>Analysis of Identity Mappings</strong>:
        <ul>
          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>
          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>
        </ul>
      </li>
      <li><strong>Proposed Residual Unit</strong>:
        <ul>
          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>
          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>
        </ul>
      </li>
      <li><strong>Empirical Validation</strong>:
        <ul>
          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>
          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>
        </ul>
      </li>
      <li><strong>Deep Residual Networks</strong>:
        <ul>
          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>
          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Experimental Results:
    <ul>
      <li><strong>CIFAR-10 and CIFAR-100</strong>:
        <ul>
          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>
          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>
        </ul>
      </li>
      <li><strong>ImageNet</strong>:
        <ul>
          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Conclusion:
    <ul>
      <li>The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.</li>
      <li>By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.</li>
      <li>The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.</li>
    </ul>
  </li>
</ul>

<h3 id="a-simple-neural-network-module-for-relational-reasoning"><a href="https://arxiv.org/abs/1706.01427">A Simple Neural Network Module for Relational Reasoning</a></h3>
<ul>
  <li><strong>Authors</strong>: Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap</li>
  <li>
    <p><strong>Affiliations</strong>: DeepMind, London, United Kingdom</p>
  </li>
  <li>
    <p>The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.</p>
  </li>
  <li>Key Contributions:
    <ol>
      <li><strong>Introduction of Relation Networks (RNs)</strong>:
        <ul>
          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>
          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>
        </ul>
      </li>
      <li><strong>Application to Visual Question Answering (CLEVR)</strong>:
        <ul>
          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>
          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>
        </ul>
      </li>
      <li><strong>Sort-of-CLEVR Dataset</strong>:
        <ul>
          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>
          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>
        </ul>
      </li>
      <li><strong>Text-Based Question Answering (bAbI)</strong>:
        <ul>
          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>
          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>
        </ul>
      </li>
      <li><strong>Dynamic Physical Systems</strong>:
        <ul>
          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>
          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Model Details:</li>
  <li><strong>Architecture</strong>:
    <ul>
      <li>RNs operate on sets of objects, where each object is represented by a feature vector.</li>
      <li>
        <p>The RN computes pairwise relations using a function<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-121" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-122"><span class="msubsup" id="MathJax-Span-123"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-124" style="font-family: STIXGeneral-Italic;">g</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.523em;"><span class="texatom" id="MathJax-Span-125"><span class="mrow" id="MathJax-Span-126"><span class="mi" id="MathJax-Span-127" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mi>θ</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-11">g_{\theta}</script> and aggregates these relations using a function<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;&amp;#x03D5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-128" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-129"><span class="msubsup" id="MathJax-Span-130"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-131" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.159em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.263em;"><span class="texatom" id="MathJax-Span-132"><span class="mrow" id="MathJax-Span-133"><span class="mi" id="MathJax-Span-134" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">ϕ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mrow class="MJX-TeXAtom-ORD"><mi>ϕ</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-12">f_{\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>
      </li>
      <li><strong>Training</strong>:
        <ul>
          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Results:</li>
  <li><strong>CLEVR</strong>:
    <ul>
      <li>
        <p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>
      </li>
      <li><strong>Sort-of-CLEVR</strong>:
        <ul>
          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>
        </ul>
      </li>
      <li><strong>bAbI</strong>:
        <ul>
          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>
        </ul>
      </li>
      <li><strong>Dynamic Physical Systems</strong>:
        <ul>
          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Conclusion:
    <ul>
      <li>The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.</li>
      <li>RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.</li>
      <li>The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.</li>
    </ul>
  </li>
</ul>

<h3 id="variational-lossy-autoencoder"><a href="https://arxiv.org/pdf/1611.02731">Variational Lossy Autoencoder</a></h3>
<ul>
  <li>Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel</li>
  <li>Published: ICLR 2017</li>
  <li>
    <p>Institutions: UC Berkeley, OpenAI</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distribution<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-135" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.57em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-136"><span class="mi" id="MathJax-Span-137" style="font-family: STIXGeneral-Italic;">p</span><span class="mo" id="MathJax-Span-138" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-139" style="font-family: STIXGeneral-Italic;">z</span><span class="mo" id="MathJax-Span-140" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-13">p(z)</script> and the decoding distribution$$ p(x</td>
          <td>z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Key Concepts:</strong>
    <ol>
      <li><strong>Representation Learning:</strong> Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.</li>
      <li><strong>Variational Autoencoder (VAE):</strong> VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.</li>
      <li><strong>Autoregressive Models:</strong> These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.</li>
    </ol>
  </li>
  <li><strong>Technical Highlights:</strong>
    <ol>
      <li><strong>Combination of VAE and Autoregressive Models:</strong>
        <ul>
          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>
          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>
        </ul>
      </li>
      <li><strong>Bits-Back Coding and Information Preference:</strong>
        <ul>
          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>
          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>
        </ul>
      </li>
      <li><strong>Lossy Code via Explicit Information Placement:</strong>
        <ul>
          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>
          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>
        </ul>
      </li>
      <li><strong>Learned Prior with Autoregressive Flow:</strong>
        <ul>
          <li>The prior distribution<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-141" style="width: 3.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-142"><span class="mi" id="MathJax-Span-143" style="font-family: STIXGeneral-Italic;">p</span><span class="mo" id="MathJax-Span-144" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-145" style="font-family: STIXGeneral-Italic;">z</span><span class="mo" id="MathJax-Span-146" style="font-family: STIXGeneral-Regular;">;</span><span class="mi" id="MathJax-Span-147" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">θ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-148" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-14">p(z; \theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>
          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Experiments and Results:</strong>
    <ol>
      <li><strong>Datasets:</strong>
        <ul>
          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>
        </ul>
      </li>
      <li><strong>Performance:</strong>
        <ul>
          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>
          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>
          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>
        </ul>
      </li>
      <li><strong>Visualization:</strong>
        <ul>
          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Conclusion:</strong></li>
  <li>The Variational Lossy Autoencoder (VLAE) effectively combines the strengths of VAEs and autoregressive models, enabling controllable representation learning and improved density estimation. The model’s design ensures that the latent code captures essential global information, making it suitable for various generative tasks. Future work includes extending VLAE to other data types, such as audio and video, and designing task-specific representations to enhance semi-supervised learning.</li>
</ul>

<h3 id="relational-recurrent-neural-networks"><a href="https://arxiv.org/pdf/1806.01822">Relational Recurrent Neural Networks</a></h3>
<ul>
  <li><strong>Authors:</strong> Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap</li>
  <li><strong>Institution:</strong> DeepMind, University College London</li>
  <li>
    <p><strong>Abstract:</strong> The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.</p>
  </li>
  <li><strong>Key Points:</strong>
    <ul>
      <li><strong>Relational Reasoning Deficits in Standard Architectures:</strong> Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.</li>
      <li><strong>Introduction of Relational Memory Core (RMC):</strong> The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.</li>
      <li><strong>Application and Results:</strong>
        <ul>
          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>
          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>
          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>
        </ul>
      </li>
      <li><strong>Model Design and Functionality:</strong>
        <ul>
          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>
          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong> The introduction of the RMC shows that explicit modeling of memory interactions can enhance the performance of neural networks on tasks that require complex relational reasoning across sequential information. The study emphasizes the importance of enabling interactions between memory vectors to improve relational reasoning capabilities in recurrent neural networks.</li>
</ul>

<h3 id="quantifying-the-rise-and-fall-of-complexity-in-closed-systems-the-coffee-automaton"><a href="https://arxiv.org/pdf/1405.6903">Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton</a></h3>
<ul>
  <li>Authors: Scott Aaronson, Sean M. Carroll, Lauren Ouellette</li>
  <li>
    <p>The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.</p>
  </li>
  <li>
    <p><strong>Introduction</strong>: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.</p>
  </li>
  <li><strong>Background</strong>: Several concepts of entropy and complexity are discussed:
    <ul>
      <li><strong>Entropy</strong>: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.</li>
      <li><strong>Complexity</strong>: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.</li>
    </ul>
  </li>
  <li>
    <p><strong>Apparent Complexity</strong>: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.</p>
  </li>
  <li>
    <p><strong>Sophistication</strong>: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.</p>
  </li>
  <li>
    <p><strong>Logical Depth</strong>: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.</p>
  </li>
  <li>
    <p><strong>Light-Cone Complexity</strong>: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.</p>
  </li>
  <li><strong>Coffee Automaton Models</strong>:
    <ul>
      <li><strong>Interacting Model</strong>: Particles interact, swapping positions if they are adjacent and different.</li>
      <li><strong>Non-Interacting Model</strong>: Particles move independently in random walks.</li>
    </ul>
  </li>
  <li><strong>Experiment and Results</strong>:
    <ul>
      <li>The automaton begins with separated coffee and cream, mixing over time.</li>
      <li><strong>Coarse-Graining</strong>: The state is averaged over local regions to produce a coarse-grained version.</li>
      <li><strong>Measurements</strong>: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.</li>
      <li>Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.</li>
    </ul>
  </li>
  <li><strong>Adjusted Coarse-Graining</strong>:
    <ul>
      <li>To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.</li>
    </ul>
  </li>
  <li><strong>Conclusions and Further Work</strong>:
    <ul>
      <li>The coarse-graining approach effectively mirrors human intuition of complexity.</li>
      <li>Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.</li>
    </ul>
  </li>
</ul>

<h3 id="neural-turing-machines"><a href="https://arxiv.org/abs/1410.5401">Neural Turing Machines</a></h3>
<ul>
  <li><strong>Author</strong>: Alex Graves, Greg Wayne, Ivo Danihelka</li>
  <li><strong>Summary</strong>:
    <ul>
      <li><strong>Introduction</strong>:
        <ul>
          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>
        </ul>
      </li>
      <li><strong>Foundational Research</strong>:
        <ul>
          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>
          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>
          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>
        </ul>
      </li>
      <li><strong>Neural Turing Machines</strong>:
        <ul>
          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>
          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>
          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>
          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>
        </ul>
      </li>
      <li><strong>Experiments</strong>:
        <ul>
          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>
          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>
          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>
          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>
          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>
          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>
        </ul>
      </li>
      <li><strong>Conclusion</strong>:
        <ul>
          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>This paper introduces the Neural Turing Machine architecture, highlighting its foundation, structure, and performance in various algorithmic tasks, demonstrating its potential to revolutionize neural network capabilities by integrating external memory and addressing mechanisms.</li>
</ul>

<h3 id="deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin"><a href="https://arxiv.org/pdf/1512.02595">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></h3>

<ul>
  <li>
    <p><strong>Authors:</strong> Baidu Research – Silicon Valley AI Lab</p>
  </li>
  <li>
    <p><strong>Abstract:</strong>
The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.</p>
  </li>
  <li>
    <p><strong>Introduction:</strong>
Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.</p>
  </li>
  <li>
    <p><strong>Model Architecture:</strong>
The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.</p>
  </li>
  <li>
    <p><strong>Training Data:</strong>
Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.</p>
  </li>
  <li><strong>Results:</strong>
    <ul>
      <li><strong>English:</strong> Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.</li>
      <li><strong>Mandarin:</strong> The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.</li>
    </ul>
  </li>
  <li>
    <p><strong>Deployment:</strong>
The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.</p>
  </li>
  <li>
    <p><strong>Conclusion:</strong>
Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.</p>
  </li>
  <li>This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.</li>
</ul>

<h3 id="scaling-laws-for-neural-language-models"><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></h3>

<ul>
  <li><strong>Authors</strong>: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</li>
  <li>
    <p><strong>Institution</strong>: OpenAI, Johns Hopkins University</p>
  </li>
  <li>The paper “Scaling Laws for Neural Language Models” explores empirical scaling laws that describe the relationship between language model performance and factors such as model size, dataset size, and computational resources used for training. The study finds that performance scales predictably according to power laws over several orders of magnitude. Key findings include:</li>
</ul>

<ol>
  <li><strong>Power-law relationships</strong>: Language model performance improves predictably with increases in model size (number of parameters), dataset size (number of tokens), and compute (floating point operations). These improvements follow simple power-law relationships.</li>
  <li><strong>Model size and data efficiency</strong>: Larger models are significantly more sample-efficient, meaning they require fewer data points to achieve the same level of performance compared to smaller models.</li>
  <li><strong>Optimal compute allocation</strong>: For a fixed compute budget, it is most efficient to train very large models on a relatively modest amount of data and to stop training before full convergence.</li>
  <li><strong>Minimal architectural effects</strong>: Performance depends strongly on scale (size, data, compute) and weakly on specific architectural hyperparameters such as network width or depth.</li>
</ol>

<ul>
  <li>Key Equations</li>
  <li><strong>Model performance as a function of parameters</strong>:
-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-149" style="width: 7.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-150"><span class="mi" id="MathJax-Span-151" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-152" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-153" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-154" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-155" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-156" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px;"><span style="position: absolute; clip: rect(2.867em, 1001.98em, 4.586em, -999.997em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-157"><span class="mo" id="MathJax-Span-158" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">(</span></span></span><span class="mfrac" id="MathJax-Span-159"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.388em, 1000.73em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.362em;"><span class="msubsup" id="MathJax-Span-160"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-161" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.471em;"><span class="mi" id="MathJax-Span-162" style="font-size: 50%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;"><span class="mi" id="MathJax-Span-163" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-164" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.737em; left: 2.138em;"><span class="texatom" id="MathJax-Span-165"><span class="mrow" id="MathJax-Span-166"><span class="msubsup" id="MathJax-Span-167"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-168" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.367em;"><span class="mi" id="MathJax-Span-169" style="font-size: 50%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>α</mi><mi>N</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-15">L(N) = \left( \frac{N_c}{N} \right)^{\alpha_N}</script>
    <ul>
      <li>Where<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-170" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-171"><span class="mi" id="MathJax-Span-172" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></span></span><script type="math/tex" id="MathJax-Element-16">L</script> is the loss,<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-173" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-174"><span class="mi" id="MathJax-Span-175" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">N</script> is the number of non-embedding parameters,<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-176" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-177"><span class="msubsup" id="MathJax-Span-178"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-179" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="mi" id="MathJax-Span-180" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>c</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-18">N_c</script> is a constant, and<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-181" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-182"><span class="msubsup" id="MathJax-Span-183"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-184" style="font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="mi" id="MathJax-Span-185" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mi>N</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-19">\alpha_N</script> is the scaling exponent.</li>
    </ul>
  </li>
  <li><strong>Dataset size relationship</strong>:
-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-186" style="width: 7.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-187"><span class="mi" id="MathJax-Span-188" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-189" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-190" style="font-family: STIXGeneral-Italic;">D</span><span class="mo" id="MathJax-Span-191" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-192" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-193" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px;"><span style="position: absolute; clip: rect(2.867em, 1002.03em, 4.586em, -999.997em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-194"><span class="mo" id="MathJax-Span-195" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">(</span></span></span><span class="mfrac" id="MathJax-Span-196"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.414em;"><span class="msubsup" id="MathJax-Span-197"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-198" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.523em;"><span class="mi" id="MathJax-Span-199" style="font-size: 50%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;"><span class="mi" id="MathJax-Span-200" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-201" style="vertical-align: -0.258em;"><span><span style="font-size: 110%; font-family: STIXSizeOneSym;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.737em; left: 2.19em;"><span class="texatom" id="MathJax-Span-202"><span class="mrow" id="MathJax-Span-203"><span class="msubsup" id="MathJax-Span-204"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-205" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.367em;"><span class="mi" id="MathJax-Span-206" style="font-size: 50%; font-family: STIXGeneral-Italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>α</mi><mi>D</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-20">L(D) = \left( \frac{D_c}{D} \right)^{\alpha_D}</script>
    <ul>
      <li>Where<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-207" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-208"><span class="mi" id="MathJax-Span-209" style="font-family: STIXGeneral-Italic;">D</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi></math></span></span><script type="math/tex" id="MathJax-Element-21">D</script> is the dataset size in tokens,<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-210" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-211"><span class="msubsup" id="MathJax-Span-212"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-213" style="font-family: STIXGeneral-Italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.732em;"><span class="mi" id="MathJax-Span-214" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>D</mi><mi>c</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-22">D_c</script> is a constant, and<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-215" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-216"><span class="msubsup" id="MathJax-Span-217"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-218" style="font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="mi" id="MathJax-Span-219" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mi>D</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-23">\alpha_D</script> is the scaling exponent.</li>
    </ul>
  </li>
  <li><strong>Compute efficiency</strong>:
-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-220" style="width: 11.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.273em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.836em, 1009.27em, 3.023em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-221"><span class="mi" id="MathJax-Span-222" style="font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-223" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-224"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-225" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-226"><span class="mrow" id="MathJax-Span-227"><span class="mtext" id="MathJax-Span-228" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">min</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-229" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-230" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="msubsup" id="MathJax-Span-231" style="padding-left: 0.315em;"><span style="display: inline-block; position: relative; width: 4.951em; height: 0px;"><span style="position: absolute; clip: rect(2.659em, 1003.08em, 4.846em, -999.997em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-232"><span class="mo" id="MathJax-Span-233" style="vertical-align: -0.414em;"><span style="font-family: STIXSizeTwoSym;">(</span></span><span class="mfrac" id="MathJax-Span-234"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.388em, 1001.67em, 4.326em, -999.997em); top: -4.581em; left: 50%; margin-left: -0.831em;"><span class="msubsup" id="MathJax-Span-235"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-236" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.471em;"><span class="texatom" id="MathJax-Span-237"><span class="mrow" id="MathJax-Span-238"><span class="mtext" id="MathJax-Span-239" style="font-size: 50%; font-family: STIXGeneral-Regular;">min</span><span class="mo" id="MathJax-Span-240" style="font-size: 50%; font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-241" style="font-size: 50%; font-family: STIXGeneral-Italic;">c</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.388em, 1001.3em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.674em;"><span class="msubsup" id="MathJax-Span-242"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-243" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.471em;"><span class="texatom" id="MathJax-Span-244"><span class="mrow" id="MathJax-Span-245"><span class="mtext" id="MathJax-Span-246" style="font-size: 50%; font-family: STIXGeneral-Regular;">min</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.77em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.773em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-247" style="vertical-align: -0.414em;"><span style="font-family: STIXSizeTwoSym;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.893em; left: 3.232em;"><span class="texatom" id="MathJax-Span-248"><span class="mrow" id="MathJax-Span-249"><span class="msubsup" id="MathJax-Span-250"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-251" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.367em;"><span class="texatom" id="MathJax-Span-252"><span class="mrow" id="MathJax-Span-253"><span class="mtext" id="MathJax-Span-254" style="font-size: 50%; font-family: STIXGeneral-Regular;">min</span><span class="mo" id="MathJax-Span-255" style="font-size: 50%; font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-256" style="font-size: 50%; font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext></mrow></msub></mfrac><mo>)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>α</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-24">L(C_{\text{min}}) = \left( \frac{C_{\text{min}, c}}{C_{\text{min}}} \right)^{\alpha_{\text{min}, C}}</script>
    <ul>
      <li>Where<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-257" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-258"><span class="msubsup" id="MathJax-Span-259"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-260" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-261"><span class="mrow" id="MathJax-Span-262"><span class="mtext" id="MathJax-Span-263" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">min</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-25">C_{\text{min}}</script> is the minimum compute required,<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-264" style="width: 2.815em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-265"><span class="msubsup" id="MathJax-Span-266"><span style="display: inline-block; position: relative; width: 2.346em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-267" style="font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.68em;"><span class="texatom" id="MathJax-Span-268"><span class="mrow" id="MathJax-Span-269"><span class="mtext" id="MathJax-Span-270" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">min</span><span class="mo" id="MathJax-Span-271" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-272" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-26">C_{\text{min}, c}</script> is a constant, and<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;min&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-273" style="width: 2.919em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1002.4em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-274"><span class="msubsup" id="MathJax-Span-275"><span style="display: inline-block; position: relative; width: 2.398em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-276" style="font-family: STIXGeneral-Italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.576em;"><span class="texatom" id="MathJax-Span-277"><span class="mrow" id="MathJax-Span-278"><span class="mtext" id="MathJax-Span-279" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">min</span><span class="mo" id="MathJax-Span-280" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-281" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mrow class="MJX-TeXAtom-ORD"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-27">\alpha_{\text{min}, C}</script> is the scaling exponent.</li>
    </ul>
  </li>
  <li><strong>Sample efficiency</strong>: Larger models trained with the same amount of data achieve better performance due to their improved ability to utilize the data.</li>
  <li><strong>Training dynamics</strong>: Training curves follow predictable power-laws, allowing early extrapolation to predict the final performance of the model.</li>
  <li><strong>Generalization</strong>: Performance on different datasets improves consistently with the performance on the training dataset, suggesting that better in-distribution performance translates to better out-of-distribution performance.</li>
  <li><strong>Model size vs. dataset size</strong>: As model size increases, the dataset size should be scaled sublinearly to avoid overfitting, implying that moderately increasing data is sufficient for much larger models.</li>
  <li>
    <p><strong>Compute-efficient training</strong>: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.</p>
  </li>
  <li>These findings provide a framework for understanding and predicting the performance of large-scale neural language models, guiding future research and practical applications in optimizing model training and deployment.</li>
</ul>

<h3 id="a-tutorial-introduction-to-the-minimum-description-length-principle"><a href="https://arxiv.org/pdf/math/0406077">A Tutorial Introduction to the Minimum Description Length Principle</a></h3>
<ul>
  <li>Authors: Peter Grünwald</li>
  <li>
    <p>This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.</p>
  </li>
  <li>Key Technical Details:
    <ol>
      <li>
        <p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>
      </li>
      <li>
        <p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>
      </li>
      <li>
        <p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>
      </li>
      <li>
        <p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>
      </li>
      <li>
        <p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>
      </li>
      <li>
        <p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>
      </li>
      <li>
        <p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>
      </li>
    </ol>
  </li>
  <li>The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .</li>
</ul>

<h3 id="machine-super-intelligence"><a href="https://www.vetta.org/documents/Machine_Super_Intelligence.pdf">Machine Super Intelligence</a></h3>

<ul>
  <li>Shane Legg’s dissertation, “Machine Super Intelligence,” presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:</li>
</ul>

<ol>
  <li>
    <p><strong>Framework for Intelligence Measures:</strong> Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.</p>
  </li>
  <li>
    <p><strong>Superintelligence Pathways:</strong> The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.</p>
  </li>
  <li>
    <p><strong>Algorithmic Insights into Intelligence:</strong> Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.</p>
  </li>
  <li>
    <p><strong>Theoretical Models of Machine Learning:</strong> Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.</p>
  </li>
  <li>
    <p><strong>Safety and Control:</strong> A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.</p>
  </li>
</ol>

<ul>
  <li>These components of Legg’s dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.</li>
</ul>

<h3 id="kolmogorov-complexity-and-algorithmic-randomness"><a href="https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf">Kolmogorov Complexity and Algorithmic Randomness</a></h3>
<ul>
  <li>
    <p>The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:</p>
  </li>
  <li><strong>Definition and Significance</strong>: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.</li>
  <li>
    <p><strong>Unpredictability and Random Sequences</strong>: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.</p>
  </li>
  <li>Theoretical Foundations
    <ul>
      <li><strong>Formalisms and Proofs</strong>: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.</li>
      <li><strong>Incompressibility Method</strong>: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.</li>
    </ul>
  </li>
  <li>Practical Applications
    <ul>
      <li><strong>Data Compression</strong>: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.</li>
      <li><strong>Psychological Models</strong>: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.</li>
    </ul>
  </li>
  <li>Advanced Topics
    <ul>
      <li><strong>Mutual Information</strong>: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.</li>
      <li><strong>Conditional Complexity</strong>: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.</li>
    </ul>
  </li>
  <li>Mathematical Rigor
    <ul>
      <li><strong>Deep Mathematical Analysis</strong>: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.</li>
    </ul>
  </li>
  <li>
    <p><strong>Future Directions</strong>: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.</p>
  </li>
  <li>This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.</li>
</ul>

<h3 id="stanfords-cs231n-convolutional-neural-networks-for-visual-recognition"><a href="https://cs231n.github.io/">Stanford’s CS231n Convolutional Neural Networks for Visual Recognition</a></h3>

<ul>
  <li><strong>Purpose</strong>: The course introduces students to the fundamental concepts in convolutional neural networks (ConvNets) and their application in image recognition and processing tasks. ConvNets are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.</li>
  <li>
    <p><strong>Architectural Advantage</strong>: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.</p>
  </li>
  <li>Core Components of ConvNets
    <ul>
      <li><strong>Layers</strong>: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).
        <ul>
          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>
          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>
          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Training ConvNets
    <ul>
      <li><strong>Loss Functions</strong>: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.</li>
      <li><strong>Backpropagation</strong>: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.</li>
    </ul>
  </li>
  <li>Practical Challenges
    <ul>
      <li><strong>Overfitting</strong>: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.</li>
      <li><strong>Hyperparameter Tuning</strong>: Includes selecting learning rates, learning rate decay, regularization constants, and more.</li>
    </ul>
  </li>
  <li>Advanced Topics
    <ul>
      <li><strong>Batch Normalization</strong>: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.</li>
      <li><strong>Transfer Learning and Fine-tuning</strong>: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.</li>
    </ul>
  </li>
</ul>

<h2 id="meta">Meta</h2>

<h3 id="better--faster-large-language-models-via-multi-token-prediction"><a href="https://arxiv.org/pdf/2404.19737">Better &amp; Faster Large Language Models Via Multi-token Prediction</a></h3>
<ul>
  <li>
    <p>Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve</p>
  </li>
  <li>The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled “Better &amp; Faster Large Language Models via Multi-token Prediction” suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.</li>
  <li>The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.</li>
  <li>At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.</li>
  <li>The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.</li>
  <li>Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.</li>
  <li>Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.</li>
  <li>Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.</li>
</ul>

<h4 id="key-takeaways">Key Takeaways:</h4>
<ul>
  <li>🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.</li>
  <li>🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.</li>
  <li>🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.</li>
  <li>🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.</li>
  <li>🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.</li>
  <li>🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.</li>
</ul>

<h3 id="dense-passage-retrieval-for-open-domain-question-answering"><a href="https://arxiv.org/pdf/2004.04906.pdf">Dense Passage Retrieval for Open-Domain Question Answering</a></h3>
<ul>
  <li>Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih</li>
  <li>In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.</li>
  <li>Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.</li>
  <li>Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.</li>
  <li>Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.</li>
</ul>

<h4 id="dense-passage-retriever-dpr">Dense Passage Retriever (DPR):</h4>
<ol>
  <li><strong>Purpose</strong>: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.</li>
  <li><strong>Key Task</strong>: Given a large number <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-282" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-283"><span class="mi" id="MathJax-Span-284" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">M</script> of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-285" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-286"><span class="mi" id="MathJax-Span-287" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-29">k</script> most relevant passages for a given input question. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-288" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-289"><span class="mi" id="MathJax-Span-290" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-30">M</script> can be very large, like 21 million passages, but <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-31-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-291" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-292"><span class="mi" id="MathJax-Span-293" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-31">k</script> (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.</li>
  <li><strong>DPR’s Mechanism</strong>:
    <ul>
      <li><strong>Dense Encoder for Passages <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-32-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-294" style="width: 2.792em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.275em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.397em, 1002.22em, 2.534em, -999.997em); top: -2.218em; left: 0em;"><span class="mrow" id="MathJax-Span-295"><span class="mi" id="MathJax-Span-296" style="font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-297" style="font-family: STIXGeneral-Italic;">P</span><span class="mo" id="MathJax-Span-298" style="font-family: STIXGeneral-Regular;">(</span><span class="mo" id="MathJax-Span-299" style="font-family: STIXGeneral-Regular;">⋅</span><span class="mo" id="MathJax-Span-300" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.224em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mi>P</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-32">EP(\cdot)</script></strong>: It converts any text passage to a <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-301" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-302"><span class="mi" id="MathJax-Span-303" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-33">d</script>-dimensional real-valued vector. This encoder processes and indexes all <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-34-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-304" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-305"><span class="mi" id="MathJax-Span-306" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-34">M</script> passages for retrieval.</li>
      <li><strong>Encoder for Questions <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-35-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-307" style="width: 2.947em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.43em; height: 0px; font-size: 121%;"><span style="position: absolute; clip: rect(1.397em, 1002.38em, 2.585em, -999.997em); top: -2.218em; left: 0em;"><span class="mrow" id="MathJax-Span-308"><span class="mi" id="MathJax-Span-309" style="font-family: STIXGeneral-Italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-310" style="font-family: STIXGeneral-Italic;">Q</span><span class="mo" id="MathJax-Span-311" style="font-family: STIXGeneral-Regular;">(</span><span class="mo" id="MathJax-Span-312" style="font-family: STIXGeneral-Regular;">⋅</span><span class="mo" id="MathJax-Span-313" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.224em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mi>Q</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-35">EQ(\cdot)</script></strong>: At runtime, when a question is posed, this encoder turns the question into a <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-36-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-314" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-315"><span class="mi" id="MathJax-Span-316" style="font-family: STIXGeneral-Italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-36">d</script>-dimensional vector.</li>
      <li><strong>Similarity Measurement</strong>: The similarity between a question and a passage is calculated using the dot product of their respective vectors: <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-37-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-317" style="width: 12.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1010.52em, 2.607em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-318"><span class="mi" id="MathJax-Span-319" style="font-family: STIXGeneral-Italic;">s</span><span class="mi" id="MathJax-Span-320" style="font-family: STIXGeneral-Italic;">i</span><span class="mi" id="MathJax-Span-321" style="font-family: STIXGeneral-Italic;">m</span><span class="mo" id="MathJax-Span-322" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-323" style="font-family: STIXGeneral-Italic;">q</span><span class="mo" id="MathJax-Span-324" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-325" style="font-family: STIXGeneral-Italic; padding-left: 0.211em;">p</span><span class="mo" id="MathJax-Span-326" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-327" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mi" id="MathJax-Span-328" style="font-family: STIXGeneral-Italic; padding-left: 0.315em;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-329" style="font-family: STIXGeneral-Italic;">Q</span><span class="mo" id="MathJax-Span-330" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-331" style="font-family: STIXGeneral-Italic;">q</span><span class="mo" id="MathJax-Span-332" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-333" style="font-family: STIXGeneral-Regular; padding-left: 0.263em;">⋅</span><span class="mi" id="MathJax-Span-334" style="font-family: STIXGeneral-Italic; padding-left: 0.263em;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-335" style="font-family: STIXGeneral-Italic;">P</span><span class="mo" id="MathJax-Span-336" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-337" style="font-family: STIXGeneral-Italic;">p</span><span class="mo" id="MathJax-Span-338" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>E</mi><mi>P</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-37">sim(q, p) = EQ(q) \cdot EP(p)</script>.</li>
    </ul>
  </li>
  <li><strong>Passage Size and Boundaries</strong>: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.</li>
  <li><strong>Encoders Implementation</strong>: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the [CLS] token as the output, meaning the output vector has 768 dimensions.</li>
  <li><strong>Inference</strong>: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-339" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-340"><span class="mi" id="MathJax-Span-341" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-38">k</script> passages with the closest embeddings are retrieved.</li>
  <li><strong>Training</strong>:
    <ul>
      <li>The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.</li>
      <li>The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.</li>
      <li>For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.</li>
    </ul>
  </li>
  <li><strong>In-batch Negatives</strong>: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.</li>
</ol>

<h3 id="retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks"><a href="https://arxiv.org/abs/2005.11401v4">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></h3>
<ul>
  <li>The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.</li>
  <li>Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.</li>
  <li>The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.</li>
  <li>The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.</li>
  <li>RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.</li>
  <li>In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.</li>
  <li>RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.</li>
  <li>On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.</li>
  <li>This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.</li>
</ul>

<h2 id="huggingface">HuggingFace</h2>

<h3 id="zephyr-direct-distillation-of-lm-alignment"><a href="https://arxiv.org/pdf/2310.16944.pdf">Zephyr: Direct Distillation of LM Alignment</a></h3>
<ul>
  <li>Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf</li>
  <li>The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:
    <ol>
      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>
      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>
      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>
    </ol>
  </li>
  <li>They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.</li>
  <li>Results:
    <ul>
      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>
      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>
      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>
    </ul>
  </li>
  <li>The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.</li>
  <li>The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.</li>
  <li>Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.</li>
  <li>The image below <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">(source)</a> gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.</li>
</ul>

<p><img src="assets/paper/1.png" alt=""></p>

<h2 id="stanford">Stanford</h2>

<h3 id="lost-in-the-middle-how-language-models-use-long-contexts"><a href="https://arxiv.org/abs/2302.12345">Lost in the Middle: How Language Models Use Long Contexts</a></h3>
<ul>
  <li>This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.</li>
  <li>The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.</li>
  <li>Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).</li>
  <li>The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.</li>
  <li>A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.</li>
  <li>The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.</li>
  <li>The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.</li>
  <li>Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.</li>
  <li>In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.</li>
</ul>

<h2 id="misc">Misc</h2>

<h3 id="precise-zero-shot-dense-retrieval-without-relevance-labels"><a href="https://arxiv.org/abs/2212.10496">Precise Zero-Shot Dense Retrieval Without Relevance Labels</a></h3>
<ul>
  <li>The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.</li>
  <li>Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.</li>
  <li>HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.</li>
  <li>Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.</li>
  <li>The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.</li>
</ul>

<h3 id="alcuna-large-language-models-meet-new-knowledge"><a href="https://arxiv.org/pdf/2310.14820.pdf">ALCUNA: Large Language Models Meet New Knowledge</a></h3>
<ul>
  <li>Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan</li>
  <li>The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.</li>
  <li>KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.</li>
  <li>ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.</li>
  <li>Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.</li>
  <li>Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.</li>
  <li>The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.</li>
  <li>The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.</li>
  <li>This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.</li>
</ul>

<h3 id="the-perils--promises-of-fact-checking-with-large-language-models"><a href="https://arxiv.org/pdf/2310.13549.pdf">The Perils &amp; Promises of Fact-checking with Large Language Models</a></h3>
<ul>
  <li>Authors: Dorian Quelle &amp; Alexandre Bovet</li>
  <li>The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.</li>
  <li>They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.</li>
  <li>
    <p>Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.</p>
  </li>
  <li>
    <p>Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.</p>
  </li>
  <li>Key Results:
    <ul>
      <li>GPT-4 outperformed GPT-3.5 overall.</li>
      <li>Providing context significantly improved accuracy, highlighting the importance of evidence gathering.</li>
      <li>Models struggled with ambiguous “half-true” type verdicts.</li>
      <li>Performance varied across languages - non-English claims saw a boost when translated to English first.</li>
      <li>No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.</li>
    </ul>
  </li>
  <li>Limitations:
    <ul>
      <li>Biased evaluation due to use of GPT-4 as a scorer.</li>
      <li>Did not explore model scaling or curating better training data.</li>
      <li>Safety/ethics of potential misinformation not addressed.</li>
    </ul>
  </li>
  <li>Implications:
    <ul>
      <li>LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.</li>
      <li>Critical examination of LLM reasoning is important before deployment.</li>
      <li>Understanding model limitations and language-specific differences is key.</li>
      <li>Continued learning after initial training needs more investigation.</li>
    </ul>
  </li>
  <li>The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895471&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Ftop-30-papers%2F&amp;pra=5&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766922993761&amp;bpp=1&amp;bdt=64&amp;idt=11&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=8415498114271&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31084128%2C31096042%2C95376241%2C95378600%2C95378749%2C95372614&amp;oid=2&amp;pvsid=1353967292117571&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=33792&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=15" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe src="https://www.google.com/recaptcha/api2/aframe" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>