[
  {
    "id": "ai-ml-comp-logistic-regression-1",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification Algorithms",
    "title": "Logistic Regression",
    "subtitle": "Classification Algorithms",
    "contentHtml": "<ul>\n  <li>To start off here, Logistic Regression is a misnomer as it does not pertain to a regression problem at all.</li>\n  <li>Logistic regression estimates the probability of an event occurring based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.</li>\n  <li>You can use your returned value in one of two ways:\n    <ul>\n      <li>You may just need an output of 0 or 1 and use it “as is”.\n        <ul>\n          <li>\n            <p>Say your model predicts the probability that your baby will cry at night as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.05</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1007.71em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-8\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0.05</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">p(cry|night)= 0.05</script>\n          </li>\n          <li>\n            <p>Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable columnalign=&quot;right left right left right left right left right left right left&quot; rowspacing=&quot;3pt&quot; columnspacing=&quot;0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em&quot; displaystyle=&quot;true&quot;><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>&amp;#x2217;</mo><mn>365</mn></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&amp;#xA0;days</mtext></mtd></mtr></mtable></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-0.102em, 1013.6em, 3.909em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mtable\" id=\"MathJax-Span-21\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 13.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.242em, 1003.34em, 5.94em, -999.997em); top: -4.372em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 3.336em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.34em, 4.378em, -999.997em); top: -5.31em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">p</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -3.956em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-56\"><span class=\"mrow\" id=\"MathJax-Span-57\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.602em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-65\"><span class=\"mrow\" id=\"MathJax-Span-66\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span><span style=\"position: absolute; clip: rect(2.451em, 1010.05em, 6.357em, -999.997em); top: -4.581em; left: 3.336em;\"><span style=\"display: inline-block; position: relative; width: 10.107em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1010.05em, 4.378em, -999.997em); top: -5.31em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\"></span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">p</span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">n</span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Italic;\">s</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1005.58em, 4.169em, -999.997em); top: -3.956em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-58\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\"></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mn\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">365</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.38em, 4.378em, -999.997em); top: -2.602em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-67\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\"></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">18</span><span class=\"mtext\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;days</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.997em; border-left: 0px solid; width: 0px; height: 4.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtable columnalign=\"right left right left right left right left right left right left\" rowspacing=\"3pt\" columnspacing=\"0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em\" displaystyle=\"true\"><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=\"false\">)</mo><mo>∗</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>∗</mo><mn>365</mn></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&nbsp;days</mtext></mtd></mtr></mtable></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-2\">\\begin{align}\nwakeUp &= p(cry|nights)*nights \\\\\n&= 0.05 * 365 \\\\\n&= 18 \\text{ days} \n\\end{align}</script>\n          </li>\n        </ul>\n      </li>\n      <li>Or you may want to convert it into a binary category such as: spam or not spam and convert it to a binary classification problem.\n        <ul>\n          <li>In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.</li>\n          <li>The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.</li>\n          <li>Sigmoid function is represented as:</li>\n        </ul>\n\n        <p><img src=\"/primers/ai/assets/ml-comp/1.jpg\" alt=\"\"></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mi>z</mi></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-73\" style=\"width: 6.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1005.32em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"msubsup\" id=\"MathJax-Span-75\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-77\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-81\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.97em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.508em;\"><span class=\"mrow\" id=\"MathJax-Span-83\"><span class=\"mn\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-86\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-88\"><span class=\"mrow\" id=\"MathJax-Span-89\"><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-3\">y^{\\prime}=\\frac{1}{1+e^{-z}}</script>\n\n        <ul>\n          <li>where,\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">y^{\\prime}</script> is the output of the logistic regression model for a particular example.</li>\n            </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>+</mo><mo>&amp;#x2026;</mo><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.75em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-105\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-107\"><span class=\"mrow\" id=\"MathJax-Span-108\"><span class=\"mn\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-112\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"mn\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-116\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-118\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mn\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-121\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-123\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mn\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">…</span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-129\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-131\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-134\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-136\"><span class=\"mrow\" id=\"MathJax-Span-137\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">z=b+w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{N} x_{N}</script>\n\n            <ul>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">w</script> values are the model’s learned weights, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">b</script> is the bias.</li>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> values are the feature values for a particular example.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Algorithm is quite simple and efficient.</li>\n      <li>Provides concrete probability scores as output.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Bad at handling a large number of categorical features.</li>\n      <li>It assumes that the data is free of missing values and predictors are independent of each other.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>Logistic regression is used when the dependent variable (target) is categorical as in binary classification problems.\n        <ul>\n          <li>For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>You may just need an output of 0 or 1 and use it “as is”.\n        <ul>\n          <li>\n            <p>Say your model predicts the probability that your baby will cry at night as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.05</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1007.71em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-8\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0.05</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">p(cry|night)= 0.05</script>\n          </li>\n          <li>\n            <p>Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable columnalign=&quot;right left right left right left right left right left right left&quot; rowspacing=&quot;3pt&quot; columnspacing=&quot;0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em&quot; displaystyle=&quot;true&quot;><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>&amp;#x2217;</mo><mn>365</mn></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&amp;#xA0;days</mtext></mtd></mtr></mtable></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-0.102em, 1013.6em, 3.909em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mtable\" id=\"MathJax-Span-21\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 13.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.242em, 1003.34em, 5.94em, -999.997em); top: -4.372em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 3.336em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.34em, 4.378em, -999.997em); top: -5.31em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">p</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -3.956em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-56\"><span class=\"mrow\" id=\"MathJax-Span-57\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.602em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-65\"><span class=\"mrow\" id=\"MathJax-Span-66\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span><span style=\"position: absolute; clip: rect(2.451em, 1010.05em, 6.357em, -999.997em); top: -4.581em; left: 3.336em;\"><span style=\"display: inline-block; position: relative; width: 10.107em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1010.05em, 4.378em, -999.997em); top: -5.31em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\"></span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">p</span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">n</span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Italic;\">s</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1005.58em, 4.169em, -999.997em); top: -3.956em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-58\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\"></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mn\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">365</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.38em, 4.378em, -999.997em); top: -2.602em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-67\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\"></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">18</span><span class=\"mtext\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;days</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.997em; border-left: 0px solid; width: 0px; height: 4.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtable columnalign=\"right left right left right left right left right left right left\" rowspacing=\"3pt\" columnspacing=\"0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em\" displaystyle=\"true\"><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=\"false\">)</mo><mo>∗</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>∗</mo><mn>365</mn></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&nbsp;days</mtext></mtd></mtr></mtable></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-2\">\\begin{align}\nwakeUp &= p(cry|nights)*nights \\\\\n&= 0.05 * 365 \\\\\n&= 18 \\text{ days} \n\\end{align}</script>\n          </li>\n        </ul>\n      </li>\n      <li>Or you may want to convert it into a binary category such as: spam or not spam and convert it to a binary classification problem.\n        <ul>\n          <li>In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.</li>\n          <li>The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.</li>\n          <li>Sigmoid function is represented as:</li>\n        </ul>\n\n        <p><img src=\"/primers/ai/assets/ml-comp/1.jpg\" alt=\"\"></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mi>z</mi></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-73\" style=\"width: 6.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1005.32em, 3.076em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"msubsup\" id=\"MathJax-Span-75\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-77\"><span class=\"mrow\" id=\"MathJax-Span-78\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-81\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.97em, 4.221em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.508em;\"><span class=\"mrow\" id=\"MathJax-Span-83\"><span class=\"mn\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-86\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-88\"><span class=\"mrow\" id=\"MathJax-Span-89\"><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-3\">y^{\\prime}=\\frac{1}{1+e^{-z}}</script>\n\n        <ul>\n          <li>where,\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">y^{\\prime}</script> is the output of the logistic regression model for a particular example.</li>\n            </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>+</mo><mo>&amp;#x2026;</mo><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.75em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-105\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-107\"><span class=\"mrow\" id=\"MathJax-Span-108\"><span class=\"mn\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-112\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"mn\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-116\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-118\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mn\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-121\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-123\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mn\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">…</span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-129\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-131\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-134\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-136\"><span class=\"mrow\" id=\"MathJax-Span-137\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">z=b+w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{N} x_{N}</script>\n\n            <ul>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">w</script> values are the model’s learned weights, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">b</script> is the bias.</li>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> values are the feature values for a particular example.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>\n            <p>Say your model predicts the probability that your baby will cry at night as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.05</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1007.71em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-8\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0.05</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">p(cry|night)= 0.05</script>\n          </li>\n          <li>\n            <p>Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable columnalign=&quot;right left right left right left right left right left right left&quot; rowspacing=&quot;3pt&quot; columnspacing=&quot;0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em&quot; displaystyle=&quot;true&quot;><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>&amp;#x2217;</mo><mn>365</mn></mtd></mtr><mtr><mtd /><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&amp;#xA0;days</mtext></mtd></mtr></mtable></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-0.102em, 1013.6em, 3.909em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mtable\" id=\"MathJax-Span-21\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 13.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.242em, 1003.34em, 5.94em, -999.997em); top: -4.372em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 3.336em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.34em, 4.378em, -999.997em); top: -5.31em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">p</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -3.956em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-56\"><span class=\"mrow\" id=\"MathJax-Span-57\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.602em; right: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-65\"><span class=\"mrow\" id=\"MathJax-Span-66\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.378em;\"></span></span><span style=\"position: absolute; clip: rect(2.451em, 1010.05em, 6.357em, -999.997em); top: -4.581em; left: 3.336em;\"><span style=\"display: inline-block; position: relative; width: 10.107em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1010.05em, 4.378em, -999.997em); top: -5.31em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\"></span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">p</span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">n</span><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Italic;\">s</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1005.58em, 4.169em, -999.997em); top: -3.956em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-58\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\"></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.05</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mn\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">365</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.38em, 4.378em, -999.997em); top: -2.602em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-67\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\"></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">18</span><span class=\"mtext\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;days</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.586em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.997em; border-left: 0px solid; width: 0px; height: 4.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtable columnalign=\"right left right left right left right left right left right left\" rowspacing=\"3pt\" columnspacing=\"0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em\" displaystyle=\"true\"><mtr><mtd><mi>w</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>U</mi><mi>p</mi></mtd><mtd><mi></mi><mo>=</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>r</mi><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy=\"false\">)</mo><mo>∗</mo><mi>n</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>0.05</mn><mo>∗</mo><mn>365</mn></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mn>18</mn><mtext>&nbsp;days</mtext></mtd></mtr></mtable></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-2\">\\begin{align}\nwakeUp &= p(cry|nights)*nights \\\\\n&= 0.05 * 365 \\\\\n&= 18 \\text{ days} \n\\end{align}</script>\n          </li>\n        </ul>\n<p>Say your model predicts the probability that your baby will cry at night as:</p>\n<p>Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:</p>\n<ul>\n          <li>In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.</li>\n          <li>The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.</li>\n          <li>Sigmoid function is represented as:</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/ml-comp/1.jpg\" alt=\"\"></p>\n<ul>\n          <li>where,\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">y^{\\prime}</script> is the output of the logistic regression model for a particular example.</li>\n            </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>+</mo><mo>&amp;#x2026;</mo><mo>+</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 16.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.75em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-105\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-107\"><span class=\"mrow\" id=\"MathJax-Span-108\"><span class=\"mn\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-112\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"mn\" id=\"MathJax-Span-114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-116\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-118\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mn\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-121\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-123\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mn\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">…</span><span class=\"mo\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-129\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-131\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-134\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-136\"><span class=\"mrow\" id=\"MathJax-Span-137\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">z=b+w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{N} x_{N}</script>\n\n            <ul>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">w</script> values are the model’s learned weights, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">b</script> is the bias.</li>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> values are the feature values for a particular example.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-variant&quot; mathvariant=&quot;normal&quot;>&amp;#x2032;</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.78em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-variant\" mathvariant=\"normal\">′</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">y^{\\prime}</script> is the output of the logistic regression model for a particular example.</li>\n            </ul>\n<ul>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">w</script> values are the model’s learned weights, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">b</script> is the bias.</li>\n              <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">x</script> values are the feature values for a particular example.</li>\n            </ul>\n<ul>\n      <li>Algorithm is quite simple and efficient.</li>\n      <li>Provides concrete probability scores as output.</li>\n    </ul>\n<ul>\n      <li>Bad at handling a large number of categorical features.</li>\n      <li>It assumes that the data is free of missing values and predictors are independent of each other.</li>\n    </ul>\n<ul>\n      <li>Logistic regression is used when the dependent variable (target) is categorical as in binary classification problems.\n        <ul>\n          <li>For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).</li>\n        </ul>",
    "contentMarkdown": "*   To start off here, Logistic Regression is a misnomer as it does not pertain to a regression problem at all.\n*   Logistic regression estimates the probability of an event occurring based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.\n*   You can use your returned value in one of two ways:\n    *   You may just need an output of 0 or 1 and use it “as is”.\n        *   Say your model predicts the probability that your baby will cry at night as:\n            \n            p(cry|night)\\=0.05p(cry|night)\\=0.05\n            \n            p(cry|night)= 0.05\n        *   Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:\n            \n            wakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 dayswakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 days\n            \n            \\\\begin{align} wakeUp &= p(cry|nights)\\*nights \\\\\\\\ &= 0.05 \\* 365 \\\\\\\\ &= 18 \\\\text{ days} \\\\end{align}\n    *   Or you may want to convert it into a binary category such as: spam or not spam and convert it to a binary classification problem.\n        \n        *   In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.\n        *   The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.\n        *   Sigmoid function is represented as:\n        \n        ![](/primers/ai/assets/ml-comp/1.jpg)\n        \n        y′\\=11+e−zy′\\=11+e−z\n        \n        y^{\\\\prime}=\\\\frac{1}{1+e^{-z}}\n        *   where,\n            \n            *   y′y′y^{\\\\prime} is the output of the logistic regression model for a particular example.\n            \n            z\\=b+w1x1+w2x2+…+wNxNz\\=b+w1x1+w2x2+…+wNxN\n            \n            z=b+w\\_{1} x\\_{1}+w\\_{2} x\\_{2}+\\\\ldots+w\\_{N} x\\_{N}\n            *   The www values are the model’s learned weights, and bbb is the bias.\n            *   The xxx values are the feature values for a particular example.\n*   **Pros:**\n    *   Algorithm is quite simple and efficient.\n    *   Provides concrete probability scores as output.\n*   **Cons:**\n    *   Bad at handling a large number of categorical features.\n    *   It assumes that the data is free of missing values and predictors are independent of each other.\n*   **Use case:**\n    *   Logistic regression is used when the dependent variable (target) is categorical as in binary classification problems.\n        *   For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).\n\n*   You may just need an output of 0 or 1 and use it “as is”.\n    *   Say your model predicts the probability that your baby will cry at night as:\n        \n        p(cry|night)\\=0.05p(cry|night)\\=0.05\n        \n        p(cry|night)= 0.05\n    *   Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:\n        \n        wakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 dayswakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 days\n        \n        \\\\begin{align} wakeUp &= p(cry|nights)\\*nights \\\\\\\\ &= 0.05 \\* 365 \\\\\\\\ &= 18 \\\\text{ days} \\\\end{align}\n*   Or you may want to convert it into a binary category such as: spam or not spam and convert it to a binary classification problem.\n    \n    *   In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.\n    *   The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.\n    *   Sigmoid function is represented as:\n    \n    ![](/primers/ai/assets/ml-comp/1.jpg)\n    \n    y′\\=11+e−zy′\\=11+e−z\n    \n    y^{\\\\prime}=\\\\frac{1}{1+e^{-z}}\n    *   where,\n        \n        *   y′y′y^{\\\\prime} is the output of the logistic regression model for a particular example.\n        \n        z\\=b+w1x1+w2x2+…+wNxNz\\=b+w1x1+w2x2+…+wNxN\n        \n        z=b+w\\_{1} x\\_{1}+w\\_{2} x\\_{2}+\\\\ldots+w\\_{N} x\\_{N}\n        *   The www values are the model’s learned weights, and bbb is the bias.\n        *   The xxx values are the feature values for a particular example.\n\n*   Say your model predicts the probability that your baby will cry at night as:\n    \n    p(cry|night)\\=0.05p(cry|night)\\=0.05\n    \n    p(cry|night)= 0.05\n*   Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:\n    \n    wakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 dayswakeUp\\=p(cry|nights)∗nights\\=0.05∗365\\=18 days\n    \n    \\\\begin{align} wakeUp &= p(cry|nights)\\*nights \\\\\\\\ &= 0.05 \\* 365 \\\\\\\\ &= 18 \\\\text{ days} \\\\end{align}\n\nSay your model predicts the probability that your baby will cry at night as:\n\nLet’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:\n\n*   In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.\n*   The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.\n*   Sigmoid function is represented as:\n\n![](/primers/ai/assets/ml-comp/1.jpg)\n\n*   where,\n    \n    *   y′y′y^{\\\\prime} is the output of the logistic regression model for a particular example.\n    \n    z\\=b+w1x1+w2x2+…+wNxNz\\=b+w1x1+w2x2+…+wNxN\n    \n    z=b+w\\_{1} x\\_{1}+w\\_{2} x\\_{2}+\\\\ldots+w\\_{N} x\\_{N}\n    *   The www values are the model’s learned weights, and bbb is the bias.\n    *   The xxx values are the feature values for a particular example.\n\n*   y′y′y^{\\\\prime} is the output of the logistic regression model for a particular example.\n\n*   The www values are the model’s learned weights, and bbb is the bias.\n*   The xxx values are the feature values for a particular example.\n\n*   Algorithm is quite simple and efficient.\n*   Provides concrete probability scores as output.\n\n*   Bad at handling a large number of categorical features.\n*   It assumes that the data is free of missing values and predictors are independent of each other.\n\n*   Logistic regression is used when the dependent variable (target) is categorical as in binary classification problems.\n    *   For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).\n\n*   For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 5,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 875,
      "contentLength": 102154
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#logistic-regression",
    "scrapedAt": "2025-12-28T11:45:45.975Z"
  },
  {
    "id": "ai-ml-comp-naive-bayes-classifier-2",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification Algorithms",
    "title": "Naive Bayes Classifier",
    "subtitle": "Classification Algorithms",
    "contentHtml": "<ul>\n  <li>Naive Bayes is a supervised learning algorithms based on Bayes’ theorem which can serve as either a binary or multi-class classifier.</li>\n  <li>It is termed “naive” because it makes the naive assumption of conditional independence between every pair of features given the value of the class variable.</li>\n  <li>The figure below below <a href=\"https://towardsdatascience.com/intro-to-bayesian-statistics-5056b43d248d\">(source)</a> shows the equation for Bayes Theorem and its individual components:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ml-comp/2.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>The thought behind naive Bayes classification is to try to classify the data by maximizing:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>O</mi><mo>&amp;#x2223;</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-148\" style=\"width: 7.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.89em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"mi\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-154\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-160\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>O</mi><mo>∣</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-9\">P(O \\mid C_i) P(C_i)</script>\n\n    <ul>\n      <li>where,\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-164\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">O</script> is the object or tuple in a dataset.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-167\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">i</script> is an index of the class.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Under-the-hood, Naive Bayes involves a multiplication (once the probability is known) which makes the algorithm simplistic and fast.</li>\n      <li>It can also be used to solve multi-class prediction problems.</li>\n      <li>This classifier performs better than other models with less training data if the assumption of independence of features holds.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>It assumes that all the features are independent. This is actually a big con because features in reality are frequently not fully independent.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>When the assumption of independence holds between features, Naive Bayes classifier typically performs better than logistic regression and requires less training data.</li>\n      <li>It performs well in case of categorical input variables compared to continuous/numerical variable(s).</li>\n    </ul>\n  </li>\n</ul>\n<p>The thought behind naive Bayes classification is to try to classify the data by maximizing:</p>\n<ul>\n      <li>where,\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-164\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">O</script> is the object or tuple in a dataset.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-167\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">i</script> is an index of the class.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-164\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">O</script> is the object or tuple in a dataset.</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-167\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">i</script> is an index of the class.</li>\n        </ul>\n<ul>\n      <li>Under-the-hood, Naive Bayes involves a multiplication (once the probability is known) which makes the algorithm simplistic and fast.</li>\n      <li>It can also be used to solve multi-class prediction problems.</li>\n      <li>This classifier performs better than other models with less training data if the assumption of independence of features holds.</li>\n    </ul>\n<ul>\n      <li>It assumes that all the features are independent. This is actually a big con because features in reality are frequently not fully independent.</li>\n    </ul>\n<ul>\n      <li>When the assumption of independence holds between features, Naive Bayes classifier typically performs better than logistic regression and requires less training data.</li>\n      <li>It performs well in case of categorical input variables compared to continuous/numerical variable(s).</li>\n    </ul>",
    "contentMarkdown": "*   Naive Bayes is a supervised learning algorithms based on Bayes’ theorem which can serve as either a binary or multi-class classifier.\n*   It is termed “naive” because it makes the naive assumption of conditional independence between every pair of features given the value of the class variable.\n*   The figure below below [(source)](https://towardsdatascience.com/intro-to-bayesian-statistics-5056b43d248d) shows the equation for Bayes Theorem and its individual components:\n\n![](/primers/ai/assets/ml-comp/2.jpg)\n\n*   The thought behind naive Bayes classification is to try to classify the data by maximizing:\n    \n    P(O∣Ci)P(Ci)P(O∣Ci)P(Ci)\n    \n    P(O \\\\mid C\\_i) P(C\\_i)\n    *   where,\n        *   OOO is the object or tuple in a dataset.\n        *   iii is an index of the class.\n*   **Pros:**\n    *   Under-the-hood, Naive Bayes involves a multiplication (once the probability is known) which makes the algorithm simplistic and fast.\n    *   It can also be used to solve multi-class prediction problems.\n    *   This classifier performs better than other models with less training data if the assumption of independence of features holds.\n*   **Cons:**\n    *   It assumes that all the features are independent. This is actually a big con because features in reality are frequently not fully independent.\n*   **Use case:**\n    *   When the assumption of independence holds between features, Naive Bayes classifier typically performs better than logistic regression and requires less training data.\n    *   It performs well in case of categorical input variables compared to continuous/numerical variable(s).\n\nThe thought behind naive Bayes classification is to try to classify the data by maximizing:\n\n*   where,\n    *   OOO is the object or tuple in a dataset.\n    *   iii is an index of the class.\n\n*   OOO is the object or tuple in a dataset.\n*   iii is an index of the class.\n\n*   Under-the-hood, Naive Bayes involves a multiplication (once the probability is known) which makes the algorithm simplistic and fast.\n*   It can also be used to solve multi-class prediction problems.\n*   This classifier performs better than other models with less training data if the assumption of independence of features holds.\n\n*   It assumes that all the features are independent. This is actually a big con because features in reality are frequently not fully independent.\n\n*   When the assumption of independence holds between features, Naive Bayes classifier typically performs better than logistic regression and requires less training data.\n*   It performs well in case of categorical input variables compared to continuous/numerical variable(s).",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "supervised learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 388,
      "contentLength": 14246
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#naive-bayes-classifier",
    "scrapedAt": "2025-12-28T11:45:45.975Z"
  },
  {
    "id": "ai-ml-comp-linear-regression-3",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Regression Algorithms",
    "title": "Linear Regression",
    "subtitle": "Regression Algorithms",
    "contentHtml": "<ul>\n  <li>Linear regression analysis is a supervised machine learning algorithm that is used to predict the value of an output variable based on the value of an input variable.</li>\n  <li>The output variable we’re looking to predict is called the dependent variable.</li>\n  <li>The input variable we’re using to predict the output variable’s value is called the independent variable.</li>\n  <li>Assumes a linear relationship between the output and input variable(s) and fits a linear equation on the data.</li>\n  <li>The goal of Linear Regression is to predict output values for inputs that are not present in the data set, with the belief that those outputs would fall on the fitted line.</li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Performs very well for linearly separated data.</li>\n      <li>Easy to implement and is interpretable.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Prone to noise and overfitting.</li>\n      <li>Very sensitive to outliers.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>Linear regression is commonly used for predictive analysis and modeling.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Performs very well for linearly separated data.</li>\n      <li>Easy to implement and is interpretable.</li>\n    </ul>\n<ul>\n      <li>Prone to noise and overfitting.</li>\n      <li>Very sensitive to outliers.</li>\n    </ul>\n<ul>\n      <li>Linear regression is commonly used for predictive analysis and modeling.</li>\n    </ul>",
    "contentMarkdown": "*   Linear regression analysis is a supervised machine learning algorithm that is used to predict the value of an output variable based on the value of an input variable.\n*   The output variable we’re looking to predict is called the dependent variable.\n*   The input variable we’re using to predict the output variable’s value is called the independent variable.\n*   Assumes a linear relationship between the output and input variable(s) and fits a linear equation on the data.\n*   The goal of Linear Regression is to predict output values for inputs that are not present in the data set, with the belief that those outputs would fall on the fitted line.\n*   **Pros:**\n    *   Performs very well for linearly separated data.\n    *   Easy to implement and is interpretable.\n*   **Cons:**\n    *   Prone to noise and overfitting.\n    *   Very sensitive to outliers.\n*   **Use case:**\n    *   Linear regression is commonly used for predictive analysis and modeling.\n\n*   Performs very well for linearly separated data.\n*   Easy to implement and is interpretable.\n\n*   Prone to noise and overfitting.\n*   Very sensitive to outliers.\n\n*   Linear regression is commonly used for predictive analysis and modeling.",
    "order": 3,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "machine learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 192,
      "contentLength": 1510
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#linear-regression",
    "scrapedAt": "2025-12-28T11:45:45.975Z"
  },
  {
    "id": "ai-ml-comp-k-nearest-neighbors-4",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification and Regression Algorithms",
    "title": "K-Nearest Neighbors",
    "subtitle": "Classification and Regression Algorithms",
    "contentHtml": "<ul>\n  <li>Based on the age-old adage “birds of a feather flock together”.</li>\n  <li>The <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-170\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">k</script>-nearest neighbors algorithm, also known as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">k</script>-NN, is a non-parametric, supervised machine learning algorithm, which uses proximity to make classifications or predictions about the grouping of an individual data point.</li>\n  <li>While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.</li>\n  <li>The value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">k</script> is a hyperparameter which represents the number of neighbors you’d like the algorithm to refer as it generates its output.</li>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-179\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">k</script>-NN answers the question that given the current data, what are the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-182\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-183\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">k</script> most similar data points to the query.\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-185\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-186\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">k</script>-NN calculates distance typically using either Euclidean or Manhattan distance:</li>\n      <li>\n        <p>Euclidean distance:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msqrt><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msup></msqrt></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-188\" style=\"width: 12.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.055em, 1010.32em, 4.013em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-193\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msqrt\" id=\"MathJax-Span-197\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 6.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.242em, 1005.26em, 5.419em, -999.997em); top: -4.008em; left: 1.148em;\"><span class=\"mrow\" id=\"MathJax-Span-198\"><span class=\"munderover\" id=\"MathJax-Span-199\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-201\"><span class=\"mrow\" id=\"MathJax-Span-202\"><span class=\"mi\" id=\"MathJax-Span-203\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-206\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-209\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.28em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-210\"><span class=\"mo\" id=\"MathJax-Span-211\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"msubsup\" id=\"MathJax-Span-213\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-215\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-219\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-221\"><span class=\"mrow\" id=\"MathJax-Span-222\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-224\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 3.336em;\"><span class=\"texatom\" id=\"MathJax-Span-225\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1005.26em, 3.388em, -999.997em); top: -5.154em; left: 1.148em;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 4.742em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.836em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.305em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.721em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.138em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.607em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.023em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.44em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.909em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 4.326em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(4.482em, 1001.2em, 8.44em, -999.997em); top: -6.612em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; font-family: STIXNonUnicode-Regular; top: -3.331em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.727em; left: 0em;\">⎷<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXNonUnicode-Regular; position: absolute; top: -2.81em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXNonUnicode-Regular; position: absolute; top: -2.185em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.638em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.059em; border-left: 0px solid; width: 0px; height: 4.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msqrt><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>−</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msup></msqrt></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-18\">d(x, y)=\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-x_{i}\\right)^{2}}</script>\n      </li>\n      <li>\n        <p>Manhattan distance:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mrow><mo>|</mo><mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-228\" style=\"width: 10.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.492em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1008.39em, 4.326em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mi\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-237\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-238\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-239\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-244\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-247\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-248\" style=\"vertical-align: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.279em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"msubsup\" id=\"MathJax-Span-250\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-256\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-258\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-261\" style=\"vertical-align: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.279em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mrow><mo>|</mo><mrow><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>−</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-19\">d(x, y)=\\sum_{i=1}^{m}\\left|x_{i}-y_{i}\\right|</script>\n      </li>\n    </ul>\n  </li>\n  <li>This is the high-level view of how the algorithm works:\n    <ul>\n      <li>For each example in the data:\n        <ul>\n          <li>Calculate distance between query example and current example from the data.</li>\n          <li>Add the distance and index to an ordered collection.</li>\n          <li>Sort in ascending order by distance.</li>\n          <li>Pick first <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">k</script> from sorted order.</li>\n          <li>Get labels of selected <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">k</script> entries.</li>\n          <li>If regression, return the mean of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">k</script> labels.</li>\n          <li>If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Easy to implement.</li>\n      <li>Needs only a few hyperparameters which are:\n        <ul>\n          <li>The value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-271\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-272\"><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">k</script>.</li>\n          <li>Distance metric used.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Does not scale well as it takes too much memory and data storage compared with other classifiers.</li>\n      <li>Prone to overfitting if the value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-274\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-275\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">k</script> is too low and will underfit if the value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-277\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">k</script> is too high.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>While <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">k</script>-NNs can be used for regression problems, they are typically used for classification.</li>\n      <li>When labelled data is too expensive or impossible to obtain.</li>\n      <li>When the dataset is relatively smaller and is noise free.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-185\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-186\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">k</script>-NN calculates distance typically using either Euclidean or Manhattan distance:</li>\n      <li>\n        <p>Euclidean distance:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msqrt><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msup></msqrt></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-188\" style=\"width: 12.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.055em, 1010.32em, 4.013em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-193\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msqrt\" id=\"MathJax-Span-197\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 6.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.242em, 1005.26em, 5.419em, -999.997em); top: -4.008em; left: 1.148em;\"><span class=\"mrow\" id=\"MathJax-Span-198\"><span class=\"munderover\" id=\"MathJax-Span-199\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-201\"><span class=\"mrow\" id=\"MathJax-Span-202\"><span class=\"mi\" id=\"MathJax-Span-203\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-206\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-209\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.28em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-210\"><span class=\"mo\" id=\"MathJax-Span-211\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"msubsup\" id=\"MathJax-Span-213\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-215\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-219\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-221\"><span class=\"mrow\" id=\"MathJax-Span-222\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-224\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 3.336em;\"><span class=\"texatom\" id=\"MathJax-Span-225\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1005.26em, 3.388em, -999.997em); top: -5.154em; left: 1.148em;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 4.742em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 0.836em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.305em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 1.721em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.138em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 2.607em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.023em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.44em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 3.909em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -4.008em; left: 4.326em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(4.482em, 1001.2em, 8.44em, -999.997em); top: -6.612em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; font-family: STIXNonUnicode-Regular; top: -3.331em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXSizeOneSym; top: -0.727em; left: 0em;\">⎷<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXNonUnicode-Regular; position: absolute; top: -2.81em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXNonUnicode-Regular; position: absolute; top: -2.185em; left: 0em;\"><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.638em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.059em; border-left: 0px solid; width: 0px; height: 4.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msqrt><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>−</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msup></msqrt></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-18\">d(x, y)=\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-x_{i}\\right)^{2}}</script>\n      </li>\n      <li>\n        <p>Manhattan distance:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mrow><mo>|</mo><mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-228\" style=\"width: 10.211em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.492em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1008.39em, 4.326em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mi\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-237\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-238\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-239\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-244\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-247\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-248\" style=\"vertical-align: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.279em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"msubsup\" id=\"MathJax-Span-250\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-256\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-258\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-261\" style=\"vertical-align: 0.732em;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.279em; left: 0em;\">∣<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mrow><mo>|</mo><mrow><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>−</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-19\">d(x, y)=\\sum_{i=1}^{m}\\left|x_{i}-y_{i}\\right|</script>\n      </li>\n    </ul>\n<p>Euclidean distance:</p>\n<p>Manhattan distance:</p>\n<ul>\n      <li>For each example in the data:\n        <ul>\n          <li>Calculate distance between query example and current example from the data.</li>\n          <li>Add the distance and index to an ordered collection.</li>\n          <li>Sort in ascending order by distance.</li>\n          <li>Pick first <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">k</script> from sorted order.</li>\n          <li>Get labels of selected <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">k</script> entries.</li>\n          <li>If regression, return the mean of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">k</script> labels.</li>\n          <li>If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Calculate distance between query example and current example from the data.</li>\n          <li>Add the distance and index to an ordered collection.</li>\n          <li>Sort in ascending order by distance.</li>\n          <li>Pick first <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">k</script> from sorted order.</li>\n          <li>Get labels of selected <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">k</script> entries.</li>\n          <li>If regression, return the mean of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">k</script> labels.</li>\n          <li>If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.</li>\n        </ul>\n<ul>\n      <li>Easy to implement.</li>\n      <li>Needs only a few hyperparameters which are:\n        <ul>\n          <li>The value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-271\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-272\"><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">k</script>.</li>\n          <li>Distance metric used.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>The value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-271\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-272\"><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">k</script>.</li>\n          <li>Distance metric used.</li>\n        </ul>\n<ul>\n      <li>Does not scale well as it takes too much memory and data storage compared with other classifiers.</li>\n      <li>Prone to overfitting if the value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-274\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-275\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">k</script> is too low and will underfit if the value of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-277\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mi\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">k</script> is too high.</li>\n    </ul>\n<ul>\n      <li>While <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">k</script>-NNs can be used for regression problems, they are typically used for classification.</li>\n      <li>When labelled data is too expensive or impossible to obtain.</li>\n      <li>When the dataset is relatively smaller and is noise free.</li>\n    </ul>",
    "contentMarkdown": "*   Based on the age-old adage “birds of a feather flock together”.\n*   The kkk\\-nearest neighbors algorithm, also known as kkk\\-NN, is a non-parametric, supervised machine learning algorithm, which uses proximity to make classifications or predictions about the grouping of an individual data point.\n*   While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n*   The value of kkk is a hyperparameter which represents the number of neighbors you’d like the algorithm to refer as it generates its output.\n*   kkk\\-NN answers the question that given the current data, what are the kkk most similar data points to the query.\n    *   kkk\\-NN calculates distance typically using either Euclidean or Manhattan distance:\n    *   Euclidean distance:\n        \n        d(x,y)\\=∑i\\=1n(yi−xi)2‾‾‾‾‾‾‾‾‾‾‾‾⎷d(x,y)\\=∑i\\=1n(yi−xi)2\n        \n        d(x, y)=\\\\sqrt{\\\\sum\\_{i=1}^{n}\\\\left(y\\_{i}-x\\_{i}\\\\right)^{2}}\n    *   Manhattan distance:\n        \n        d(x,y)\\=∑i\\=1m∣∣xi−yi∣∣d(x,y)\\=∑i\\=1m|xi−yi|\n        \n        d(x, y)=\\\\sum\\_{i=1}^{m}\\\\left|x\\_{i}-y\\_{i}\\\\right|\n*   This is the high-level view of how the algorithm works:\n    *   For each example in the data:\n        *   Calculate distance between query example and current example from the data.\n        *   Add the distance and index to an ordered collection.\n        *   Sort in ascending order by distance.\n        *   Pick first kkk from sorted order.\n        *   Get labels of selected kkk entries.\n        *   If regression, return the mean of kkk labels.\n        *   If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.\n*   **Pros:**\n    *   Easy to implement.\n    *   Needs only a few hyperparameters which are:\n        *   The value of kkk.\n        *   Distance metric used.\n*   **Cons:**\n    *   Does not scale well as it takes too much memory and data storage compared with other classifiers.\n    *   Prone to overfitting if the value of kkk is too low and will underfit if the value of kkk is too high.\n*   **Use case:**\n    *   While kkk\\-NNs can be used for regression problems, they are typically used for classification.\n    *   When labelled data is too expensive or impossible to obtain.\n    *   When the dataset is relatively smaller and is noise free.\n\n*   kkk\\-NN calculates distance typically using either Euclidean or Manhattan distance:\n*   Euclidean distance:\n    \n    d(x,y)\\=∑i\\=1n(yi−xi)2‾‾‾‾‾‾‾‾‾‾‾‾⎷d(x,y)\\=∑i\\=1n(yi−xi)2\n    \n    d(x, y)=\\\\sqrt{\\\\sum\\_{i=1}^{n}\\\\left(y\\_{i}-x\\_{i}\\\\right)^{2}}\n*   Manhattan distance:\n    \n    d(x,y)\\=∑i\\=1m∣∣xi−yi∣∣d(x,y)\\=∑i\\=1m|xi−yi|\n    \n    d(x, y)=\\\\sum\\_{i=1}^{m}\\\\left|x\\_{i}-y\\_{i}\\\\right|\n\nEuclidean distance:\n\nManhattan distance:\n\n*   For each example in the data:\n    *   Calculate distance between query example and current example from the data.\n    *   Add the distance and index to an ordered collection.\n    *   Sort in ascending order by distance.\n    *   Pick first kkk from sorted order.\n    *   Get labels of selected kkk entries.\n    *   If regression, return the mean of kkk labels.\n    *   If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.\n\n*   Calculate distance between query example and current example from the data.\n*   Add the distance and index to an ordered collection.\n*   Sort in ascending order by distance.\n*   Pick first kkk from sorted order.\n*   Get labels of selected kkk entries.\n*   If regression, return the mean of kkk labels.\n*   If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.\n\n*   Easy to implement.\n*   Needs only a few hyperparameters which are:\n    *   The value of kkk.\n    *   Distance metric used.\n\n*   The value of kkk.\n*   Distance metric used.\n\n*   Does not scale well as it takes too much memory and data storage compared with other classifiers.\n*   Prone to overfitting if the value of kkk is too low and will underfit if the value of kkk is too high.\n\n*   While kkk\\-NNs can be used for regression problems, they are typically used for classification.\n*   When labelled data is too expensive or impossible to obtain.\n*   When the dataset is relatively smaller and is noise free.",
    "order": 4,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "algorithmsarchitecture",
      "machine learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": false,
      "wordCount": 619,
      "contentLength": 74594
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#k-nearest-neighbors",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-support-vector-machines-5",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification and Regression Algorithms",
    "title": "Support Vector Machines",
    "subtitle": "Classification and Regression Algorithms",
    "contentHtml": "<ul>\n  <li>The objective of Support Vector Machines (SVMs) is to find a hyperplane in an <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-283\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">N</script>-dimensional space (where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-286\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">N</script> is the number of features) that distinctly classifies the data points.\n    <ul>\n      <li>Note that a hyperplane is a decision boundary that helps classify the data points. If the number of input features is 2, hyperplane is just a line, if input features is 3, it becomes a 2D plane.</li>\n    </ul>\n  </li>\n  <li>The subset of training data points utilized in the decision function are called “support vectors”, hence the name Support Vector Machines.</li>\n  <li>In the instance that the data is not linearly separable, we need to use the kernel trick (also called the polynomial trick). The SVM kernel is a function that takes low dimensional input space and transforms it into higher-dimensional space, i.e., it enables learning a non-linear decision boundary to separate datapoints. The gist of the kernel trick is that learning a linear model in the higher-dimensional space is equivalent to learning a non-linear model in the original lower-dimensional input space. More on this in the article on <a href=\"../svm-kernel-trick\">SVM Kernel/Polynomial Trick</a>.</li>\n  <li>The image below <a href=\"https://www.geeksforgeeks.org/support-vector-machine-algorithm/\">(source)</a> displays the linear hyperplane separating the two classes such that the distance from the hyperplane to the nearest data point on each side is maximized. This hyperplane is known as the maximum-margin hyperplane/hard margin.</li>\n</ul>\n<ul>\n      <li>Note that a hyperplane is a decision boundary that helps classify the data points. If the number of input features is 2, hyperplane is just a line, if input features is 3, it becomes a 2D plane.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/ml-comp/3.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Effective in high dimensional spaces.</li>\n      <li>Effective in cases where the number of dimensions is greater than the number of samples.</li>\n      <li>Works well when there is a clear margin of separation between classes.</li>\n      <li>Memory efficient as it uses a subset of training points in the decision function (“support vectors”).</li>\n      <li>Versatile since different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Doesn’t perform well when we have large datasets because the required training time is higher.</li>\n      <li>If the number of features is much greater than the number of samples, avoiding over-fitting in choosing kernel functions and regularization term is crucial.</li>\n      <li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.</li>\n      <li>Doesn’t perform very well when the dataset has more noise, i.e., when target classes are overlapping.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>While SVMs can be used for regression problems, they are typically used for classification (similar to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">k</script>-NN) and outliers detection.</li>\n      <li>Works great if the number of features is high and they occur in high dimensional spaces.</li>\n    </ul>\n  </li>\n  <li>The following figure <a href=\"https://www.linkedin.com/in/essy-ruben-kouakou-83a4201b4/?originalSubdomain=ci\">(source)</a> shows SVM flavors.</li>\n</ul>\n<ul>\n      <li>Effective in high dimensional spaces.</li>\n      <li>Effective in cases where the number of dimensions is greater than the number of samples.</li>\n      <li>Works well when there is a clear margin of separation between classes.</li>\n      <li>Memory efficient as it uses a subset of training points in the decision function (“support vectors”).</li>\n      <li>Versatile since different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>\n    </ul>\n<ul>\n      <li>Doesn’t perform well when we have large datasets because the required training time is higher.</li>\n      <li>If the number of features is much greater than the number of samples, avoiding over-fitting in choosing kernel functions and regularization term is crucial.</li>\n      <li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.</li>\n      <li>Doesn’t perform very well when the dataset has more noise, i.e., when target classes are overlapping.</li>\n    </ul>\n<ul>\n      <li>While SVMs can be used for regression problems, they are typically used for classification (similar to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">k</script>-NN) and outliers detection.</li>\n      <li>Works great if the number of features is high and they occur in high dimensional spaces.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/ml-comp/svms.jpeg\" alt=\"\"></p>\n<h5 id=\"explain-the-kernel-trick-in-svm-and-why-we-use-it-and-how-to-choose-what-kernel-to-use\">Explain the Kernel Trick in SVM and Why We Use It and How to Choose What Kernel to Use?</h5>\n<ul>\n  <li>Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance.\n    <ul>\n      <li>For e.g.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to a RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.</li>\n    </ul>\n  </li>\n  <li>Typically without the kernel trick, in order to calculate support vectors and support vector classifiers, we need first to transform data points one by one to the higher dimensional space, and do the calculations based on SVM equations in the higher dimensional space, then return the results. The ‘trick’ in the kernel trick is that we design the kernels based on some conditions as mathematical functions that are equivalent to a dot product in the higher dimensional space without even having to transform data points to the higher dimensional space. i.e we can calculate support vectors and support vector classifiers in the same space where the data is provided which saves a lot of time and calculations.</li>\n  <li>Having domain knowledge can be very helpful in choosing the optimal kernel for your problem, however in the absence of such knowledge following this default rule can be helpful:\nFor linear problems, we can try linear or logistic kernels and for nonlinear problems, we can use RBF or Gaussian kernels.</li>\n</ul>\n<ul>\n      <li>For e.g.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to a RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/interview/svmkt.jpeg\" alt=\"\"></p>",
    "contentMarkdown": "*   The objective of Support Vector Machines (SVMs) is to find a hyperplane in an NNN\\-dimensional space (where NNN is the number of features) that distinctly classifies the data points.\n    *   Note that a hyperplane is a decision boundary that helps classify the data points. If the number of input features is 2, hyperplane is just a line, if input features is 3, it becomes a 2D plane.\n*   The subset of training data points utilized in the decision function are called “support vectors”, hence the name Support Vector Machines.\n*   In the instance that the data is not linearly separable, we need to use the kernel trick (also called the polynomial trick). The SVM kernel is a function that takes low dimensional input space and transforms it into higher-dimensional space, i.e., it enables learning a non-linear decision boundary to separate datapoints. The gist of the kernel trick is that learning a linear model in the higher-dimensional space is equivalent to learning a non-linear model in the original lower-dimensional input space. More on this in the article on [SVM Kernel/Polynomial Trick](../svm-kernel-trick).\n*   The image below [(source)](https://www.geeksforgeeks.org/support-vector-machine-algorithm/) displays the linear hyperplane separating the two classes such that the distance from the hyperplane to the nearest data point on each side is maximized. This hyperplane is known as the maximum-margin hyperplane/hard margin.\n\n*   Note that a hyperplane is a decision boundary that helps classify the data points. If the number of input features is 2, hyperplane is just a line, if input features is 3, it becomes a 2D plane.\n\n![](/primers/ai/assets/ml-comp/3.jpg)\n\n*   **Pros:**\n    *   Effective in high dimensional spaces.\n    *   Effective in cases where the number of dimensions is greater than the number of samples.\n    *   Works well when there is a clear margin of separation between classes.\n    *   Memory efficient as it uses a subset of training points in the decision function (“support vectors”).\n    *   Versatile since different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n*   **Cons:**\n    *   Doesn’t perform well when we have large datasets because the required training time is higher.\n    *   If the number of features is much greater than the number of samples, avoiding over-fitting in choosing kernel functions and regularization term is crucial.\n    *   SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n    *   Doesn’t perform very well when the dataset has more noise, i.e., when target classes are overlapping.\n*   **Use case:**\n    *   While SVMs can be used for regression problems, they are typically used for classification (similar to kkk\\-NN) and outliers detection.\n    *   Works great if the number of features is high and they occur in high dimensional spaces.\n*   The following figure [(source)](https://www.linkedin.com/in/essy-ruben-kouakou-83a4201b4/?originalSubdomain=ci) shows SVM flavors.\n\n*   Effective in high dimensional spaces.\n*   Effective in cases where the number of dimensions is greater than the number of samples.\n*   Works well when there is a clear margin of separation between classes.\n*   Memory efficient as it uses a subset of training points in the decision function (“support vectors”).\n*   Versatile since different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\n*   Doesn’t perform well when we have large datasets because the required training time is higher.\n*   If the number of features is much greater than the number of samples, avoiding over-fitting in choosing kernel functions and regularization term is crucial.\n*   SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n*   Doesn’t perform very well when the dataset has more noise, i.e., when target classes are overlapping.\n\n*   While SVMs can be used for regression problems, they are typically used for classification (similar to kkk\\-NN) and outliers detection.\n*   Works great if the number of features is high and they occur in high dimensional spaces.\n\n![](/primers/ai/assets/ml-comp/svms.jpeg)\n\n##### Explain the Kernel Trick in SVM and Why We Use It and How to Choose What Kernel to Use?\n\n*   Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance.\n    *   For e.g.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to a RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.\n*   Typically without the kernel trick, in order to calculate support vectors and support vector classifiers, we need first to transform data points one by one to the higher dimensional space, and do the calculations based on SVM equations in the higher dimensional space, then return the results. The ‘trick’ in the kernel trick is that we design the kernels based on some conditions as mathematical functions that are equivalent to a dot product in the higher dimensional space without even having to transform data points to the higher dimensional space. i.e we can calculate support vectors and support vector classifiers in the same space where the data is provided which saves a lot of time and calculations.\n*   Having domain knowledge can be very helpful in choosing the optimal kernel for your problem, however in the absence of such knowledge following this default rule can be helpful: For linear problems, we can try linear or logistic kernels and for nonlinear problems, we can use RBF or Gaussian kernels.\n\n*   For e.g.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to a RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.\n\n![](/primers/ai/assets/interview/svmkt.jpeg)",
    "order": 5,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 6,
    "tags": [
      "algorithmsarchitecture",
      "regularization",
      "cross-validation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 1015,
      "contentLength": 12361
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#support-vector-machines",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-decision-trees-6",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification and Regression Algorithms",
    "title": "Decision Trees",
    "subtitle": "Classification and Regression Algorithms",
    "contentHtml": "<ul>\n  <li>\n    <p>A Decision Tree is a tree with a flowchart-like structure consisting of 3 elements as shown in the following image <a href=\"https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/\">(source)</a>:</p>\n\n    <p><img src=\"/primers/ai/assets/ml-comp/5.jpg\" alt=\"\"></p>\n\n    <ul>\n      <li>The internal node denotes a test on an attribute.</li>\n      <li>Each branch represents an outcome of the test.</li>\n      <li>Each leaf node (terminal node) holds a class label.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Here is a figure (source)](https://python.plainenglish.io/decision-trees-easy-intuitive-way-with-python-23131eaad311) that illustrates an example decision tree with the thought process behind deciding to play tennis:</p>\n  </li>\n</ul>\n<p>A Decision Tree is a tree with a flowchart-like structure consisting of 3 elements as shown in the following image <a href=\"https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/\">(source)</a>:</p>\n<p><img src=\"/primers/ai/assets/ml-comp/5.jpg\" alt=\"\"></p>\n<ul>\n      <li>The internal node denotes a test on an attribute.</li>\n      <li>Each branch represents an outcome of the test.</li>\n      <li>Each leaf node (terminal node) holds a class label.</li>\n    </ul>\n<p>Here is a figure (source)](https://python.plainenglish.io/decision-trees-easy-intuitive-way-with-python-23131eaad311) that illustrates an example decision tree with the thought process behind deciding to play tennis:</p>\n<p><img src=\"/primers/ai/assets/ml-comp/4.jpg\" alt=\"\"></p>\n<ul>\n  <li>The objective of a Decision Tree is to create a training model that can to predict the class of the target variable by learning simple decision rules inferred from prior data (training data).</li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Interpretability is high due to the intuitive nature of a tree.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Decision trees are susceptible to overfitting (<a href=\"#random-forests\">random forests</a> are a great way to fix this issue).</li>\n      <li>Small changes in data can lead to large structural changes on the tree.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>When you want to be able to lay out all the possible outcomes of a problem and work on challenging each option.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Interpretability is high due to the intuitive nature of a tree.</li>\n    </ul>\n<ul>\n      <li>Decision trees are susceptible to overfitting (<a href=\"#random-forests\">random forests</a> are a great way to fix this issue).</li>\n      <li>Small changes in data can lead to large structural changes on the tree.</li>\n    </ul>\n<ul>\n      <li>When you want to be able to lay out all the possible outcomes of a problem and work on challenging each option.</li>\n    </ul>",
    "contentMarkdown": "*   A Decision Tree is a tree with a flowchart-like structure consisting of 3 elements as shown in the following image [(source)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/):\n    \n    ![](/primers/ai/assets/ml-comp/5.jpg)\n    \n    *   The internal node denotes a test on an attribute.\n    *   Each branch represents an outcome of the test.\n    *   Each leaf node (terminal node) holds a class label.\n*   Here is a figure (source)\\](https://python.plainenglish.io/decision-trees-easy-intuitive-way-with-python-23131eaad311) that illustrates an example decision tree with the thought process behind deciding to play tennis:\n    \n\nA Decision Tree is a tree with a flowchart-like structure consisting of 3 elements as shown in the following image [(source)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/):\n\n![](/primers/ai/assets/ml-comp/5.jpg)\n\n*   The internal node denotes a test on an attribute.\n*   Each branch represents an outcome of the test.\n*   Each leaf node (terminal node) holds a class label.\n\nHere is a figure (source)\\](https://python.plainenglish.io/decision-trees-easy-intuitive-way-with-python-23131eaad311) that illustrates an example decision tree with the thought process behind deciding to play tennis:\n\n![](/primers/ai/assets/ml-comp/4.jpg)\n\n*   The objective of a Decision Tree is to create a training model that can to predict the class of the target variable by learning simple decision rules inferred from prior data (training data).\n*   **Pros:**\n    *   Interpretability is high due to the intuitive nature of a tree.\n*   **Cons:**\n    *   Decision trees are susceptible to overfitting ([random forests](#random-forests) are a great way to fix this issue).\n    *   Small changes in data can lead to large structural changes on the tree.\n*   **Use case:**\n    *   When you want to be able to lay out all the possible outcomes of a problem and work on challenging each option.\n\n*   Interpretability is high due to the intuitive nature of a tree.\n\n*   Decision trees are susceptible to overfitting ([random forests](#random-forests) are a great way to fix this issue).\n*   Small changes in data can lead to large structural changes on the tree.\n\n*   When you want to be able to lay out all the possible outcomes of a problem and work on challenging each option.",
    "order": 6,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 318,
      "contentLength": 2904
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#decision-trees",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-model-ensembles-7",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Classification and Regression Algorithms",
    "title": "Model Ensembles",
    "subtitle": "Classification and Regression Algorithms",
    "contentHtml": "<h4 id=\"bagging-and-boosting\">Bagging and Boosting</h4>\n<ul>\n  <li>Bagging and boosting are two popular ensemble learning techniques used in machine learning to improve the performance of predictive models by combining multiple weaker models. In other words, they combine multiple models to produce a more stable and accurate final model compared to a single classifier. They aim to decrease the bias and variance of a model (with the end goal of having low bias and modest variance to model the nuances of the training data, while not underfitting/overfitting it). While they have similar goals, they differ in their approach and how they create the ensemble.</li>\n  <li>Ensemble learning is a powerful approach that combines multiple models to improve the predictive performance of machine learning algorithms. By leveraging the diversity of these models, ensemble learning helps mitigate the issues of bias, variance, and noise commonly encountered in individual models. It achieves this by training a set of classifiers or experts and allowing them to vote or contribute to the final prediction or classification.</li>\n</ul>\n<h6 id=\"bootstrapping\">Bootstrapping</h6>\n<ul>\n  <li>Before diving into the specifics of bagging and boosting, let’s first understand bootstrapping. Bootstrapping is a sampling technique that involves creating subsets of observations from the original dataset with replacement.</li>\n  <li>Each subset has the same size as the original dataset, and the random sampling allows us to better understand the bias and variance within the dataset. It helps estimate the mean and standard deviation by resampling from the dataset.</li>\n</ul>\n<h6 id=\"bagging\">Bagging</h6>\n<ul>\n  <li>Bagging, short for Bootstrap Aggregation, is a straightforward yet powerful ensemble method. It applies the bootstrap procedure to high-variance machine learning algorithms, typically decision trees. The idea behind bagging is to combine the results of multiple models, such as decision trees, to obtain a more generalized and robust prediction. It creates subsets (bags) from the original dataset using random sampling with replacement, and each subset is used to train a base model or weak model independently. These models run in parallel and are independent of each other.</li>\n  <li>The final prediction is determined by combining the predictions from all the models, often through averaging or majority voting.</li>\n</ul>\n<h6 id=\"boosting\">Boosting</h6>\n<ul>\n  <li>Boosting is a sequential process where each subsequent model attempts to correct the errors made by the previous model. Unlike bagging, boosting involves training learners sequentially, with early learners fitting simple models to the data and subsequent learners analyzing the data for errors. The goal is to solve for net error from the prior model by adjusting the weights assigned to each data point. Boosting assigns higher weights to misclassified data points, so subsequent learners focus more on these difficult cases.</li>\n  <li>Through this iterative process, boosting aims to convert a collection of weak learners into a stronger and more accurate model. The final model, often referred to as a strong learner, is a weighted combination of all the models.</li>\n</ul>\n<h6 id=\"bagging-vs-boosting\">Bagging vs. Boosting</h6>\n<ul>\n  <li>Bagging and Boosting are both ensemble learning techniques used to improve the performance of machine learning models. However, they differ in their approach and objectives. Here are the key differences between Bagging and Boosting:</li>\n  <li><strong>Data Sampling:</strong>\n    <ul>\n      <li>Bagging: In Bagging (short for Bootstrap Aggregating), multiple training datasets are created by randomly sampling from the original dataset with replacement. Each dataset is of the same size as the original dataset.</li>\n      <li>Boosting: In Boosting, the training datasets are also created by random sampling with replacement. However, each new dataset gives more weight to the instances that were misclassified by previous models. This allows subsequent models to focus more on difficult cases.</li>\n    </ul>\n  </li>\n  <li><strong>Model Independence:</strong>\n    <ul>\n      <li>Bagging: In Bagging, each model is built independently of the others. They are trained on different subsets of the data and can be constructed in parallel.</li>\n      <li>Boosting: In Boosting, models are built sequentially. Each new model is influenced by the performance of previously built models. Misclassified instances are given higher weights, and subsequent models try to correct those errors.</li>\n    </ul>\n  </li>\n  <li><strong>Weighting of Models:</strong>\n    <ul>\n      <li>Bagging: In Bagging, all models have equal weight when making predictions. The final prediction is often obtained by averaging the predictions of all models or using majority voting.</li>\n      <li>Boosting: In Boosting, models are weighted based on their performance. Models with better classification results are given higher weights. The final prediction is obtained by combining the weighted predictions of all models.</li>\n    </ul>\n  </li>\n  <li><strong>Objective:</strong>\n    <ul>\n      <li>Bagging: Bagging aims to reduce the variance of a single model. It helps to improve stability and reduce overfitting by combining multiple models trained on different subsets of the data.</li>\n      <li>Boosting: Boosting aims to reduce the bias of a single model. It focuses on difficult instances and tries to correct the model’s mistakes by giving more weight to misclassified instances. Boosting can improve the overall accuracy of the model but may be more prone to overfitting.</li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li>Bagging: Random Forest is an extension of Bagging that uses decision trees as base models and combines their predictions to make final predictions.</li>\n      <li>Boosting: Gradient Boosting is a popular Boosting algorithm that sequentially adds decision trees to the model, with each new tree correcting the mistakes of the previous ones.</li>\n    </ul>\n  </li>\n  <li>The image below <a href=\"https://www.kaggle.com/code/prashant111/bagging-vs-boosting\">(source)</a> is an illustrated example of bagging and boosting.</li>\n</ul>\n<ul>\n      <li>Bagging: In Bagging (short for Bootstrap Aggregating), multiple training datasets are created by randomly sampling from the original dataset with replacement. Each dataset is of the same size as the original dataset.</li>\n      <li>Boosting: In Boosting, the training datasets are also created by random sampling with replacement. However, each new dataset gives more weight to the instances that were misclassified by previous models. This allows subsequent models to focus more on difficult cases.</li>\n    </ul>\n<ul>\n      <li>Bagging: In Bagging, each model is built independently of the others. They are trained on different subsets of the data and can be constructed in parallel.</li>\n      <li>Boosting: In Boosting, models are built sequentially. Each new model is influenced by the performance of previously built models. Misclassified instances are given higher weights, and subsequent models try to correct those errors.</li>\n    </ul>\n<ul>\n      <li>Bagging: In Bagging, all models have equal weight when making predictions. The final prediction is often obtained by averaging the predictions of all models or using majority voting.</li>\n      <li>Boosting: In Boosting, models are weighted based on their performance. Models with better classification results are given higher weights. The final prediction is obtained by combining the weighted predictions of all models.</li>\n    </ul>\n<ul>\n      <li>Bagging: Bagging aims to reduce the variance of a single model. It helps to improve stability and reduce overfitting by combining multiple models trained on different subsets of the data.</li>\n      <li>Boosting: Boosting aims to reduce the bias of a single model. It focuses on difficult instances and tries to correct the model’s mistakes by giving more weight to misclassified instances. Boosting can improve the overall accuracy of the model but may be more prone to overfitting.</li>\n    </ul>\n<ul>\n      <li>Bagging: Random Forest is an extension of Bagging that uses decision trees as base models and combines their predictions to make final predictions.</li>\n      <li>Boosting: Gradient Boosting is a popular Boosting algorithm that sequentially adds decision trees to the model, with each new tree correcting the mistakes of the previous ones.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/ml-comp/9.png\" alt=\"\"></p>\n<h4 id=\"random-forests\">Random Forests</h4>\n<ul>\n  <li>A random forest is a robust ML algorithm that relies on having an ensemble of different models and making them vote for the prediction.</li>\n  <li>An essential feature of random forest is to encourage diversity in the models; that way, we ensure the models have different predictions that will improve their model performance.</li>\n  <li>Random forest encourages diversity by using random sampling with a replacement but also changing the root node, which is the first feature in which we split our data.</li>\n  <li>\n    <p>The results are a set of decision trees with different root nodes taken from similar (not equal) datasets, and each has a vote in the prediction of the model.</p>\n  </li>\n  <li><strong>Pros:</strong>\n    <ul>\n      <li>Less prone to overfitting compared to <a href=\"#decision-trees\">decision trees</a>.</li>\n    </ul>\n  </li>\n  <li><strong>Cons:</strong>\n    <ul>\n      <li>Interpretability is low compared to <a href=\"#decision-trees\">decision trees</a>.</li>\n    </ul>\n  </li>\n  <li><strong>Use case:</strong>\n    <ul>\n      <li>When the model is overfitting and you want better generalization.</li>\n    </ul>\n  </li>\n</ul>\n<p>The results are a set of decision trees with different root nodes taken from similar (not equal) datasets, and each has a vote in the prediction of the model.</p>\n<ul>\n      <li>Less prone to overfitting compared to <a href=\"#decision-trees\">decision trees</a>.</li>\n    </ul>\n<ul>\n      <li>Interpretability is low compared to <a href=\"#decision-trees\">decision trees</a>.</li>\n    </ul>\n<ul>\n      <li>When the model is overfitting and you want better generalization.</li>\n    </ul>\n<h4 id=\"gradient-boosting\">Gradient Boosting</h4>\n<ul>\n  <li>Gradient Boosting is an ensemble machine learning algorithm that combines multiple weak models to create a strong model.</li>\n  <li>It is an iterative process where each iteration, a new model is fit to the residual errors made by the previous model, with the goal of decreasing the overall prediction error.</li>\n  <li>The algorithm works as follows:\n    <ol>\n      <li>Initialize the model with a weak learner, typically a decision tree with a single split.</li>\n      <li>Compute the negative gradient of the loss function with respect to the current prediction.</li>\n      <li>Fit a new model to the negative gradient.</li>\n      <li>Update the prediction by adding the prediction from the new model.</li>\n      <li>Repeat steps 2-4 for a specified number of iterations, or until a stopping criterion is met.</li>\n      <li>Combine the predictions from all models to get the final prediction.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li>Initialize the model with a weak learner, typically a decision tree with a single split.</li>\n      <li>Compute the negative gradient of the loss function with respect to the current prediction.</li>\n      <li>Fit a new model to the negative gradient.</li>\n      <li>Update the prediction by adding the prediction from the new model.</li>\n      <li>Repeat steps 2-4 for a specified number of iterations, or until a stopping criterion is met.</li>\n      <li>Combine the predictions from all models to get the final prediction.</li>\n    </ol>\n<h5 id=\"xgboost\">XGBoost</h5>\n<ul>\n  <li>XGBoost algorithm is a gradient boosting algorithm that is highly efficient and scalable.</li>\n  <li>Here’s a high-level overview of the XGBoost algorithm:\n    <ol>\n      <li>Initialize the model with a weak learner, usually a decision tree stump (a decision tree with a single split)</li>\n      <li>Compute the negative gradient of the loss function with respect to the current prediction</li>\n      <li>Fit a decision tree to the negative gradient to make a new prediction</li>\n      <li>Add the prediction from this tree to the current prediction</li>\n      <li>Repeat steps 2-4 for a specified number of trees, or until a stopping criterion is met</li>\n      <li>Combine the predictions from all trees to get the final prediction</li>\n    </ol>\n  </li>\n  <li>The following figure summarizes parallelizing XGBoost <a href=\"https://theaiedge.io\">(source)</a>.</li>\n  <li>The content below is taken from <a href=\"\">Damien Benveniste’s LinkedIn post linked here</a></li>\n  <li>But why XGBoost became so popular? There are 2 aspects that made the success of XGBoost back in 2014.</li>\n  <li>The first one is the regularized learning objective that allows for better pruning of the trees.</li>\n  <li>The second one, is the ability to distribute the Gradient Boosting learning process across multiple threads or machines, allowing it to handle larger scales of data. Boosting algorithms have been known to perform very well on most large data sets, but the iterative process of boosting makes those painfully slow!</li>\n  <li>How do you parallelize a boosting algorithm then? In the case of Random Forest, it is easy, you just distribute the data across threads, build independent trees there, and average the resulting tree predictions. In the case of an iterative process like boosting, you need to parallelize the tree building itself. It all comes down to how you find an optimal split in a tree: for each feature, sort the data and linearly scan the feature to find the best split. If you have N samples and M features, it is O(NM log(N)) time complexity at each node. In pseudo-code:</li>\n</ul>\n<ol>\n      <li>Initialize the model with a weak learner, usually a decision tree stump (a decision tree with a single split)</li>\n      <li>Compute the negative gradient of the loss function with respect to the current prediction</li>\n      <li>Fit a decision tree to the negative gradient to make a new prediction</li>\n      <li>Add the prediction from this tree to the current prediction</li>\n      <li>Repeat steps 2-4 for a specified number of trees, or until a stopping criterion is met</li>\n      <li>Combine the predictions from all trees to get the final prediction</li>\n    </ol>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"n\">best_split</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"k\">for</span> <span class=\"n\">feature</span> <span class=\"ow\">in</span> <span class=\"n\">features</span><span class=\"p\">:</span>\n     <span class=\"k\">for</span> <span class=\"n\">sample</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span> <span class=\"n\">samples</span><span class=\"p\">:</span>\n   <span class=\"k\">if</span> <span class=\"n\">split</span> <span class=\"ow\">is</span> <span class=\"n\">better</span> <span class=\"n\">than</span> <span class=\"n\">best_split</span><span class=\"p\">:</span>\n          <span class=\"n\">best_split</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">feature</span><span class=\"p\">,</span> <span class=\"n\">sample</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"n\">best_split</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"k\">for</span> <span class=\"n\">feature</span> <span class=\"ow\">in</span> <span class=\"n\">features</span><span class=\"p\">:</span>\n     <span class=\"k\">for</span> <span class=\"n\">sample</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span> <span class=\"n\">samples</span><span class=\"p\">:</span>\n   <span class=\"k\">if</span> <span class=\"n\">split</span> <span class=\"ow\">is</span> <span class=\"n\">better</span> <span class=\"n\">than</span> <span class=\"n\">best_split</span><span class=\"p\">:</span>\n          <span class=\"n\">best_split</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">feature</span><span class=\"p\">,</span> <span class=\"n\">sample</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>So you can parallelize split search by scanning each feature independently and reduce the resulting splits to the optimal one.</li>\n  <li>XGBoost is not the first attempt to parallelize GBM, but they used a series of tricks that made it very efficient:</li>\n  <li>First, all the columns are pre-sorted while keeping a pointer to the original index of the entry. This removes the need to sort the feature at every search.</li>\n  <li>They used a Compressed Column Format for a more efficient distribution of the data.</li>\n  <li>They used a cache-aware prefetching algorithm to minimize the non-contiguous memory \naccess that results from the pre-sorting step.</li>\n  <li>Not directly about parallelization, but they came out with an approximated split search algorithm that speeds the tree building further.</li>\n  <li>\n    <p>As of today, you can train XGBoost across cores on the same machine, but also on AWS YARN, Kubernetes, Spark, and GPU and you can use Dask or Ray to do it (https://lnkd.in/dtQTfu62).</p>\n  </li>\n  <li>One thing to look out for is that there is a limit to how much XGBoost can be parallelized. With too many threads, the data communication between threads becomes a bottleneck and the training speed plateaus. Here is a example explaining that effect: https://lnkd.in/d9SEcQuV.\n<img src=\"assets/ml-comp/10.png\" alt=\"\"><br>\n<img src=\"/primers/ai/assets/ml-comp/7.png\" alt=\"\"><br>\n&lt;!–\n    <h2 id=\"clustering-in-machine-learning-latent-dirichlet-allocation-and-k-means\">Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means</h2>\n  </li>\n  <li>Clustering is a type of unsupervised learning method in machine learning, as it involves grouping unlabeled data based on their underlying patterns. This article will delve into two popular clustering algorithms: Latent Dirichlet Allocation (LDA) and K-Means.</li>\n</ul>\n<p>As of today, you can train XGBoost across cores on the same machine, but also on AWS YARN, Kubernetes, Spark, and GPU and you can use Dask or Ray to do it (https://lnkd.in/dtQTfu62).</p>",
    "contentMarkdown": "#### Bagging and Boosting\n\n*   Bagging and boosting are two popular ensemble learning techniques used in machine learning to improve the performance of predictive models by combining multiple weaker models. In other words, they combine multiple models to produce a more stable and accurate final model compared to a single classifier. They aim to decrease the bias and variance of a model (with the end goal of having low bias and modest variance to model the nuances of the training data, while not underfitting/overfitting it). While they have similar goals, they differ in their approach and how they create the ensemble.\n*   Ensemble learning is a powerful approach that combines multiple models to improve the predictive performance of machine learning algorithms. By leveraging the diversity of these models, ensemble learning helps mitigate the issues of bias, variance, and noise commonly encountered in individual models. It achieves this by training a set of classifiers or experts and allowing them to vote or contribute to the final prediction or classification.\n\n###### Bootstrapping\n\n*   Before diving into the specifics of bagging and boosting, let’s first understand bootstrapping. Bootstrapping is a sampling technique that involves creating subsets of observations from the original dataset with replacement.\n*   Each subset has the same size as the original dataset, and the random sampling allows us to better understand the bias and variance within the dataset. It helps estimate the mean and standard deviation by resampling from the dataset.\n\n###### Bagging\n\n*   Bagging, short for Bootstrap Aggregation, is a straightforward yet powerful ensemble method. It applies the bootstrap procedure to high-variance machine learning algorithms, typically decision trees. The idea behind bagging is to combine the results of multiple models, such as decision trees, to obtain a more generalized and robust prediction. It creates subsets (bags) from the original dataset using random sampling with replacement, and each subset is used to train a base model or weak model independently. These models run in parallel and are independent of each other.\n*   The final prediction is determined by combining the predictions from all the models, often through averaging or majority voting.\n\n###### Boosting\n\n*   Boosting is a sequential process where each subsequent model attempts to correct the errors made by the previous model. Unlike bagging, boosting involves training learners sequentially, with early learners fitting simple models to the data and subsequent learners analyzing the data for errors. The goal is to solve for net error from the prior model by adjusting the weights assigned to each data point. Boosting assigns higher weights to misclassified data points, so subsequent learners focus more on these difficult cases.\n*   Through this iterative process, boosting aims to convert a collection of weak learners into a stronger and more accurate model. The final model, often referred to as a strong learner, is a weighted combination of all the models.\n\n###### Bagging vs. Boosting\n\n*   Bagging and Boosting are both ensemble learning techniques used to improve the performance of machine learning models. However, they differ in their approach and objectives. Here are the key differences between Bagging and Boosting:\n*   **Data Sampling:**\n    *   Bagging: In Bagging (short for Bootstrap Aggregating), multiple training datasets are created by randomly sampling from the original dataset with replacement. Each dataset is of the same size as the original dataset.\n    *   Boosting: In Boosting, the training datasets are also created by random sampling with replacement. However, each new dataset gives more weight to the instances that were misclassified by previous models. This allows subsequent models to focus more on difficult cases.\n*   **Model Independence:**\n    *   Bagging: In Bagging, each model is built independently of the others. They are trained on different subsets of the data and can be constructed in parallel.\n    *   Boosting: In Boosting, models are built sequentially. Each new model is influenced by the performance of previously built models. Misclassified instances are given higher weights, and subsequent models try to correct those errors.\n*   **Weighting of Models:**\n    *   Bagging: In Bagging, all models have equal weight when making predictions. The final prediction is often obtained by averaging the predictions of all models or using majority voting.\n    *   Boosting: In Boosting, models are weighted based on their performance. Models with better classification results are given higher weights. The final prediction is obtained by combining the weighted predictions of all models.\n*   **Objective:**\n    *   Bagging: Bagging aims to reduce the variance of a single model. It helps to improve stability and reduce overfitting by combining multiple models trained on different subsets of the data.\n    *   Boosting: Boosting aims to reduce the bias of a single model. It focuses on difficult instances and tries to correct the model’s mistakes by giving more weight to misclassified instances. Boosting can improve the overall accuracy of the model but may be more prone to overfitting.\n*   **Examples:**\n    *   Bagging: Random Forest is an extension of Bagging that uses decision trees as base models and combines their predictions to make final predictions.\n    *   Boosting: Gradient Boosting is a popular Boosting algorithm that sequentially adds decision trees to the model, with each new tree correcting the mistakes of the previous ones.\n*   The image below [(source)](https://www.kaggle.com/code/prashant111/bagging-vs-boosting) is an illustrated example of bagging and boosting.\n\n*   Bagging: In Bagging (short for Bootstrap Aggregating), multiple training datasets are created by randomly sampling from the original dataset with replacement. Each dataset is of the same size as the original dataset.\n*   Boosting: In Boosting, the training datasets are also created by random sampling with replacement. However, each new dataset gives more weight to the instances that were misclassified by previous models. This allows subsequent models to focus more on difficult cases.\n\n*   Bagging: In Bagging, each model is built independently of the others. They are trained on different subsets of the data and can be constructed in parallel.\n*   Boosting: In Boosting, models are built sequentially. Each new model is influenced by the performance of previously built models. Misclassified instances are given higher weights, and subsequent models try to correct those errors.\n\n*   Bagging: In Bagging, all models have equal weight when making predictions. The final prediction is often obtained by averaging the predictions of all models or using majority voting.\n*   Boosting: In Boosting, models are weighted based on their performance. Models with better classification results are given higher weights. The final prediction is obtained by combining the weighted predictions of all models.\n\n*   Bagging: Bagging aims to reduce the variance of a single model. It helps to improve stability and reduce overfitting by combining multiple models trained on different subsets of the data.\n*   Boosting: Boosting aims to reduce the bias of a single model. It focuses on difficult instances and tries to correct the model’s mistakes by giving more weight to misclassified instances. Boosting can improve the overall accuracy of the model but may be more prone to overfitting.\n\n*   Bagging: Random Forest is an extension of Bagging that uses decision trees as base models and combines their predictions to make final predictions.\n*   Boosting: Gradient Boosting is a popular Boosting algorithm that sequentially adds decision trees to the model, with each new tree correcting the mistakes of the previous ones.\n\n![](/primers/ai/assets/ml-comp/9.png)\n\n#### Random Forests\n\n*   A random forest is a robust ML algorithm that relies on having an ensemble of different models and making them vote for the prediction.\n*   An essential feature of random forest is to encourage diversity in the models; that way, we ensure the models have different predictions that will improve their model performance.\n*   Random forest encourages diversity by using random sampling with a replacement but also changing the root node, which is the first feature in which we split our data.\n*   The results are a set of decision trees with different root nodes taken from similar (not equal) datasets, and each has a vote in the prediction of the model.\n    \n*   **Pros:**\n    *   Less prone to overfitting compared to [decision trees](#decision-trees).\n*   **Cons:**\n    *   Interpretability is low compared to [decision trees](#decision-trees).\n*   **Use case:**\n    *   When the model is overfitting and you want better generalization.\n\nThe results are a set of decision trees with different root nodes taken from similar (not equal) datasets, and each has a vote in the prediction of the model.\n\n*   Less prone to overfitting compared to [decision trees](#decision-trees).\n\n*   Interpretability is low compared to [decision trees](#decision-trees).\n\n*   When the model is overfitting and you want better generalization.\n\n#### Gradient Boosting\n\n*   Gradient Boosting is an ensemble machine learning algorithm that combines multiple weak models to create a strong model.\n*   It is an iterative process where each iteration, a new model is fit to the residual errors made by the previous model, with the goal of decreasing the overall prediction error.\n*   The algorithm works as follows:\n    1.  Initialize the model with a weak learner, typically a decision tree with a single split.\n    2.  Compute the negative gradient of the loss function with respect to the current prediction.\n    3.  Fit a new model to the negative gradient.\n    4.  Update the prediction by adding the prediction from the new model.\n    5.  Repeat steps 2-4 for a specified number of iterations, or until a stopping criterion is met.\n    6.  Combine the predictions from all models to get the final prediction.\n\n1.  Initialize the model with a weak learner, typically a decision tree with a single split.\n2.  Compute the negative gradient of the loss function with respect to the current prediction.\n3.  Fit a new model to the negative gradient.\n4.  Update the prediction by adding the prediction from the new model.\n5.  Repeat steps 2-4 for a specified number of iterations, or until a stopping criterion is met.\n6.  Combine the predictions from all models to get the final prediction.\n\n##### XGBoost\n\n*   XGBoost algorithm is a gradient boosting algorithm that is highly efficient and scalable.\n*   Here’s a high-level overview of the XGBoost algorithm:\n    1.  Initialize the model with a weak learner, usually a decision tree stump (a decision tree with a single split)\n    2.  Compute the negative gradient of the loss function with respect to the current prediction\n    3.  Fit a decision tree to the negative gradient to make a new prediction\n    4.  Add the prediction from this tree to the current prediction\n    5.  Repeat steps 2-4 for a specified number of trees, or until a stopping criterion is met\n    6.  Combine the predictions from all trees to get the final prediction\n*   The following figure summarizes parallelizing XGBoost [(source)](https://theaiedge.io).\n*   The content below is taken from Damien Benveniste’s LinkedIn post linked here\n*   But why XGBoost became so popular? There are 2 aspects that made the success of XGBoost back in 2014.\n*   The first one is the regularized learning objective that allows for better pruning of the trees.\n*   The second one, is the ability to distribute the Gradient Boosting learning process across multiple threads or machines, allowing it to handle larger scales of data. Boosting algorithms have been known to perform very well on most large data sets, but the iterative process of boosting makes those painfully slow!\n*   How do you parallelize a boosting algorithm then? In the case of Random Forest, it is easy, you just distribute the data across threads, build independent trees there, and average the resulting tree predictions. In the case of an iterative process like boosting, you need to parallelize the tree building itself. It all comes down to how you find an optimal split in a tree: for each feature, sort the data and linearly scan the feature to find the best split. If you have N samples and M features, it is O(NM log(N)) time complexity at each node. In pseudo-code:\n\n1.  Initialize the model with a weak learner, usually a decision tree stump (a decision tree with a single split)\n2.  Compute the negative gradient of the loss function with respect to the current prediction\n3.  Fit a decision tree to the negative gradient to make a new prediction\n4.  Add the prediction from this tree to the current prediction\n5.  Repeat steps 2-4 for a specified number of trees, or until a stopping criterion is met\n6.  Combine the predictions from all trees to get the final prediction\n\n![](https://aman.ai/images/copy.png)\n\n`best_split = None for feature in features:      for sample in sorted samples:    if split is better than best_split:           best_split = f(feature, sample)`\n\n![](https://aman.ai/images/copy.png)\n\n`best_split = None for feature in features:      for sample in sorted samples:    if split is better than best_split:           best_split = f(feature, sample)`\n\n*   So you can parallelize split search by scanning each feature independently and reduce the resulting splits to the optimal one.\n*   XGBoost is not the first attempt to parallelize GBM, but they used a series of tricks that made it very efficient:\n*   First, all the columns are pre-sorted while keeping a pointer to the original index of the entry. This removes the need to sort the feature at every search.\n*   They used a Compressed Column Format for a more efficient distribution of the data.\n*   They used a cache-aware prefetching algorithm to minimize the non-contiguous memory access that results from the pre-sorting step.\n*   Not directly about parallelization, but they came out with an approximated split search algorithm that speeds the tree building further.\n*   As of today, you can train XGBoost across cores on the same machine, but also on AWS YARN, Kubernetes, Spark, and GPU and you can use Dask or Ray to do it (https://lnkd.in/dtQTfu62).\n    \n*   One thing to look out for is that there is a limit to how much XGBoost can be parallelized. With too many threads, the data communication between threads becomes a bottleneck and the training speed plateaus. Here is a example explaining that effect: https://lnkd.in/d9SEcQuV. ![](assets/ml-comp/10.png)  \n    ![](/primers/ai/assets/ml-comp/7.png)  \n    <!–\n    \n    ## Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means\n    \n*   Clustering is a type of unsupervised learning method in machine learning, as it involves grouping unlabeled data based on their underlying patterns. This article will delve into two popular clustering algorithms: Latent Dirichlet Allocation (LDA) and K-Means.\n\nAs of today, you can train XGBoost across cores on the same machine, but also on AWS YARN, Kubernetes, Spark, and GPU and you can use Dask or Ray to do it (https://lnkd.in/dtQTfu62).",
    "order": 7,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 12,
    "tags": [
      "algorithmsarchitecture",
      "machine learning",
      "supervised learning",
      "unsupervised learning",
      "loss function"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 2374,
      "contentLength": 18972
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#model-ensembles",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-what-is-k-means-clustering-8",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "title": "What is K-Means Clustering?",
    "subtitle": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "contentHtml": "<ul>\n  <li>K-Means is a centroid-based clustering method, meaning it clusters the data into k different groups by trying to minimize the distance between data points in the same group, often measured by Euclidean distance. The center of each group (the centroid) is calculated as the mean of all the data points in the cluster.</li>\n  <li>Here’s how the algorithm works:</li>\n</ul>\n<ol>\n  <li>Select k initial centroids, where k is a user-defined number of clusters.</li>\n  <li>Assign each data point to the nearest centroid. These clusters will form the initial clusters.</li>\n  <li>Recalculate the centroid (mean) of each cluster.</li>\n  <li>Repeat steps 2 and 3 until the centroids don’t change significantly, or a maximum number of iterations is reached.</li>\n</ol>\n<ul>\n  <li>K-Means is a simple yet powerful algorithm, but it has its drawbacks. The algorithm’s performance can be greatly affected by the choice of initial centroids and the value of k. Additionally, it works best on datasets where clusters are spherical and roughly the same size.</li>\n</ul>",
    "contentMarkdown": "*   K-Means is a centroid-based clustering method, meaning it clusters the data into k different groups by trying to minimize the distance between data points in the same group, often measured by Euclidean distance. The center of each group (the centroid) is calculated as the mean of all the data points in the cluster.\n*   Here’s how the algorithm works:\n\n1.  Select k initial centroids, where k is a user-defined number of clusters.\n2.  Assign each data point to the nearest centroid. These clusters will form the initial clusters.\n3.  Recalculate the centroid (mean) of each cluster.\n4.  Repeat steps 2 and 3 until the centroids don’t change significantly, or a maximum number of iterations is reached.\n\n*   K-Means is a simple yet powerful algorithm, but it has its drawbacks. The algorithm’s performance can be greatly affected by the choice of initial centroids and the value of k. Additionally, it works best on datasets where clusters are spherical and roughly the same size.",
    "order": 8,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 163,
      "contentLength": 1064
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#what-is-k-means-clustering?",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-what-is-latent-dirichlet-allocation-lda-9",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "title": "What is Latent Dirichlet Allocation (LDA)?",
    "subtitle": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "contentHtml": "<ul>\n  <li>Latent Dirichlet Allocation (LDA) is a generative statistical model widely used for topic modeling in natural language processing. Rather than clustering based on distances, LDA assumes that each document in a corpus is a mixture of a certain number of topics, and each word in the document is attributable to one of the document’s topics.</li>\n  <li>In LDA:\n    <ol>\n      <li>You specify the number of topics (k) you believe exist in your corpus.</li>\n      <li>The algorithm assigns every word in every document to a temporary topic (initially, this assignment is random).</li>\n      <li>For each document, the algorithm goes through each word, and for each topic, calculates:</li>\n    </ol>\n    <ul>\n      <li>How often the topic occurs in the document, and</li>\n      <li>How often the word occurs with the topic throughout the corpus.\n        <ol>\n          <li>Based on these calculations, the algorithm reassigns the word to a new topic.</li>\n          <li>Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.</li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n  <li>LDA’s major advantage is its ability to uncover hidden thematic structure in the corpus. However, its interpretability relies heavily on the quality of the text preprocessing and the choice of the number of topics, which often requires domain knowledge.</li>\n</ul>\n<ol>\n      <li>You specify the number of topics (k) you believe exist in your corpus.</li>\n      <li>The algorithm assigns every word in every document to a temporary topic (initially, this assignment is random).</li>\n      <li>For each document, the algorithm goes through each word, and for each topic, calculates:</li>\n    </ol>\n<ul>\n      <li>How often the topic occurs in the document, and</li>\n      <li>How often the word occurs with the topic throughout the corpus.\n        <ol>\n          <li>Based on these calculations, the algorithm reassigns the word to a new topic.</li>\n          <li>Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.</li>\n        </ol>\n      </li>\n    </ul>\n<ol>\n          <li>Based on these calculations, the algorithm reassigns the word to a new topic.</li>\n          <li>Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.</li>\n        </ol>",
    "contentMarkdown": "*   Latent Dirichlet Allocation (LDA) is a generative statistical model widely used for topic modeling in natural language processing. Rather than clustering based on distances, LDA assumes that each document in a corpus is a mixture of a certain number of topics, and each word in the document is attributable to one of the document’s topics.\n*   In LDA:\n    \n    1.  You specify the number of topics (k) you believe exist in your corpus.\n    2.  The algorithm assigns every word in every document to a temporary topic (initially, this assignment is random).\n    3.  For each document, the algorithm goes through each word, and for each topic, calculates:\n    \n    *   How often the topic occurs in the document, and\n    *   How often the word occurs with the topic throughout the corpus.\n        1.  Based on these calculations, the algorithm reassigns the word to a new topic.\n        2.  Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.\n*   LDA’s major advantage is its ability to uncover hidden thematic structure in the corpus. However, its interpretability relies heavily on the quality of the text preprocessing and the choice of the number of topics, which often requires domain knowledge.\n\n1.  You specify the number of topics (k) you believe exist in your corpus.\n2.  The algorithm assigns every word in every document to a temporary topic (initially, this assignment is random).\n3.  For each document, the algorithm goes through each word, and for each topic, calculates:\n\n*   How often the topic occurs in the document, and\n*   How often the word occurs with the topic throughout the corpus.\n    1.  Based on these calculations, the algorithm reassigns the word to a new topic.\n    2.  Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.\n\n1.  Based on these calculations, the algorithm reassigns the word to a new topic.\n2.  Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.",
    "order": 9,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 372,
      "contentLength": 2612
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#what-is-latent-dirichlet-allocation-(lda)?",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-machine-learning-or-deep-learning-10",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "title": "Machine Learning or Deep Learning?",
    "subtitle": "Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means",
    "contentHtml": "<ul>\n  <li>Both K-means and LDA are traditional machine learning algorithms. They rely on explicit programming and feature engineering, whereas deep learning algorithms automatically discover the features to be used for learning through the learning process by building high-level features from data.</li>\n  <li>Nevertheless, they remain essential tools in the data scientist’s arsenal. The choice between machine learning or deep learning methods will depend on the problem at hand, the nature of the data available, and the computational resources at your disposal.</li>\n</ul>",
    "contentMarkdown": "*   Both K-means and LDA are traditional machine learning algorithms. They rely on explicit programming and feature engineering, whereas deep learning algorithms automatically discover the features to be used for learning through the learning process by building high-level features from data.\n*   Nevertheless, they remain essential tools in the data scientist’s arsenal. The choice between machine learning or deep learning methods will depend on the problem at hand, the nature of the data available, and the computational resources at your disposal.",
    "order": 10,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "deep learning",
      "machine learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 81,
      "contentLength": 578
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#machine-learning-or-deep-learning?",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-what-is-a-hidden-markov-model-11",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Hidden Markov Model (HMM) in Machine Learning",
    "title": "What is a Hidden Markov Model?",
    "subtitle": "Hidden Markov Model (HMM) in Machine Learning",
    "contentHtml": "<ul>\n  <li>A Hidden Markov Model is a statistical model where the system being modeled is assumed to be a Markov process — i.e., a random process where the future states depend only on the current state and not on the sequence of events that preceded it — with hidden states.</li>\n  <li>\n    <p>In an HMM, we deal with two types of sequences:</p>\n  </li>\n  <li>An observable sequence (also known as emission sequence)</li>\n  <li>A hidden sequence, which corresponds to the hidden states that generate the observable sequence</li>\n  <li>The term “hidden” in HMM refers to the fact that while the output states (observable sequence) are visible to an observer, the sequence of states that led to those outputs (hidden sequence) is unknown or hidden.</li>\n</ul>\n<p>In an HMM, we deal with two types of sequences:</p>",
    "contentMarkdown": "*   A Hidden Markov Model is a statistical model where the system being modeled is assumed to be a Markov process — i.e., a random process where the future states depend only on the current state and not on the sequence of events that preceded it — with hidden states.\n*   In an HMM, we deal with two types of sequences:\n    \n*   An observable sequence (also known as emission sequence)\n*   A hidden sequence, which corresponds to the hidden states that generate the observable sequence\n*   The term “hidden” in HMM refers to the fact that while the output states (observable sequence) are visible to an observer, the sequence of states that led to those outputs (hidden sequence) is unknown or hidden.\n\nIn an HMM, we deal with two types of sequences:",
    "order": 11,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 132,
      "contentLength": 813
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#what-is-a-hidden-markov-model?",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-key-components-of-an-hmm-12",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Hidden Markov Model (HMM) in Machine Learning",
    "title": "Key Components of an HMM",
    "subtitle": "Hidden Markov Model (HMM) in Machine Learning",
    "contentHtml": "<ul>\n  <li>A Hidden Markov Model is characterized by the following components:</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>States:</strong> These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.</p>\n  </li>\n  <li>\n    <p><strong>Observations:</strong> These are the output states which are directly visible.</p>\n  </li>\n  <li>\n    <p><strong>Transition Probabilities:</strong> These are the probabilities of transitioning from one hidden state to another.</p>\n  </li>\n  <li>\n    <p><strong>Emission Probabilities:</strong> These are the probabilities of an observable state being generated from a hidden state.</p>\n  </li>\n  <li>\n    <p><strong>Initial State Probabilities:</strong> These are the probabilities of starting in each hidden state.</p>\n  </li>\n</ol>\n<p><strong>States:</strong> These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.</p>\n<p><strong>Observations:</strong> These are the output states which are directly visible.</p>\n<p><strong>Transition Probabilities:</strong> These are the probabilities of transitioning from one hidden state to another.</p>\n<p><strong>Emission Probabilities:</strong> These are the probabilities of an observable state being generated from a hidden state.</p>\n<p><strong>Initial State Probabilities:</strong> These are the probabilities of starting in each hidden state.</p>",
    "contentMarkdown": "*   A Hidden Markov Model is characterized by the following components:\n\n1.  **States:** These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.\n    \n2.  **Observations:** These are the output states which are directly visible.\n    \n3.  **Transition Probabilities:** These are the probabilities of transitioning from one hidden state to another.\n    \n4.  **Emission Probabilities:** These are the probabilities of an observable state being generated from a hidden state.\n    \n5.  **Initial State Probabilities:** These are the probabilities of starting in each hidden state.\n    \n\n**States:** These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.\n\n**Observations:** These are the output states which are directly visible.\n\n**Transition Probabilities:** These are the probabilities of transitioning from one hidden state to another.\n\n**Emission Probabilities:** These are the probabilities of an observable state being generated from a hidden state.\n\n**Initial State Probabilities:** These are the probabilities of starting in each hidden state.",
    "order": 12,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 168,
      "contentLength": 1465
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#key-components-of-an-hmm",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-applications-of-hmms-in-nlp-13",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Hidden Markov Model (HMM) in Machine Learning",
    "title": "Applications of HMMs in NLP",
    "subtitle": "Hidden Markov Model (HMM) in Machine Learning",
    "contentHtml": "<ul>\n  <li>In the field of Natural Language Processing, HMMs have been extensively used, especially for tasks like Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and even in speech recognition and machine translation systems. For instance, in POS tagging, the hidden states could represent the parts of speech, while the visible states could represent the words in sentences.</li>\n</ul>",
    "contentMarkdown": "*   In the field of Natural Language Processing, HMMs have been extensively used, especially for tasks like Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and even in speech recognition and machine translation systems. For instance, in POS tagging, the hidden states could represent the parts of speech, while the visible states could represent the words in sentences.",
    "order": 13,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 57,
      "contentLength": 401
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#applications-of-hmms-in-nlp",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  },
  {
    "id": "ai-ml-comp-pros-and-cons-of-hmms-14",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "ML Algorithms Comparative Analysis",
    "articleSlug": "ml-comp",
    "chapter": "Hidden Markov Model (HMM) in Machine Learning",
    "title": "Pros and Cons of HMMs",
    "subtitle": "Hidden Markov Model (HMM) in Machine Learning",
    "contentHtml": "<ul>\n  <li>HMMs are beneficial due to their simplicity, interpretability, and their effectiveness in dealing with temporal data. However, they make some strong assumptions, like the Markov assumption and the assumption of independence among observable states given the hidden states, which may not hold true in all scenarios. Additionally, HMMs may suffer from issues with scalability and can struggle to model complex, long-distance dependencies in the data.</li>\n  <li>Despite these limitations, HMMs remain a valuable tool in the machine learning toolbox and serve as a foundation for more complex models in sequence prediction tasks. –&gt;</li>\n</ul>",
    "contentMarkdown": "*   HMMs are beneficial due to their simplicity, interpretability, and their effectiveness in dealing with temporal data. However, they make some strong assumptions, like the Markov assumption and the assumption of independence among observable states given the hidden states, which may not hold true in all scenarios. Additionally, HMMs may suffer from issues with scalability and can struggle to model complex, long-distance dependencies in the data.\n*   Despite these limitations, HMMs remain a valuable tool in the machine learning toolbox and serve as a foundation for more complex models in sequence prediction tasks. –>",
    "order": 14,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "machine learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 94,
      "contentLength": 654
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ml-comp/#pros-and-cons-of-hmms",
    "scrapedAt": "2025-12-28T11:45:45.976Z"
  }
]