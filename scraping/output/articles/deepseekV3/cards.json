[
  {
    "id": "ai-deepseekV3-attention-mechanism-multi-head-latent-attention-ml-1",
    "domain": "ai_primers",
    "category": "Models",
    "article": "DeepSeek V3",
    "articleSlug": "deepseekV3",
    "chapter": "Architecture",
    "title": "Attention Mechanism: Multi-head Latent Attention (MLA)",
    "subtitle": "Architecture",
    "contentHtml": "<p>MLA is the self-attention mechanism, primarily designed to significantly reduce the Key-Value (KV) cache size during inference via <strong>low-rank joint compression</strong>.</p>\n<ul>\n  <li><strong>KV Compression:</strong> The compressed latent vector for keys and values, $c_{KV_t} \\in \\mathbb{R}^{d_c}$, and the decoupled key, $k^R_t$, are the only vectors that need to be cached during generation. The KV compression dimension ($d_c$) is set to 512.</li>\n  <li><strong>Query Compression:</strong> Attention queries are also compressed into a latent vector $c_{Q_t} \\in \\mathbb{R}^{d’_c}$ to reduce activation memory during training. The query compression dimension ($d’_c$) is set to 1536.</li>\n  <li><strong>Dimensionality:</strong> The model uses $n_h=128$ attention heads, a dimension per head $d_h=128$, and a per-head dimension for the decoupled RoPE-carrying key/query $d^R_h=64$.</li>\n</ul>",
    "contentMarkdown": "MLA is the self-attention mechanism, primarily designed to significantly reduce the Key-Value (KV) cache size during inference via **low-rank joint compression**.\n\n*   **KV Compression:** The compressed latent vector for keys and values, $c\\_{KV\\_t} \\\\in \\\\mathbb{R}^{d\\_c}$, and the decoupled key, $k^R\\_t$, are the only vectors that need to be cached during generation. The KV compression dimension ($d\\_c$) is set to 512.\n*   **Query Compression:** Attention queries are also compressed into a latent vector $c\\_{Q\\_t} \\\\in \\\\mathbb{R}^{d’\\_c}$ to reduce activation memory during training. The query compression dimension ($d’\\_c$) is set to 1536.\n*   **Dimensionality:** The model uses $n\\_h=128$ attention heads, a dimension per head $d\\_h=128$, and a per-head dimension for the decoupled RoPE-carrying key/query $d^R\\_h=64$.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "attention",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 113,
      "contentLength": 904
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/deepseekV3/#attention-mechanism:-multi-head-latent-attention-(mla)",
    "scrapedAt": "2025-12-28T11:52:23.793Z"
  },
  {
    "id": "ai-deepseekV3-moe-configuration-and-load-balancing-2",
    "domain": "ai_primers",
    "category": "Models",
    "article": "DeepSeek V3",
    "articleSlug": "deepseekV3",
    "chapter": "Architecture",
    "title": "MoE Configuration and Load Balancing",
    "subtitle": "Architecture",
    "contentHtml": "<p>DeepSeek-V3 utilizes the DeepSeekMoE architecture.</p>\n<ul>\n  <li><strong>Expert Structure:</strong> Each MoE layer consists of <strong>1 shared expert and 256 routed experts</strong>. The intermediate hidden dimension of each expert is 2048.</li>\n  <li><strong>Activation:</strong> <strong>8 routed experts</strong> ($K_r=8$) are activated for each token.</li>\n  <li><strong>Auxiliary-Loss-Free Load Balancing:</strong> DeepSeek-V3 pioneers an <strong>auxiliary-loss-free strategy</strong> for load balancing to mitigate the performance degradation associated with conventional auxiliary losses. This is achieved by introducing a <strong>dynamic bias term ($b_i$)</strong> for each expert, which is only used for determining the top-$K_r$ routing, not for the final gating value calculation. Ablation studies show this strategy leads to <strong>greater expert specialization patterns</strong> compared to auxiliary-loss-based models. The bias update speed ($\\gamma$) was $0.001$ for most of the training.</li>\n  <li><strong>Complementary Sequence-Wise Loss:</strong> A complementary sequence-wise balance loss ($\\mathcal{L}_{\\text{Bal}}$) is retained with an <strong>extremely small weighting factor ($\\alpha = 0.0001$)</strong>, solely to prevent extreme imbalance within any single sequence.</li>\n  <li><strong>Routing Constraint:</strong> A restricted routing mechanism limits communication costs by ensuring each token is sent to at most <strong>$M=4$ nodes</strong>.</li>\n  <li><strong>No Token Dropping:</strong> Due to the effective load balancing strategy, <strong>DeepSeek-V3 does not drop any tokens</strong> during training or inference.</li>\n</ul>",
    "contentMarkdown": "DeepSeek-V3 utilizes the DeepSeekMoE architecture.\n\n*   **Expert Structure:** Each MoE layer consists of **1 shared expert and 256 routed experts**. The intermediate hidden dimension of each expert is 2048.\n*   **Activation:** **8 routed experts** ($K\\_r=8$) are activated for each token.\n*   **Auxiliary-Loss-Free Load Balancing:** DeepSeek-V3 pioneers an **auxiliary-loss-free strategy** for load balancing to mitigate the performance degradation associated with conventional auxiliary losses. This is achieved by introducing a **dynamic bias term ($b\\_i$)** for each expert, which is only used for determining the top-$K\\_r$ routing, not for the final gating value calculation. Ablation studies show this strategy leads to **greater expert specialization patterns** compared to auxiliary-loss-based models. The bias update speed ($\\\\gamma$) was $0.001$ for most of the training.\n*   **Complementary Sequence-Wise Loss:** A complementary sequence-wise balance loss ($\\\\mathcal{L}\\_{\\\\text{Bal}}$) is retained with an **extremely small weighting factor ($\\\\alpha = 0.0001$)**, solely to prevent extreme imbalance within any single sequence.\n*   **Routing Constraint:** A restricted routing mechanism limits communication costs by ensuring each token is sent to at most **$M=4$ nodes**.\n*   **No Token Dropping:** Due to the effective load balancing strategy, **DeepSeek-V3 does not drop any tokens** during training or inference.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 190,
      "contentLength": 1663
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/deepseekV3/#moe-configuration-and-load-balancing",
    "scrapedAt": "2025-12-28T11:52:23.793Z"
  },
  {
    "id": "ai-deepseekV3-training-objective-multi-token-prediction-mtp-3",
    "domain": "ai_primers",
    "category": "Models",
    "article": "DeepSeek V3",
    "articleSlug": "deepseekV3",
    "chapter": "Architecture",
    "title": "Training Objective: Multi-Token Prediction (MTP)",
    "subtitle": "Architecture",
    "contentHtml": "<p>DeepSeek-V3 employs a <strong>Multi-Token Prediction (MTP) objective</strong>, which densifies training signals and enhances model performance.</p>\n<ul>\n  <li><strong>Prediction Depth:</strong> The model predicts <strong>one additional token</strong> ($D=1$) besides the next token.</li>\n  <li><strong>Implementation:</strong> The MTP module uses shared embedding layers and output heads with the main model to maintain memory efficiency. The MTP loss weight ($\\lambda$) was set to $0.3$ for the first 10T tokens and $0.1$ for the remaining 4.8T tokens.</li>\n  <li><strong>Inference Acceleration:</strong> MTP can be repurposed for speculative decoding. The acceptance rate of the second token prediction ranges from <strong>85% to 90%</strong>, enabling a significantly improved decoding speed of <strong>1.8 times Tokens Per Second (TPS)</strong>.</li>\n</ul>",
    "contentMarkdown": "DeepSeek-V3 employs a **Multi-Token Prediction (MTP) objective**, which densifies training signals and enhances model performance.\n\n*   **Prediction Depth:** The model predicts **one additional token** ($D=1$) besides the next token.\n*   **Implementation:** The MTP module uses shared embedding layers and output heads with the main model to maintain memory efficiency. The MTP loss weight ($\\\\lambda$) was set to $0.3$ for the first 10T tokens and $0.1$ for the remaining 4.8T tokens.\n*   **Inference Acceleration:** MTP can be repurposed for speculative decoding. The acceptance rate of the second token prediction ranges from **85% to 90%**, enabling a significantly improved decoding speed of **1.8 times Tokens Per Second (TPS)**.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "embedding"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 106,
      "contentLength": 863
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/deepseekV3/#training-objective:-multi-token-prediction-(mtp)",
    "scrapedAt": "2025-12-28T11:52:23.793Z"
  },
  {
    "id": "ai-deepseekV3-fp8-mixed-precision-training-4",
    "domain": "ai_primers",
    "category": "Models",
    "article": "DeepSeek V3",
    "articleSlug": "deepseekV3",
    "chapter": "Training Infrastructure and Optimization",
    "title": "FP8 Mixed Precision Training",
    "subtitle": "Training Infrastructure and Optimization",
    "contentHtml": "<p>DeepSeek-V3 validated an <strong>FP8 mixed precision training framework</strong> for the first time on an extremely large-scale model, reducing memory usage and accelerating training.</p>\n<ul>\n  <li><strong>Data Format:</strong> The <strong>E4M3 format</strong> (4-bit exponent, 3-bit mantissa) is adopted for all FP8 tensors, prioritizing mantissa over exponent bits for higher precision.</li>\n  <li><strong>Fine-Grained Quantization:</strong> To mitigate quantization errors from outliers, a fine-grained strategy is used:\n    <ul>\n      <li><strong>Activations:</strong> Quantized on a <strong>tile-wise 1x128</strong> basis.</li>\n      <li><strong>Weights:</strong> Quantized on a <strong>block-wise 128x128</strong> basis.</li>\n    </ul>\n  </li>\n  <li><strong>Increased Accumulation Precision:</strong> To ensure numerical stability, intermediate results in FP8 GEMM operations are promoted to <strong>FP32 registers on CUDA Cores</strong> at specific intervals ($N_C=128$ elements) for high-precision accumulation. This addresses the limitation of standard Tensor Core accumulation precision.</li>\n  <li><strong>Stability:</strong> The relative loss error of the FP8-trained model remained consistently <strong>below 0.25%</strong> compared to the BF16 baseline.</li>\n</ul>\n<ul>\n      <li><strong>Activations:</strong> Quantized on a <strong>tile-wise 1x128</strong> basis.</li>\n      <li><strong>Weights:</strong> Quantized on a <strong>block-wise 128x128</strong> basis.</li>\n    </ul>",
    "contentMarkdown": "DeepSeek-V3 validated an **FP8 mixed precision training framework** for the first time on an extremely large-scale model, reducing memory usage and accelerating training.\n\n*   **Data Format:** The **E4M3 format** (4-bit exponent, 3-bit mantissa) is adopted for all FP8 tensors, prioritizing mantissa over exponent bits for higher precision.\n*   **Fine-Grained Quantization:** To mitigate quantization errors from outliers, a fine-grained strategy is used:\n    *   **Activations:** Quantized on a **tile-wise 1x128** basis.\n    *   **Weights:** Quantized on a **block-wise 128x128** basis.\n*   **Increased Accumulation Precision:** To ensure numerical stability, intermediate results in FP8 GEMM operations are promoted to **FP32 registers on CUDA Cores** at specific intervals ($N\\_C=128$ elements) for high-precision accumulation. This addresses the limitation of standard Tensor Core accumulation precision.\n*   **Stability:** The relative loss error of the FP8-trained model remained consistently **below 0.25%** compared to the BF16 baseline.\n\n*   **Activations:** Quantized on a **tile-wise 1x128** basis.\n*   **Weights:** Quantized on a **block-wise 128x128** basis.",
    "order": 4,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 152,
      "contentLength": 1496
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/deepseekV3/#fp8-mixed-precision-training",
    "scrapedAt": "2025-12-28T11:52:23.793Z"
  },
  {
    "id": "ai-deepseekV3-dualpipe-parallelism-5",
    "domain": "ai_primers",
    "category": "Models",
    "article": "DeepSeek V3",
    "articleSlug": "deepseekV3",
    "chapter": "Training Infrastructure and Optimization",
    "title": "DualPipe Parallelism",
    "subtitle": "Training Infrastructure and Optimization",
    "contentHtml": "<p>The custom <strong>DualPipe</strong> algorithm was designed for efficient pipeline parallelism, reducing pipeline bubbles compared to existing methods (Table 2 in source).</p>\n<ul>\n  <li><strong>Computation-Communication Overlap:</strong> DualPipe overlaps the computation and communication within forward and backward chunks, ensuring that both <strong>all-to-all and PP communication can be fully hidden</strong> during execution. This co-design results in a <strong>near-zero all-to-all communication overhead</strong>.</li>\n  <li><strong>Communication Kernels:</strong> Customized cross-node all-to-all communication kernels were developed to utilize InfiniBand (IB) and NVLink bandwidths efficiently. Only <strong>20 SMs</strong> are needed to fully utilize the bandwidths for communication.</li>\n</ul>",
    "contentMarkdown": "The custom **DualPipe** algorithm was designed for efficient pipeline parallelism, reducing pipeline bubbles compared to existing methods (Table 2 in source).\n\n*   **Computation-Communication Overlap:** DualPipe overlaps the computation and communication within forward and backward chunks, ensuring that both **all-to-all and PP communication can be fully hidden** during execution. This co-design results in a **near-zero all-to-all communication overhead**.\n*   **Communication Kernels:** Customized cross-node all-to-all communication kernels were developed to utilize InfiniBand (IB) and NVLink bandwidths efficiently. Only **20 SMs** are needed to fully utilize the bandwidths for communication.",
    "order": 5,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 87,
      "contentLength": 810
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/deepseekV3/#dualpipe-parallelism",
    "scrapedAt": "2025-12-28T11:52:23.793Z"
  }
]