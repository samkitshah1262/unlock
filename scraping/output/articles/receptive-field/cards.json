[
  {
    "id": "ai-receptive-field-add-more-convolutional-layers-1",
    "domain": "ai_primers",
    "category": "Vision",
    "article": "Receptive Field",
    "articleSlug": "receptive-field",
    "chapter": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "title": "Add More Convolutional Layers",
    "subtitle": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "contentHtml": "<ul>\n  <li>Per <a href=\"https://arxiv.org/abs/1701.04128\">“Understanding the effective receptive field in deep convolutional neural networks”</a> by Luo et al. (2016), adding more convolutional layers increases the receptive field size linearly, as each extra layer increases the <strong>receptive field size by the kernel size</strong>. Moreover, it is experimentally validated that as the theoretical receptive field is increasing but the effective (experimental) receptive field (ERF) is reducing. The following figure (taken from <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>) shows the impact of increasing the number of layers decreases the ERF ration:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/receptive-field/0e.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Per [“Understanding the effective receptive field in deep convolutional neural networks”](https://arxiv.org/abs/1701.04128) by Luo et al. (2016), adding more convolutional layers increases the receptive field size linearly, as each extra layer increases the **receptive field size by the kernel size**. Moreover, it is experimentally validated that as the theoretical receptive field is increasing but the effective (experimental) receptive field (ERF) is reducing. The following figure (taken from [Luo et al. (2016)](https://arxiv.org/abs/1701.04128)) shows the impact of increasing the number of layers decreases the ERF ration:\n\n![](/primers/ai/assets/receptive-field/0e.png)",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "vision",
      "neural network",
      "convolution"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 85,
      "contentLength": 756
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/receptive-field/#add-more-convolutional-layers",
    "scrapedAt": "2025-12-28T11:52:39.828Z"
  },
  {
    "id": "ai-receptive-field-sub-sampling-and-dilated-convolutions-2",
    "domain": "ai_primers",
    "category": "Vision",
    "article": "Receptive Field",
    "articleSlug": "receptive-field",
    "chapter": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "title": "Sub-sampling and Dilated Convolutions",
    "subtitle": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "contentHtml": "<ul>\n  <li>\n    <p>Sub-sampling techniques like pooling on the other hand, increases the receptive field size multiplicatively. Modern architectures like ResNet combine adding more convolutional layers and sub-sampling. On the other hand, sequentially placed dilated convolutions, increase the RF exponentially.</p>\n  </li>\n  <li>\n    <p>But first, let’s revisit the idea of dilated convolutions.</p>\n  </li>\n  <li>\n    <p>In essence, dilated convolutions introduce another parameter, denoted as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">r</script>, called the dilation rate. Dilations introduce “holes” in a convolutional kernel. The “holes” basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-143\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">r</script> introduces a kind of striding of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-146\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">r</script>.</p>\n  </li>\n  <li>\n    <p>The equation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.1em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">(1)</script> in the <a href=\"#closed-form-calculations-of-the-receptive-field-for-single-path-networks\">above</a> section can be reused by simply replacing the kernel size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">k</script> for all layers using dilations:</p>\n  </li>\n</ul>\n<p>Sub-sampling techniques like pooling on the other hand, increases the receptive field size multiplicatively. Modern architectures like ResNet combine adding more convolutional layers and sub-sampling. On the other hand, sequentially placed dilated convolutions, increase the RF exponentially.</p>\n<p>But first, let’s revisit the idea of dilated convolutions.</p>\n<p>In essence, dilated convolutions introduce another parameter, denoted as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">r</script>, called the dilation rate. Dilations introduce “holes” in a convolutional kernel. The “holes” basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-143\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">r</script> introduces a kind of striding of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-146\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">r</script>.</p>\n<p>The equation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.1em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">(1)</script> in the <a href=\"#closed-form-calculations-of-the-receptive-field-for-single-path-networks\">above</a> section can be reused by simply replacing the kernel size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">k</script> for all layers using dilations:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>k</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>k</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>1</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 8.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1006.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"msup\" id=\"MathJax-Span-159\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mo\" id=\"MathJax-Span-161\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mi>r</mi><mo stretchy=\"false\">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>+</mo><mn>1</mn></math></span></span></div>\n<ul>\n  <li>All the above can be illustrated in the following animation, from <a href=\"https://github.com/vdumoulin/conv_arithmetic\">“A guide to convolutional arithmetic”</a> by Dumoulin (2016).</li>\n</ul>\n<p><img src=\"/primers/ai/assets/receptive-field/1.gif\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>Now, let’s briefly inspect how dilated convolutions can influence the receptive field.</p>\n  </li>\n  <li>\n    <p>Let’s dig into three sequential conv. Layers (denoted by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-171\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mi\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">a, b, c</script>) that are illustrated in the image with normal convolution, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-178\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-179\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">r = 2</script> dilation factor, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">r = 4</script> dilation factor. We will intuitively understand why dilation supports an exponential expansion of the receptive field without loss of resolution (i.e., pooling) or coverage. Refer to the following image from <a href=\"https://arxiv.org/abs/1511.07122\">“Multi-scale context aggregation by dilated convolutions”</a> by Yu and Koltun (2015):</p>\n  </li>\n</ul>\n<p>Now, let’s briefly inspect how dilated convolutions can influence the receptive field.</p>\n<p>Let’s dig into three sequential conv. Layers (denoted by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-171\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mi\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">a, b, c</script>) that are illustrated in the image with normal convolution, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-178\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-179\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">r = 2</script> dilation factor, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 2.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">r = 4</script> dilation factor. We will intuitively understand why dilation supports an exponential expansion of the receptive field without loss of resolution (i.e., pooling) or coverage. Refer to the following image from <a href=\"https://arxiv.org/abs/1511.07122\">“Multi-scale context aggregation by dilated convolutions”</a> by Yu and Koltun (2015):</p>\n<p><img src=\"/primers/ai/assets/receptive-field/2.png\" alt=\"\"></p>\n<h4 id=\"analysis\">Analysis</h4>\n<ul>\n  <li>In (a), we have a normal <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-188\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mn\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3</mn><mo>×</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">3 \\times 3</script> convolution with receptive field <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-193\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mn\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3</mn><mo>×</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">3 \\times 3</script>.</li>\n  <li>In (b), we have a 2-dilated <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-198\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-199\"><span class=\"mn\" id=\"MathJax-Span-200\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-202\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3</mn><mo>×</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">3 \\times 3</script> convolution that is applied in the output of layer (a) which is a normal convolution. As a result, each element in the 2 coupled layers now has a receptive field of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>7</mn><mo>&amp;#x00D7;</mo><mn>7</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-203\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-204\"><span class=\"mn\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Regular;\">7</span><span class=\"mo\" id=\"MathJax-Span-206\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">7</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>7</mn><mo>×</mo><mn>7</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">7 \\times 7</script>. If we studied 2-dilated conv alone the receptive field would be simply <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mo>&amp;#x00D7;</mo><mn>5</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-208\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">5</span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">5</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>5</mn><mo>×</mo><mn>5</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">5 \\times 5</script> with the same number of parameters.</li>\n  <li>\n    <p>In (c), by applying a 4-dilated convolution, each element in the third sequential conv layer now has a receptive field of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>15</mn><mo>&amp;#x00D7;</mo><mn>15</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-213\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.02em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"mn\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Regular;\">15</span><span class=\"mo\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">15</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>15</mn><mo>×</mo><mn>15</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">15 \\times 15</script>. As a result, the receptive field grows exponentially while the number of parameters grows linearly per <a href=\"https://arxiv.org/abs/1511.07122\">Yu and Koltun (2015)</a>.</p>\n  </li>\n  <li>In other words, a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-218\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-219\"><span class=\"mn\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3</mn><mo>×</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">3 \\times 3</script> kernel with a dilation rate of 2 will have the same receptive field as a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mo>&amp;#x00D7;</mo><mn>5</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-223\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-224\"><span class=\"mn\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Regular;\">5</span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">5</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>5</mn><mo>×</mo><mn>5</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">5 \\times 5</script> kernel, while only using 9 parameters. Similarly, a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-228\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-229\"><span class=\"mn\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular;\">3</span><span class=\"mo\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3</mn><mo>×</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">3 \\times 3</script> kernel with a dilation rate of 4 will have the same receptive field as a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9</mn><mo>&amp;#x00D7;</mo><mn>9</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-233\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mn\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">9</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">9</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>9</mn><mo>×</mo><mn>9</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">9 \\times 9</script> kernel without dilation. Mathematically:</li>\n</ul>\n<p>In (c), by applying a 4-dilated convolution, each element in the third sequential conv layer now has a receptive field of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>15</mn><mo>&amp;#x00D7;</mo><mn>15</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-213\" style=\"width: 3.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.02em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"mn\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Regular;\">15</span><span class=\"mo\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">15</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>15</mn><mo>×</mo><mn>15</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">15 \\times 15</script>. As a result, the receptive field grows exponentially while the number of parameters grows linearly per <a href=\"https://arxiv.org/abs/1511.07122\">Yu and Koltun (2015)</a>.</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>k</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>1</mn><mo>=</mo><msub><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-238\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.66em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"mi\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-248\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-249\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-251\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-255\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>r</mi><mo stretchy=\"false\">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>+</mo><mn>1</mn><mo>=</mo><msub><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi></mrow></msub></math></span></span></div>\n<blockquote>\n  <p>Insight: In deep architectures, we often introduce dilated convolutions in the last convolutional layers.</p>\n</blockquote>\n<p>Insight: In deep architectures, we often introduce dilated convolutions in the last convolutional layers.</p>\n<ul>\n  <li>Below you can observe the resulting ERF (effective receptive field) when introducing pooling and dilation in an experimental study performed by <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>. The receptive field is larger in both cases than the vanilla conv, but is largest with pooling. We’ll see more about the effective receptive field in a <a href=\"#understanding-the-effective-receptive-field\">later</a> section. The following figure (taken from <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>) shows a visualization of the effective receptive field (ERF) by introducing pooling strategies and dilation:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/receptive-field/3.png\" alt=\"\"></p>\n<blockquote>\n  <p>Insight: Based on <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>, pooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.</p>\n</blockquote>\n<p>Insight: Based on <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>, pooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.</p>",
    "contentMarkdown": "*   Sub-sampling techniques like pooling on the other hand, increases the receptive field size multiplicatively. Modern architectures like ResNet combine adding more convolutional layers and sub-sampling. On the other hand, sequentially placed dilated convolutions, increase the RF exponentially.\n    \n*   But first, let’s revisit the idea of dilated convolutions.\n    \n*   In essence, dilated convolutions introduce another parameter, denoted as rrr, called the dilation rate. Dilations introduce “holes” in a convolutional kernel. The “holes” basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of rrr introduces a kind of striding of rrr.\n    \n*   The equation (1)(1)(1) in the [above](#closed-form-calculations-of-the-receptive-field-for-single-path-networks) section can be reused by simply replacing the kernel size kkk for all layers using dilations:\n    \n\nSub-sampling techniques like pooling on the other hand, increases the receptive field size multiplicatively. Modern architectures like ResNet combine adding more convolutional layers and sub-sampling. On the other hand, sequentially placed dilated convolutions, increase the RF exponentially.\n\nBut first, let’s revisit the idea of dilated convolutions.\n\nIn essence, dilated convolutions introduce another parameter, denoted as rrr, called the dilation rate. Dilations introduce “holes” in a convolutional kernel. The “holes” basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of rrr introduces a kind of striding of rrr.\n\nThe equation (1)(1)(1) in the [above](#closed-form-calculations-of-the-receptive-field-for-single-path-networks) section can be reused by simply replacing the kernel size kkk for all layers using dilations:\n\nk′\\=r(k−1)+1k′\\=r(k−1)+1\n\n*   All the above can be illustrated in the following animation, from [“A guide to convolutional arithmetic”](https://github.com/vdumoulin/conv_arithmetic) by Dumoulin (2016).\n\n![](/primers/ai/assets/receptive-field/1.gif)\n\n*   Now, let’s briefly inspect how dilated convolutions can influence the receptive field.\n    \n*   Let’s dig into three sequential conv. Layers (denoted by a,b,ca,b,ca, b, c) that are illustrated in the image with normal convolution, r\\=2r\\=2r = 2 dilation factor, and r\\=4r\\=4r = 4 dilation factor. We will intuitively understand why dilation supports an exponential expansion of the receptive field without loss of resolution (i.e., pooling) or coverage. Refer to the following image from [“Multi-scale context aggregation by dilated convolutions”](https://arxiv.org/abs/1511.07122) by Yu and Koltun (2015):\n    \n\nNow, let’s briefly inspect how dilated convolutions can influence the receptive field.\n\nLet’s dig into three sequential conv. Layers (denoted by a,b,ca,b,ca, b, c) that are illustrated in the image with normal convolution, r\\=2r\\=2r = 2 dilation factor, and r\\=4r\\=4r = 4 dilation factor. We will intuitively understand why dilation supports an exponential expansion of the receptive field without loss of resolution (i.e., pooling) or coverage. Refer to the following image from [“Multi-scale context aggregation by dilated convolutions”](https://arxiv.org/abs/1511.07122) by Yu and Koltun (2015):\n\n![](/primers/ai/assets/receptive-field/2.png)\n\n#### Analysis\n\n*   In (a), we have a normal 3×33×33 \\\\times 3 convolution with receptive field 3×33×33 \\\\times 3.\n*   In (b), we have a 2-dilated 3×33×33 \\\\times 3 convolution that is applied in the output of layer (a) which is a normal convolution. As a result, each element in the 2 coupled layers now has a receptive field of 7×77×77 \\\\times 7. If we studied 2-dilated conv alone the receptive field would be simply 5×55×55 \\\\times 5 with the same number of parameters.\n*   In (c), by applying a 4-dilated convolution, each element in the third sequential conv layer now has a receptive field of 15×1515×1515 \\\\times 15. As a result, the receptive field grows exponentially while the number of parameters grows linearly per [Yu and Koltun (2015)](https://arxiv.org/abs/1511.07122).\n    \n*   In other words, a 3×33×33 \\\\times 3 kernel with a dilation rate of 2 will have the same receptive field as a 5×55×55 \\\\times 5 kernel, while only using 9 parameters. Similarly, a 3×33×33 \\\\times 3 kernel with a dilation rate of 4 will have the same receptive field as a 9×99×99 \\\\times 9 kernel without dilation. Mathematically:\n\nIn (c), by applying a 4-dilated convolution, each element in the third sequential conv layer now has a receptive field of 15×1515×1515 \\\\times 15. As a result, the receptive field grows exponentially while the number of parameters grows linearly per [Yu and Koltun (2015)](https://arxiv.org/abs/1511.07122).\n\nr(k−1)+1\\=kprevr(k−1)+1\\=kprev\n\n> Insight: In deep architectures, we often introduce dilated convolutions in the last convolutional layers.\n\nInsight: In deep architectures, we often introduce dilated convolutions in the last convolutional layers.\n\n*   Below you can observe the resulting ERF (effective receptive field) when introducing pooling and dilation in an experimental study performed by [Luo et al. (2016)](https://arxiv.org/abs/1701.04128). The receptive field is larger in both cases than the vanilla conv, but is largest with pooling. We’ll see more about the effective receptive field in a [later](#understanding-the-effective-receptive-field) section. The following figure (taken from [Luo et al. (2016)](https://arxiv.org/abs/1701.04128)) shows a visualization of the effective receptive field (ERF) by introducing pooling strategies and dilation:\n\n![](/primers/ai/assets/receptive-field/3.png)\n\n> Insight: Based on [Luo et al. (2016)](https://arxiv.org/abs/1701.04128), pooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.\n\nInsight: Based on [Luo et al. (2016)](https://arxiv.org/abs/1701.04128), pooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 5,
    "tags": [
      "vision",
      "convolution"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 862,
      "contentLength": 52454
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/receptive-field/#sub-sampling-and-dilated-convolutions",
    "scrapedAt": "2025-12-28T11:52:39.828Z"
  },
  {
    "id": "ai-receptive-field-depth-wise-convolutions-3",
    "domain": "ai_primers",
    "category": "Vision",
    "article": "Receptive Field",
    "articleSlug": "receptive-field",
    "chapter": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "title": "Depth-wise Convolutions",
    "subtitle": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "contentHtml": "<ul>\n  <li>Finally as described in <a href=\"https://distill.pub/2019/computing-receptive-fields\">“Computing receptive fields of convolutional neural networks”</a> by Araujo et al. (2019), with depth-wise convolutions the receptive field is increased with a small compute footprint, so it is considered a compact way to increase the receptive field with fewer parameters. Depthwise convolution is the channel-wise spatial convolution. However, note that depth-wise convolutions do not directly increase the receptive field. But since we use fewer parameters with more compact computations, we can add more layers. Thus, with roughly the same number of parameters, we can get a bigger receptive field. <a href=\"https://arxiv.org/abs/1704.04861\">MobileNet</a> by Howard et al. (2017) achieves high recognition performance based on this idea.</li>\n</ul>",
    "contentMarkdown": "*   Finally as described in [“Computing receptive fields of convolutional neural networks”](https://distill.pub/2019/computing-receptive-fields) by Araujo et al. (2019), with depth-wise convolutions the receptive field is increased with a small compute footprint, so it is considered a compact way to increase the receptive field with fewer parameters. Depthwise convolution is the channel-wise spatial convolution. However, note that depth-wise convolutions do not directly increase the receptive field. But since we use fewer parameters with more compact computations, we can add more layers. Thus, with roughly the same number of parameters, we can get a bigger receptive field. [MobileNet](https://arxiv.org/abs/1704.04861) by Howard et al. (2017) achieves high recognition performance based on this idea.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "vision",
      "neural network",
      "convolution"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 108,
      "contentLength": 849
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/receptive-field/#depth-wise-convolutions",
    "scrapedAt": "2025-12-28T11:52:39.829Z"
  },
  {
    "id": "ai-receptive-field-skip-connections-and-receptive-field-4",
    "domain": "ai_primers",
    "category": "Vision",
    "article": "Receptive Field",
    "articleSlug": "receptive-field",
    "chapter": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "title": "Skip-connections and Receptive Field",
    "subtitle": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "contentHtml": "<ul>\n  <li>\n    <p>If you want to revisit the intuition behind skip connections, check out our article on <a href=\"../skip-connections\">Skip Connections</a>.</p>\n  </li>\n  <li>\n    <p>In a model without any skip-connections, the receptive field is considered fixed. However, when introducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">n</script> skip-residual blocks, the networks utilize <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-260\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"msubsup\" id=\"MathJax-Span-262\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">2^n</script> different paths and therefore <strong>features can be learned with a large range of different receptive fields</strong> by <a href=\"https://arxiv.org/abs/1707.01992\">Li et al. (2017)</a>. For example, the <a href=\"https://arxiv.org/abs/1707.01992\">HighResNet architecture</a> has a maximum receptive field of 87 pixels, coming from 29 unique paths. In the following figure, we can observe the distribution of the receptive field of these paths in the architecture. The receptive field, in this case, ranges from 3 to 87, following a <a href=\"https://mathworld.wolfram.com/BinomialDistribution.html\">binomial distribution</a>. The histogram of receptive field distribution from <a href=\"https://arxiv.org/abs/1707.01992\">“On the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a pretext task”</a> by Li et al. (2017) is as follows:</p>\n  </li>\n</ul>\n<p>If you want to revisit the intuition behind skip connections, check out our article on <a href=\"../skip-connections\">Skip Connections</a>.</p>\n<p>In a model without any skip-connections, the receptive field is considered fixed. However, when introducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">n</script> skip-residual blocks, the networks utilize <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-260\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"msubsup\" id=\"MathJax-Span-262\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">2^n</script> different paths and therefore <strong>features can be learned with a large range of different receptive fields</strong> by <a href=\"https://arxiv.org/abs/1707.01992\">Li et al. (2017)</a>. For example, the <a href=\"https://arxiv.org/abs/1707.01992\">HighResNet architecture</a> has a maximum receptive field of 87 pixels, coming from 29 unique paths. In the following figure, we can observe the distribution of the receptive field of these paths in the architecture. The receptive field, in this case, ranges from 3 to 87, following a <a href=\"https://mathworld.wolfram.com/BinomialDistribution.html\">binomial distribution</a>. The histogram of receptive field distribution from <a href=\"https://arxiv.org/abs/1707.01992\">“On the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a pretext task”</a> by Li et al. (2017) is as follows:</p>\n<p><img src=\"/primers/ai/assets/receptive-field/4.png\" alt=\"\"></p>\n<blockquote>\n  <p>Insight: Skip-connections may provide more paths, however, based on <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>, they tend to make the effective receptive field smaller.</p>\n</blockquote>\n<p>Insight: Skip-connections may provide more paths, however, based on <a href=\"https://arxiv.org/abs/1701.04128\">Luo et al. (2016)</a>, they tend to make the effective receptive field smaller.</p>",
    "contentMarkdown": "*   If you want to revisit the intuition behind skip connections, check out our article on [Skip Connections](../skip-connections).\n    \n*   In a model without any skip-connections, the receptive field is considered fixed. However, when introducing nnn skip-residual blocks, the networks utilize 2n2n2^n different paths and therefore **features can be learned with a large range of different receptive fields** by [Li et al. (2017)](https://arxiv.org/abs/1707.01992). For example, the [HighResNet architecture](https://arxiv.org/abs/1707.01992) has a maximum receptive field of 87 pixels, coming from 29 unique paths. In the following figure, we can observe the distribution of the receptive field of these paths in the architecture. The receptive field, in this case, ranges from 3 to 87, following a [binomial distribution](https://mathworld.wolfram.com/BinomialDistribution.html). The histogram of receptive field distribution from [“On the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a pretext task”](https://arxiv.org/abs/1707.01992) by Li et al. (2017) is as follows:\n    \n\nIf you want to revisit the intuition behind skip connections, check out our article on [Skip Connections](../skip-connections).\n\nIn a model without any skip-connections, the receptive field is considered fixed. However, when introducing nnn skip-residual blocks, the networks utilize 2n2n2^n different paths and therefore **features can be learned with a large range of different receptive fields** by [Li et al. (2017)](https://arxiv.org/abs/1707.01992). For example, the [HighResNet architecture](https://arxiv.org/abs/1707.01992) has a maximum receptive field of 87 pixels, coming from 29 unique paths. In the following figure, we can observe the distribution of the receptive field of these paths in the architecture. The receptive field, in this case, ranges from 3 to 87, following a [binomial distribution](https://mathworld.wolfram.com/BinomialDistribution.html). The histogram of receptive field distribution from [“On the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a pretext task”](https://arxiv.org/abs/1707.01992) by Li et al. (2017) is as follows:\n\n![](/primers/ai/assets/receptive-field/4.png)\n\n> Insight: Skip-connections may provide more paths, however, based on [Luo et al. (2016)](https://arxiv.org/abs/1701.04128), they tend to make the effective receptive field smaller.\n\nInsight: Skip-connections may provide more paths, however, based on [Luo et al. (2016)](https://arxiv.org/abs/1701.04128), they tend to make the effective receptive field smaller.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "vision",
      "convolution"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 334,
      "contentLength": 8894
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/receptive-field/#skip-connections-and-receptive-field",
    "scrapedAt": "2025-12-28T11:52:39.829Z"
  },
  {
    "id": "ai-receptive-field-receptive-field-and-transposed-convolutions-upsamp-5",
    "domain": "ai_primers",
    "category": "Vision",
    "article": "Receptive Field",
    "articleSlug": "receptive-field",
    "chapter": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "title": "Receptive Field and Transposed Convolutions, Upsampling, Separable Convolutions, and Batch Normalization",
    "subtitle": "How Can We Increase the Receptive Field in a Convolutional Network?",
    "contentHtml": "<h4 id=\"upsampling\">Upsampling</h4>\n<ul>\n  <li>Upsampling is also a local operation. For the purposes of RF computation, we can consider the kernel size to be equal to the number of input features involved in the computation of an output feature. Since we usually double the spatial dimension, as shown in the figure below, the kernel is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi><mo>=</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi><mo>=</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">k=1</script>.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/receptive-field/5.png\" alt=\"\"></p>\n<h4 id=\"separable-convolutions\">Separable Convolutions</h4>\n<ul>\n  <li>In short, the RF properties of the separable convolution are identical to its corresponding equivalent non-separable convolution. So, practically nothing changes in terms of the receptive field.</li>\n</ul>\n<h4 id=\"batch-normalization\">Batch Normalization</h4>\n<ul>\n  <li>During training, batch normalization parameters are computed based on all the channel elements of the feature map. Thus, one can state that its receptive field is the whole input image.</li>\n</ul>",
    "contentMarkdown": "#### Upsampling\n\n*   Upsampling is also a local operation. For the purposes of RF computation, we can consider the kernel size to be equal to the number of input features involved in the computation of an output feature. Since we usually double the spatial dimension, as shown in the figure below, the kernel is k\\=1k\\=1k=1.\n\n![](/primers/ai/assets/receptive-field/5.png)\n\n#### Separable Convolutions\n\n*   In short, the RF properties of the separable convolution are identical to its corresponding equivalent non-separable convolution. So, practically nothing changes in terms of the receptive field.\n\n#### Batch Normalization\n\n*   During training, batch normalization parameters are computed based on all the channel elements of the feature map. Thus, one can state that its receptive field is the whole input image.",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "vision",
      "convolution",
      "batch normalization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 121,
      "contentLength": 2491
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/receptive-field/#receptive-field-and-transposed-convolutions,-upsampling,-separable-convolutions,-and-batch-normalization",
    "scrapedAt": "2025-12-28T11:52:39.829Z"
  }
]