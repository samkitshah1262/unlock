<!DOCTYPE html><html lang="en"><head><style type="text/css" id="nanobarcss">.nanobar{width:100%;height:4px;z-index:9999;top:0}.bar{width:0;height:100%;transition:height .3s;background:#000}</style><style>#back-to-top{background:#fff;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#333;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:24px;margin:16px auto 0;width:24px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Primers • OpenAI o1</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/primers/ai/o1/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon">

  <!-- Google ads -->
  <script src="https://pagead2.googlesyndication.com/pagead/managed/js/adsense/m202512100101/show_ads_impl.js"></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213" crossorigin="anonymous" data-checked-head="true"></script>
<meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9nrunKdU5m96PSN1XsSGr3qOP0lvPFUB2AiAylCDlN5DTl17uDFkpQuHj1AFtgWLxpLaiBZuhrtb2WOu7ofHwEAAACKeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A93bovR+QVXNx2/38qDbmeYYf1wdte9EO37K9eMq3r+541qo0byhYU899BhPB7Cv9QqD7wIbR1B6OAc9kEfYCA4AAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A1S5fojrAunSDrFbD8OfGmFHdRFZymSM/1ss3G+NEttCLfHkXvlcF6LGLH8Mo5PakLO1sCASXU1/gQf6XGuTBgwAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQUlQcm9tcHRBUElNdWx0aW1vZGFsSW5wdXQiLCJleHBpcnkiOjE3NzQzMTA0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script async="" src="https://fundingchoicesmessages.google.com/i/ca-pub-5905744527956213?href=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fo1&amp;ers=2"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxVGd0H9qAFm7iTvPSSIZeXkYUjGqFYGGOoH7ITYwo9W0TuEuckmAapqynUwQHYrNq8uoRv0bsbvD6BrVF9hD820UCDeaqp2eLBSvtiHZFe_kyyQ_x0Dkt1bMA4s8d4jGcCisJzFVQ==?fccs=W1siQUtzUm9sLTFpRzFmRzNTMTJVWV8wOVJuRDh4YkhFekpUU3U1WTFOV2tLQmh1OFhMRkJwZnEzOEF1TXFxN2ZjM2d1b0dwYmxCaW0yNTBXbnlIM3ZhQmpLNXVveXV2THJZSEEybm1RbXZwQjFGb1M3dnhYQjcyNkpGM3gxTndLOUprV3lSd2pzeHAxWEo2bE9rM2E5M0tmaDdNSzhnTW84ZjdRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI3MDMsODYxMDAwMDAwXSxudWxsLG51bGwsbnVsbCxbbnVsbCxbN11dLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9vMS8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxXHcNajMTuqg7mzpw8zAbsUoXx462cOGBI4xg2JKUGWF80OV2GOkb4p8Tk_WTDdo8snBHSCjMGKSQjyI81EQWpd9Kfg_49-p3YM-XJmG2KanANhVjj5Ob9DNuhuSk8DlrH5yWrkuA==?fccs=W1siQUtzUm9sLTFpRzFmRzNTMTJVWV8wOVJuRDh4YkhFekpUU3U1WTFOV2tLQmh1OFhMRkJwZnEzOEF1TXFxN2ZjM2d1b0dwYmxCaW0yNTBXbnlIM3ZhQmpLNXVveXV2THJZSEEybm1RbXZwQjFGb1M3dnhYQjcyNkpGM3gxTndLOUprV3lSd2pzeHAxWEo2bE9rM2E5M0tmaDdNSzhnTW84ZjdRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI3MDQsNjYwMDAwMDBdLG51bGwsbnVsbCxudWxsLFtudWxsLFs3LDldLG51bGwsMixudWxsLCJlbiJdLCJodHRwczovL2FtYW4uYWkvcHJpbWVycy9haS9vMS8iLG51bGwsW1s4LCJzQ2hOSDVPc2FrMCJdLFs5LCJlbi1VUyJdLFsxOSwiMiJdLFsxNywiWzBdIl0sWzI0LCIiXSxbMjksImZhbHNlIl1dXQ"></script><script async="" src="https://fundingchoicesmessages.google.com/f/AGSKWxW8-VuYLtC62wxBWrVySb4K-FYvUhlxambxB4DldTzx8Nn4FxDEoAqBG7G_UXTJW_ufurk45tZbv_D462ocVRSteXmueaw3vEK7MKwgYQ3dJPp3WPkovifOC3E5FNRx4P4ZfzZpuQ==?fccs=W1siQUtzUm9sLTFpRzFmRzNTMTJVWV8wOVJuRDh4YkhFekpUU3U1WTFOV2tLQmh1OFhMRkJwZnEzOEF1TXFxN2ZjM2d1b0dwYmxCaW0yNTBXbnlIM3ZhQmpLNXVveXV2THJZSEEybm1RbXZwQjFGb1M3dnhYQjcyNkpGM3gxTndLOUprV3lSd2pzeHAxWEo2bE9rM2E5M0tmaDdNSzhnTW84ZjdRPT0iXSxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsWzE3NjY5MjI3MDUsNjAwMDAwMF0sbnVsbCxudWxsLG51bGwsW251bGwsWzcsOSw2XSxudWxsLDIsbnVsbCwiZW4iLG51bGwsbnVsbCxudWxsLG51bGwsbnVsbCwxXSwiaHR0cHM6Ly9hbWFuLmFpL3ByaW1lcnMvYWkvbzEvIixudWxsLFtbOCwic0NoTkg1T3NhazAiXSxbOSwiZW4tVVMiXSxbMTksIjIiXSxbMTcsIlswXSJdLFsyNCwiIl0sWzI5LCJmYWxzZSJdXV0"></script></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script><div id="back-to-top" class="hidden">Back to Top</div>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • OpenAI O1</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a>    <ul>
      <li><a href="#key-ideas" id="markdown-toc-key-ideas">Key Ideas</a></li>
    </ul>
  </li>
  <li><a href="#test-time-compute-shifting-focus-to-inference-scaling" id="markdown-toc-test-time-compute-shifting-focus-to-inference-scaling">Test-time Compute: Shifting Focus to Inference Scaling</a>    <ul>
      <li><a href="#inference-scaling-a-new-paradigm" id="markdown-toc-inference-scaling-a-new-paradigm">Inference Scaling: a New Paradigm</a></li>
      <li><a href="#the-role-of-reinforcement-learning-in-o1" id="markdown-toc-the-role-of-reinforcement-learning-in-o1">The Role of Reinforcement Learning in O1</a></li>
    </ul>
  </li>
  <li><a href="#test-results" id="markdown-toc-test-results">Test Results</a>    <ul>
      <li><a href="#advanced-math-competitions" id="markdown-toc-advanced-math-competitions">Advanced Math Competitions</a></li>
      <li><a href="#coding-competitions" id="markdown-toc-coding-competitions">Coding Competitions</a></li>
      <li><a href="#stem-expertise" id="markdown-toc-stem-expertise">STEM Expertise</a></li>
    </ul>
  </li>
  <li><a href="#limitations" id="markdown-toc-limitations">Limitations</a></li>
  <li><a href="#use-cases-of-openai-o1" id="markdown-toc-use-cases-of-openai-o1">Use-Cases of OpenAI O1</a></li>
  <li><a href="#pricing-and-accessibility" id="markdown-toc-pricing-and-accessibility">Pricing and Accessibility</a></li>
  <li><a href="#safety-and-advancements" id="markdown-toc-safety-and-advancements">Safety and Advancements</a>    <ul>
      <li><a href="#preparedness-framework" id="markdown-toc-preparedness-framework">Preparedness Framework</a></li>
      <li><a href="#external-red-teaming" id="markdown-toc-external-red-teaming">External Red-teaming</a></li>
      <li><a href="#safety-performance-metrics" id="markdown-toc-safety-performance-metrics">Safety Performance Metrics</a></li>
      <li><a href="#key-safety-enhancements" id="markdown-toc-key-safety-enhancements">Key Safety Enhancements</a></li>
    </ul>
  </li>
  <li><a href="#deployment-patterns-for-o1-using-dynamic-task-routing" id="markdown-toc-deployment-patterns-for-o1-using-dynamic-task-routing">Deployment Patterns for O1 Using Dynamic Task Routing</a>    <ul>
      <li><a href="#adding-a-side-network-to-openais-o1-models-a-dual-net-approach" id="markdown-toc-adding-a-side-network-to-openais-o1-models-a-dual-net-approach">Adding a Side Network to OpenAI’s O1 Models: a Dual-Net Approach</a>        <ul>
          <li><a href="#side-net-for-task-routing-and-efficiency" id="markdown-toc-side-net-for-task-routing-and-efficiency">Side Net for Task Routing and Efficiency</a></li>
          <li><a href="#practical-implementation-in-o1" id="markdown-toc-practical-implementation-in-o1">Practical Implementation in O1</a></li>
          <li><a href="#routing-and-use-case-scenarios" id="markdown-toc-routing-and-use-case-scenarios">Routing and Use-Case Scenarios</a></li>
          <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
        </ul>
      </li>
      <li><a href="#adopting-a-router-setup-to-divert-queries" id="markdown-toc-adopting-a-router-setup-to-divert-queries">Adopting a Router Setup to Divert Queries</a>        <ul>
          <li><a href="#routellm-intelligent-query-routing-between-big-and-small-models" id="markdown-toc-routellm-intelligent-query-routing-between-big-and-small-models">RouteLLM: Intelligent Query Routing Between Big and Small Models</a></li>
          <li><a href="#implementation-with-o1-models" id="markdown-toc-implementation-with-o1-models">Implementation with O1 Models</a></li>
        </ul>
      </li>
      <li><a href="#integration-with-openais-o1-test-time-compute" id="markdown-toc-integration-with-openais-o1-test-time-compute">Integration with OpenAI’s O1 Test-Time Compute</a></li>
      <li><a href="#conclusion-1" id="markdown-toc-conclusion-1">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#conclusion-2" id="markdown-toc-conclusion-2">Conclusion</a></li>
  <li><a href="#related-papers" id="markdown-toc-related-papers">Related Papers</a>    <ul>
      <li><a href="#lets-verify-step-by-step" id="markdown-toc-lets-verify-step-by-step">Let’s Verify Step by Step</a></li>
      <li><a href="#scaling-llm-test-time-compute-optimally-can-be-more-effective-than-scaling-model-parameters" id="markdown-toc-scaling-llm-test-time-compute-optimally-can-be-more-effective-than-scaling-model-parameters">Scaling LLM Test-Time Compute Optimally Can be More Effective Than Scaling Model Parameters</a></li>
      <li><a href="#star-self-taught-reasoner-bootstrapping-reasoning-with-reasoning" id="markdown-toc-star-self-taught-reasoner-bootstrapping-reasoning-with-reasoning">STaR: Self-Taught Reasoner: Bootstrapping Reasoning with Reasoning</a></li>
      <li><a href="#quiet-star-language-models-can-teach-themselves-to-think-before-speaking" id="markdown-toc-quiet-star-language-models-can-teach-themselves-to-think-before-speaking">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></li>
      <li><a href="#large-language-monkeys-scaling-inference-compute-with-repeated-sampling" id="markdown-toc-large-language-monkeys-scaling-inference-compute-with-repeated-sampling">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a></li>
      <li><a href="#learn-beyond-the-answer-training-language-models-with-reflection-for-mathematical-reasoning" id="markdown-toc-learn-beyond-the-answer-training-language-models-with-reflection-for-mathematical-reasoning">Learn Beyond the Answer: Training Language Models with Reflection for Mathematical Reasoning</a></li>
      <li><a href="#agent-q-advanced-reasoning-and-learning-for-autonomous-ai-agents" id="markdown-toc-agent-q-advanced-reasoning-and-learning-for-autonomous-ai-agents">Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</a></li>
      <li><a href="#v-star-training-verifiers-for-self-taught-reasoners" id="markdown-toc-v-star-training-verifiers-for-self-taught-reasoners">V-STaR: Training Verifiers for Self-Taught Reasoners</a></li>
      <li><a href="#improve-mathematical-reasoning-in-language-models-by-automated-process-supervision" id="markdown-toc-improve-mathematical-reasoning-in-language-models-by-automated-process-supervision">Improve Mathematical Reasoning in Language Models by Automated Process Supervision</a></li>
      <li><a href="#chain-of-thought-reasoning-without-prompting" id="markdown-toc-chain-of-thought-reasoning-without-prompting">Chain-of-Thought Reasoning Without Prompting</a></li>
      <li><a href="#training-language-models-to-self-correct-via-reinforcement-learning" id="markdown-toc-training-language-models-to-self-correct-via-reinforcement-learning">Training Language Models to Self-Correct Via Reinforcement Learning</a></li>
    </ul>
  </li>
  <li><a href="#further-reading" id="markdown-toc-further-reading">Further Reading</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="overview">Overview</h2>

<ul>
  <li>OpenAI’s o1 models are a leap forward in reasoning and problem-solving capabilities for complex tasks, especially in the domains of STEM, coding, and scientific reasoning. The o1 series introduces a novel training method based on reinforcement learning (RL) combined with chain-of-thought reasoning, allowing the model to “think” through problems before producing an output. This deeper reasoning approach significantly enhances the model’s performance on tasks requiring logic, math, and technical expertise.</li>
  <li>The key models in this series include <code class="language-plaintext highlighter-rouge">o1-preview</code>, a high-performance reasoning model, and <code class="language-plaintext highlighter-rouge">o1-mini</code>, a cost-efficient alternative designed for faster performance and lower computational costs, especially for coding and technical reasoning tasks. <code class="language-plaintext highlighter-rouge">o1-mini</code> model offers a faster, more efficient alternative to <code class="language-plaintext highlighter-rouge">o1-preview</code>, optimizing for cost-effectiveness and speed while sacrificing some breadth of knowledge. This makes it ideal for developers focused on coding and technical tasks. Meanwhile, <code class="language-plaintext highlighter-rouge">o1-preview</code> excels at handling more complex reasoning tasks by taking advantage of extended test-time compute.</li>
</ul>

<h3 id="key-ideas">Key Ideas</h3>

<ul>
  <li><strong>Chain-of-thought reasoning</strong> using large-scale RL for improved problem-solving, enabling the model to break down tasks into manageable steps.</li>
  <li><strong>Longer response times</strong>, ranging from seconds to minutes, allowing the model to generate more thoughtful and accurate results by producing more internal reasoning tokens (these tokens are hidden from the user but billed).</li>
  <li>The models <strong>do not support tool usage</strong>, <strong>batch calls</strong>, or <strong>image inputs</strong>. They are designed for reasoning-based tasks without relying on external information.</li>
  <li>API access is limited to high-tier users with a minimum <strong>$1,000 spend</strong>, emphasizing exclusivity for advanced applications.</li>
  <li>Increased <strong>output token limits</strong> (32,768 for <code class="language-plaintext highlighter-rouge">o1-preview</code> and 65,536 for <code class="language-plaintext highlighter-rouge">o1-mini</code>) ensure enough space for reasoning and responses in complex tasks.</li>
</ul>

<h2 id="test-time-compute-shifting-focus-to-inference-scaling">Test-time Compute: Shifting Focus to Inference Scaling</h2>

<ul>
  <li>The release of o1 signals a paradigm shift in how test-time/inference-time compute is utilized, marking the most significant change since the foundational <a href="https://arxiv.org/abs/2203.15556">Chinchilla scaling law</a> of 2022. While many had predicted a plateau in LLM capabilities by focusing exclusively on training scaling, o1 introduces a breakthrough by highlighting the critical role of inference scaling in overcoming diminishing returns. This shift recognizes the interplay between training and inference curves, rather than relying solely on training compute for improvements.</li>
  <li>No self-improving LLM algorithm has achieved significant gains beyond three iterations. Unlike AlphaGo’s success in using additional compute to extend its performance beyond human-level abilities, no LLM has been able to replicate this in the context of LLM training. However, with the introduction of test-time compute scaling in o1, this may represent a new frontier for AI models, enabling them to solve increasingly complex problems by refining their reasoning in real-time.</li>
  <li>Instead of focusing exclusively on the trade-offs associated with training-time compute, the o1 model emphasizes test-time/inference-time compute, allowing the model to “think” during inference by generating chains of reasoning. Much like AlphaGo’s Monte Carlo Tree Search (MCTS), o1 models simulate various strategies and scenarios during inference to converge on optimal solutions. This strategy marks a key departure from earlier approaches, such as GPT-4o, which relied heavily on large pre-trained models.</li>
</ul>

<h3 id="inference-scaling-a-new-paradigm">Inference Scaling: a New Paradigm</h3>

<ul>
  <li>The key innovation in o1 lies in the interplay between training and inference scaling, recognizing that the two curves must work together. While many AI researchers expected diminishing returns by focusing exclusively on training scaling laws, o1 has shifted the focus to inference scaling, which allows the model to think through tasks during inference by simulating possible solutions. This approach enables o1 to overcome the diminishing returns seen in training alone, leading to significantly enhanced capabilities during inference.</li>
  <li>Unlike earlier models, o1 leverages inference scaling to handle complex reasoning tasks more efficiently. During inference, the model refines its outputs through real-time search, similar to AlphaGo’s approach to gameplay. This search-based refinement allows the model to reach better solutions by rolling out multiple strategies in real-time.</li>
  <li>Below are some of the key ideas in test-time compute:</li>
</ul>

<ol>
  <li>
    <p><strong>Decoupling Reasoning from Knowledge</strong>: The o1 models demonstrate that massive models aren’t necessary for reasoning tasks. In previous models, a large portion of parameters was dedicated to memorizing facts for benchmarks like Trivia QA. However, o1 introduces a smaller, more efficient “reasoning core”, separating reasoning from knowledge. This core can call on external tools (e.g., browser, code verifier) to handle specialized tasks, reducing the need for large pre-trained models and allowing for a reduction in pre-training compute.</p>
  </li>
  <li>
    <p><strong>Inference Scaling and Longer Response Times</strong>: o1 models shift a substantial amount of compute to serving inference instead of pre- or post-training. This shift allows the models to generate longer, more detailed reasoning outputs, especially for complex tasks in coding, mathematics, and scientific problem-solving. The extended response times (ranging from several seconds to minutes) allow for deeper reasoning, as the models generate additional internal reasoning tokens during inference, improving overall accuracy and depth.</p>
  </li>
  <li>
    <p><strong>Dynamic Resource Allocation</strong>: Leveraging more test-time compute, o1 models can dynamically enhance performance by adjusting computational resources based on task complexity. This contrasts with earlier models like GPT-4o, which had fixed test-time compute limits. By enabling dynamic resource allocation, o1 ensures that more compute is used where necessary, improving performance on tasks that require deeper thought.</p>
  </li>
  <li>
    <p><strong>Data Flywheel Effect</strong>: The o1 models have the potential to create a data flywheel by generating search traces during inference. When a correct answer is produced, the search trace becomes a mini dataset of training examples containing both positive and negative rewards. These examples can improve the reasoning core for future iterations of the model, much like how AlphaGo’s value network refined itself through MCTS-generated training data.</p>
  </li>
</ol>

<h3 id="the-role-of-reinforcement-learning-in-o1">The Role of Reinforcement Learning in O1</h3>

<ul>
  <li>o1’s large-scale RL algorithm teaches the model how to think productively using its chain of thought. This RL-based training process is highly data-efficient, allowing the model to improve with more RL (train-time compute) and more time spent thinking (test-time compute). As the model spends more time on reasoning during inference, its performance consistently improves, highlighting the importance of test-time compute scaling.</li>
  <li>The constraints on scaling this RL-based approach differ substantially from those of traditional LLM pretraining. Rather than relying on vast datasets and expensive training regimes, o1 models can achieve significant improvements by focusing on how to think during inference, as shown in the plot from <a href="https://openai.com/index/learning-to-reason-with-llms/">Learning to Reason with LLMs</a>:</li>
</ul>

<p><img src="/primers/ai/assets/o1/ttc.jpg" alt="Test-time Compute"></p>

<ul>
  <li>This process mirrors how AlphaGo used additional compute to extend its performance envelope beyond human-level abilities by integrating real-time search during gameplay. In a similar vein, o1’s test-time compute scaling may signal the beginning of a new era in LLM capabilities.</li>
</ul>

<h2 id="test-results">Test Results</h2>

<ul>
  <li>OpenAI tested the o1 models against a series of benchmarks, including human exams and machine learning tasks, where they outperformed GPT-4o in most reasoning-heavy areas. Here are a few highlights:</li>
</ul>

<h3 id="advanced-math-competitions">Advanced Math Competitions</h3>

<ul>
  <li>On the AIME (American Invitational Mathematics Examination):
    <ul>
      <li>GPT-4o solved ~12% of the problems.</li>
      <li><code class="language-plaintext highlighter-rouge">o1-preview</code> solved 74% of the problems on the first try, reaching up to 93% when using more advanced techniques like consensus re-ranking.</li>
    </ul>
  </li>
</ul>

<h3 id="coding-competitions">Coding Competitions</h3>

<ul>
  <li>In Codeforces competitions, <code class="language-plaintext highlighter-rouge">o1-mini</code> achieved a performance level in the 86th percentile, showcasing advanced abilities in solving and debugging code. This far exceeded the previous generation’s GPT-4o performance.</li>
</ul>

<h3 id="stem-expertise">STEM Expertise</h3>

<ul>
  <li>In evaluations like the GPQA benchmark, which tests advanced knowledge in chemistry, physics, and biology, o1 models outperformed human experts, marking a significant milestone in AI-driven scientific problem-solving.</li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>While the o1 models excel in reasoning and technical tasks, there are some limitations:
    <ul>
      <li><strong>Limited non-STEM knowledge</strong>: <code class="language-plaintext highlighter-rouge">o1-mini</code>, optimized for STEM reasoning, underperforms in non-STEM factual tasks compared to larger models. It lacks the broad world knowledge required for answering questions on biographies, trivia, or literature, which limits its utility in those domains.</li>
      <li><strong>Response speed and cost</strong>: The longer reasoning times and additional token generation required for deep problem-solving can make <code class="language-plaintext highlighter-rouge">o1-preview</code> slower and more costly compared to simpler models like GPT-4o.</li>
      <li><strong>Hidden reasoning tokens</strong>: The model generates internal reasoning tokens during inference, which are billed but not visible to the user. This can lead to higher costs, especially in use cases where extensive internal reasoning is not necessary.</li>
      <li><strong>No tool or image support</strong>: The models do not support external tool usage or image inputs, limiting their versatility in multimodal tasks that require these capabilities.</li>
    </ul>
  </li>
</ul>

<h2 id="use-cases-of-openai-o1">Use-Cases of OpenAI O1</h2>

<ul>
  <li>The o1 models shine in fields requiring high-level reasoning and structured thinking. Below are key use cases:
    <ol>
      <li><strong>Coding</strong>: <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code> excel in code generation, debugging, and solving algorithmic problems. In Codeforces competitions, o1 models performed better than 90% of human participants.</li>
      <li><strong>Scientific Problem-solving</strong>: These models have shown superior performance on challenging scientific benchmarks such as GPQA, solving complex problems in physics, chemistry, and biology. They outperform human PhD experts on some evaluations, demonstrating advanced problem-solving abilities.</li>
      <li><strong>Math</strong>: o1 models have achieved outstanding results in mathematics competitions like AIME, solving 93% of problems when given multiple attempts and consensus voting, surpassing even top human performers.</li>
    </ol>
  </li>
</ul>

<h2 id="pricing-and-accessibility">Pricing and Accessibility</h2>

<ul>
  <li>Pricing for the o1 models is comparable to GPT-4 levels at launch, with <code class="language-plaintext highlighter-rouge">o1-preview</code> costing $15 per 1 million tokens (input/output) and $60 for completion tokens, while <code class="language-plaintext highlighter-rouge">o1-mini</code> is offered at $3 and $12 respectively, making it a more cost-efficient option for high-throughput applications.</li>
  <li>API Access to <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code> is restricted to high-tier accounts with at least $1,000 spent, reflecting their advanced capabilities and premium pricing. These models are not yet available for broad public use.</li>
</ul>

<h2 id="safety-and-advancements">Safety and Advancements</h2>

<ul>
  <li>The o1 models introduce a significant leap in safety and alignment through the use of chain-of-thought reasoning, allowing the models to not only solve complex problems but also to think through safety rules and apply them in real-time. This built-in safety mechanism is designed to improve the robustness of the models, making them less prone to harmful outputs or jailbreaking attempts.</li>
</ul>

<h3 id="preparedness-framework">Preparedness Framework</h3>

<ul>
  <li>OpenAI follows a comprehensive <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">Preparedness Framework</a> for ensuring the safety and reliability of the o1 models. This framework includes a series of internal and external safety tests, red-teaming exercises, and real-world evaluations across challenging scenarios to assess how well the models adhere to ethical standards and avoid harmful behavior.</li>
  <li>Chain-of-thought reasoning plays a critical role in safety by helping the models reason through the implications of their responses, particularly when faced with sensitive or potentially dangerous prompts. By embedding safety rules directly into their reasoning processes, o1 models can make more informed and cautious decisions.</li>
  <li>In evaluations, <code class="language-plaintext highlighter-rouge">o1-preview</code> showed significant improvements over GPT-4o, with a substantial increase in safe completions, particularly on jailbreaks and edge cases, such as avoiding violent content, illegal activities, and harmful advice.</li>
</ul>

<h3 id="external-red-teaming">External Red-teaming</h3>

<ul>
  <li>Both <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code> underwent rigorous external red-teaming evaluations to test their responses to adversarial inputs. These tests, conducted by third-party experts in cybersecurity, content policy, and international security, confirmed the models’ robustness in resisting manipulation and providing responsible outputs.</li>
</ul>

<h3 id="safety-performance-metrics">Safety Performance Metrics</h3>
<ul>
  <li>On difficult safety benchmarks like StrongREJECT, <code class="language-plaintext highlighter-rouge">o1-preview</code> scored 84 out of 100, compared to GPT-4o’s 22, reflecting a major improvement in the model’s ability to adhere to ethical guidelines.</li>
  <li>In other safety categories, such as avoiding illegal sexual content, violent harassment, and self-harm encouragement, <code class="language-plaintext highlighter-rouge">o1-preview</code> showed a marked increase in safe completions, surpassing earlier models and ensuring better alignment with human values.</li>
</ul>

<h3 id="key-safety-enhancements">Key Safety Enhancements</h3>

<ul>
  <li><strong>Reduced Jailbreak Vulnerability</strong>: The o1 models performed exceptionally well in preventing jailbreak attempts, where malicious users attempt to manipulate the model into producing harmful or unethical content.</li>
  <li><strong>Improved Compliance</strong>: The models demonstrated enhanced compliance with safety guidelines, even in challenging or ambiguous situations, outperforming earlier versions like GPT-4o across a wide range of harmful prompts.</li>
  <li>
    <p><strong>Edge Case Handling</strong>: The models were also tested on benign edge cases, where the goal was to avoid unnecessary refusals while still adhering to safety standards. <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code> both showed a high level of compliance, maintaining a balance between safety and responsiveness.</p>
  </li>
  <li>By integrating chain-of-thought reasoning directly into the models’ architecture, OpenAI has made significant strides in ensuring that the o1 models not only solve complex tasks but do so in a way that aligns with ethical standards and safety regulations.</li>
</ul>

<h2 id="deployment-patterns-for-o1-using-dynamic-task-routing">Deployment Patterns for O1 Using Dynamic Task Routing</h2>

<ul>
  <li>To maximize the efficiency and performance of OpenAI’s o1 models, two complementary approaches can be integrated: a dual-net architecture and an intelligent routing system. These methods aim to dynamically allocate computational resources based on task complexity, ensuring that both cost and performance are optimized.</li>
  <li>Below are two key strategies to achieve this goal.</li>
</ul>

<h3 id="adding-a-side-network-to-openais-o1-models-a-dual-net-approach">Adding a Side Network to OpenAI’s O1 Models: a Dual-Net Approach</h3>

<ul>
  <li>Based on the concept proposed in <a href="https://arxiv.org/abs/2007.13512">Add a SideNet to your MainNet</a>, a potential enhancement to OpenAI’s o1 series involves integrating a smaller, faster “side network” alongside the primary “main network” to optimize performance across various tasks. The side net acts as a preliminary filter, handling simpler tasks while routing complex problems to the more powerful o1 model. This dynamic delegation improves efficiency, reduces costs, and speeds up response times by reserving o1’s deep reasoning capabilities for tasks that genuinely require it. This concept mirrors strategies from earlier AI systems, where a smaller network is paired with a larger one, creating an efficient and scalable architecture.</li>
  <li>Below, we detail how this approach could work with o1.</li>
</ul>

<h4 id="side-net-for-task-routing-and-efficiency">Side Net for Task Routing and Efficiency</h4>

<p>The primary challenge in AI performance optimization lies in dynamically determining the computational resources necessary for each task. A side network—a smaller, less resource-intensive model—could serve as a preliminary filter, assessing the complexity of the task before deciding whether to route it to the more powerful main o1 network.</p>

<p>The side net would operate as follows:</p>
<ul>
  <li>
    <p><strong>Confidence Estimation</strong>: For any given input, the side net would first estimate the difficulty and provide a confidence score based on its ability to solve the task. If the side net is confident in solving the task (e.g., a simpler coding or reasoning query), it handles the problem, saving both time and cost.</p>
  </li>
  <li>
    <p><strong>Routing Complex Tasks</strong>: If the side net exhibits low confidence or identifies the task as too complex (e.g., tasks requiring deep mathematical reasoning or intricate multi-step logic), it passes the task to the o1 model for more in-depth reasoning.</p>
  </li>
</ul>

<h4 id="practical-implementation-in-o1">Practical Implementation in O1</h4>

<ul>
  <li>For o1, the addition of a side net would be beneficial in several ways:</li>
</ul>

<ol>
  <li><strong>Efficiency Gains</strong>:
    <ul>
      <li><strong>Faster Task Resolution</strong>: Tasks that are less complex can be quickly handled by the side net, allowing for faster response times and reducing the need to invoke the more resource-heavy o1 model.</li>
      <li><strong>Cost-Effectiveness</strong>: By delegating simpler tasks to the side net, users could minimize the computational costs associated with the o1 model’s deep reasoning cycles, as lower-tier models generally cost less to operate.</li>
    </ul>
  </li>
  <li><strong>Dynamic Resource Allocation</strong>:
    <ul>
      <li>In practice, this would mean reserving the high-level reasoning capabilities of o1 for problems that genuinely require it, thereby reducing the need for extended computation times on tasks where simpler models could suffice. This aligns with the philosophy of <strong>test-time compute scaling</strong>, where resources are allocated dynamically based on the task’s complexity.</li>
    </ul>
  </li>
  <li><strong>Estimation-Based Computation</strong>:
    <ul>
      <li>Estimating the difficulty of tasks is often easier than solving them outright. This allows the side net to perform a lightweight analysis and decide whether it’s worth utilizing the more advanced reasoning processes of o1 or solving it directly. Such a strategy can prevent the o1 network from being overused on simple queries, thereby maximizing overall throughput.</li>
    </ul>
  </li>
</ol>

<h4 id="routing-and-use-case-scenarios">Routing and Use-Case Scenarios</h4>

<ul>
  <li>The side net approach could improve the performance of o1 in several use cases:
    <ul>
      <li>
        <p><strong>Routine Coding Tasks</strong>: In scenarios like basic debugging or syntax correction, the side net could handle the tasks independently without involving the <code class="language-plaintext highlighter-rouge">o1-preview</code> model, significantly reducing response times and token consumption.</p>
      </li>
      <li>
        <p><strong>Complex STEM Reasoning</strong>: On more complex STEM tasks (e.g., physics problem-solving or advanced algorithmic challenges), the side net might quickly recognize its limitations and forward the problem to the o1 network, ensuring a thorough and accurate solution.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="conclusion">Conclusion</h4>

<ul>
  <li>Integrating a side net into OpenAI’s o1 architecture offers a promising pathway for enhancing performance and cost-effectiveness. By dynamically assessing the complexity of tasks and delegating them appropriately, this dual-net approach could optimize resource use, reduce computation times, and offer users a more efficient solution tailored to the difficulty of the task at hand. This fusion of side and main nets, where confidence in simpler tasks dictates the computational path, has the potential to push o1’s capabilities even further while controlling costs and maximizing response efficiency.</li>
</ul>

<h3 id="adopting-a-router-setup-to-divert-queries">Adopting a Router Setup to Divert Queries</h3>

<ul>
  <li>In addition to integrating a smaller side network alongside OpenAI’s o1 models, another compelling paradigm involves leveraging <a href="https://arxiv.org/abs/2406.18665">RouteLLM</a>, a routing framework proposed for dynamic model routing between a large model (such as o1) and a smaller, cost-efficient model. This will lead to a system that intelligently decides which model—a large, accurate model (<code class="language-plaintext highlighter-rouge">o1-preview</code>) or a smaller, cost-efficient one (<code class="language-plaintext highlighter-rouge">o1-mini</code> or side net)—should handle a given task. By using preference data and performance thresholds, this setup balances the cost and quality trade-off, reducing reliance on larger models without sacrificing significant response quality. This approach builds on the foundational idea of routing queries based on task complexity, allowing for cost-effective and high-performance inference.</li>
</ul>

<h4 id="routellm-intelligent-query-routing-between-big-and-small-models">RouteLLM: Intelligent Query Routing Between Big and Small Models</h4>

<ul>
  <li>RouteLLM introduces an efficient way to determine the appropriate model for a task dynamically, based on the complexity and expected performance requirements. This strategy is useful when deciding between a smaller model (such as <code class="language-plaintext highlighter-rouge">o1-mini</code> or even a simpler side net) and a larger model like <code class="language-plaintext highlighter-rouge">o1-preview</code>. The router system evaluates the complexity of a given query and decides which model to invoke, thereby balancing cost and performance. The main idea is to use lightweight “routers” trained on preference data and additional techniques, such as data augmentation, to optimize the decision-making process.</li>
</ul>

<p><strong>Key aspects of RouteLLM routing</strong>:</p>
<ul>
  <li><strong>Performance vs. Cost Trade-off</strong>: The larger model (<code class="language-plaintext highlighter-rouge">o1-preview</code>) is typically more accurate but costly to run, while the smaller model (<code class="language-plaintext highlighter-rouge">o1-mini</code> or side net) is less accurate but cheaper. The router determines which model to invoke based on the task complexity.</li>
  <li><strong>Cost Savings</strong>: According to RouteLLM’s results, routing can save up to <strong>2x in cost</strong> without sacrificing significant response quality. The router essentially ensures that only complex queries are passed to the more powerful model.</li>
  <li><strong>Human Preference Data</strong>: RouteLLM uses human preference data to learn which types of queries require the larger model and which can be handled by the smaller one. This learning allows the router to generalize across a wide variety of tasks, making it effective for real-world applications.</li>
</ul>

<h4 id="implementation-with-o1-models">Implementation with O1 Models</h4>

<ul>
  <li>For OpenAI’s o1 architecture, RouteLLM could provide the following advantages:</li>
</ul>

<ol>
  <li><strong>Dynamic Query Evaluation</strong>:
    <ul>
      <li>For tasks requiring deeper reasoning, the RouteLLM system would send the query to <code class="language-plaintext highlighter-rouge">o1-preview</code>, where more extensive test-time compute can be allocated.</li>
      <li>For simpler tasks, it could route queries to a smaller model (e.g., <code class="language-plaintext highlighter-rouge">o1-mini</code>) or a dedicated side net to reduce computational load and costs.</li>
    </ul>
  </li>
  <li><strong>Training the Router</strong>:
    <ul>
      <li>The router can be trained using preference data similar to the RouteLLM framework, which captures the complexity of queries and the performance differences between <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code>.</li>
      <li><strong>Data Augmentation</strong>: To improve the router’s generalization across different domains, synthetic data (e.g., from MMLU benchmarks) or automatically generated preference labels could be used to train the routing system further.</li>
    </ul>
  </li>
  <li><strong>Cost and Performance Optimization</strong>:
    <ul>
      <li>RouteLLM demonstrated the effectiveness of this system by achieving <strong>50% less reliance on larger models</strong> while maintaining quality through intelligent routing. This method would allow developers using o1 models to optimize for budget constraints while ensuring top-tier performance for complex tasks.</li>
      <li><strong>Metrics like Cost-Performance Threshold (CPT)</strong> can help determine the best balance, with thresholds adjusted based on user preferences or task criticality.</li>
    </ul>
  </li>
</ol>

<h3 id="integration-with-openais-o1-test-time-compute">Integration with OpenAI’s O1 Test-Time Compute</h3>

<ul>
  <li>
    <p><strong>Extended Decision-Making</strong>: By integrating RouteLLM’s router system, the decision of how long to compute or which model to use becomes more dynamic and data-driven. The router helps decide if a task truly requires the deep reasoning capabilities of <code class="language-plaintext highlighter-rouge">o1-preview</code> or if the faster <code class="language-plaintext highlighter-rouge">o1-mini</code> would suffice.</p>
  </li>
  <li>
    <p><strong>Use Case Adaptability</strong>: In environments where budget and speed are priorities, RouteLLM’s architecture ensures that only necessary computational resources are allocated for complex queries, while simpler queries are handled more efficiently. For instance, in a <strong>coding task</strong>, basic syntax fixes can be routed to <code class="language-plaintext highlighter-rouge">o1-mini</code>, while complex algorithm debugging is processed by <code class="language-plaintext highlighter-rouge">o1-preview</code>.</p>
  </li>
</ul>

<h3 id="conclusion-1">Conclusion</h3>

<ul>
  <li>Incorporating a dual-model routing system like RouteLLM into OpenAI’s o1 framework enhances the decision-making process around model usage. It allows for intelligent, cost-effective routing, balancing the performance of the smaller <code class="language-plaintext highlighter-rouge">o1-mini</code> with the deeper reasoning capabilities of <code class="language-plaintext highlighter-rouge">o1-preview</code>. This ensures that resources are used efficiently, optimizing both cost and task performance dynamically based on query complexity【6†source】.</li>
</ul>

<h2 id="conclusion-2">Conclusion</h2>

<ul>
  <li>OpenAI’s <code class="language-plaintext highlighter-rouge">o1-preview</code> and <code class="language-plaintext highlighter-rouge">o1-mini</code> models represent a major advancement in reasoning, safety, and coding capabilities. With chain-of-thought reasoning and RL, they can tackle complex problems with greater accuracy than previous models, making them invaluable tools for developers, scientists, and researchers. While there are still some limitations, especially in non-STEM tasks, o1 sets a new standard for AI’s reasoning capabilities.</li>
</ul>

<h2 id="related-papers">Related Papers</h2>

<h3 id="lets-verify-step-by-step"><a href="https://arxiv.org/abs/2305.20050">Let’s Verify Step by Step</a></h3>

<ul>
  <li>This paper by Lightman et al. from OpenAI presents a detailed investigation into the effectiveness of process supervision compared to outcome supervision in training language models for complex multi-step reasoning.</li>
  <li>The authors explore the concepts of outcome and process supervision. Outcome-supervised reward models (ORMs) focus on the final result of a model’s reasoning chain, while process-supervised reward models (PRMs) receive feedback at each step in the reasoning chain.</li>
  <li>To collect process supervision data, they present human data-labelers with step-by-step solutions to MATH problems sampled by the large-scale generator.
Their task is to assign each step in the solution a label of positive, negative, or neutral, as shown in the below figure. A positive label indicates that the step is correct and reasonable. A negative label indicates that the step is either incorrect or unreasonable. A neutral label indicates ambiguity. In practice, a step may be labelled neutral if it is subtly misleading, or if it is a poor suggestion that is technically still valid. Neutral labels allows them to defer the decision about how to handle ambiguity: at test time, we can treat neutral labels as either positive or negative.  The following figure from the paper shows a screenshot of the interface used to collect feedback for each step in a solution.</li>
</ul>

<p><img src="../../../images/papers/PRM1.jpg" alt=""></p>

<ul>
  <li>The following figure from the paper shows two solutions to the same problem, graded by the PRM. The solution on the left is correct while the solution on the right is incorrect. A green background indicates a high PRM score, and a red background indicates a low score. The PRM correctly identifies the mistake in the incorrect solution.</li>
</ul>

<p><img src="../../../images/papers/PRM2.jpg" alt=""></p>

<ul>
  <li>For their experiments, they used large-scale models fine-tuned from GPT-4 and smaller models for detailed comparisons. These models were trained on the MATH dataset, which includes complex mathematical problems.</li>
  <li>The paper introduces a new dataset, PRM800K, comprising 800,000 step-level human feedback labels, which was instrumental in training their PRM models.</li>
  <li>The key findings show that process supervision significantly outperforms outcome supervision in training models to solve complex problems. Specifically, their PRM model solved 78.2% of problems from a representative subset of the MATH test set.</li>
  <li>The researchers also demonstrate that active learning significantly improves the efficiency of process supervision, leading to better data utilization.</li>
  <li>They conducted out-of-distribution generalization tests using recent STEM tests like AP Physics and Calculus exams, where the PRM continued to outperform other methods.</li>
  <li>The paper discusses the implications of their findings for AI alignment, highlighting the advantages of process supervision in producing more interpretable and aligned models.</li>
  <li>They acknowledge potential limitations related to test set contamination but argue that the relative comparisons made in their work are robust against such issues.</li>
  <li>This research contributes to the field by showing the effectiveness of process supervision and active learning in improving the reasoning capabilities of language models, especially in complex domains like mathematics.</li>
</ul>

<h3 id="scaling-llm-test-time-compute-optimally-can-be-more-effective-than-scaling-model-parameters"><a href="https://arxiv.org/abs/2408.03314">Scaling LLM Test-Time Compute Optimally Can be More Effective Than Scaling Model Parameters</a></h3>

<ul>
  <li>This paper by Snell et al. from UC Berkeley and Google DeepMind explores the scaling of inference-time computation in large language models (LLMs) and addresses the question of how much a fixed amount of test-time compute can improve model performance, particularly on difficult prompts. The authors focus on two primary mechanisms to scale test-time compute: (1) searching against dense process-based verifier reward models (PRMs) and (2) adaptively updating the model’s response distribution during test time.</li>
  <li>The study reveals that the optimal approach for scaling test-time compute depends heavily on prompt difficulty. Based on this insight, the authors propose a “compute-optimal” scaling strategy, which adaptively allocates test-time compute depending on the problem’s complexity. This strategy improves efficiency by more than 4× compared to standard best-of-N sampling and can, under certain conditions, outperform models 14× larger with matched FLOPs.</li>
  <li>In their experimental setup, they use PaLM 2-S (Codey) models fine-tuned for revision and verification tasks, evaluated on the challenging MATH benchmark. They evaluate methods to scale test-time compute, including revising answers iteratively and searching for correct solutions using PRMs. Key findings include:
    <ul>
      <li><strong>Revisions</strong>: When the LLM iteratively refines its responses, it achieves better performance on easier tasks by revising and optimizing its original answers. For more complex problems, parallel sampling (best-of-N) is generally more effective, especially when multiple high-level solution approaches must be explored.</li>
      <li><strong>PRM-based Search</strong>: Process-based verifiers perform step-by-step evaluations of solutions, offering better guidance on complex problems. Beam search and lookahead search methods were explored, with beam search showing higher efficiency on more difficult prompts when the compute budget is limited. The following figure from the paper shows a comparison of different PRM search methods. Left: Best-of-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">N</script> samples <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">N</script> full answers and then selects the best answer according to the PRM final score. Center: Beam search samples <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">N</script> candidates at each step, and selects the top <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">M</script> according to the PRM to continue the search from. Right: lookahead-search extends each step in beam-search to utilize a k-step lookahead while assessing which steps to retain and continue the search from. Thus lookahead-search needs more compute.</li>
    </ul>

    <p><img src="../../../images/papers/PRMSearch.jpg" alt=""></p>
  </li>
  <li>The following figure from the paper shows parallel sampling (e.g., Best-of-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-5">N</script>) verses sequential revisions. Left: Parallel sampling generates <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-6">N</script> answers independently in parallel, whereas sequential revisions generates each one in sequence conditioned on previous attempts. Right: In both the sequential and parallel cases, we can use the verifier to determine the best-of-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-7">N</script> answers (e.g. by applying best-of-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="mi" id="MathJax-Span-24" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-8">N</script> weighted). We can also allocate some of our budget to parallel and some to sequential, effectively enabling a combination of the two sampling strategies. In this case, we use the verifier to first select the best answer within each sequential chain and then select the best answer accross chains.</li>
</ul>

<p><img src="../../../images/papers/ParallelSeqverifier.jpg" alt=""></p>

<ul>
  <li>The paper emphasizes that adaptive test-time compute scaling, based on the difficulty of the question, is essential. The proposed compute-optimal scaling strategy outperforms best-of-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-25" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-26"><span class="mi" id="MathJax-Span-27" style="font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-9">N</script> with significantly less computation, particularly for easy and intermediate tasks. By dynamically choosing between search-based and revision-based methods, the authors demonstrate a practical way to optimize LLM performance within a constrained computational budget.</li>
  <li>In addition, they show that test-time compute can be a viable substitute for additional pretraining, especially when handling easier questions or lower inference workloads. On the other hand, for harder questions, additional pretraining remains more effective. This tradeoff suggests that in specific deployment scenarios (e.g., where smaller models are desirable), emphasizing test-time compute scaling might reduce the need for training significantly larger models.</li>
  <li>Finally, the authors propose future directions, including combining different methods of test-time compute (e.g., revisions and PRM-based search) and refining difficulty assessment during inference to further optimize test-time compute allocation.</li>
</ul>

<h3 id="star-self-taught-reasoner-bootstrapping-reasoning-with-reasoning"><a href="https://arxiv.org/abs/2203.14465">STaR: Self-Taught Reasoner: Bootstrapping Reasoning with Reasoning</a></h3>

<ul>
  <li>This paper by Zelikman et al. from Stanford and Google introduces the Self-Taught Reasoner (STaR), a technique for bootstrapping the ability of large language models (LLMs) to generate reasoning-based answers (rationales) iteratively. The goal of STaR is to improve the LLM’s performance on complex reasoning tasks like arithmetic and commonsense question answering without the need for manually curated large datasets of rationales. Instead, the method iteratively generates and fine-tunes rationales using a small set of initial examples, allowing the model to “teach itself” more complex reasoning over time.</li>
  <li>The core of the STaR approach relies on a simple yet iterative loop:
    <ol>
      <li><strong>Rationale Generation</strong>: A pretrained LLM is prompted with a few rationale examples (e.g., 10 for arithmetic) and tasked with generating rationales for a set of questions. Only rationales that yield correct answers are retained.</li>
      <li><strong>Fine-Tuning</strong>: The model is fine-tuned on these filtered correct rationales to improve its ability to generate them.</li>
      <li><strong>Rationalization</strong>: For problems where the model fails to generate correct answers, it is provided with the correct answer and asked to “rationalize” it by generating a rationale. This technique allows the model to improve by reasoning backward from the correct answer.</li>
    </ol>
  </li>
  <li>This process is repeated across multiple iterations, with the model learning to solve increasingly complex tasks through rationale generation and rationalization.</li>
  <li>The following figure from the paper shows an overview of STaR and a STaR-generated rationale on CommonsenseQA. We indicate the fine-tuning outer loop with a dashed line. The questions and ground truth answers are expected to be present in the dataset, while the rationales are generated using STaR.</li>
</ul>

<p><img src="../../../images/papers/STaR.jpg" alt=""></p>

<ul>
  <li><strong>Implementation Details:</strong>
    <ul>
      <li><strong>Initial Setup</strong>: STaR starts with a small prompt set of rationale-annotated examples (e.g., 10 examples in the case of arithmetic). Each example in the dataset is then augmented with these few-shot rationales, encouraging the model to generate a rationale for the given question.</li>
      <li><strong>Filtering</strong>: Rationales are filtered by whether they result in the correct final answer, and only correct rationales are used for fine-tuning.</li>
      <li><strong>Training Process</strong>: The model is fine-tuned in a loop, with the number of fine-tuning steps increased by 20% per iteration. Fine-tuning starts with 40 training steps and slowly scales up.</li>
      <li><strong>Rationalization</strong>: When the model fails to generate a correct rationale, it is prompted with the correct answer and asked to generate a rationale based on this information. These rationales are added to the fine-tuning dataset for further improvement.</li>
      <li><strong>Avoiding Overfitting</strong>: The model is always retrained from the original pre-trained model rather than continuing to train the same model across iterations, to prevent overfitting.</li>
    </ul>
  </li>
  <li><strong>Results:</strong>
    <ul>
      <li><strong>Arithmetic</strong>: The model’s performance improved significantly after each iteration. Without rationalization, STaR improved performance on n-digit addition problems in a stage-wise fashion (improving on simpler problems first), while rationalization enabled the model to learn across different problem sizes simultaneously.</li>
      <li><strong>CommonsenseQA</strong>: STaR outperformed a GPT-J model fine-tuned directly on answers, achieving 72.5% accuracy compared to 73.0% for a 30× larger GPT-3 model. STaR with rationalization outperformed models without rationalization, indicating the added benefit of rationalizing incorrect answers.</li>
      <li><strong>Generalization</strong>: The STaR approach also demonstrated the ability to generalize beyond training data, solving unseen, out-of-distribution problems in arithmetic.</li>
    </ul>
  </li>
  <li><strong>Key Contributions:</strong>
    <ol>
      <li>STaR provides a scalable bootstrapping technique that allows models to iteratively improve their reasoning abilities without relying on large rationale-annotated datasets.</li>
      <li>The inclusion of rationalization as a mechanism for solving problems that the model initially fails to answer correctly enhances the training process by exposing the model to more difficult problems.</li>
      <li>STaR’s iterative approach makes it a broadly applicable method for improving model reasoning across domains, including arithmetic and commonsense reasoning tasks.</li>
    </ol>
  </li>
  <li>In summary, STaR introduces a novel iterative reasoning-based training approach that improves the reasoning capability of LLMs using a small set of rationale examples and a large dataset without rationales. This method significantly enhances model performance on both symbolic and natural language reasoning tasks.</li>
</ul>

<h3 id="quiet-star-language-models-can-teach-themselves-to-think-before-speaking"><a href="https://arxiv.org/abs/2403.09629">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></h3>

<ul>
  <li>This paper by Zelikman from Stanford and Notbad AI, generalizes the Self-Taught Reasoner (STaR) method by teaching language models (LMs) to reason implicitly through continuous text without relying on curated reasoning datasets. The key idea behind Quiet-STaR is to allow LMs to generate “thoughts” or rationales at each token position, helping predict future tokens and improving overall performance. The authors extend the STaR approach, which previously focused on specific tasks like question-answering, to a more general framework where the LM generates and learns from rationales embedded in arbitrary text.</li>
  <li>The core implementation of Quiet-STaR involves three major steps:
    <ol>
      <li><strong>Parallel Rationale Generation</strong>: The LM generates rationales after every token to explain the continuation of future tokens. A key challenge resolved here is the computational inefficiency of generating continuations at every token. The authors propose a token-wise parallel sampling algorithm that allows for efficient generation by caching forward passes and employing diagonal attention masking.</li>
      <li><strong>Mixing Head for Prediction Integration</strong>: A “mixing head” model is trained to combine predictions made with and without rationales. This helps manage the distribution shift caused by introducing rationales in a pre-trained model. The head outputs a weight that determines how much the rationale-based prediction should influence the overall token prediction.</li>
      <li><strong>Rationale Optimization with REINFORCE</strong>: The model’s rationale generation is optimized using a REINFORCE-based objective, rewarding rationales that improve the likelihood of future token predictions. This method allows the LM to learn to generate rationales that help predict the next tokens more effectively, based on feedback from their impact on future token prediction.</li>
    </ol>
  </li>
  <li>The following figure from the paper shows Quiet-STaR visualized as applied during training to a single thought. We generate thoughts, in parallel, following all tokens in the text (think). The model produces a mixture of its next-token predictions with and without a thought (talk). They apply REINFORCE, as in STaR, to increase the likelihood of thoughts that help the model predict future text while discarding thoughts that make the future text less likely (learn).</li>
</ul>

<p><img src="../../../images/papers/Quiet-STaR.jpg" alt=""></p>

<ul>
  <li>The following figure from the paper shows parallel generation. By constructing an attention mask that allows all thought tokens to pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text, we can generate continuations of all of the thoughts in parallel. Each inference call is used to generate one additional thought token for all text tokens.</li>
</ul>

<p><img src="../../../images/papers/Quiet-STaR1.jpg" alt=""></p>

<ul>
  <li>The following figure from the paper shows the forward pass and teacher forcing. We visualize a single forward pass of our algorithm. Solid lines denote language model computation, while dashed lines indicate tokens are inserted via teacher forcing, and the mixer represents the mixing head. In particular, we visualize predicting three tokens ahead. Thought generation is shown in more detail in the above two figures.</li>
</ul>

<p><img src="../../../images/papers/Quiet-STaR2.jpg" alt=""></p>

<ul>
  <li>The authors also introduce custom tokens, specifically <code class="language-plaintext highlighter-rouge">&lt;|startofthought|&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;|endofthought|&gt;</code>, which mark the beginning and end of the rationale generation. These tokens are initialized based on the LM’s existing knowledge (e.g., em dash “<code class="language-plaintext highlighter-rouge">−−−</code>”) and fine-tuned for optimal performance.</li>
  <li>One of the significant findings from the experiments was that training with Quiet-STaR on diverse text datasets (like C4 and OpenWebMath) improved zero-shot reasoning abilities on commonsense reasoning tasks like GSM8K and CommonsenseQA. The LM showed improved performance in reasoning tasks without any task-specific fine-tuning, demonstrating the effectiveness of Quiet-STaR in enhancing reasoning in LMs in a generalizable and scalable way.</li>
  <li>For example, zero-shot performance on GSM8K improved from 5.9% to 10.9%, and on CommonsenseQA from 36.3% to 47.2%. These improvements are primarily driven by difficult-to-predict tokens, where Quiet-STaR’s rationales prove most beneficial. Furthermore, longer thought sequences resulted in better predictions, suggesting that more detailed reasoning steps enhance token prediction accuracy.</li>
  <li>The computational overhead of Quiet-STaR is notable, as generating rationales adds complexity. However, the authors argue that this overhead can be leveraged to improve the model’s performance in tasks that require deeper reasoning. The results suggest that Quiet-STaR can enhance not only language modeling but also chain-of-thought reasoning, where reasoning steps are crucial for solving more complex tasks.</li>
  <li>In conclusion, Quiet-STaR represents a significant step towards generalizable reasoning in language models by embedding continuous rationales in text generation, ultimately leading to better zero-shot reasoning performance across a range of tasks. The method also opens up potential future directions, such as dynamically predicting when rationale generation is needed and ensembling rationales for further improvements in reasoning capabilities.</li>
</ul>

<h3 id="large-language-monkeys-scaling-inference-compute-with-repeated-sampling"><a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a></h3>

<ul>
  <li>This paper by Brown et al. from Stanford, University of Oxford, and Google DeepMind, explores a novel methodology for scaling inference compute in large language models (LLMs) by utilizing repeated sampling. Instead of relying on a single inference attempt per problem, the authors propose increasing the number of generated samples to improve task coverage, particularly in tasks where answers can be automatically verified.</li>
  <li>The paper investigates two key aspects of the repeated sampling strategy:
    <ol>
      <li><strong>Coverage</strong>: The fraction of problems that can be solved by any generated sample.</li>
      <li><strong>Precision</strong>: The ability to identify the correct solution from the generated samples.</li>
    </ol>
  </li>
  <li><strong>Technical Details:</strong>
  The authors demonstrate that by scaling the number of inference samples, task coverage can increase exponentially across various domains such as coding and formal proofs, where answers are verifiable. For instance, using the SWE-bench Lite benchmark, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increased from 15.9% with one sample to 56% with 250 samples, surpassing the state-of-the-art performance of 43% by more capable models like GPT-4o and Claude 3.5 Sonnet.</li>
  <li><strong>Key Observations:</strong>
    <ul>
      <li><strong>Log-linear scaling of coverage</strong>: Across multiple models (e.g., Llama-3 and Gemma), the coverage exhibits a nearly log-linear relationship with the number of generated samples. This scaling behavior is modeled with an exponentiated power law, indicating the existence of potential inference-time scaling laws.</li>
      <li><strong>Cost-efficiency</strong>: Repeated sampling of cheaper models like DeepSeek can outperform single-sample inferences from premium models (e.g., GPT-4o) in terms of both performance and cost-effectiveness, providing up to 3x cost savings.</li>
    </ul>
  </li>
  <li>The following figure from the paper shows the proposed repeated sampling procedure: (i) Generate many candidate solutions for a given problem by sampling from an LLM with a positive temperature. (ii) Use a domain-specific verifier (ex. unit tests for code) to select a final answer from the generated samples.</li>
</ul>

<p><img src="../../../images/papers/LLMonkeys.jpg" alt=""></p>

<ul>
  <li><strong>Implementation:</strong>
  The repeated sampling methodology is implemented through the following steps:
    <ol>
      <li><strong>Sample generation</strong>: For each problem, multiple candidate solutions are generated by the LLM with a positive sampling temperature.</li>
      <li><strong>Verification</strong>: Solutions are verified using domain-specific verifiers (e.g., unit tests for code or proof checkers for formal proofs). In domains like coding, verification is fully automatic, translating the increased coverage into better performance.</li>
      <li><strong>Evaluation of Coverage</strong>: Coverage is evaluated using metrics such as pass@k, where k is the number of generated samples. For example, pass@10,000 was used to evaluate the CodeContests and MATH datasets.</li>
    </ol>
  </li>
  <li><strong>Empirical Results:</strong>
    <ul>
      <li><strong>Programming tasks</strong>: On the CodeContests dataset, the coverage of weaker models like Gemma-2B increased from 0.02% with one sample to 7.1% with 10,000 samples.</li>
      <li><strong>Mathematical problems</strong>: For math word problems from the GSM8K and MATH datasets, coverage increased to over 95% with 10,000 samples. However, methods to select the correct solution, such as majority voting or reward models, plateau after several hundred samples, highlighting the need for better solution selection mechanisms.</li>
    </ul>
  </li>
  <li><strong>Future Directions:</strong>
  The paper points out that identifying correct solutions from multiple samples remains a challenge in domains without automatic verifiers (e.g., math word problems). Additionally, the work opens up further research avenues, including optimizing sample diversity and leveraging multi-turn interactions for iterative problem-solving.</li>
  <li>This work underscores the potential of scaling inference compute through repeated sampling, demonstrating significant improvements in model performance while offering a cost-effective alternative to using larger, more expensive models.</li>
</ul>

<h3 id="learn-beyond-the-answer-training-language-models-with-reflection-for-mathematical-reasoning"><a href="https://arxiv.org/abs/2406.12050">Learn Beyond the Answer: Training Language Models with Reflection for Mathematical Reasoning</a></h3>

<ul>
  <li>This paper by Zhang et al. from the University of Notre Dame and Tencent AI Lab introduces Reflective Augmentation (RefAug), a novel method designed to improve the performance of language models (LMs) in mathematical reasoning tasks, particularly those requiring deeper comprehension through reflection. Traditional data augmentation approaches have focused on increasing the quantity of training instances, which improves problem-solving skills in simple, single-round question-answering (QA) tasks. However, these methods are less effective for complex reasoning scenarios where a more reflective approach is needed. RefAug addresses this limitation by adding reflective components to the training sequences, encouraging LMs to engage in alternative reasoning and <strong>follow-up reasoning</strong>.</li>
  <li><strong>Key Contributions</strong>:
    <ol>
      <li><strong>Reflective Augmentation</strong>:
        <ul>
          <li>RefAug enhances each training instance by appending a reflective section after the standard solution. This section helps the LM reflect on the problem, promoting deeper understanding and enabling it to consider alternative methods and apply abstractions or analogies.</li>
          <li>Two types of reflection are included:
            <ul>
              <li><strong>Alternative reasoning</strong>: Encourages the model to consider different methods for solving the problem.</li>
              <li><strong>Follow-up reasoning</strong>: Either focuses on abstraction (generalizing the problem) or analogy (applying the same technique to more complex problems).</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Implementation</strong>:
        <ul>
          <li>The paper uses <strong>GPT-4-turbo</strong> as an expert model to annotate reflective sections for training, minimizing human involvement and ensuring high-quality reasoning.</li>
          <li>The training objective is extended to optimize for the concatenation of the original answer and the reflective section. During training, the model learns the full reasoning sequence but during inference, the reflective part is excluded to maintain efficiency.</li>
          <li>Experiments were conducted with LMs such as Mistral-7B and Gemma-7B, testing them on mathematical reasoning tasks with and without reflective augmentation.</li>
          <li>The following figure from the paper shows that question augmentation creates new questions based on existing ones. Answer augmentation re-samples answers for each problem to increase diversity. Both methods expand the size of the training set. Reflective augmentation appends the original answer with a reflective section, which is complementary to traditional approaches. Corresponding training sequences are shown in an (input, output) format, where augmented parts are in red.</li>
        </ul>

        <p><img src="../../../images/papers/RefAug.jpg" alt=""></p>

        <ul>
          <li>The following figure from the paper shows that the model that learned the standard solution does not fully understand when and how to apply substitution when facing a different scenario. In contrast, the model trained with reflection on the substitution technique gains a deeper understanding of its principles, patterns, and its flexible application in new contexts.</li>
        </ul>

        <p><img src="../../../images/papers/RefAug2.jpg" alt=""></p>
      </li>
      <li><strong>Performance</strong>:
        <ul>
          <li><strong>Substantial improvement in standard QA</strong>: RefAug enhances performance in single-round QA by +7.2 accuracy points, demonstrating its ability to help models learn problem-solving skills more effectively.</li>
          <li><strong>Superior results in reflective reasoning tasks</strong>: RefAug significantly boosts the model’s capabilities in handling follow-up questions and error correction, areas where traditional augmentation techniques falter.</li>
          <li><strong>Complementary to traditional augmentation</strong>: Combining RefAug with other augmentation methods (such as question and answer augmentation) leads to further gains, showing its effectiveness as a complementary approach.</li>
        </ul>
      </li>
      <li><strong>Scalability</strong>:
        <ul>
          <li>RefAug proved effective even when applied to large datasets, like MetaMath, with results improving by 2 percentage points over baseline models trained on the same data without reflective sections.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Experimental Results</strong>:
    <ul>
      <li>Models trained with RefAug outperformed their standard counterparts in both in-distribution and out-of-distribution mathematical tasks (such as GSM8k, MATH, MAWPS, etc.).</li>
      <li>On reflective reasoning tasks (e.g., MathChat and MINT), RefAug-augmented models demonstrated a marked improvement, particularly in multi-step and follow-up questions.</li>
    </ul>
  </li>
  <li><strong>Significance</strong>:
    <ul>
      <li>RefAug goes beyond conventional data expansion techniques by embedding reflective thinking into training data, which strengthens a model’s ability to generalize and reason in diverse mathematical contexts. This method shows great promise for enhancing LMs in tasks requiring <strong>flexible problem-solving</strong> and deeper conceptual understanding.</li>
    </ul>
  </li>
  <li>The approach is designed to be easily integrated with other augmentation methods, improving the overall efficiency and effectiveness of language models in mathematical reasoning tasks.</li>
</ul>

<h3 id="agent-q-advanced-reasoning-and-learning-for-autonomous-ai-agents"><a href="https://arxiv.org/abs/2408.07199">Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</a></h3>

<ul>
  <li>This paper by Putta et al. from MultiOn and Stanford, presents “Agent Q,” a novel framework that enhances the reasoning and decision-making capabilities of large language models (LLMs) in agentic, multi-step tasks in dynamic environments such as web navigation. The framework tackles challenges related to compounding errors and limited exploration data that hinder LLMs from excelling in autonomous, real-time decision-making scenarios.</li>
  <li>The authors propose a method that integrates guided Monte Carlo Tree Search (MCTS) with a self-critique mechanism. This enables the agent to iteratively improve by learning from both successful and unsuccessful trajectories, using an off-policy variant of Direct Preference Optimization (DPO). The agent operates in a Partially Observable Markov Decision Process (POMDP) framework, where the LLM is responsible for planning, reasoning, and interacting with the environment, such as executing commands on web pages.
-<strong>Key Components of Agent Q</strong>:
    <ol>
      <li>
        <p><strong>Monte Carlo Tree Search (MCTS)</strong>: MCTS is used for exploring multiple action trajectories. It evaluates possible actions at each node (web page) by calculating rewards and assigning values to each action. The Upper Confidence Bound (UCB1) strategy guides the exploration versus exploitation trade-off. To handle the sparse reward environment, an AI-based feedback mechanism is employed to rank actions and provide step-level guidance.</p>
      </li>
      <li>
        <p><strong>Direct Preference Optimization (DPO)</strong>: DPO helps optimize the agent by using preference pairs (successful vs. unsuccessful actions) collected during interaction. This approach mitigates the need for a large number of online samples, making it computationally efficient for offline training. The DPO algorithm allows the agent to refine its decision-making policy by comparing trajectory outcomes and constructing preference pairs for learning.</p>
      </li>
      <li>
        <p><strong>Self-Critique Mechanism</strong>: To overcome credit assignment problems (where small errors can lead to overall task failure), the model incorporates a self-critique mechanism. At each step, the LLM provides intermediate feedback, which serves as an implicit reward, helping the agent refine its future actions.
-<strong>Implementation Details</strong>:</p>
        <ul>
          <li><strong>Initial Setup</strong>: The LLaMA-3 70B model serves as the base agent. The agent is evaluated in the WebShop environment (a simulated e-commerce platform) and a real-world reservation system (OpenTable). Initial observations and user queries are represented as HTML DOM trees, and the agent’s actions are composite, consisting of planning, reasoning, environment interaction, and explanation steps.</li>
        </ul>
      </li>
    </ol>

    <ul>
      <li><strong>Training Process</strong>: The agent is trained using a combination of offline and online learning methods. Offline learning leverages the DPO algorithm to learn from past trajectories, while online learning uses MCTS to guide real-time action selection. The model continuously improves through iterative fine-tuning based on the outcomes of the agent’s decisions.</li>
    </ul>
  </li>
  <li>The following figure from the paper shows the use of Monte Carlo Tree Search (MCTS) to guide trajectory collection and iteratively improve model performance using direct preference optimization (DPO). We begin on the left by sampling a user query from the list of tasks in the dataset. We iteratively expand the search tree using UCB1 as a heuristic to balance exploration and exploitation of different actions. We store the accumulated reward obtained for each node in the tree, where in this image darker green indicates higher reward and darker red indicates lower reward. To construct the preference dataset, we compute a weighted score of the MCTS average Q-value and score generated by a feedback language model to construct contrastive pairs for DPO. The policy is optimized and can be iteratively improved.</li>
</ul>

<p><img src="../../../images/papers/AgentQ1.jpg" alt=""></p>

<ul>
  <li>The following figure from the paper shows that they provide the following input format to the Agent, consisting of the system prompt, execution history, the current observation as a DOM representation, and the user query containing the goal. We divide our Agent output format into an overall step-by-step plan, thought, a command, and a status code.</li>
</ul>

<p><img src="../../../images/papers/AgentQ2.jpg" alt=""></p>

<p>-<strong>Results</strong>:
    - In WebShop, Agent Q outperforms baseline models such as behavior cloning and reinforced fine-tuning, achieving a success rate of 50.5%, surpassing the average human performance of 50%.
    - In real-world experiments on OpenTable, the Agent Q framework improves the LLaMA-3 model’s zero-shot performance from 18.6% to 81.7%, with a further increase to 95.4% when MCTS is utilized during inference.</p>
<ul>
  <li>This framework demonstrates significant progress in building autonomous web agents that can generalize and learn from their experiences in complex, multi-step reasoning tasks.</li>
</ul>

<h3 id="v-star-training-verifiers-for-self-taught-reasoners"><a href="https://arxiv.org/abs/2402.06457">V-STaR: Training Verifiers for Self-Taught Reasoners</a></h3>

<ul>
  <li>This paper by Hosseini et al. from Mila, MSR, University of Edinburgh, and Google Deepmind, published in COLM 2024, introduces V-STaR, a novel approach designed to improve the reasoning capabilities of large language models (LLMs) by training both a verifier and a generator using correct and incorrect solutions. The authors aim to address a key limitation in previous self-improvement approaches, such as STaR and Rejection Fine-Tuning (RFT), which discard incorrect solutions, potentially missing valuable learning opportunities. V-STaR instead leverages both correct and incorrect model-generated solutions in an iterative self-improvement process, leading to better performance in tasks like math problem-solving and code generation.</li>
  <li>The core idea of V-STaR is to iteratively train a generator to produce solutions and a verifier to judge their correctness using Direct Preference Optimization (DPO). By utilizing both correct and incorrect solutions, V-STaR ensures that the verifier learns from the generator’s errors, making it more robust.</li>
  <li><strong>Methodology and Implementation Details</strong>:
    <ol>
      <li><strong>Training the Generator (GSFT)</strong>: The generator is initially fine-tuned using supervised fine-tuning (SFT) on the original dataset, producing solutions for various problem instances. After each iteration, correct solutions are added to the training data for future iterations.</li>
      <li><strong>Training the Verifier (VT)</strong>: Both correct and incorrect generated solutions are added to the verifier’s training data. The verifier is trained using DPO, which optimizes for preference learning by contrasting correct and incorrect solutions, improving its ability to rank solutions based on correctness.</li>
      <li><strong>Iterative Process</strong>: This process is repeated for multiple iterations. In each iteration, the generator produces solutions, and the verifier learns from both the correct and incorrect solutions, progressively improving the overall performance of both models.</li>
      <li><strong>Test-time Verification</strong>: At test time, the generator produces multiple candidate solutions for a problem, and the verifier selects the best one by ranking them.</li>
    </ol>
  </li>
  <li>The following figure from the paper shows generator and verifier training in V-STaR. Left: In each training iteration, the generator $G^t$ is fine-tuned (from a pretrained LLM) on the current buffer of problem instances and correct solutions $\mathcal{D}<em>{\text {GEN }}$. Generated solutions that yielded a correct answer are added to $\mathcal{D}</em>{\mathrm{GEN}}$ to be used in future iterations, and all the generated solutions (correct and incorrect) are added to $\mathcal{D}<em>{\text {VER }}$. The verifier $V^t$ is trained using DPO with a preference dataset constructed from pairs of correct and incorrect solutions from $\mathcal{D}</em>{\text {VER }}$. Right: At test time, the verifier is used to rank solutions produced by the generator. Such iterative training and inference-time ranking yields large improvements over generator-only self-improvement.</li>
</ul>

<p><img src="../../../images/papers/V-STaR.jpg" alt=""></p>

<ul>
  <li><strong>Key Results</strong>:
    <ul>
      <li>V-STaR demonstrates a <strong>4% to 17% improvement</strong> in test accuracy over baseline self-improvement and verification methods in tasks like code generation and math reasoning. In some cases, it even surpasses much larger models.</li>
      <li>When evaluated on math reasoning benchmarks such as GSM8K and MATH, and code-generation datasets like MBPP and HumanEval, V-STaR outperforms prior approaches by combining both correct and incorrect examples for training the verifier.</li>
    </ul>
  </li>
  <li><strong>Empirical Findings</strong>:
    <ul>
      <li>The paper compares V-STaR against several baselines, including non-iterative versions of STaR and RFT combined with a verifier, and demonstrates significant improvements in Pass@1 and Best-of-64 metrics.</li>
      <li>V-STaR is highly data-efficient, with the iterative collection of correct and incorrect solutions leading to more challenging examples for the verifier, which enhances both the generator and verifier over time.</li>
    </ul>
  </li>
  <li><strong>Conclusions</strong>:
  The V-STaR approach significantly enhances reasoning tasks by training LLMs to learn from both correct and incorrect solutions. The iterative training process allows both the generator and verifier to continuously improve, and the use of DPO for training verifiers has been shown to outperform more traditional ORM-style verification methods. This framework is simple to implement and applicable to a wide range of reasoning problems, provided there is access to correctness feedback during training.</li>
</ul>

<h3 id="improve-mathematical-reasoning-in-language-models-by-automated-process-supervision"><a href="https://arxiv.org/abs/2406.06592">Improve Mathematical Reasoning in Language Models by Automated Process Supervision</a></h3>

<ul>
  <li>This paper from Luo et al. from Google DeepMind introduces a novel approach to enhance the mathematical reasoning capabilities of large language models (LLMs) through automated process supervision, focusing on intermediate reasoning steps rather than just final outcomes. Traditional techniques like Outcome Reward Models (ORM) verify the final answer’s correctness, but these models do not reward or penalize the intermediate steps, leading to challenges in solving complex multi-step reasoning tasks such as mathematical problem solving.</li>
  <li>The authors propose a new divide-and-conquer Monte Carlo Tree Search (MCTS) algorithm, named OmegaPRM, for the efficient collection of process supervision data. OmegaPRM improves upon previous methods by utilizing a binary search technique to locate errors in the chain of thought, identifying the first error in a reasoning path. This method ensures a balanced collection of positive and negative examples, leading to high-quality data collection.</li>
  <li>The implementation of OmegaPRM allows the automatic generation of over 1.5 million process supervision annotations, which are used to train a Process Reward Model (PRM). These annotations are generated without human intervention, making the process both cost-effective and scalable. The PRM, when integrated with a weighted self-consistency algorithm, achieves a 69.4% success rate on the MATH benchmark, which is a significant 36% relative improvement over the base model performance of 51%.</li>
  <li>The following figure from the paper shows an example tree structure built with our proposed OmegaPRM algorithm. Each node in the tree indicates a state of partial chain-of-thought solution, with information including accuracy of rollouts and other statistics. Each edge indicates an action, i.e., a reasoning step, from the last state. Yellow edges are correct steps and blue edges are wrong.</li>
</ul>

<p><img src="../../../images/papers/OmegaPRM1.jpg" alt=""></p>

<ul>
  <li>Key implementation details include:
    <ul>
      <li><strong>Monte Carlo Tree Search (MCTS)</strong>: A tree is built for each mathematical question, with nodes representing intermediate reasoning steps. The binary search efficiently narrows down to the first incorrect step by performing rollouts at each stage.</li>
      <li><strong>Tree Structure</strong>: Each node stores statistics, such as visit counts and Monte Carlo estimations, to guide future rollouts. This structure enables reuse of rollouts, reducing computational redundancy and enhancing the training data collection efficiency.</li>
      <li><strong>PRM Training</strong>: The PRM is trained using pointwise soft labels derived from the Monte Carlo rollouts. The soft label approach provides better performance compared to hard labels and pairwise loss functions, achieving 70.1% accuracy in classifying per-step correctness.</li>
      <li><strong>Data Generation</strong>: The PRM was trained on the MATH dataset, generating process annotations using OmegaPRM’s binary search method. This process reduced the number of policy calls while maintaining high-quality annotations.</li>
    </ul>
  </li>
  <li>The following figure from the paper shows an illustration of the process supervision rollouts, Monte Carlo estimation using binary search and the MCTS process. (a) An example of Monte Carlo estimation of a prefix solution. Two out of the three rollouts are correct, producing the Monte Carlo estimation <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;MC&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mn&gt;0.67&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-28" style="width: 11.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1009.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-29"><span class="mi" id="MathJax-Span-30" style="font-family: STIXGeneral-Regular;">MC</span><span class="mo" id="MathJax-Span-31"></span><span class="mrow" id="MathJax-Span-32"><span class="mo" id="MathJax-Span-33" style="vertical-align: 0em;"><span style="font-family: STIXGeneral-Regular;">(</span></span><span class="mrow" id="MathJax-Span-34"><span class="mi" id="MathJax-Span-35" style="font-family: STIXGeneral-Italic;">q</span><span class="mo" id="MathJax-Span-36" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-37" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-38" style="font-family: STIXGeneral-Italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.852em; left: 0.471em;"><span class="texatom" id="MathJax-Span-39"><span class="mrow" id="MathJax-Span-40"><span class="mn" id="MathJax-Span-41" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span class="mo" id="MathJax-Span-42" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">:</span><span class="mi" id="MathJax-Span-43" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">t<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span class="mo" id="MathJax-Span-44" style="vertical-align: 0em;"><span style="font-family: STIXGeneral-Regular;">)</span></span></span><span class="mo" id="MathJax-Span-45" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">=</span><span class="mn" id="MathJax-Span-46" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">2</span><span class="texatom" id="MathJax-Span-47"><span class="mrow" id="MathJax-Span-48"><span class="mo" id="MathJax-Span-49" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mn" id="MathJax-Span-50" style="font-family: STIXGeneral-Regular;">3</span><span class="mo" id="MathJax-Span-51" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">≈</span><span class="mn" id="MathJax-Span-52" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">0.67</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>MC</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></mrow><mo>)</mo></mrow><mo>=</mo><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn><mo>≈</mo><mn>0.67</mn></math></span></span><script type="math/tex" id="MathJax-Element-10">\operatorname{MC}\left(q, x_{1: t}\right)=2 / 3 \approx 0.67</script>. (b) An example of error locating using binary search. The first error step is located at the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext&gt;th&amp;#xA0;&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-53" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-54"><span class="msubsup" id="MathJax-Span-55"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-56" style="font-family: STIXGeneral-Regular;">7</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.523em;"><span class="texatom" id="MathJax-Span-57"><span class="mrow" id="MathJax-Span-58"><span class="mtext" id="MathJax-Span-59" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">th&nbsp;</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>7</mn><mrow class="MJX-TeXAtom-ORD"><mtext>th&nbsp;</mtext></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-11">7^{\text {th }}</script> step after three divide-and-rollouts, where the rollout positions are indicated by the vertical dashed lines. (c) The MCTS process. The dotted lines in Select stage represent the available rollouts for binary search. The bold colored edges represent steps with correctness estimations. The yellow color indicates a correct step, i.e., with a preceding state <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-60" style="width: 0.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-61"><span class="mi" id="MathJax-Span-62" style="font-family: STIXGeneral-Italic;">s</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></span></span><script type="math/tex" id="MathJax-Element-12">s</script> that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;M&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;C&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-63" style="width: 5.211em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1004.33em, 2.503em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-64"><span class="texatom" id="MathJax-Span-65"><span class="mrow" id="MathJax-Span-66"><span class="mi" id="MathJax-Span-67" style="font-family: STIXGeneral-Regular;">M</span><span class="mi" id="MathJax-Span-68" style="font-family: STIXGeneral-Regular;">C</span></span></span><span class="mo" id="MathJax-Span-69" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-70" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-71" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-72" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">&gt;</span><span class="mn" id="MathJax-Span-73" style="font-family: STIXGeneral-Regular; padding-left: 0.315em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">M</mi><mi mathvariant="normal">C</mi></mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-13">\mathrm{MC}(s)>0</script> and the blue color indicates an incorrect step, i.e., with $$\mathrm{MC}(s)=0$. The number of dashes in each colored edge indicates the number of steps.</li>
</ul>

<p><img src="../../../images/papers/OmegaPRM2.jpg" alt=""></p>

<ul>
  <li>The paper also highlights the cost-effectiveness of the OmegaPRM approach, as it automates the collection of process supervision data without human annotators, overcoming the limitations of previous methods that required expensive and labor-intensive human annotations. The resulting PRM significantly improves the reasoning accuracy of LLMs in multi-step mathematical tasks.</li>
</ul>

<h3 id="chain-of-thought-reasoning-without-prompting"><a href="https://arxiv.org/abs/2402.10200">Chain-of-Thought Reasoning Without Prompting</a></h3>

<ul>
  <li>This paper by Wang and Zhou from Google DeepMind investigates an alternative approach to elicit reasoning capabilities from large language models (LLMs) without the need for prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. The key focus is on altering the decoding process to reveal inherent CoT reasoning paths within the models, avoiding the conventional reliance on manual prompt engineering.</li>
  <li>The authors demonstrate that by adjusting the decoding procedure, particularly moving away from greedy decoding to exploring top-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-74" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-75"><span class="mi" id="MathJax-Span-76" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-14">k</script> alternative tokens, it is possible to uncover CoT paths that the model naturally possesses. This method, termed CoT-decoding, effectively bypasses the limitations of standard prompting strategies and instead emphasizes a more task-agnostic way of assessing LLMs’ intrinsic reasoning capabilities.</li>
  <li>The following figure from the paper shows an illustration of CoT-decoding. Pre-trained LLMs are capable of inherent reasoning without prompting by considering alternative top-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-77" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-78"><span class="mi" id="MathJax-Span-79" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-15">k</script> tokens, rather than solely relying on the top-1 greedy decoding path. Moreover, these models tend to display higher confidence in decoding the final answer (indicated by a darker shaded color) when a CoT reasoning path is present.</li>
</ul>

<p><img src="../../../images/papers/CoT-decoding.jpg" alt=""></p>

<ul>
  <li><strong>Technical Contributions</strong>:
    <ul>
      <li>The authors show that when alternative decoding paths (top-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-80" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-81"><span class="mi" id="MathJax-Span-82" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-16">k</script> tokens) are considered, CoT reasoning paths often emerge naturally. This reveals reasoning capabilities that are typically hidden when models rely on the standard greedy decoding path.</li>
      <li>The presence of CoT in a decoding path correlates with a higher confidence in the final decoded answer. This confidence measure can be leveraged to differentiate between CoT and non-CoT paths.</li>
      <li>The method enables LLMs to solve reasoning tasks such as mathematical problems and commonsense reasoning more effectively compared to using prompting methods or greedy decoding alone.</li>
    </ul>
  </li>
  <li><strong>Implementation Details</strong>:
    <ul>
      <li><strong>CoT-decoding</strong>: This involves generating multiple alternative decoding paths by selecting the top-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-83" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-84"><span class="mi" id="MathJax-Span-85" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">k</script> tokens at each decoding step. After generating the paths, the model’s confidence in each path is evaluated based on the difference between the top two token probabilities in the final answer. This probability difference helps identify the most reliable CoT paths.</li>
      <li>The method is tested on various reasoning benchmarks such as GSM8K (math reasoning) and commonsense tasks (e.g., year parity questions). For example, in a mathematical task, a correct CoT path was found by selecting alternative decoding paths that consider intermediate steps, such as calculating the price of items and applying discounts sequentially, rather than providing a direct answer.</li>
      <li>The authors implemented CoT-decoding across several language models, including PaLM-2 and Mistral-7B. Across these models, CoT-decoding consistently yielded significant accuracy improvements over greedy decoding in tasks that require multi-step reasoning.</li>
      <li>The paper also explores the effect of different values for top-<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-86" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-87"><span class="mi" id="MathJax-Span-88" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-18">k</script>, showing that higher values generally improve model performance by increasing the likelihood of finding the correct reasoning paths.</li>
    </ul>
  </li>
  <li><strong>Experimental Results</strong>:
    <ul>
      <li>CoT-decoding yielded a significant accuracy boost compared to greedy decoding, particularly on complex reasoning tasks. For example, on GSM8K, CoT-decoding improved accuracy from 9.9% to 25.1% on the Mistral-7B model, showcasing the effectiveness of this approach.</li>
      <li>The method partially closes the performance gap between pre-trained and instruction-tuned models. For instance, CoT-decoding improved the pre-trained PaLM-2 large model’s accuracy to levels close to that of instruction-tuned models without requiring any additional supervised data.</li>
      <li>This work presents a significant step forward in understanding LLMs’ intrinsic reasoning abilities without relying on human intervention through prompt engineering, making it easier to evaluate and harness these capabilities across a broader range of tasks.</li>
    </ul>
  </li>
</ul>

<h3 id="training-language-models-to-self-correct-via-reinforcement-learning"><a href="https://arxiv.org/abs/2409.12917">Training Language Models to Self-Correct Via Reinforcement Learning</a></h3>

<ul>
  <li>This paper by Kumar et al. from Google DeepMind introduces SCoRe, a novel multi-turn online reinforcement learning (RL) approach that enhances LLMs’ intrinsic self-correction capabilities by training on self-generated data. Unlike previous self-correction methods that require multiple models or external supervision, SCoRe is designed to operate without any oracle feedback, relying entirely on data generated by the model itself.</li>
  <li>The authors first highlight the limitations of traditional supervised fine-tuning (SFT) approaches, which suffer from a distribution mismatch between training data and model outputs. These approaches often bias the model towards minimal edits that fail to generalize well to unseen problems, leading to ineffective self-correction. In response, SCoRe addresses these issues through multi-turn RL, allowing the model to iteratively refine its outputs across multiple attempts.</li>
  <li>The SCoRe method is implemented in two distinct stages:
    <ol>
      <li><strong>Stage I: Initialization</strong> – The model is fine-tuned to produce high-reward second-attempt responses while constraining its first attempt to remain close to the base model’s outputs. This initialization is critical in preventing collapse into trivial strategies like minimal edits.</li>
      <li><strong>Stage II: Multi-turn RL with Reward Shaping</strong> – The RL training optimizes both the first and second attempts, with an emphasis on improving from the first to the second attempt. A reward shaping mechanism is introduced to amplify self-correction behaviors by providing bonus rewards for meaningful improvements between attempts.</li>
    </ol>
  </li>
  <li>The following figure from the paper shows an example trace and the problem setting of self-correction.</li>
</ul>

<p><img src="../../../images/papers/SCoRe.jpg" alt=""></p>

<ul>
  <li>Key components of the implementation include:
    <ul>
      <li><strong>On-policy RL</strong>: SCoRe uses on-policy policy gradient methods to optimize for self-correction, ensuring that the model learns from its own mistakes and adapts to its own response distribution.</li>
      <li><strong>Reward Shaping</strong>: The model receives additional rewards for making corrections that change an incorrect response to correct, encouraging it to explore self-correction strategies.</li>
      <li><strong>KL-Divergence Regularization</strong>: The model’s first attempt is constrained via KL-divergence to stay close to the base model’s outputs, avoiding drastic changes that could degrade initial performance.</li>
    </ul>
  </li>
  <li>The approach was tested on two key tasks: mathematical reasoning (using the MATH dataset) and coding (using the HumanEval and MBPP datasets). SCoRe demonstrated significant improvements over baseline models, achieving a 15.6% gain in self-correction accuracy on MATH and a 9.1% improvement on coding benchmarks. Importantly, the model improved not only in its ability to correct mistakes but also in maintaining correct responses across attempts.</li>
  <li>In terms of evaluation, SCoRe’s success was measured using metrics such as accuracy at each attempt (t1 and t2), the net improvement between attempts (Δ(t1, t2)), and the frequency of problems corrected in subsequent attempts. SCoRe outperformed other approaches by substantially reducing the number of correct answers that became incorrect in the second attempt and improving the rate of incorrect-to-correct transformations.</li>
  <li>The paper also provides ablation studies demonstrating the importance of each component in the SCoRe pipeline, highlighting the necessity of multi-turn training and reward shaping in achieving effective self-correction behavior.</li>
  <li>In summary, SCoRe presents a robust solution to the problem of self-correction in LLMs by leveraging reinforcement learning techniques that focus on improving performance iteratively over multiple attempts.</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://openai.com/index/openai-o1-system-card/">OpenAI o1 System Card</a></li>
  <li><a href="https://openai.com/index/learning-to-reason-with-llms/">Learning to Reason with LLMs</a></li>
  <li><a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">OpenAI’s Preparedness Framework</a></li>
  <li><a href="https://openai.com/index/openai-`o1-mini`-advancing-cost-efficient-reasoning/#model-speed">OpenAI <code class="language-plaintext highlighter-rouge">o1-mini</code></a></li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.deeplearning.ai/the-batch/issue-245/">The Batch – Weekly Issues: Issue 245</a></li>
</ul>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><div><button class="btn-copy" data-clipboard-action="copy" data-clipboard-target="#code0"><img src="https://aman.ai/images/copy.png" style="margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;"></button></div><code id="code0">@article{Chadha2020DistilledOpenAIo1,
  title   = {OpenAI o1},
  author  = {Chadha, Aman and Jain, Vinija},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://vinija.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div><ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;" data-ad-status="unfilled"><div id="aswift_0_host" style="border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;min-height:auto;max-height:none;min-width:auto;max-width:none;" sandbox="allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allow="attribution-reporting; run-ad-auction" src="https://googleads.g.doubleclick.net/pagead/ads?client=ca-pub-5905744527956213&amp;output=html&amp;adk=1812271804&amp;adf=3025194257&amp;lmt=1766895470&amp;plaf=1%3A2%2C2%3A2%2C7%3A2&amp;plat=1%3A128%2C2%3A128%2C3%3A128%2C4%3A128%2C8%3A128%2C9%3A32776%2C16%3A8388608%2C17%3A32%2C24%3A32%2C25%3A32%2C30%3A1048576%2C32%3A32%2C41%3A32%2C42%3A32&amp;format=0x0&amp;url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fo1%2F&amp;pra=5&amp;wgl=1&amp;asro=0&amp;aiapm=0.1542&amp;aiapmd=0.1423&amp;aiapmi=0.16&amp;aiapmid=1&amp;aiact=0.5423&amp;aiactd=0.7&amp;aicct=0.7&amp;aicctd=0.5799&amp;ailct=0.5849&amp;ailctd=0.65&amp;aimart=4&amp;aimartd=4&amp;aieuf=1&amp;aicrs=1&amp;uach=WyIiLCIiLCIiLCIiLCIiLG51bGwsMCxudWxsLCIiLG51bGwsMF0.&amp;abgtt=6&amp;dt=1766922703359&amp;bpp=2&amp;bdt=14&amp;idt=24&amp;shv=r20251211&amp;mjsv=m202512100101&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie_enabled=1&amp;eoidce=1&amp;nras=1&amp;correlator=2783934209472&amp;frm=20&amp;pv=2&amp;u_tz=330&amp;u_his=50&amp;u_h=600&amp;u_w=800&amp;u_ah=600&amp;u_aw=800&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=-12245933&amp;ady=-12245933&amp;biw=800&amp;bih=600&amp;scr_x=0&amp;scr_y=0&amp;eid=31096041%2C42531706%2C95376241%2C95378749%2C95380023&amp;oid=2&amp;pvsid=5718627176724529&amp;tmod=112874479&amp;uas=0&amp;nvt=1&amp;fsapi=1&amp;fc=1920&amp;brdim=22%2C22%2C22%2C22%2C800%2C0%2C756%2C556%2C800%2C600&amp;vis=1&amp;rsz=%7C%7Cs%7C&amp;abl=NS&amp;fu=32768&amp;bc=31&amp;bz=0.95&amp;psd=W251bGwsW251bGwsbnVsbCxudWxsLCJkZXByZWNhdGVkX2thbm9uIl1d&amp;ifi=1&amp;uci=a!1&amp;fsb=1&amp;dtd=28" data-google-container-id="a!1" tabindex="0" title="Advertisement" aria-label="Advertisement" data-load-complete="true"></iframe></div></ins>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC"></image>
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="mailto:hi@aman.ai">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg=="></image>
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script><div class="nanobar my-class" id="my-id" style="position: fixed;"><div class="bar"></div></div>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    

<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="googlefcPresent" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="__tcfapiLocator" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcInactive" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe name="googlefcLoaded" src="about:blank" style="display: none; width: 0px; height: 0px; border: none; z-index: -1000; left: -1000px; top: -1000px;"></iframe><iframe src="https://www.google.com/recaptcha/api2/aframe" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="https://googleads.g.doubleclick.net/pagead/html/r20251211/r20190131/zrt_lookup.html" style="display: none;"></iframe></html>